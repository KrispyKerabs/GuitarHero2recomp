#include "ppc_recomp_shared.h"

__attribute__((alias("__imp__sub_825CE570"))) PPC_WEAK_FUNC(sub_825CE570);
PPC_FUNC_IMPL(__imp__sub_825CE570) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,21184(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21184);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ce608
	if (ctx.cr6.eq) goto loc_825CE608;
	// lwz r11,14824(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ce5b0
	if (ctx.cr6.eq) goto loc_825CE5B0;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825f1cd8
	ctx.lr = 0x825CE5A8;
	sub_825F1CD8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f17a0
	ctx.lr = 0x825CE5B0;
	sub_825F17A0(ctx, base);
loc_825CE5B0:
	// li r30,1
	ctx.r30.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r30,3676(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3676, ctx.r30.u32);
	// bl 0x825eefd0
	ctx.lr = 0x825CE5C0;
	sub_825EEFD0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,160(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	// lwz r4,156(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// bl 0x825efc00
	ctx.lr = 0x825CE5D0;
	sub_825EFC00(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ce698
	if (!ctx.cr6.eq) goto loc_825CE698;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,160(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	// lwz r4,156(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// bl 0x825ef868
	ctx.lr = 0x825CE5E8;
	sub_825EF868(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825ce5f8
	if (ctx.cr6.eq) goto loc_825CE5F8;
	// stw r30,3676(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3676, ctx.r30.u32);
	// b 0x825ce698
	goto loc_825CE698;
loc_825CE5F8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ebc08
	ctx.lr = 0x825CE600;
	sub_825EBC08(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825dd688
	ctx.lr = 0x825CE608;
	sub_825DD688(ctx, base);
loc_825CE608:
	// lwz r11,212(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// mullw r5,r10,r11
	ctx.r5.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// bl 0x8239cb70
	ctx.lr = 0x825CE620;
	sub_8239CB70(ctx, base);
	// lwz r11,216(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// bl 0x8239cb70
	ctx.lr = 0x825CE638;
	sub_8239CB70(ctx, base);
	// lwz r11,216(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239cb70
	ctx.lr = 0x825CE650;
	sub_8239CB70(ctx, base);
	// lwz r11,204(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r10,212(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// lwz r3,3776(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3776);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239cb70
	ctx.lr = 0x825CE668;
	sub_8239CB70(ctx, base);
	// lwz r11,216(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// lwz r3,3780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3780);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239cb70
	ctx.lr = 0x825CE680;
	sub_8239CB70(ctx, base);
	// lwz r11,216(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// lwz r3,3784(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3784);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239cb70
	ctx.lr = 0x825CE698;
	sub_8239CB70(ctx, base);
loc_825CE698:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CE6B0"))) PPC_WEAK_FUNC(sub_825CE6B0);
PPC_FUNC_IMPL(__imp__sub_825CE6B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825CE6B8;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r6,3720(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// lwz r9,3776(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3776);
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// add r29,r9,r10
	ctx.r29.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lwz r3,3780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3780);
	// lwz r4,3784(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3784);
	// add r30,r6,r10
	ctx.r30.u64 = ctx.r6.u64 + ctx.r10.u64;
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// add r25,r3,r11
	ctx.r25.u64 = ctx.r3.u64 + ctx.r11.u64;
	// lwz r8,3724(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// add r24,r4,r11
	ctx.r24.u64 = ctx.r4.u64 + ctx.r11.u64;
	// lwz r7,3728(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// add r10,r7,r9
	ctx.r10.u64 = ctx.r7.u64 + ctx.r9.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// add r28,r8,r11
	ctx.r28.u64 = ctx.r8.u64 + ctx.r11.u64;
	// add r26,r10,r11
	ctx.r26.u64 = ctx.r10.u64 + ctx.r11.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825CE718;
	sub_8239CB70(ctx, base);
	// lwz r5,204(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// add r29,r5,r29
	ctx.r29.u64 = ctx.r5.u64 + ctx.r29.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825CE72C;
	sub_8239CB70(ctx, base);
	// lwz r11,204(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// li r27,0
	ctx.r27.s64 = 0;
	// add r9,r11,r29
	ctx.r9.u64 = ctx.r11.u64 + ctx.r29.u64;
	// lwz r11,188(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// addze. r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x825ce7d4
	if (!ctx.cr0.gt) goto loc_825CE7D4;
loc_825CE748:
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825ce794
	if (!ctx.cr6.gt) goto loc_825CE794;
	// subf r6,r9,r30
	ctx.r6.s64 = ctx.r30.s64 - ctx.r9.s64;
loc_825CE75C:
	// lwz r8,204(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// add r10,r11,r9
	ctx.r10.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// lbzx r7,r6,r10
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lbzx r8,r8,r30
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r30.u32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// stb r8,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r8.u8);
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825ce75c
	if (ctx.cr6.lt) goto loc_825CE75C;
loc_825CE794:
	// lwz r11,204(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r29,r11,r9
	ctx.r29.u64 = ctx.r11.u64 + ctx.r9.u64;
	// add r30,r10,r30
	ctx.r30.u64 = ctx.r10.u64 + ctx.r30.u64;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825CE7B4;
	sub_8239CB70(ctx, base);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// lwz r11,204(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r9,r11,r29
	ctx.r9.u64 = ctx.r11.u64 + ctx.r29.u64;
	// addze r11,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r11.s64 = temp.s64;
	// cmpw cr6,r27,r11
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825ce748
	if (ctx.cr6.lt) goto loc_825CE748;
loc_825CE7D4:
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825CE7E4;
	sub_8239CB70(ctx, base);
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// add r30,r5,r25
	ctx.r30.u64 = ctx.r5.u64 + ctx.r25.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825CE7F8;
	sub_8239CB70(ctx, base);
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// li r29,0
	ctx.r29.s64 = 0;
	// add r9,r11,r30
	ctx.r9.u64 = ctx.r11.u64 + ctx.r30.u64;
	// lwz r11,200(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// addze. r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x825ce8a0
	if (!ctx.cr0.gt) goto loc_825CE8A0;
loc_825CE814:
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825ce860
	if (!ctx.cr6.gt) goto loc_825CE860;
	// subf r6,r9,r28
	ctx.r6.s64 = ctx.r28.s64 - ctx.r9.s64;
loc_825CE828:
	// lwz r8,208(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// add r10,r11,r9
	ctx.r10.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// lbzx r7,r6,r10
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lbzx r8,r8,r28
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r28.u32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// stb r8,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r8.u8);
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825ce828
	if (ctx.cr6.lt) goto loc_825CE828;
loc_825CE860:
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r11,r9
	ctx.r30.u64 = ctx.r11.u64 + ctx.r9.u64;
	// add r28,r10,r28
	ctx.r28.u64 = ctx.r10.u64 + ctx.r28.u64;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825CE880;
	sub_8239CB70(ctx, base);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r9,r11,r30
	ctx.r9.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addze r11,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r11.s64 = temp.s64;
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825ce814
	if (ctx.cr6.lt) goto loc_825CE814;
loc_825CE8A0:
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825CE8B0;
	sub_8239CB70(ctx, base);
	// lwz r5,208(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// add r30,r5,r24
	ctx.r30.u64 = ctx.r5.u64 + ctx.r24.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825CE8C4;
	sub_8239CB70(ctx, base);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// li r29,0
	ctx.r29.s64 = 0;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r9,r11,r30
	ctx.r9.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addze. r11,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r11.s64 = temp.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble 0x825ce96c
	if (!ctx.cr0.gt) goto loc_825CE96C;
loc_825CE8E0:
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825ce92c
	if (!ctx.cr6.gt) goto loc_825CE92C;
	// subf r6,r9,r26
	ctx.r6.s64 = ctx.r26.s64 - ctx.r9.s64;
loc_825CE8F4:
	// lwz r8,208(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// add r10,r11,r9
	ctx.r10.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// lbzx r7,r6,r10
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r6.u32 + ctx.r10.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lbzx r8,r8,r26
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r26.u32);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// srawi r8,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// stb r8,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r8.u8);
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825ce8f4
	if (ctx.cr6.lt) goto loc_825CE8F4;
loc_825CE92C:
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r11,r9
	ctx.r30.u64 = ctx.r11.u64 + ctx.r9.u64;
	// add r26,r10,r26
	ctx.r26.u64 = ctx.r10.u64 + ctx.r26.u64;
	// rotlwi r5,r11,0
	ctx.r5.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825CE94C;
	sub_8239CB70(ctx, base);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r9,r11,r30
	ctx.r9.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addze r11,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r11.s64 = temp.s64;
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825ce8e0
	if (ctx.cr6.lt) goto loc_825CE8E0;
loc_825CE96C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825CE974"))) PPC_WEAK_FUNC(sub_825CE974);
PPC_FUNC_IMPL(__imp__sub_825CE974) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CE978"))) PPC_WEAK_FUNC(sub_825CE978);
PPC_FUNC_IMPL(__imp__sub_825CE978) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825CE980;
	sub_8239BA1C(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825ce99c
	if (!ctx.cr6.eq) goto loc_825CE99C;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
loc_825CE99C:
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ce9b4
	if (ctx.cr6.eq) goto loc_825CE9B4;
	// li r3,13
	ctx.r3.s64 = 13;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
loc_825CE9B4:
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r10,15472(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r10,6
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 6, ctx.xer);
	// stw r11,3668(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3668, ctx.r11.u32);
	// sth r11,3684(r31)
	PPC_STORE_U16(ctx.r31.u32 + 3684, ctx.r11.u16);
	// stw r11,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r11.u32);
	// bne cr6,0x825ceaa0
	if (!ctx.cr6.eq) goto loc_825CEAA0;
	// li r9,-3
	ctx.r9.s64 = -3;
	// lwz r10,15300(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15300);
	// li r8,1
	ctx.r8.s64 = 1;
	// std r11,3576(r31)
	PPC_STORE_U64(ctx.r31.u32 + 3576, ctx.r11.u64);
	// stw r11,284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 284, ctx.r11.u32);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r11,3380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3380, ctx.r11.u32);
	// stw r11,3396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3396, ctx.r11.u32);
	// stw r9,3376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3376, ctx.r9.u32);
	// stw r8,14788(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14788, ctx.r8.u32);
	// stw r11,3384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3384, ctx.r11.u32);
	// stw r11,3400(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3400, ctx.r11.u32);
	// stw r11,3452(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3452, ctx.r11.u32);
	// beq cr6,0x825ceaa0
	if (ctx.cr6.eq) goto loc_825CEAA0;
	// lwz r11,19696(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19696);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r6,188(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r7,180(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// lwz r10,19700(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19700);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lwz r8,200(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// add r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lwz r9,192(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// mullw r29,r6,r11
	ctx.r29.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r11.s32);
	// add r11,r8,r10
	ctx.r11.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mullw r30,r11,r10
	ctx.r30.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239ca70
	ctx.lr = 0x825CEA50;
	sub_8239CA70(ctx, base);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// li r4,128
	ctx.r4.s64 = 128;
	// bl 0x8239ca70
	ctx.lr = 0x825CEA60;
	sub_8239CA70(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// li r4,128
	ctx.r4.s64 = 128;
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// bl 0x8239ca70
	ctx.lr = 0x825CEA70;
	sub_8239CA70(ctx, base);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,3732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// bl 0x8239ca70
	ctx.lr = 0x825CEA80;
	sub_8239CA70(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// li r4,128
	ctx.r4.s64 = 128;
	// lwz r3,3736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// bl 0x8239ca70
	ctx.lr = 0x825CEA90;
	sub_8239CA70(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// li r4,128
	ctx.r4.s64 = 128;
	// lwz r3,3740(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// bl 0x8239ca70
	ctx.lr = 0x825CEAA0;
	sub_8239CA70(ctx, base);
loc_825CEAA0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825CEAAC"))) PPC_WEAK_FUNC(sub_825CEAAC);
PPC_FUNC_IMPL(__imp__sub_825CEAAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CEAB0"))) PPC_WEAK_FUNC(sub_825CEAB0);
PPC_FUNC_IMPL(__imp__sub_825CEAB0) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,3412(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3412);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ceac8
	if (ctx.cr6.eq) goto loc_825CEAC8;
	// lwz r11,3708(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3708);
	// stw r11,3716(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3716, ctx.r11.u32);
	// blr 
	return;
loc_825CEAC8:
	// lwz r11,15564(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15564);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ceaec
	if (ctx.cr6.eq) goto loc_825CEAEC;
	// lwz r11,3396(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3396);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ceaec
	if (ctx.cr6.eq) goto loc_825CEAEC;
	// lwz r11,3704(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3704);
	// stw r11,3716(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3716, ctx.r11.u32);
	// blr 
	return;
loc_825CEAEC:
	// lwz r11,3396(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3396);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ceb04
	if (ctx.cr6.eq) goto loc_825CEB04;
	// lwz r11,3688(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3688);
	// stw r11,3716(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3716, ctx.r11.u32);
	// blr 
	return;
loc_825CEB04:
	// lwz r11,3692(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3692);
	// stw r11,3716(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3716, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CEB10"))) PPC_WEAK_FUNC(sub_825CEB10);
PPC_FUNC_IMPL(__imp__sub_825CEB10) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba18
	ctx.lr = 0x825CEB18;
	sub_8239BA18(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r28,0
	ctx.r28.s64 = 0;
	// mr r29,r28
	ctx.r29.u64 = ctx.r28.u64;
	// lwz r11,21580(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21580);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ceb38
	if (ctx.cr6.eq) goto loc_825CEB38;
	// bl 0x825e92c8
	ctx.lr = 0x825CEB38;
	sub_825E92C8(ctx, base);
loc_825CEB38:
	// lwz r11,15564(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r11,156(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// beq cr6,0x825ceb64
	if (ctx.cr6.eq) goto loc_825CEB64;
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// stw r11,596(r10)
	PPC_STORE_U32(ctx.r10.u32 + 596, ctx.r11.u32);
	// lwz r11,3704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// lwz r10,160(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	// stw r10,600(r11)
	PPC_STORE_U32(ctx.r11.u32 + 600, ctx.r10.u32);
	// lwz r11,3704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// b 0x825ceb7c
	goto loc_825CEB7C;
loc_825CEB64:
	// lwz r10,3688(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// stw r11,596(r10)
	PPC_STORE_U32(ctx.r10.u32 + 596, ctx.r11.u32);
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// lwz r10,160(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	// stw r10,600(r11)
	PPC_STORE_U32(ctx.r11.u32 + 600, ctx.r10.u32);
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
loc_825CEB7C:
	// lwz r10,20972(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20972);
	// stw r10,604(r11)
	PPC_STORE_U32(ctx.r11.u32 + 604, ctx.r10.u32);
	// lwz r10,14772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825cebb4
	if (!ctx.cr6.gt) goto loc_825CEBB4;
	// lwz r11,3408(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3408);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825cebd0
	if (!ctx.cr6.eq) goto loc_825CEBD0;
	// lwz r11,3384(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3384);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825cebd0
	if (!ctx.cr6.eq) goto loc_825CEBD0;
	// lwz r11,3452(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3452);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825cebd0
	if (!ctx.cr6.eq) goto loc_825CEBD0;
loc_825CEBB4:
	// lwz r11,15564(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825cebc8
	if (ctx.cr6.eq) goto loc_825CEBC8;
	// lwz r11,3704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// b 0x825cebcc
	goto loc_825CEBCC;
loc_825CEBC8:
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
loc_825CEBCC:
	// stw r11,3716(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3716, ctx.r11.u32);
loc_825CEBD0:
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// li r30,1
	ctx.r30.s64 = 1;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825cec18
	if (ctx.cr6.eq) goto loc_825CEC18;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825cec18
	if (ctx.cr6.eq) goto loc_825CEC18;
	// ld r9,3576(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 3576);
	// cmpdi cr6,r9,1
	ctx.cr6.compare<int64_t>(ctx.r9.s64, 1, ctx.xer);
	// ble cr6,0x825cec10
	if (!ctx.cr6.gt) goto loc_825CEC10;
	// lwz r9,3404(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3404);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x825cec10
	if (!ctx.cr6.eq) goto loc_825CEC10;
	// lwz r9,15564(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// bne cr6,0x825cec10
	if (!ctx.cr6.eq) goto loc_825CEC10;
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
loc_825CEC10:
	// lwz r9,15564(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// stw r9,3404(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3404, ctx.r9.u32);
loc_825CEC18:
	// lwz r9,15472(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r9,7
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 7, ctx.xer);
	// beq cr6,0x825cec28
	if (ctx.cr6.eq) goto loc_825CEC28;
	// stw r28,21184(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21184, ctx.r28.u32);
loc_825CEC28:
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825ced64
	if (ctx.cr6.eq) goto loc_825CED64;
	// ld r9,3576(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 3576);
	// cmpdi cr6,r9,1
	ctx.cr6.compare<int64_t>(ctx.r9.s64, 1, ctx.xer);
	// bne cr6,0x825cec74
	if (!ctx.cr6.eq) goto loc_825CEC74;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825cec4c
	if (ctx.cr6.eq) goto loc_825CEC4C;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825cec74
	if (!ctx.cr6.eq) goto loc_825CEC74;
loc_825CEC4C:
	// lwz r11,15564(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825cec6c
	if (ctx.cr6.eq) goto loc_825CEC6C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d8138
	ctx.lr = 0x825CEC60;
	sub_825D8138(ctx, base);
	// lwz r11,3708(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3708);
	// stw r30,3412(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3412, ctx.r30.u32);
	// b 0x825ced60
	goto loc_825CED60;
loc_825CEC6C:
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// b 0x825ced60
	goto loc_825CED60;
loc_825CEC74:
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825ced64
	if (ctx.cr6.eq) goto loc_825CED64;
	// lwz r9,3408(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3408);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x825ceca0
	if (!ctx.cr6.eq) goto loc_825CECA0;
	// lwz r9,3384(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3384);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x825ceca0
	if (!ctx.cr6.eq) goto loc_825CECA0;
	// lwz r9,3452(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3452);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x825cecd4
	if (ctx.cr6.eq) goto loc_825CECD4;
loc_825CECA0:
	// lwz r10,3384(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3384);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825ced64
	if (ctx.cr6.eq) goto loc_825CED64;
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// bne cr6,0x825ced5c
	if (!ctx.cr6.eq) goto loc_825CED5C;
	// lwz r11,3396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3396);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ced5c
	if (ctx.cr6.eq) goto loc_825CED5C;
	// lwz r11,15564(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825cec6c
	if (ctx.cr6.eq) goto loc_825CEC6C;
	// lwz r11,3704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// b 0x825ced60
	goto loc_825CED60;
loc_825CECD4:
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825ced64
	if (ctx.cr6.eq) goto loc_825CED64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825cecf4
	if (ctx.cr6.eq) goto loc_825CECF4;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x825cecf4
	if (ctx.cr6.eq) goto loc_825CECF4;
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// bne cr6,0x825ced64
	if (!ctx.cr6.eq) goto loc_825CED64;
loc_825CECF4:
	// lwz r10,3412(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3412);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lwz r10,15564(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// beq cr6,0x825ced28
	if (ctx.cr6.eq) goto loc_825CED28;
	// lwz r11,3708(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3708);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r28,3412(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3412, ctx.r28.u32);
	// stw r11,3716(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3716, ctx.r11.u32);
	// beq cr6,0x825ced64
	if (ctx.cr6.eq) goto loc_825CED64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d8138
	ctx.lr = 0x825CED20;
	sub_825D8138(ctx, base);
	// stw r30,3412(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3412, ctx.r30.u32);
	// b 0x825ced64
	goto loc_825CED64;
loc_825CED28:
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825ced54
	if (ctx.cr6.eq) goto loc_825CED54;
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// beq cr6,0x825ced64
	if (ctx.cr6.eq) goto loc_825CED64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d8138
	ctx.lr = 0x825CED40;
	sub_825D8138(ctx, base);
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// stw r30,3412(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3412, ctx.r30.u32);
	// bne cr6,0x825ced5c
	if (!ctx.cr6.eq) goto loc_825CED5C;
	// lwz r11,3704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// b 0x825ced60
	goto loc_825CED60;
loc_825CED54:
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// beq cr6,0x825ced64
	if (ctx.cr6.eq) goto loc_825CED64;
loc_825CED5C:
	// lwz r11,3696(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3696);
loc_825CED60:
	// stw r11,3716(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3716, ctx.r11.u32);
loc_825CED64:
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825ced9c
	if (!ctx.cr6.gt) goto loc_825CED9C;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825ced9c
	if (ctx.cr6.eq) goto loc_825CED9C;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825ced9c
	if (ctx.cr6.eq) goto loc_825CED9C;
	// lwz r10,20980(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20980);
	// lwz r11,20972(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20972);
	// stw r10,20976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20976, ctx.r10.u32);
	// stw r11,20980(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20980, ctx.r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
loc_825CED9C:
	// lwz r11,20972(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20972);
	// stw r11,20976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20976, ctx.r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
}

__attribute__((alias("__imp__sub_825CEDAC"))) PPC_WEAK_FUNC(sub_825CEDAC);
PPC_FUNC_IMPL(__imp__sub_825CEDAC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CEDB0"))) PPC_WEAK_FUNC(sub_825CEDB0);
PPC_FUNC_IMPL(__imp__sub_825CEDB0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// stw r4,21236(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21236, ctx.r4.u32);
	// bne cr6,0x825cee44
	if (!ctx.cr6.eq) goto loc_825CEE44;
	// lwz r10,21240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// li r11,1
	ctx.r11.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r30,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r30.u32);
	// lwz r10,21276(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21276);
	// lwz r3,21240(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// rlwinm r5,r10,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,21244(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21244, ctx.r11.u32);
	// bl 0x8239ca70
	ctx.lr = 0x825CEE00;
	sub_8239CA70(ctx, base);
	// lwz r11,21276(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21276);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,21252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21252);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239ca70
	ctx.lr = 0x825CEE14;
	sub_8239CA70(ctx, base);
	// lwz r11,21276(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21276);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,21268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21268);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239ca70
	ctx.lr = 0x825CEE28;
	sub_8239CA70(ctx, base);
	// lwz r11,21268(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21268);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r30,21272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21272, ctx.r30.u32);
	// stw r30,21256(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21256, ctx.r30.u32);
	// stw r30,21260(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21260, ctx.r30.u32);
	// stw r11,21264(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21264, ctx.r11.u32);
	// b 0x825cf048
	goto loc_825CF048;
loc_825CEE44:
	// lwz r11,188(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// srawi r10,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 4;
	// lwz r11,21244(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21244);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825cee60
	if (ctx.cr6.lt) goto loc_825CEE60;
	// li r3,4
	ctx.r3.s64 = 4;
	// b 0x825cf048
	goto loc_825CF048;
loc_825CEE60:
	// lwz r10,21240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r11,-4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// cmplw cr6,r9,r11
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825cee84
	if (ctx.cr6.gt) goto loc_825CEE84;
	// lwz r11,21276(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21276);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
loc_825CEE84:
	// lwz r11,0(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// lwz r10,21276(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21276);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825cefc4
	if (ctx.cr6.lt) goto loc_825CEFC4;
	// lwz r9,21244(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21244);
	// li r3,4
	ctx.r3.s64 = 4;
	// cmplwi cr6,r9,2
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 2, ctx.xer);
	// ble cr6,0x825ceefc
	if (!ctx.cr6.gt) goto loc_825CEEFC;
	// mr r8,r30
	ctx.r8.u64 = ctx.r30.u64;
	// cmpwi cr6,r9,2
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 2, ctx.xer);
	// ble cr6,0x825ceee0
	if (!ctx.cr6.gt) goto loc_825CEEE0;
	// lwz r11,21240(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// rotlwi r10,r9,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// addi r10,r10,-2
	ctx.r10.s64 = ctx.r10.s64 + -2;
loc_825CEEC0:
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r30,-4(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// subf r4,r30,r4
	ctx.r4.s64 = ctx.r4.s64 - ctx.r30.s64;
	// add r8,r4,r8
	ctx.r8.u64 = ctx.r4.u64 + ctx.r8.u64;
	// bne cr6,0x825ceec0
	if (!ctx.cr6.eq) goto loc_825CEEC0;
loc_825CEEE0:
	// lwz r10,21240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,-2
	ctx.r9.s64 = ctx.r9.s64 + -2;
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// divwu r11,r8,r9
	ctx.r11.u32 = ctx.r8.u32 / ctx.r9.u32;
	// twllei r9,0
	// b 0x825cef88
	goto loc_825CEF88;
loc_825CEEFC:
	// bne cr6,0x825cef10
	if (!ctx.cr6.eq) goto loc_825CEF10;
	// lwz r11,21240(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// b 0x825cef90
	goto loc_825CEF90;
loc_825CEF10:
	// lwz r11,21280(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21280);
	// lwz r10,21288(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21288);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// ble cr6,0x825cef60
	if (!ctx.cr6.gt) goto loc_825CEF60;
	// cmpwi cr6,r10,2
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 2, ctx.xer);
	// ble cr6,0x825cef60
	if (!ctx.cr6.gt) goto loc_825CEF60;
	// lwz r8,92(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 92);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r4,21240(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// srawi r30,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r8.s32 >> 4;
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r11,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mullw r10,r30,r10
	ctx.r10.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r10.s32);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// twllei r11,0
	// divwu r11,r10,r11
	ctx.r11.u32 = ctx.r10.u32 / ctx.r11.u32;
	// lwz r10,-4(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + -4);
	// b 0x825cef8c
	goto loc_825CEF8C;
loc_825CEF60:
	// lwz r8,21284(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21284);
	// cmplwi cr6,r8,1
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 1, ctx.xer);
	// ble cr6,0x825cef94
	if (!ctx.cr6.gt) goto loc_825CEF94;
	// lwz r10,21240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,92(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 92);
	// twllei r8,0
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// srawi r11,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r9.s32 >> 4;
	// divwu r11,r11,r8
	ctx.r11.u32 = ctx.r11.u32 / ctx.r8.u32;
loc_825CEF88:
	// lwz r10,-4(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
loc_825CEF8C:
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
loc_825CEF90:
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
loc_825CEF94:
	// lwz r11,21276(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21276);
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825cefc4
	if (!ctx.cr6.gt) goto loc_825CEFC4;
	// lwz r10,21244(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21244);
	// lwz r9,21240(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// ble cr6,0x825cf010
	if (!ctx.cr6.gt) goto loc_825CF010;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
loc_825CEFC4:
	// lwz r11,0(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r9,21268(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21268);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r10,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, ctx.r10.u32);
	// lwz r11,21244(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21244);
	// lwz r10,21240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// lwz r9,0(r5)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// beq cr6,0x825cf018
	if (ctx.cr6.eq) goto loc_825CF018;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r11,r10
	PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// lwz r11,21244(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21244);
	// lwz r10,21252(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21252);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r6,r11,r10
	PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, ctx.r6.u32);
	// b 0x825cf030
	goto loc_825CF030;
loc_825CF010:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x825cf048
	goto loc_825CF048;
loc_825CF018:
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r9,r11,r10
	PPC_STORE_U32(ctx.r11.u32 + ctx.r10.u32, ctx.r9.u32);
	// lwz r11,21244(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21244);
	// lwz r10,21252(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21252);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r6,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, ctx.r6.u32);
loc_825CF030:
	// lwz r11,21244(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21244);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,21244(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21244, ctx.r11.u32);
	// lwz r11,21280(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21280);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,21280(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21280, ctx.r11.u32);
loc_825CF048:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CF060"))) PPC_WEAK_FUNC(sub_825CF060);
PPC_FUNC_IMPL(__imp__sub_825CF060) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x825cf070
	if (!ctx.cr6.eq) goto loc_825CF070;
	// li r3,7
	ctx.r3.s64 = 7;
	// blr 
	return;
loc_825CF070:
	// lwz r11,16(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825cf084
	if (ctx.cr6.eq) goto loc_825CF084;
	// li r3,13
	ctx.r3.s64 = 13;
	// blr 
	return;
loc_825CF084:
	// lwz r11,15472(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bge cr6,0x825cf0a8
	if (!ctx.cr6.lt) goto loc_825CF0A8;
	// lwz r11,14824(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14824);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825cf0fc
	if (ctx.cr6.eq) goto loc_825CF0FC;
	// lwz r9,14848(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14848);
	// lwz r11,14852(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14852);
	// b 0x825cf104
	goto loc_825CF104;
loc_825CF0A8:
	// lwz r11,21184(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21184);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825cf0fc
	if (!ctx.cr6.eq) goto loc_825CF0FC;
	// lwz r11,14772(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825cf0fc
	if (!ctx.cr6.gt) goto loc_825CF0FC;
	// ld r11,3576(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 3576);
	// cmpdi cr6,r11,1
	ctx.cr6.compare<int64_t>(ctx.r11.s64, 1, ctx.xer);
	// ble cr6,0x825cf0fc
	if (!ctx.cr6.gt) goto loc_825CF0FC;
	// lwz r10,21352(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21352);
	// lwz r11,21380(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21380);
	// mr r9,r10
	ctx.r9.u64 = ctx.r10.u64;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bgt cr6,0x825cf0e4
	if (ctx.cr6.gt) goto loc_825CF0E4;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
loc_825CF0E4:
	// lwz r11,21384(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21384);
	// lwz r10,21356(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21356);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// ble cr6,0x825cf104
	if (!ctx.cr6.gt) goto loc_825CF104;
	// mr r11,r10
	ctx.r11.u64 = ctx.r10.u64;
	// b 0x825cf104
	goto loc_825CF104;
loc_825CF0FC:
	// lwz r11,160(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// lwz r9,156(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
loc_825CF104:
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x825cf110
	if (ctx.cr6.eq) goto loc_825CF110;
	// stw r9,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r9.u32);
loc_825CF110:
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x825cf11c
	if (ctx.cr6.eq) goto loc_825CF11C;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
loc_825CF11C:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CF124"))) PPC_WEAK_FUNC(sub_825CF124);
PPC_FUNC_IMPL(__imp__sub_825CF124) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CF128"))) PPC_WEAK_FUNC(sub_825CF128);
PPC_FUNC_IMPL(__imp__sub_825CF128) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825CF130;
	sub_8239BA0C(ctx, base);
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// mr r30,r6
	ctx.r30.u64 = ctx.r6.u64;
	// mr r29,r7
	ctx.r29.u64 = ctx.r7.u64;
	// mr r27,r8
	ctx.r27.u64 = ctx.r8.u64;
	// mr r26,r9
	ctx.r26.u64 = ctx.r9.u64;
	// mr r25,r10
	ctx.r25.u64 = ctx.r10.u64;
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// bne cr6,0x825cf160
	if (!ctx.cr6.eq) goto loc_825CF160;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825CF160:
	// li r11,40
	ctx.r11.s64 = 40;
	// stw r30,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r30.u32);
	// stw r29,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r29.u32);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// stw r4,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r4.u32);
	// sth r5,190(r1)
	PPC_STORE_U16(ctx.r1.u32 + 190, ctx.r5.u16);
	// stw r11,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r11.u32);
	// li r11,1
	ctx.r11.s64 = 1;
	// sth r11,188(r1)
	PPC_STORE_U16(ctx.r1.u32 + 188, ctx.r11.u16);
	// beq cr6,0x825cf1a8
	if (ctx.cr6.eq) goto loc_825CF1A8;
	// cmplwi cr6,r4,3
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 3, ctx.xer);
	// beq cr6,0x825cf1a8
	if (ctx.cr6.eq) goto loc_825CF1A8;
	// clrlwi r11,r5,16
	ctx.r11.u64 = ctx.r5.u32 & 0xFFFF;
	// mullw r11,r11,r30
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r30.s32);
	// mullw r11,r11,r29
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r29.s32);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// b 0x825cf1d4
	goto loc_825CF1D4;
loc_825CF1A8:
	// clrlwi r11,r5,16
	ctx.r11.u64 = ctx.r5.u32 & 0xFFFF;
	// mr r10,r29
	ctx.r10.u64 = ctx.r29.u64;
	// mullw r11,r11,r30
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r30.s32);
	// srawi r9,r10,31
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7FFFFFFF) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 31;
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// subf r10,r9,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r9.s64;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// rlwinm r11,r11,0,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFC;
	// mullw r11,r10,r11
	ctx.r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
loc_825CF1D4:
	// li r31,0
	ctx.r31.s64 = 0;
	// stw r11,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r11.u32);
	// addi r5,r1,152
	ctx.r5.s64 = ctx.r1.s64 + 152;
	// addi r4,r1,148
	ctx.r4.s64 = ctx.r1.s64 + 148;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stw r31,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r31.u32);
	// stw r31,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r31.u32);
	// stw r31,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r31.u32);
	// stw r31,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r31.u32);
	// bl 0x825dd6d8
	ctx.lr = 0x825CF1FC;
	sub_825DD6D8(ctx, base);
	// lwz r11,148(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r26,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r26.u32);
	// addi r11,r11,-2
	ctx.r11.s64 = ctx.r11.s64 + -2;
	// stw r27,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r27.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r29.u32);
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// stw r31,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r31.u32);
	// mr r7,r26
	ctx.r7.u64 = ctx.r26.u64;
	// stw r31,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r31.u32);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// stw r31,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r31.u32);
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// xori r11,r11,1
	ctx.r11.u64 = ctx.r11.u64 ^ 1;
	// addi r5,r1,144
	ctx.r5.s64 = ctx.r1.s64 + 144;
	// addi r4,r1,176
	ctx.r4.s64 = ctx.r1.s64 + 176;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r11.u32);
	// bl 0x825f6140
	ctx.lr = 0x825CF250;
	sub_825F6140(ctx, base);
	// lhz r11,190(r1)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r1.u32 + 190);
	// addi r9,r1,156
	ctx.r9.s64 = ctx.r1.s64 + 156;
	// addi r6,r1,160
	ctx.r6.s64 = ctx.r1.s64 + 160;
	// lwz r7,372(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 372);
	// mullw r11,r11,r27
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r27.s32);
	// lwz r5,196(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 196);
	// lwz r3,144(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// mullw r11,r11,r26
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r26.s32);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// addze r8,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r8.s64 = temp.s64;
	// bl 0x825f5fb0
	ctx.lr = 0x825CF280;
	sub_825F5FB0(ctx, base);
	// lwz r3,144(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// bl 0x825f4a78
	ctx.lr = 0x825CF288;
	sub_825F4A78(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825CF294"))) PPC_WEAK_FUNC(sub_825CF294);
PPC_FUNC_IMPL(__imp__sub_825CF294) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CF298"))) PPC_WEAK_FUNC(sub_825CF298);
PPC_FUNC_IMPL(__imp__sub_825CF298) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba18
	ctx.lr = 0x825CF2A0;
	sub_8239BA18(ctx, base);
	// stfd f30,-56(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -56, ctx.f30.u64);
	// stfd f31,-48(r1)
	PPC_STORE_U64(ctx.r1.u32 + -48, ctx.f31.u64);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// mr r28,r10
	ctx.r28.u64 = ctx.r10.u64;
	// addi r10,r1,80
	ctx.r10.s64 = ctx.r1.s64 + 80;
	// fmr f30,f2
	ctx.f30.f64 = ctx.f2.f64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// mr r6,r8
	ctx.r6.u64 = ctx.r8.u64;
	// mr r7,r9
	ctx.r7.u64 = ctx.r9.u64;
	// lwz r8,80(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// stw r11,3340(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3340, ctx.r11.u32);
	// stw r29,21428(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21428, ctx.r29.u32);
	// fctiwz f0,f31
	ctx.f0.s64 = (ctx.f31.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f31.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f0.u32);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x825f05d8
	ctx.lr = 0x825CF2F4;
	sub_825F05D8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825cf3e8
	if (!ctx.cr6.eq) goto loc_825CF3E8;
	// stfs f31,3648(r31)
	ctx.fpscr.disableFlushMode();
	temp.f32 = float(ctx.f31.f64);
	PPC_STORE_U32(ctx.r31.u32 + 3648, temp.u32);
	// cmpwi cr6,r28,4
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 4, ctx.xer);
	// stfs f30,3652(r31)
	temp.f32 = float(ctx.f30.f64);
	PPC_STORE_U32(ctx.r31.u32 + 3652, temp.u32);
	// sth r29,3684(r31)
	PPC_STORE_U16(ctx.r31.u32 + 3684, ctx.r29.u16);
	// stw r30,3640(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3640, ctx.r30.u32);
	// stw r28,3644(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3644, ctx.r28.u32);
	// ble cr6,0x825cf320
	if (!ctx.cr6.gt) goto loc_825CF320;
	// li r11,4
	ctx.r11.s64 = 4;
	// b 0x825cf32c
	goto loc_825CF32C;
loc_825CF320:
	// cmpwi cr6,r28,-1
	ctx.cr6.compare<int32_t>(ctx.r28.s32, -1, ctx.xer);
	// bge cr6,0x825cf330
	if (!ctx.cr6.lt) goto loc_825CF330;
	// li r11,-1
	ctx.r11.s64 = -1;
loc_825CF32C:
	// stw r11,3644(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3644, ctx.r11.u32);
loc_825CF330:
	// lwz r11,228(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,15504(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15504, ctx.r11.u32);
	// beq cr6,0x825cf34c
	if (ctx.cr6.eq) goto loc_825CF34C;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825cf34c
	if (ctx.cr6.eq) goto loc_825CF34C;
	// stw r29,15504(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15504, ctx.r29.u32);
loc_825CF34C:
	// lwz r6,216(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 216);
	// lwz r11,204(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// lwz r10,156(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// lwz r8,160(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	// rlwinm r5,r9,3,0,28
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r6,116(r31)
	PPC_STORE_U32(ctx.r31.u32 + 116, ctx.r6.u32);
	// rlwinm r6,r11,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r7,212(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// stw r11,96(r31)
	PPC_STORE_U32(ctx.r31.u32 + 96, ctx.r11.u32);
	// stw r10,88(r31)
	PPC_STORE_U32(ctx.r31.u32 + 88, ctx.r10.u32);
	// stw r9,108(r31)
	PPC_STORE_U32(ctx.r31.u32 + 108, ctx.r9.u32);
	// stw r8,92(r31)
	PPC_STORE_U32(ctx.r31.u32 + 92, ctx.r8.u32);
	// stw r7,104(r31)
	PPC_STORE_U32(ctx.r31.u32 + 104, ctx.r7.u32);
	// stw r6,100(r31)
	PPC_STORE_U32(ctx.r31.u32 + 100, ctx.r6.u32);
	// stw r5,112(r31)
	PPC_STORE_U32(ctx.r31.u32 + 112, ctx.r5.u32);
	// bne cr6,0x825cf3a0
	if (!ctx.cr6.eq) goto loc_825CF3A0;
	// cmpw cr6,r8,r7
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, ctx.xer);
	// li r8,1
	ctx.r8.s64 = 1;
	// beq cr6,0x825cf3a4
	if (ctx.cr6.eq) goto loc_825CF3A4;
loc_825CF3A0:
	// mr r8,r29
	ctx.r8.u64 = ctx.r29.u64;
loc_825CF3A4:
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// lwz r7,188(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r9,r11,-22728
	ctx.r9.s64 = ctx.r11.s64 + -22728;
	// stw r8,120(r31)
	PPC_STORE_U32(ctx.r31.u32 + 120, ctx.r8.u32);
	// srawi r11,r10,4
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r10.s32 >> 4;
	// stw r29,3456(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3456, ctx.r29.u32);
	// addi r9,r9,384
	ctx.r9.s64 = ctx.r9.s64 + 384;
	// stw r29,3460(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3460, ctx.r29.u32);
	// srawi r10,r7,4
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 4;
	// stw r29,3464(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3464, ctx.r29.u32);
	// stw r11,128(r31)
	PPC_STORE_U32(ctx.r31.u32 + 128, ctx.r11.u32);
	// stw r9,256(r31)
	PPC_STORE_U32(ctx.r31.u32 + 256, ctx.r9.u32);
	// mullw r9,r10,r11
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// stw r10,132(r31)
	PPC_STORE_U32(ctx.r31.u32 + 132, ctx.r10.u32);
	// stw r9,124(r31)
	PPC_STORE_U32(ctx.r31.u32 + 124, ctx.r9.u32);
loc_825CF3E8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// lfd f30,-56(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -56);
	// lfd f31,-48(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -48);
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
}

__attribute__((alias("__imp__sub_825CF3F8"))) PPC_WEAK_FUNC(sub_825CF3F8);
PPC_FUNC_IMPL(__imp__sub_825CF3F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825CF400;
	sub_8239BA1C(ctx, base);
	// stfd f31,-40(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -40, ctx.f31.u64);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// li r11,1
	ctx.r11.s64 = 1;
	// li r29,-1
	ctx.r29.s64 = -1;
	// std r30,19744(r31)
	PPC_STORE_U64(ctx.r31.u32 + 19744, ctx.r30.u64);
	// std r30,19752(r31)
	PPC_STORE_U64(ctx.r31.u32 + 19752, ctx.r30.u64);
	// stw r30,19768(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19768, ctx.r30.u32);
	// std r30,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r30.u64);
	// stw r30,19772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19772, ctx.r30.u32);
	// stw r30,19824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19824, ctx.r30.u32);
	// stw r30,3420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3420, ctx.r30.u32);
	// stw r30,19828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19828, ctx.r30.u32);
	// stw r29,19924(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19924, ctx.r29.u32);
	// stw r11,19724(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19724, ctx.r11.u32);
	// bl 0x82692698
	ctx.lr = 0x825CF444;
	sub_82692698(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// addi r10,r11,-150
	ctx.r10.s64 = ctx.r11.s64 + -150;
	// stw r11,19936(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19936, ctx.r11.u32);
	// stw r10,19932(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19932, ctx.r10.u32);
	// bl 0x823b5290
	ctx.lr = 0x825CF45C;
	sub_823B5290(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825cf468
	if (!ctx.cr6.eq) goto loc_825CF468;
	// stw r30,19724(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19724, ctx.r30.u32);
loc_825CF468:
	// lis r11,-7341
	ctx.r11.s64 = -481099776;
	// ld r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lis r8,8388
	ctx.r8.s64 = 549715968;
	// lwz r10,3656(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3656);
	// ori r11,r11,63439
	ctx.r11.u64 = ctx.r11.u64 | 63439;
	// ori r8,r8,39845
	ctx.r8.u64 = ctx.r8.u64 | 39845;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// rldimi r11,r8,32,0
	ctx.r11.u64 = (__builtin_rotateleft64(ctx.r8.u64, 32) & 0xFFFFFFFF00000000) | (ctx.r11.u64 & 0xFFFFFFFF);
	// mulhd r11,r9,r11
	// sradi r11,r11,7
	ctx.xer.ca = (ctx.r11.s64 < 0) & ((ctx.r11.u64 & 0x7F) != 0);
	ctx.r11.s64 = ctx.r11.s64 >> 7;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// stw r11,19928(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19928, ctx.r11.u32);
	// bgt cr6,0x825cf4a4
	if (ctx.cr6.gt) goto loc_825CF4A4;
	// li r10,30
	ctx.r10.s64 = 30;
loc_825CF4A4:
	// li r11,1000
	ctx.r11.s64 = 1000;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// twllei r10,0
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// tdllei r8,0
	// divw. r11,r11,r10
	ctx.r11.s32 = ctx.r11.s32 / ctx.r10.s32;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotldi r10,r9,1
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u64, 1);
	// divd r9,r9,r8
	ctx.r9.s64 = ctx.r9.s64 / ctx.r8.s64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// andc r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 & ~ctx.r10.u64;
	// stw r11,19820(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19820, ctx.r11.u32);
	// tdlgei r10,-1
	// stw r9,19832(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19832, ctx.r9.u32);
	// bgt 0x825cf4e4
	if (ctx.cr0.gt) goto loc_825CF4E4;
	// li r11,33
	ctx.r11.s64 = 33;
	// stw r11,19820(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19820, ctx.r11.u32);
loc_825CF4E4:
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,114
	ctx.r4.s64 = 114;
	// li r3,6
	ctx.r3.s64 = 6;
	// bl 0x825f6c70
	ctx.lr = 0x825CF4F4;
	sub_825F6C70(ctx, base);
	// cmpwi cr6,r3,100
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 100, ctx.xer);
	// blt cr6,0x825cf504
	if (ctx.cr6.lt) goto loc_825CF504;
	// cmpwi cr6,r3,32000
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 32000, ctx.xer);
	// ble cr6,0x825cf508
	if (!ctx.cr6.gt) goto loc_825CF508;
loc_825CF504:
	// li r3,100
	ctx.r3.s64 = 100;
loc_825CF508:
	// lwz r11,19928(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19928);
	// extsw r10,r3
	ctx.r10.s64 = ctx.r3.s32;
	// li r5,0
	ctx.r5.s64 = 0;
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// li r4,114
	ctx.r4.s64 = 114;
	// li r3,5
	ctx.r3.s64 = 5;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lwz r11,188(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r11.u64);
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lfd f13,96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// lfd f0,88(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f0
	ctx.f12.f64 = double(float(ctx.f0.f64));
	// lfs f0,-23956(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23956);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// fmuls f12,f12,f0
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f0.f64));
	// lfs f0,-23960(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -23960);
	ctx.f0.f64 = double(temp.f32);
	// fdivs f13,f0,f13
	ctx.f13.f64 = double(float(ctx.f0.f64 / ctx.f13.f64));
	// lfd f0,104(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lis r11,-32255
	ctx.r11.s64 = -2113863680;
	// frsp f10,f0
	ctx.f10.f64 = double(float(ctx.f0.f64));
	// lfs f0,-4904(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -4904);
	ctx.f0.f64 = double(temp.f32);
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// fmuls f10,f10,f0
	ctx.f10.f64 = double(float(ctx.f10.f64 * ctx.f0.f64));
	// lfs f0,1780(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 1780);
	ctx.f0.f64 = double(temp.f32);
	// lfd f11,88(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fmuls f12,f12,f10
	ctx.f12.f64 = double(float(ctx.f12.f64 * ctx.f10.f64));
	// frsp f11,f11
	ctx.f11.f64 = double(float(ctx.f11.f64));
	// fmuls f13,f12,f13
	ctx.f13.f64 = double(float(ctx.f12.f64 * ctx.f13.f64));
	// fmuls f0,f11,f0
	ctx.f0.f64 = double(float(ctx.f11.f64 * ctx.f0.f64));
	// fmuls f31,f13,f0
	ctx.f31.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// bl 0x825f6c70
	ctx.lr = 0x825CF5B0;
	sub_825F6C70(ctx, base);
	// stw r29,19736(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19736, ctx.r29.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// stw r3,19732(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19732, ctx.r3.u32);
	// bne cr6,0x825cf5c8
	if (!ctx.cr6.eq) goto loc_825CF5C8;
	// stw r30,19724(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19724, ctx.r30.u32);
	// b 0x825cf614
	goto loc_825CF614;
loc_825CF5C8:
	// cmpwi cr6,r3,15
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 15, ctx.xer);
	// ble cr6,0x825cf614
	if (!ctx.cr6.gt) goto loc_825CF614;
	// lwz r11,19832(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19832);
	// srawi r10,r3,4
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r3.s32 >> 4;
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r11.u64);
	// lis r11,-32248
	ctx.r11.s64 = -2113404928;
	// std r10,96(r1)
	PPC_STORE_U64(ctx.r1.u32 + 96, ctx.r10.u64);
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// lfd f0,96(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 96);
	// fcfid f12,f0
	ctx.f12.f64 = double(ctx.f0.s64);
	// lfd f0,-26816(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -26816);
	// fmul f0,f12,f0
	ctx.f0.f64 = ctx.f12.f64 * ctx.f0.f64;
	// fmul f0,f0,f13
	ctx.f0.f64 = ctx.f0.f64 * ctx.f13.f64;
	// fctiwz f0,f0
	ctx.f0.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// li r12,19832
	ctx.r12.s64 = 19832;
	// stfiwx f0,r31,r12
	PPC_STORE_U32(ctx.r31.u32 + ctx.r12.u32, ctx.f0.u32);
loc_825CF614:
	// lwz r11,19832(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19832);
	// addi r10,r31,19836
	ctx.r10.s64 = ctx.r31.s64 + 19836;
	// stw r30,19812(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19812, ctx.r30.u32);
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// std r11,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r11.u64);
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f13,f0
	ctx.f13.f64 = double(ctx.f0.s64);
	// lfd f0,-23968(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -23968);
	// fmul f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 * ctx.f0.f64;
	// fctiwz f0,f0
	ctx.f0.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f0.u32);
loc_825CF644:
	// lwz r11,19812(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19812);
	// lwz r10,19832(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19832);
	// addi r11,r11,4944
	ctx.r11.s64 = ctx.r11.s64 + 4944;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stwx r10,r11,r31
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, ctx.r10.u32);
	// lwz r11,19812(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19812);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,19812(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19812, ctx.r11.u32);
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// blt cr6,0x825cf644
	if (ctx.cr6.lt) goto loc_825CF644;
	// lwz r11,19832(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19832);
	// lis r8,-32244
	ctx.r8.s64 = -2113142784;
	// addi r9,r31,19864
	ctx.r9.s64 = ctx.r31.s64 + 19864;
	// stw r30,19812(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19812, ctx.r30.u32);
	// rlwinm r10,r11,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r31,19904
	ctx.r11.s64 = ctx.r31.s64 + 19904;
	// subfic r7,r31,-19904
	ctx.xer.ca = ctx.r31.u32 <= 4294947392;
	ctx.r7.s64 = -19904 - ctx.r31.s64;
	// addi r6,r8,-24056
	ctx.r6.s64 = ctx.r8.s64 + -24056;
	// stw r10,19808(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19808, ctx.r10.u32);
	// li r10,5
	ctx.r10.s64 = 5;
loc_825CF698:
	// lwz r8,3924(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// addi r8,r6,20
	ctx.r8.s64 = ctx.r6.s64 + 20;
	// bne cr6,0x825cf6ac
	if (!ctx.cr6.eq) goto loc_825CF6AC;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
loc_825CF6AC:
	// add r5,r7,r11
	ctx.r5.u64 = ctx.r7.u64 + ctx.r11.u64;
	// addi r4,r11,-64
	ctx.r4.s64 = ctx.r11.s64 + -64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// lwzx r8,r5,r8
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r8.u32);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// std r8,104(r1)
	PPC_STORE_U64(ctx.r1.u32 + 104, ctx.r8.u64);
	// lfd f0,104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f0,f0
	ctx.f0.f64 = double(float(ctx.f0.f64));
	// fmuls f0,f0,f31
	ctx.f0.f64 = double(float(ctx.f0.f64 * ctx.f31.f64));
	// fctiwz f0,f0
	ctx.f0.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f0,0,r4
	PPC_STORE_U32(ctx.r4.u32, ctx.f0.u32);
	// std r30,0(r9)
	PPC_STORE_U64(ctx.r9.u32 + 0, ctx.r30.u64);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// stw r30,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r30.u32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// bne cr6,0x825cf698
	if (!ctx.cr6.eq) goto loc_825CF698;
	// lwz r11,19856(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19856);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,188(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// lwz r4,180(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// stw r11,19860(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19860, ctx.r11.u32);
	// bl 0x825f6c78
	ctx.lr = 0x825CF70C;
	sub_825F6C78(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// lfd f31,-40(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -40);
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825CF718"))) PPC_WEAK_FUNC(sub_825CF718);
PPC_FUNC_IMPL(__imp__sub_825CF718) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r30,1
	ctx.r30.s64 = 1;
loc_825CF734:
	// lwz r11,15548(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15548);
	// addi r11,r11,4976
	ctx.r11.s64 = ctx.r11.s64 + 4976;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	// cmpwi cr6,r11,50
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 50, ctx.xer);
	// ble cr6,0x825cf7cc
	if (!ctx.cr6.gt) goto loc_825CF7CC;
	// lwz r9,180(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// lwz r11,15548(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15548);
	// mullw r10,r10,r9
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r9.s32);
	// addi r9,r11,4976
	ctx.r9.s64 = ctx.r11.s64 + 4976;
	// addi r11,r11,2483
	ctx.r11.s64 = ctx.r11.s64 + 2483;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// srawi r10,r10,8
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// lwzx r9,r9,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r31.u32);
	// ldx r8,r11,r31
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r11.u32 + ctx.r31.u32);
	// extsw r9,r9
	ctx.r9.s64 = ctx.r9.s32;
	// rotldi r11,r8,1
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u64, 1);
	// divd r8,r8,r9
	ctx.r8.s64 = ctx.r8.s64 / ctx.r9.s64;
	// addi r7,r11,-1
	ctx.r7.s64 = ctx.r11.s64 + -1;
	// tdllei r9,0
	// extsw r11,r8
	ctx.r11.s64 = ctx.r8.s32;
	// andc r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 & ~ctx.r7.u64;
	// cmpwi cr6,r11,50
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 50, ctx.xer);
	// tdlgei r9,-1
	// blt cr6,0x825cf7cc
	if (ctx.cr6.lt) goto loc_825CF7CC;
	// cmpwi cr6,r10,50
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 50, ctx.xer);
	// blt cr6,0x825cf7cc
	if (ctx.cr6.lt) goto loc_825CF7CC;
	// rotlwi r9,r11,1
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
	// divw r5,r11,r10
	ctx.r5.s32 = ctx.r11.s32 / ctx.r10.s32;
	// addi r11,r9,-1
	ctx.r11.s64 = ctx.r9.s64 + -1;
	// li r4,119
	ctx.r4.s64 = 119;
	// andc r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 & ~ctx.r11.u64;
	// addi r3,r30,6
	ctx.r3.s64 = ctx.r30.s64 + 6;
	// twllei r10,0
	// twlgei r11,-1
	// bl 0x825f6c70
	ctx.lr = 0x825CF7CC;
	sub_825F6C70(ctx, base);
loc_825CF7CC:
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// cmpwi cr6,r30,4
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 4, ctx.xer);
	// ble cr6,0x825cf734
	if (!ctx.cr6.gt) goto loc_825CF734;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CF7F0"))) PPC_WEAK_FUNC(sub_825CF7F0);
PPC_FUNC_IMPL(__imp__sub_825CF7F0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825CF7F8;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r24,0
	ctx.r24.s64 = 0;
	// mr r25,r24
	ctx.r25.u64 = ctx.r24.u64;
	// lwz r11,19824(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19824);
	// lwz r10,3924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// lwz r8,15556(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15556);
	// addi r9,r11,1
	ctx.r9.s64 = ctx.r11.s64 + 1;
	// subfic r11,r10,0
	ctx.xer.ca = ctx.r10.u32 <= 0;
	ctx.r11.s64 = 0 - ctx.r10.s64;
	// rlwinm r10,r8,28,0,3
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0xF0000000;
	// subfe r8,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r8.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// srawi. r11,r10,28
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFFFFFFF) != 0);
	ctx.r11.s64 = ctx.r10.s32 >> 28;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rlwinm r10,r8,0,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFE;
	// stw r9,19824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19824, ctx.r9.u32);
	// addi r27,r10,4
	ctx.r27.s64 = ctx.r10.s64 + 4;
	// blt 0x825cf840
	if (ctx.cr0.lt) goto loc_825CF840;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// ble cr6,0x825cf848
	if (!ctx.cr6.gt) goto loc_825CF848;
loc_825CF840:
	// cmpwi cr6,r11,-2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, -2, ctx.xer);
	// bne cr6,0x825cf8fc
	if (!ctx.cr6.eq) goto loc_825CF8FC;
loc_825CF848:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bge cr6,0x825cf858
	if (!ctx.cr6.lt) goto loc_825CF858;
	// mr r11,r24
	ctx.r11.u64 = ctx.r24.u64;
	// b 0x825cf864
	goto loc_825CF864;
loc_825CF858:
	// cmpw cr6,r11,r27
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r27.s32, ctx.xer);
	// ble cr6,0x825cf864
	if (!ctx.cr6.gt) goto loc_825CF864;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_825CF864:
	// lwz r10,15548(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15548);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// beq cr6,0x825cf894
	if (ctx.cr6.eq) goto loc_825CF894;
	// bge cr6,0x825cf890
	if (!ctx.cr6.lt) goto loc_825CF890;
	// lwz r10,19832(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19832);
	// mulli r9,r10,218
	ctx.r9.s64 = ctx.r10.s64 * 218;
	// mulli r10,r10,243
	ctx.r10.s64 = ctx.r10.s64 * 243;
	// srawi r9,r9,8
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// srawi r10,r10,8
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// stw r9,19836(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19836, ctx.r9.u32);
	// stw r10,19832(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19832, ctx.r10.u32);
loc_825CF890:
	// stw r11,15548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15548, ctx.r11.u32);
loc_825CF894:
	// lwz r11,19768(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19768);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825cf8bc
	if (ctx.cr6.eq) goto loc_825CF8BC;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826926a8
	ctx.lr = 0x825CF8A8;
	sub_826926A8(ctx, base);
	// ld r11,19744(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 19744);
	// ld r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// stw r24,19768(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19768, ctx.r24.u32);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// std r11,19744(r31)
	PPC_STORE_U64(ctx.r31.u32 + 19744, ctx.r11.u64);
loc_825CF8BC:
	// lwz r11,19768(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19768);
	// std r24,19752(r31)
	PPC_STORE_U64(ctx.r31.u32 + 19752, ctx.r24.u64);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r24,3420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3420, ctx.r24.u32);
	// std r24,19744(r31)
	PPC_STORE_U64(ctx.r31.u32 + 19744, ctx.r24.u64);
	// bne cr6,0x825cfd74
	if (!ctx.cr6.eq) goto loc_825CFD74;
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// bl 0x826926a8
	ctx.lr = 0x825CF8DC;
	sub_826926A8(ctx, base);
	// ld r11,19744(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 19744);
	// ld r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// li r10,1
	ctx.r10.s64 = 1;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// stw r10,19768(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19768, ctx.r10.u32);
	// std r11,19744(r31)
	PPC_STORE_U64(ctx.r31.u32 + 19744, ctx.r11.u64);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825CF8FC:
	// cmpwi cr6,r9,5
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 5, ctx.xer);
	// bge cr6,0x825cf93c
	if (!ctx.cr6.lt) goto loc_825CF93C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc7d0
	ctx.lr = 0x825CF90C;
	sub_825CC7D0(ctx, base);
	// lwz r11,19824(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19824);
	// std r24,19744(r31)
	PPC_STORE_U64(ctx.r31.u32 + 19744, ctx.r24.u64);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// std r24,19752(r31)
	PPC_STORE_U64(ctx.r31.u32 + 19752, ctx.r24.u64);
	// stw r24,3420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3420, ctx.r24.u32);
	// bgt cr6,0x825cfd6c
	if (ctx.cr6.gt) goto loc_825CFD6C;
	// bl 0x82692698
	ctx.lr = 0x825CF928;
	sub_82692698(ctx, base);
	// stw r3,19932(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19932, ctx.r3.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc770
	ctx.lr = 0x825CF934;
	sub_825CC770(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825CF93C:
	// bl 0x82692698
	ctx.lr = 0x825CF940;
	sub_82692698(ctx, base);
	// lwz r11,19936(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19936);
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// addi r11,r11,8000
	ctx.r11.s64 = ctx.r11.s64 + 8000;
	// cmplw cr6,r26,r11
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825cf9b0
	if (!ctx.cr6.gt) goto loc_825CF9B0;
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r26,19936(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19936, ctx.r26.u32);
	// li r4,114
	ctx.r4.s64 = 114;
	// li r3,0
	ctx.r3.s64 = 0;
	// bl 0x825f6c70
	ctx.lr = 0x825CF968;
	sub_825F6C70(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,114
	ctx.r4.s64 = 114;
	// li r3,5
	ctx.r3.s64 = 5;
	// bl 0x825f6c70
	ctx.lr = 0x825CF97C;
	sub_825F6C70(ctx, base);
	// cmplwi cr6,r30,4
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 4, ctx.xer);
	// stw r3,19732(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19732, ctx.r3.u32);
	// bgt cr6,0x825cf9b0
	if (ctx.cr6.gt) goto loc_825CF9B0;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r30,15548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15548, ctx.r30.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,15544(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15544, ctx.r11.u32);
	// bl 0x825f6a28
	ctx.lr = 0x825CF99C;
	sub_825F6A28(ctx, base);
	// lwz r11,19732(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19732);
	// addi r11,r11,-2
	ctx.r11.s64 = ctx.r11.s64 + -2;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// stw r11,19724(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19724, ctx.r11.u32);
loc_825CF9B0:
	// lwz r11,19724(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19724);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825cfd74
	if (ctx.cr6.eq) goto loc_825CFD74;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc7d0
	ctx.lr = 0x825CF9C4;
	sub_825CC7D0(ctx, base);
	// lwz r11,19728(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19728);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// mr r28,r11
	ctx.r28.u64 = ctx.r11.u64;
	// bgt cr6,0x825cf9d8
	if (ctx.cr6.gt) goto loc_825CF9D8;
	// lwz r28,15548(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15548);
loc_825CF9D8:
	// lwz r11,19812(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19812);
	// ld r10,19744(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 19744);
	// addi r11,r11,4944
	ctx.r11.s64 = ctx.r11.s64 + 4944;
	// lwz r9,19808(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19808);
	// extsw r5,r10
	ctx.r5.s64 = ctx.r10.s32;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// add r10,r10,r5
	ctx.r10.u64 = ctx.r10.u64 + ctx.r5.u64;
	// stw r10,19808(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19808, ctx.r10.u32);
	// stwx r5,r11,r31
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, ctx.r5.u32);
	// lwz r10,19812(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19812);
	// ld r3,19752(r31)
	ctx.r3.u64 = PPC_LOAD_U64(ctx.r31.u32 + 19752);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r11,3420(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3420);
	// lwz r9,19808(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19808);
	// extsw r30,r3
	ctx.r30.s64 = ctx.r3.s32;
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// srawi r4,r9,3
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r4.s64 = ctx.r9.s32 >> 3;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r10,19812(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19812, ctx.r10.u32);
	// bgt cr6,0x825cfa44
	if (ctx.cr6.gt) goto loc_825CFA44;
	// lwz r10,15556(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15556);
	// lis r9,12288
	ctx.r9.s64 = 805306368;
	// rlwinm r10,r10,16,0,3
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 16) & 0xF0000000;
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x825cfac0
	if (ctx.cr6.lt) goto loc_825CFAC0;
loc_825CFA44:
	// lwz r10,19828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19828);
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r11,19828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19828, ctx.r11.u32);
	// ble cr6,0x825cfa88
	if (!ctx.cr6.gt) goto loc_825CFA88;
	// addi r28,r28,-1
	ctx.r28.s64 = ctx.r28.s64 + -1;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x825cfa68
	if (!ctx.cr6.gt) goto loc_825CFA68;
	// addi r28,r28,-1
	ctx.r28.s64 = ctx.r28.s64 + -1;
loc_825CFA68:
	// lwz r11,19832(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19832);
	// mulli r11,r11,230
	ctx.r11.s64 = ctx.r11.s64 * 230;
	// srawi r11,r11,8
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xFF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 8;
	// mulli r10,r11,230
	ctx.r10.s64 = ctx.r11.s64 * 230;
	// stw r11,19832(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19832, ctx.r11.u32);
	// srawi r10,r10,8
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// stw r10,19836(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19836, ctx.r10.u32);
	// b 0x825cfa90
	goto loc_825CFA90;
loc_825CFA88:
	// stw r26,19932(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19932, ctx.r26.u32);
	// stw r24,19924(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19924, ctx.r24.u32);
loc_825CFA90:
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x825cfabc
	if (!ctx.cr6.gt) goto loc_825CFABC;
	// addi r11,r28,4961
	ctx.r11.s64 = ctx.r28.s64 + 4961;
	// addi r10,r28,4960
	ctx.r10.s64 = ctx.r28.s64 + 4960;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	// lwzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r31.u32);
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// bne cr6,0x825cfabc
	if (!ctx.cr6.eq) goto loc_825CFABC;
	// addi r28,r28,-1
	ctx.r28.s64 = ctx.r28.s64 + -1;
loc_825CFABC:
	// stw r24,3420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3420, ctx.r24.u32);
loc_825CFAC0:
	// lwz r7,15556(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15556);
	// rlwinm r11,r7,16,0,15
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 16) & 0xFFFF0000;
	// srawi r6,r11,28
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xFFFFFFF) != 0);
	ctx.r6.s64 = ctx.r11.s32 >> 28;
	// cmpwi cr6,r6,2
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 2, ctx.xer);
	// bne cr6,0x825cfb00
	if (!ctx.cr6.eq) goto loc_825CFB00;
	// lwz r11,19820(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19820);
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// srawi r9,r7,16
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r9.s64 = ctx.r7.s32 >> 16;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// ble cr6,0x825cfb00
	if (!ctx.cr6.gt) goto loc_825CFB00;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x825cfafc
	if (!ctx.cr6.gt) goto loc_825CFAFC;
	// addi r28,r28,-1
	ctx.r28.s64 = ctx.r28.s64 + -1;
loc_825CFAFC:
	// stw r26,19932(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19932, ctx.r26.u32);
loc_825CFB00:
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x825cfb7c
	if (!ctx.cr6.gt) goto loc_825CFB7C;
	// addi r11,r28,4960
	ctx.r11.s64 = ctx.r28.s64 + 4960;
	// lwz r8,19832(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19832);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r31
	ctx.r9.u64 = ctx.r11.u64 + ctx.r31.u64;
loc_825CFB18:
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r10,r11,r5
	ctx.r10.u64 = ctx.r11.u64 + ctx.r5.u64;
	// cmpw cr6,r10,r8
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, ctx.xer);
	// ble cr6,0x825cfb7c
	if (!ctx.cr6.gt) goto loc_825CFB7C;
	// lwz r29,19816(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19816);
	// cmpwi cr6,r29,7
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 7, ctx.xer);
	// ble cr6,0x825cfb4c
	if (!ctx.cr6.gt) goto loc_825CFB4C;
	// add r11,r11,r4
	ctx.r11.u64 = ctx.r11.u64 + ctx.r4.u64;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// ble cr6,0x825cfb4c
	if (!ctx.cr6.gt) goto loc_825CFB4C;
	// addi r28,r28,-1
	ctx.r28.s64 = ctx.r28.s64 + -1;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// b 0x825cfb74
	goto loc_825CFB74;
loc_825CFB4C:
	// rlwinm r11,r8,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x825cfb7c
	if (!ctx.cr6.gt) goto loc_825CFB7C;
	// lwz r11,19728(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19728);
	// addi r28,r28,-1
	ctx.r28.s64 = ctx.r28.s64 + -1;
	// addi r9,r9,-4
	ctx.r9.s64 = ctx.r9.s64 + -4;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// mr r25,r24
	ctx.r25.u64 = ctx.r24.u64;
	// bgt cr6,0x825cfb74
	if (ctx.cr6.gt) goto loc_825CFB74;
	// lwz r25,15548(r31)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15548);
loc_825CFB74:
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// bgt cr6,0x825cfb18
	if (ctx.cr6.gt) goto loc_825CFB18;
loc_825CFB7C:
	// lwz r8,15548(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15548);
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// cmpw cr6,r28,r8
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r8.s32, ctx.xer);
	// addi r29,r11,-23996
	ctx.r29.s64 = ctx.r11.s64 + -23996;
	// bne cr6,0x825cfc54
	if (!ctx.cr6.eq) goto loc_825CFC54;
	// cmpw cr6,r28,r27
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r27.s32, ctx.xer);
	// bge cr6,0x825cfc54
	if (!ctx.cr6.lt) goto loc_825CFC54;
	// cmpwi cr6,r6,1
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 1, ctx.xer);
	// ble cr6,0x825cfbc0
	if (!ctx.cr6.gt) goto loc_825CFBC0;
	// cmpwi cr6,r6,2
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 2, ctx.xer);
	// bne cr6,0x825cfc54
	if (!ctx.cr6.eq) goto loc_825CFC54;
	// lwz r11,19820(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19820);
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// srawi r10,r7,16
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0xFFFF) != 0);
	ctx.r10.s64 = ctx.r7.s32 >> 16;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x825cfc54
	if (!ctx.cr6.gt) goto loc_825CFC54;
loc_825CFBC0:
	// lwz r6,19924(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19924);
	// lwz r11,19932(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19932);
	// cmpw cr6,r8,r6
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, ctx.xer);
	// subf r7,r11,r26
	ctx.r7.s64 = ctx.r26.s64 - ctx.r11.s64;
	// bge cr6,0x825cfbdc
	if (!ctx.cr6.lt) goto loc_825CFBDC;
	// lwz r10,19832(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19832);
	// b 0x825cfbe0
	goto loc_825CFBE0;
loc_825CFBDC:
	// lwz r10,19836(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19836);
loc_825CFBE0:
	// addi r9,r28,1
	ctx.r9.s64 = ctx.r28.s64 + 1;
	// addi r11,r9,4960
	ctx.r11.s64 = ctx.r9.s64 + 4960;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	// add r5,r11,r5
	ctx.r5.u64 = ctx.r11.u64 + ctx.r5.u64;
	// cmpw cr6,r5,r10
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x825cfc44
	if (!ctx.cr6.lt) goto loc_825CFC44;
	// add r11,r11,r4
	ctx.r11.u64 = ctx.r11.u64 + ctx.r4.u64;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x825cfc44
	if (!ctx.cr6.lt) goto loc_825CFC44;
	// addi r11,r29,-20
	ctx.r11.s64 = ctx.r29.s64 + -20;
	// rlwinm r10,r28,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// cmplw cr6,r7,r11
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825cfc24
	if (ctx.cr6.gt) goto loc_825CFC24;
	// cmpw cr6,r8,r6
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, ctx.xer);
	// bge cr6,0x825cfc54
	if (!ctx.cr6.lt) goto loc_825CFC54;
loc_825CFC24:
	// addi r11,r29,-20
	ctx.r11.s64 = ctx.r29.s64 + -20;
	// rlwinm r10,r27,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r28,r9
	ctx.r28.u64 = ctx.r9.u64;
	// lwzx r11,r10,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// cmplw cr6,r7,r11
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825cfc44
	if (!ctx.cr6.gt) goto loc_825CFC44;
	// addi r11,r26,-500
	ctx.r11.s64 = ctx.r26.s64 + -500;
	// stw r11,19932(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19932, ctx.r11.u32);
loc_825CFC44:
	// cmpw cr6,r8,r6
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r6.s32, ctx.xer);
	// bge cr6,0x825cfc54
	if (!ctx.cr6.lt) goto loc_825CFC54;
	// addi r11,r6,-1
	ctx.r11.s64 = ctx.r6.s64 + -1;
	// stw r11,19924(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19924, ctx.r11.u32);
loc_825CFC54:
	// cmpdi cr6,r3,0
	ctx.cr6.compare<int64_t>(ctx.r3.s64, 0, ctx.xer);
	// ble cr6,0x825cfc9c
	if (!ctx.cr6.gt) goto loc_825CFC9C;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// ble cr6,0x825cfc9c
	if (!ctx.cr6.gt) goto loc_825CFC9C;
	// cmpw cr6,r8,r27
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r27.s32, ctx.xer);
	// bgt cr6,0x825cfc9c
	if (ctx.cr6.gt) goto loc_825CFC9C;
	// addi r11,r8,2483
	ctx.r11.s64 = ctx.r8.s64 + 2483;
	// extsw r10,r30
	ctx.r10.s64 = ctx.r30.s32;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// ldx r9,r11,r31
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r11.u32 + ctx.r31.u32);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stdx r10,r11,r31
	PPC_STORE_U64(ctx.r11.u32 + ctx.r31.u32, ctx.r10.u64);
	// lwz r11,15548(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15548);
	// addi r11,r11,4976
	ctx.r11.s64 = ctx.r11.s64 + 4976;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stwx r10,r11,r31
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, ctx.r10.u32);
loc_825CFC9C:
	// lwz r11,15548(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15548);
	// cmpw cr6,r28,r11
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r11.s32, ctx.xer);
	// bne cr6,0x825cfcc4
	if (!ctx.cr6.eq) goto loc_825CFCC4;
	// lwz r10,19728(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19728);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bgt cr6,0x825cfcc4
	if (ctx.cr6.gt) goto loc_825CFCC4;
	// lwz r11,19816(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19816);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,19816(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19816, ctx.r11.u32);
	// b 0x825cfd64
	goto loc_825CFD64;
loc_825CFCC4:
	// rlwinm r30,r28,2,0,29
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r30,r29
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r29.u32);
	// clrlwi r11,r11,31
	ctx.r11.u64 = ctx.r11.u32 & 0x1;
	// stw r11,15508(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15508, ctx.r11.u32);
	// lwzx r11,r30,r29
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r29.u32);
	// rlwinm r11,r11,31,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x1;
	// stw r11,15512(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15512, ctx.r11.u32);
	// lwzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r29.u32);
	// lwzx r11,r30,r29
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r29.u32);
	// xor r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r11.u64;
	// rlwinm r10,r10,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825cfd08
	if (ctx.cr6.eq) goto loc_825CFD08;
	// rlwinm r3,r11,30,31,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x1;
	// lwz r4,3924(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// bl 0x825f8c30
	ctx.lr = 0x825CFD08;
	sub_825F8C30(ctx, base);
loc_825CFD08:
	// lwz r11,15548(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15548);
	// cmpw cr6,r28,r11
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x825cfd54
	if (!ctx.cr6.lt) goto loc_825CFD54;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// stw r26,19932(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19932, ctx.r26.u32);
	// ble cr6,0x825cfd40
	if (!ctx.cr6.gt) goto loc_825CFD40;
	// addi r10,r29,-20
	ctx.r10.s64 = ctx.r29.s64 + -20;
	// addi r9,r29,-20
	ctx.r9.s64 = ctx.r29.s64 + -20;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r30,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r9.u32);
	// lwzx r10,r8,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r10.u32);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// add r10,r10,r26
	ctx.r10.u64 = ctx.r10.u64 + ctx.r26.u64;
	// stw r10,19932(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19932, ctx.r10.u32);
loc_825CFD40:
	// lwz r10,19924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19924);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x825cfd54
	if (!ctx.cr6.lt) goto loc_825CFD54;
	// stw r11,19924(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19924, ctx.r11.u32);
loc_825CFD54:
	// stw r28,15548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15548, ctx.r28.u32);
	// stw r28,3644(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3644, ctx.r28.u32);
	// stw r25,19728(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19728, ctx.r25.u32);
	// stw r24,19816(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19816, ctx.r24.u32);
loc_825CFD64:
	// std r24,19744(r31)
	PPC_STORE_U64(ctx.r31.u32 + 19744, ctx.r24.u64);
	// std r24,19752(r31)
	PPC_STORE_U64(ctx.r31.u32 + 19752, ctx.r24.u64);
loc_825CFD6C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc770
	ctx.lr = 0x825CFD74;
	sub_825CC770(ctx, base);
loc_825CFD74:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825CFD7C"))) PPC_WEAK_FUNC(sub_825CFD7C);
PPC_FUNC_IMPL(__imp__sub_825CFD7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CFD80"))) PPC_WEAK_FUNC(sub_825CFD80);
PPC_FUNC_IMPL(__imp__sub_825CFD80) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825cfdac
	if (!ctx.cr6.eq) goto loc_825CFDAC;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_825CFDAC:
	// lwz r10,21332(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21332);
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825cfdd0
	if (!ctx.cr6.eq) goto loc_825CFDD0;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_825CFDD0:
	// lwz r9,21328(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21328);
	// lfd f0,20856(r11)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + 20856);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// addi r10,r1,104
	ctx.r10.s64 = ctx.r1.s64 + 104;
	// fctiwz f0,f0
	ctx.f0.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// lwz r9,3908(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 3908);
	// stw r9,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r9.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// lwz r9,21228(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21228);
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// lwz r9,21160(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21160);
	// lwz r11,3660(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 3660);
	// stw r9,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r9.u32);
	// stfiwx f0,0,r10
	PPC_STORE_U32(ctx.r10.u32, ctx.f0.u32);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r11.u32);
	// bl 0x825e9150
	ctx.lr = 0x825CFE1C;
	sub_825E9150(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CFE2C"))) PPC_WEAK_FUNC(sub_825CFE2C);
PPC_FUNC_IMPL(__imp__sub_825CFE2C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CFE30"))) PPC_WEAK_FUNC(sub_825CFE30);
PPC_FUNC_IMPL(__imp__sub_825CFE30) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825cfe64
	if (!ctx.cr6.eq) goto loc_825CFE64;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825CFE64:
	// lwz r11,21332(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21332);
	// rlwinm r11,r11,0,30,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825cfe8c
	if (!ctx.cr6.eq) goto loc_825CFE8C;
	// li r3,12
	ctx.r3.s64 = 12;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825CFE8C:
	// lwz r11,21328(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21328);
	// addi r3,r1,92
	ctx.r3.s64 = ctx.r1.s64 + 92;
	// li r5,92
	ctx.r5.s64 = 92;
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r31.u32);
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r11.u32);
	// bl 0x8239cb70
	ctx.lr = 0x825CFEAC;
	sub_8239CB70(ctx, base);
	// lwz r11,21184(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21184);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,21352(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21352);
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// xori r11,r11,1
	ctx.r11.u64 = ctx.r11.u64 ^ 1;
	// lwz r10,21356(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21356);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// bl 0x825e9150
	ctx.lr = 0x825CFED8;
	sub_825E9150(ctx, base);
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CFEEC"))) PPC_WEAK_FUNC(sub_825CFEEC);
PPC_FUNC_IMPL(__imp__sub_825CFEEC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CFEF0"))) PPC_WEAK_FUNC(sub_825CFEF0);
PPC_FUNC_IMPL(__imp__sub_825CFEF0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825cff1c
	if (!ctx.cr6.eq) goto loc_825CFF1C;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_825CFF1C:
	// lwz r10,21332(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21332);
	// rlwinm r10,r10,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825cff40
	if (!ctx.cr6.eq) goto loc_825CFF40;
	// li r3,12
	ctx.r3.s64 = 12;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_825CFF40:
	// li r8,2
	ctx.r8.s64 = 2;
	// lwz r10,21184(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21184);
	// lwz r9,14772(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14772);
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// stw r4,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r4.u32);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// stw r8,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r8.u32);
	// lwz r8,21328(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21328);
	// xori r10,r10,1
	ctx.r10.u64 = ctx.r10.u64 ^ 1;
	// stw r8,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r8.u32);
	// lwz r8,21208(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21208);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// lwz r8,156(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 156);
	// stw r8,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r8.u32);
	// lwz r8,160(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 160);
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// bgt cr6,0x825cff98
	if (ctx.cr6.gt) goto loc_825CFF98;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825CFF98:
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,21364(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21364);
	// stw r10,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r10.u32);
	// lwz r10,21368(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21368);
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
	// lwz r10,21372(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21372);
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// lwz r10,21376(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21376);
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// lwz r10,21212(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21212);
	// stw r10,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r10.u32);
	// lwz r10,21220(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21220);
	// stw r10,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r10.u32);
	// lwz r10,21216(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21216);
	// lwz r11,21224(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21224);
	// stw r10,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r10.u32);
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r11.u32);
	// bl 0x825e9150
	ctx.lr = 0x825CFFE4;
	sub_825E9150(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825CFFF4"))) PPC_WEAK_FUNC(sub_825CFFF4);
PPC_FUNC_IMPL(__imp__sub_825CFFF4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825CFFF8"))) PPC_WEAK_FUNC(sub_825CFFF8);
PPC_FUNC_IMPL(__imp__sub_825CFFF8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825d0024
	if (!ctx.cr6.eq) goto loc_825D0024;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_825D0024:
	// lwz r10,21332(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21332);
	// rlwinm r10,r10,0,28,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x8;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825d0048
	if (!ctx.cr6.eq) goto loc_825D0048;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_825D0048:
	// lwz r9,21328(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21328);
	// lwz r10,19976(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 19976);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// li r9,3
	ctx.r9.s64 = 3;
	// stw r9,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r9.u32);
	// beq cr6,0x825d00a0
	if (ctx.cr6.eq) goto loc_825D00A0;
	// lwz r10,19980(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 19980);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825d00a0
	if (ctx.cr6.eq) goto loc_825D00A0;
	// lwz r10,21076(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21076);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// lwz r8,21072(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21072);
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r9,r9,-24200
	ctx.r9.s64 = ctx.r9.s64 + -24200;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// b 0x825d00a4
	goto loc_825D00A4;
loc_825D00A0:
	// lwz r10,284(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 284);
loc_825D00A4:
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lwz r10,21160(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21160);
	// stw r4,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r4.u32);
	// stw r5,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r5.u32);
	// stw r10,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r10.u32);
	// lwz r10,20836(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20836);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// lwz r10,20840(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20840);
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// lwz r10,21164(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21164);
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// lwz r10,20976(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20976);
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// lwz r10,3444(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 3444);
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// lwz r10,3448(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 3448);
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// lwz r10,20868(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20868);
	// stw r10,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r10.u32);
	// lwz r10,20872(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20872);
	// lwz r11,21436(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21436);
	// stw r10,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r10.u32);
	// stw r11,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r11.u32);
	// bl 0x825e9150
	ctx.lr = 0x825D0108;
	sub_825E9150(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D0118"))) PPC_WEAK_FUNC(sub_825D0118);
PPC_FUNC_IMPL(__imp__sub_825D0118) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825d0144
	if (!ctx.cr6.eq) goto loc_825D0144;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_825D0144:
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r9,21332(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21332);
	// slw r10,r10,r4
	ctx.r10.u64 = ctx.r4.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r4.u8 & 0x3F));
	// and r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 & ctx.r10.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825d0170
	if (!ctx.cr6.eq) goto loc_825D0170;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_825D0170:
	// lwz r10,21328(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21328);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// stw r4,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r4.u32);
	// stw r6,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r6.u32);
	// stw r5,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r5.u32);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// stw r7,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r7.u32);
	// bl 0x825e9150
	ctx.lr = 0x825D0194;
	sub_825E9150(ctx, base);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D01A4"))) PPC_WEAK_FUNC(sub_825D01A4);
PPC_FUNC_IMPL(__imp__sub_825D01A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D01A8"))) PPC_WEAK_FUNC(sub_825D01A8);
PPC_FUNC_IMPL(__imp__sub_825D01A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825D01B0;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// lwz r24,23968(r27)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r27.u32 + 23968);
	// ble cr6,0x825d0214
	if (!ctx.cr6.gt) goto loc_825D0214;
	// lis r11,21845
	ctx.r11.s64 = 1431633920;
	// addi r31,r24,17696
	ctx.r31.s64 = ctx.r24.s64 + 17696;
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// li r26,-1
	ctx.r26.s64 = -1;
	// ori r28,r11,21845
	ctx.r28.u64 = ctx.r11.u64 | 21845;
loc_825D01DC:
	// addi r29,r31,264
	ctx.r29.s64 = ctx.r31.s64 + 264;
	// lwz r3,15204(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 15204);
	// li r5,-1
	ctx.r5.s64 = -1;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82626170
	ctx.lr = 0x825D01F0;
	sub_82626170(ctx, base);
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// stw r26,612(r11)
	PPC_STORE_U32(ctx.r11.u32 + 612, ctx.r26.u32);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r31,r31,4
	ctx.r31.s64 = ctx.r31.s64 + 4;
	// stw r28,616(r11)
	PPC_STORE_U32(ctx.r11.u32 + 616, ctx.r28.u32);
	// bne cr6,0x825d01dc
	if (!ctx.cr6.eq) goto loc_825D01DC;
loc_825D0214:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r25,18216(r24)
	PPC_STORE_U32(ctx.r24.u32 + 18216, ctx.r25.u32);
	// stw r25,17956(r24)
	PPC_STORE_U32(ctx.r24.u32 + 17956, ctx.r25.u32);
	// stw r11,17952(r24)
	PPC_STORE_U32(ctx.r24.u32 + 17952, ctx.r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825D022C"))) PPC_WEAK_FUNC(sub_825D022C);
PPC_FUNC_IMPL(__imp__sub_825D022C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D0230"))) PPC_WEAK_FUNC(sub_825D0230);
PPC_FUNC_IMPL(__imp__sub_825D0230) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba04
	ctx.lr = 0x825D0238;
	sub_8239BA04(ctx, base);
	// stfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.f30.u64);
	// stfd f31,-88(r1)
	PPC_STORE_U64(ctx.r1.u32 + -88, ctx.f31.u64);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// fmr f31,f1
	ctx.f31.f64 = ctx.f1.f64;
	// lis r3,1
	ctx.r3.s64 = 65536;
	// fmr f30,f2
	ctx.f30.f64 = ctx.f2.f64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// li r28,0
	ctx.r28.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r3,r3,33760
	ctx.r3.u64 = ctx.r3.u64 | 33760;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// mr r26,r8
	ctx.r26.u64 = ctx.r8.u64;
	// mr r25,r9
	ctx.r25.u64 = ctx.r9.u64;
	// mr r23,r10
	ctx.r23.u64 = ctx.r10.u64;
	// mr r24,r28
	ctx.r24.u64 = ctx.r28.u64;
	// bl 0x825edb18
	ctx.lr = 0x825D027C;
	sub_825EDB18(ctx, base);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825d02a0
	if (!ctx.cr6.eq) goto loc_825D02A0;
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r28,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r28.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// lfd f31,-88(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x8239ba54
	// ERROR 8239BA54
	return;
loc_825D02A0:
	// lis r5,1
	ctx.r5.s64 = 65536;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r5,r5,33760
	ctx.r5.u64 = ctx.r5.u64 | 33760;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8239ca70
	ctx.lr = 0x825D02B4;
	sub_8239CA70(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,56
	ctx.r3.s64 = 56;
	// stw r28,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r28.u32);
	// stw r28,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r28.u32);
	// stw r31,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r31.u32);
	// bl 0x825edb18
	ctx.lr = 0x825D02CC;
	sub_825EDB18(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,80(r31)
	PPC_STORE_U32(ctx.r31.u32 + 80, ctx.r3.u32);
	// bne cr6,0x825d02ec
	if (!ctx.cr6.eq) goto loc_825D02EC;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// lfd f31,-88(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x8239ba54
	// ERROR 8239BA54
	return;
loc_825D02EC:
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x825d4d18
	ctx.lr = 0x825D02F4;
	sub_825D4D18(ctx, base);
	// lis r11,22358
	ctx.r11.s64 = 1465253888;
	// lwz r9,284(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// ori r11,r11,17201
	ctx.r11.u64 = ctx.r11.u64 | 17201;
	// lwz r10,276(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x825d039c
	if (!ctx.cr6.eq) goto loc_825D039C;
	// lis r30,22349
	ctx.r30.s64 = 1464664064;
	// li r24,1
	ctx.r24.s64 = 1;
	// ori r30,r30,22081
	ctx.r30.u64 = ctx.r30.u64 | 22081;
loc_825D0318:
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r11,80(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// stw r10,19704(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19704, ctx.r10.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r9,19708(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19708, ctx.r9.u32);
	// mr r10,r23
	ctx.r10.u64 = ctx.r23.u64;
	// mr r9,r25
	ctx.r9.u64 = ctx.r25.u64;
	// fmr f2,f30
	ctx.fpscr.disableFlushMode();
	ctx.f2.f64 = ctx.f30.f64;
	// stw r8,15236(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15236, ctx.r8.u32);
	// mr r8,r26
	ctx.r8.u64 = ctx.r26.u64;
	// stw r31,40(r11)
	PPC_STORE_U32(ctx.r11.u32 + 40, ctx.r31.u32);
	// fmr f1,f31
	ctx.f1.f64 = ctx.f31.f64;
	// stw r26,21412(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21412, ctx.r26.u32);
	// stw r25,21416(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21416, ctx.r25.u32);
	// stw r28,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r28.u32);
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r28.u32);
	// bl 0x825cf298
	ctx.lr = 0x825D0364;
	sub_825CF298(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r24,0
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// beq cr6,0x825d0378
	if (ctx.cr6.eq) goto loc_825D0378;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,21480(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21480, ctx.r11.u32);
loc_825D0378:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825d0388
	if (!ctx.cr6.eq) goto loc_825D0388;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cf3f8
	ctx.lr = 0x825D0388;
	sub_825CF3F8(ctx, base);
loc_825D0388:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// lfd f31,-88(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x8239ba54
	// ERROR 8239BA54
	return;
loc_825D039C:
	// lis r11,22349
	ctx.r11.s64 = 1464664064;
	// ori r11,r11,22067
	ctx.r11.u64 = ctx.r11.u64 | 22067;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825d03dc
	if (ctx.cr6.eq) goto loc_825D03DC;
	// lis r11,30573
	ctx.r11.s64 = 2003632128;
	// ori r11,r11,30259
	ctx.r11.u64 = ctx.r11.u64 | 30259;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825d03dc
	if (ctx.cr6.eq) goto loc_825D03DC;
	// lis r11,22349
	ctx.r11.s64 = 1464664064;
	// ori r11,r11,22096
	ctx.r11.u64 = ctx.r11.u64 | 22096;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825d03dc
	if (ctx.cr6.eq) goto loc_825D03DC;
	// lis r11,30573
	ctx.r11.s64 = 2003632128;
	// ori r11,r11,30320
	ctx.r11.u64 = ctx.r11.u64 | 30320;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x825d0318
	if (!ctx.cr6.eq) goto loc_825D0318;
loc_825D03DC:
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825d0318
	if (ctx.cr6.eq) goto loc_825D0318;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x825d0318
	if (ctx.cr6.eq) goto loc_825D0318;
	// lbz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// stw r11,3924(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3924, ctx.r11.u32);
	// stw r11,3936(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3936, ctx.r11.u32);
	// lbz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lwz r8,3924(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// rlwinm r11,r11,28,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 28) & 0x1;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// stw r11,3940(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3940, ctx.r11.u32);
	// stw r11,15300(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15300, ctx.r11.u32);
	// beq cr6,0x825d0318
	if (ctx.cr6.eq) goto loc_825D0318;
	// li r3,6
	ctx.r3.s64 = 6;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// lfd f30,-96(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// lfd f31,-88(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -88);
	// b 0x8239ba54
	// ERROR 8239BA54
	return;
}

__attribute__((alias("__imp__sub_825D042C"))) PPC_WEAK_FUNC(sub_825D042C);
PPC_FUNC_IMPL(__imp__sub_825D042C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D0430"))) PPC_WEAK_FUNC(sub_825D0430);
PPC_FUNC_IMPL(__imp__sub_825D0430) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825d0458
	if (!ctx.cr6.eq) goto loc_825D0458;
	// li r3,7
	ctx.r3.s64 = 7;
	// b 0x825d04ac
	goto loc_825D04AC;
loc_825D0458:
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d046c
	if (ctx.cr6.eq) goto loc_825D046C;
	// li r3,13
	ctx.r3.s64 = 13;
	// b 0x825d04ac
	goto loc_825D04AC;
loc_825D046C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cf718
	ctx.lr = 0x825D0474;
	sub_825CF718(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ee1c0
	ctx.lr = 0x825D047C;
	sub_825EE1C0(ctx, base);
	// lis r5,1
	ctx.r5.s64 = 65536;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// ori r5,r5,33760
	ctx.r5.u64 = ctx.r5.u64 | 33760;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8239ca70
	ctx.lr = 0x825D0494;
	sub_8239CA70(ctx, base);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825d04a8
	if (!ctx.cr6.eq) goto loc_825D04A8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825edb28
	ctx.lr = 0x825D04A8;
	sub_825EDB28(ctx, base);
loc_825D04A8:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
loc_825D04AC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D04C4"))) PPC_WEAK_FUNC(sub_825D04C4);
PPC_FUNC_IMPL(__imp__sub_825D04C4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D04C8"))) PPC_WEAK_FUNC(sub_825D04C8);
PPC_FUNC_IMPL(__imp__sub_825D04C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9f8
	ctx.lr = 0x825D04D0;
	sub_8239B9F8(ctx, base);
	// stwu r1,-512(r1)
	ea = -512 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r21,0
	ctx.r21.s64 = 0;
	// li r28,1
	ctx.r28.s64 = 1;
	// mr r22,r3
	ctx.r22.u64 = ctx.r3.u64;
	// cmplwi cr6,r22,0
	ctx.cr6.compare<uint32_t>(ctx.r22.u32, 0, ctx.xer);
	// stw r21,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r21.u32);
	// stw r28,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r28.u32);
	// bne cr6,0x825d04fc
	if (!ctx.cr6.eq) goto loc_825D04FC;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825D04FC:
	// lwz r31,0(r22)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d0518
	if (ctx.cr6.eq) goto loc_825D0518;
	// li r3,13
	ctx.r3.s64 = 13;
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825D0518:
	// lwz r20,3340(r31)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r20
	ctx.r3.u64 = ctx.r20.u64;
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// bl 0x8248c788
	ctx.lr = 0x825D053C;
	sub_8248C788(ctx, base);
	// lwz r11,120(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d065c
	if (ctx.cr6.eq) goto loc_825D065C;
loc_825D0548:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825d05a0
	if (!ctx.cr6.eq) goto loc_825D05A0;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r29,r5,r30
	ctx.r29.u64 = ctx.r5.u64 + ctx.r30.u64;
	// cmplwi cr6,r29,256
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 256, ctx.xer);
	// bge cr6,0x825d05a4
	if (!ctx.cr6.lt) goto loc_825D05A4;
	// addi r11,r1,144
	ctx.r11.s64 = ctx.r1.s64 + 144;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// add r3,r30,r11
	ctx.r3.u64 = ctx.r30.u64 + ctx.r11.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825D0574;
	sub_8239CB70(ctx, base);
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwz r3,3340(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r30,r29
	ctx.r30.u64 = ctx.r29.u64;
	// bl 0x8248c788
	ctx.lr = 0x825D0594;
	sub_8248C788(ctx, base);
	// lwz r11,120(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d0548
	if (!ctx.cr6.eq) goto loc_825D0548;
loc_825D05A0:
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
loc_825D05A4:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825d0630
	if (ctx.cr6.eq) goto loc_825D0630;
	// lwz r11,23248(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23248);
	// add r29,r5,r30
	ctx.r29.u64 = ctx.r5.u64 + ctx.r30.u64;
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x825d05f8
	if (!ctx.cr6.gt) goto loc_825D05F8;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d05cc
	if (ctx.cr6.eq) goto loc_825D05CC;
	// lwz r3,23252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23252);
	// bl 0x825edb28
	ctx.lr = 0x825D05CC;
	sub_825EDB28(ctx, base);
loc_825D05CC:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x825edb18
	ctx.lr = 0x825D05D8;
	sub_825EDB18(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,23252(r31)
	PPC_STORE_U32(ctx.r31.u32 + 23252, ctx.r3.u32);
	// bne cr6,0x825d05f4
	if (!ctx.cr6.eq) goto loc_825D05F4;
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r21,23248(r31)
	PPC_STORE_U32(ctx.r31.u32 + 23248, ctx.r21.u32);
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825D05F4:
	// stw r29,23248(r31)
	PPC_STORE_U32(ctx.r31.u32 + 23248, ctx.r29.u32);
loc_825D05F8:
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r3,23252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23252);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// bl 0x8239cb70
	ctx.lr = 0x825D0608;
	sub_8239CB70(ctx, base);
	// lwz r11,23252(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23252);
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r3,r11,r30
	ctx.r3.u64 = ctx.r11.u64 + ctx.r30.u64;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x8239cb70
	ctx.lr = 0x825D061C;
	sub_8239CB70(ctx, base);
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r5,r11,r30
	ctx.r5.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
	// lwz r11,23252(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23252);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r11.u32);
loc_825D0630:
	// lwz r11,120(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d065c
	if (ctx.cr6.eq) goto loc_825D065C;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x825d0650
	if (ctx.cr6.eq) goto loc_825D0650;
	// lwz r11,116(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825d065c
	if (!ctx.cr6.eq) goto loc_825D065C;
loc_825D0650:
	// li r3,11
	ctx.r3.s64 = 11;
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825D065C:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// stw r21,21396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21396, ctx.r21.u32);
	// stw r21,21400(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21400, ctx.r21.u32);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// stw r21,21392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21392, ctx.r21.u32);
	// stw r21,21408(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21408, ctx.r21.u32);
	// bne cr6,0x825d0720
	if (!ctx.cr6.eq) goto loc_825D0720;
	// lwz r11,116(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lbz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// rlwinm r10,r11,0,30,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825d0698
	if (ctx.cr6.eq) goto loc_825D0698;
	// stw r28,21408(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21408, ctx.r28.u32);
	// stw r21,14772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14772, ctx.r21.u32);
	// b 0x825d06a0
	goto loc_825D06A0;
loc_825D0698:
	// stw r21,21408(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21408, ctx.r21.u32);
	// stw r28,14772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14772, ctx.r28.u32);
loc_825D06A0:
	// rlwinm r10,r11,30,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 30) & 0x1;
	// rlwinm r11,r11,0,26,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r10,21392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21392, ctx.r10.u32);
	// beq cr6,0x825d06bc
	if (ctx.cr6.eq) goto loc_825D06BC;
	// stw r28,21404(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21404, ctx.r28.u32);
	// b 0x825d06c0
	goto loc_825D06C0;
loc_825D06BC:
	// stw r21,21404(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21404, ctx.r21.u32);
loc_825D06C0:
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r5,r11,-1
	ctx.r5.s64 = ctx.r11.s64 + -1;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
	// bne cr6,0x825d0704
	if (!ctx.cr6.eq) goto loc_825D0704;
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// lwz r3,3340(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8248c788
	ctx.lr = 0x825D06F0;
	sub_8248C788(ctx, base);
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// addi r11,r11,-7
	ctx.r11.s64 = ctx.r11.s64 + -7;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// b 0x825d0724
	goto loc_825D0724;
loc_825D0704:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r11,r11,-7
	ctx.r11.s64 = ctx.r11.s64 + -7;
	// addi r4,r10,1
	ctx.r4.s64 = ctx.r10.s64 + 1;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// b 0x825d072c
	goto loc_825D072C;
loc_825D0720:
	// li r7,0
	ctx.r7.s64 = 0;
loc_825D0724:
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
loc_825D072C:
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// bl 0x825d5100
	ctx.lr = 0x825D0738;
	sub_825D5100(ctx, base);
	// lwz r11,80(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r10,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r10.u32);
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// lwz r3,0(r22)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	// bl 0x825cc938
	ctx.lr = 0x825D0754;
	sub_825CC938(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825d0920
	if (!ctx.cr6.eq) goto loc_825D0920;
	// lwz r3,0(r22)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825d0918
	if (ctx.cr6.eq) goto loc_825D0918;
	// lwz r11,15300(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15300);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d0784
	if (!ctx.cr6.eq) goto loc_825D0784;
	// lwz r11,15368(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15368);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d0918
	if (ctx.cr6.eq) goto loc_825D0918;
loc_825D0784:
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// mr r25,r21
	ctx.r25.u64 = ctx.r21.u64;
	// beq cr6,0x825d0798
	if (ctx.cr6.eq) goto loc_825D0798;
	// lwz r25,156(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// bne cr6,0x825d07a0
	if (!ctx.cr6.eq) goto loc_825D07A0;
loc_825D0798:
	// mr r26,r21
	ctx.r26.u64 = ctx.r21.u64;
	// b 0x825d07a4
	goto loc_825D07A4;
loc_825D07A0:
	// lwz r26,160(r3)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
loc_825D07A4:
	// lwz r24,88(r3)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 88);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// lwz r23,92(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 92);
	// lwz r29,19704(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19704);
	// lwz r28,19708(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19708);
	// lwz r27,15236(r3)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15236);
	// beq cr6,0x825d07e0
	if (ctx.cr6.eq) goto loc_825D07E0;
	// lwz r11,16(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d07d4
	if (ctx.cr6.eq) goto loc_825D07D4;
	// li r30,13
	ctx.r30.s64 = 13;
	// b 0x825d07e4
	goto loc_825D07E4;
loc_825D07D4:
	// lwz r30,3656(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3656);
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bgt cr6,0x825d07e4
	if (ctx.cr6.gt) goto loc_825D07E4;
loc_825D07E0:
	// li r30,30
	ctx.r30.s64 = 30;
loc_825D07E4:
	// lwz r11,15368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15368);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d0844
	if (ctx.cr6.eq) goto loc_825D0844;
	// bl 0x825d0430
	ctx.lr = 0x825D07F4;
	sub_825D0430(ctx, base);
	// extsw r7,r30
	ctx.r7.s64 = ctx.r30.s32;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// stw r27,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r27.u32);
	// lis r5,22358
	ctx.r5.s64 = 1465253888;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r28.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r29.u32);
	// mr r9,r26
	ctx.r9.u64 = ctx.r26.u64;
	// stw r21,0(r22)
	PPC_STORE_U32(ctx.r22.u32 + 0, ctx.r21.u32);
	// std r7,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r7.u64);
	// mr r8,r25
	ctx.r8.u64 = ctx.r25.u64;
	// ori r5,r5,20530
	ctx.r5.u64 = ctx.r5.u64 | 20530;
	// lfs f2,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2480);
	ctx.f2.f64 = double(temp.f32);
	// mr r4,r20
	ctx.r4.u64 = ctx.r20.u64;
	// mr r3,r22
	ctx.r3.u64 = ctx.r22.u64;
	// lfd f0,128(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// bl 0x825d0230
	ctx.lr = 0x825D0840;
	sub_825D0230(ctx, base);
	// b 0x825d0894
	goto loc_825D0894;
loc_825D0844:
	// bl 0x825d0430
	ctx.lr = 0x825D0848;
	sub_825D0430(ctx, base);
	// extsw r7,r30
	ctx.r7.s64 = ctx.r30.s32;
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// stw r27,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r27.u32);
	// lis r5,22349
	ctx.r5.s64 = 1464664064;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r28.u32);
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r29.u32);
	// mr r9,r26
	ctx.r9.u64 = ctx.r26.u64;
	// stw r21,0(r22)
	PPC_STORE_U32(ctx.r22.u32 + 0, ctx.r21.u32);
	// std r7,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r7.u64);
	// mr r8,r25
	ctx.r8.u64 = ctx.r25.u64;
	// ori r5,r5,22067
	ctx.r5.u64 = ctx.r5.u64 | 22067;
	// lfs f2,2480(r11)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 2480);
	ctx.f2.f64 = double(temp.f32);
	// mr r4,r20
	ctx.r4.u64 = ctx.r20.u64;
	// mr r3,r22
	ctx.r3.u64 = ctx.r22.u64;
	// lfd f0,128(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f1,f0
	ctx.f1.f64 = double(float(ctx.f0.f64));
	// bl 0x825d0230
	ctx.lr = 0x825D0894;
	sub_825D0230(ctx, base);
loc_825D0894:
	// lwz r11,0(r22)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d0924
	if (!ctx.cr6.eq) goto loc_825D0924;
	// stw r24,15308(r11)
	PPC_STORE_U32(ctx.r11.u32 + 15308, ctx.r24.u32);
	// lwz r10,0(r22)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	// stw r23,15312(r10)
	PPC_STORE_U32(ctx.r10.u32 + 15312, ctx.r23.u32);
	// lwz r10,15472(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 15472);
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r10,7
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 7, ctx.xer);
	// bne cr6,0x825d08e8
	if (!ctx.cr6.eq) goto loc_825D08E8;
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r5,r10,-1
	ctx.r5.s64 = ctx.r10.s64 + -1;
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
	// lwz r10,15472(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 15472);
	// lwz r3,80(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 80);
	// addi r11,r10,-7
	ctx.r11.s64 = ctx.r10.s64 + -7;
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// addi r4,r10,1
	ctx.r4.s64 = ctx.r10.s64 + 1;
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// b 0x825d08f8
	goto loc_825D08F8;
loc_825D08E8:
	// lwz r3,80(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + 80);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
loc_825D08F8:
	// bl 0x825d5100
	ctx.lr = 0x825D08FC;
	sub_825D5100(ctx, base);
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r3,0(r22)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x825cc938
	ctx.lr = 0x825D090C;
	sub_825CC938(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825D0918:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cf3f8
	ctx.lr = 0x825D0920;
	sub_825CF3F8(ctx, base);
loc_825D0920:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
loc_825D0924:
	// addi r1,r1,512
	ctx.r1.s64 = ctx.r1.s64 + 512;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
}

__attribute__((alias("__imp__sub_825D092C"))) PPC_WEAK_FUNC(sub_825D092C);
PPC_FUNC_IMPL(__imp__sub_825D092C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D0930"))) PPC_WEAK_FUNC(sub_825D0930);
PPC_FUNC_IMPL(__imp__sub_825D0930) {
	PPC_FUNC_PROLOGUE();
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x825d0940
	if (!ctx.cr6.eq) goto loc_825D0940;
	// li r3,7
	ctx.r3.s64 = 7;
	// blr 
	return;
loc_825D0940:
	// b 0x825ce978
	sub_825CE978(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825D0944"))) PPC_WEAK_FUNC(sub_825D0944);
PPC_FUNC_IMPL(__imp__sub_825D0944) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D0948"))) PPC_WEAK_FUNC(sub_825D0948);
PPC_FUNC_IMPL(__imp__sub_825D0948) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825D0950;
	sub_8239BA08(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// li r24,0
	ctx.r24.s64 = 0;
	// mr r26,r4
	ctx.r26.u64 = ctx.r4.u64;
	// mr r25,r5
	ctx.r25.u64 = ctx.r5.u64;
	// li r30,1
	ctx.r30.s64 = 1;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d09d8
	if (!ctx.cr6.lt) goto loc_825D09D8;
loc_825D0980:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d09d8
	if (ctx.cr6.eq) goto loc_825D09D8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d09c8
	if (!ctx.cr0.lt) goto loc_825D09C8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D09C8;
	sub_825D5398(ctx, base);
loc_825D09C8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d0980
	if (ctx.cr6.gt) goto loc_825D0980;
loc_825D09D8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d0a14
	if (!ctx.cr0.lt) goto loc_825D0A14;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0A14;
	sub_825D5398(ctx, base);
loc_825D0A14:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,21364(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21364, ctx.r28.u32);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d0a8c
	if (!ctx.cr6.lt) goto loc_825D0A8C;
loc_825D0A34:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d0a8c
	if (ctx.cr6.eq) goto loc_825D0A8C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d0a7c
	if (!ctx.cr0.lt) goto loc_825D0A7C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0A7C;
	sub_825D5398(ctx, base);
loc_825D0A7C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d0a34
	if (ctx.cr6.gt) goto loc_825D0A34;
loc_825D0A8C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d0ac8
	if (!ctx.cr0.lt) goto loc_825D0AC8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0AC8;
	sub_825D5398(ctx, base);
loc_825D0AC8:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,21368(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21368, ctx.r28.u32);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d0b40
	if (!ctx.cr6.lt) goto loc_825D0B40;
loc_825D0AE8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d0b40
	if (ctx.cr6.eq) goto loc_825D0B40;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d0b30
	if (!ctx.cr0.lt) goto loc_825D0B30;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0B30;
	sub_825D5398(ctx, base);
loc_825D0B30:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d0ae8
	if (ctx.cr6.gt) goto loc_825D0AE8;
loc_825D0B40:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d0b7c
	if (!ctx.cr0.lt) goto loc_825D0B7C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0B7C;
	sub_825D5398(ctx, base);
loc_825D0B7C:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,21372(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21372, ctx.r28.u32);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d0bf4
	if (!ctx.cr6.lt) goto loc_825D0BF4;
loc_825D0B9C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d0bf4
	if (ctx.cr6.eq) goto loc_825D0BF4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d0be4
	if (!ctx.cr0.lt) goto loc_825D0BE4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0BE4;
	sub_825D5398(ctx, base);
loc_825D0BE4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d0b9c
	if (ctx.cr6.gt) goto loc_825D0B9C;
loc_825D0BF4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d0c30
	if (!ctx.cr0.lt) goto loc_825D0C30;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0C30;
	sub_825D5398(ctx, base);
loc_825D0C30:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,21376(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21376, ctx.r28.u32);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d0ca8
	if (!ctx.cr6.lt) goto loc_825D0CA8;
loc_825D0C50:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d0ca8
	if (ctx.cr6.eq) goto loc_825D0CA8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d0c98
	if (!ctx.cr0.lt) goto loc_825D0C98;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0C98;
	sub_825D5398(ctx, base);
loc_825D0C98:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d0c50
	if (ctx.cr6.gt) goto loc_825D0C50;
loc_825D0CA8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d0ce4
	if (!ctx.cr0.lt) goto loc_825D0CE4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0CE4;
	sub_825D5398(ctx, base);
loc_825D0CE4:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,3892(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3892, ctx.r28.u32);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d0d5c
	if (!ctx.cr6.lt) goto loc_825D0D5C;
loc_825D0D04:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d0d5c
	if (ctx.cr6.eq) goto loc_825D0D5C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d0d4c
	if (!ctx.cr0.lt) goto loc_825D0D4C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0D4C;
	sub_825D5398(ctx, base);
loc_825D0D4C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d0d04
	if (ctx.cr6.gt) goto loc_825D0D04;
loc_825D0D5C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d0d98
	if (!ctx.cr0.lt) goto loc_825D0D98;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0D98;
	sub_825D5398(ctx, base);
loc_825D0D98:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,1792(r27)
	PPC_STORE_U32(ctx.r27.u32 + 1792, ctx.r28.u32);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d0e10
	if (!ctx.cr6.lt) goto loc_825D0E10;
loc_825D0DB8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d0e10
	if (ctx.cr6.eq) goto loc_825D0E10;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d0e00
	if (!ctx.cr0.lt) goto loc_825D0E00;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0E00;
	sub_825D5398(ctx, base);
loc_825D0E00:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d0db8
	if (ctx.cr6.gt) goto loc_825D0DB8;
loc_825D0E10:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d0e4c
	if (!ctx.cr0.lt) goto loc_825D0E4C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0E4C;
	sub_825D5398(ctx, base);
loc_825D0E4C:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// stw r28,20864(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20864, ctx.r28.u32);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825d0ec4
	if (!ctx.cr6.lt) goto loc_825D0EC4;
loc_825D0E6C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d0ec4
	if (ctx.cr6.eq) goto loc_825D0EC4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d0eb4
	if (!ctx.cr0.lt) goto loc_825D0EB4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0EB4;
	sub_825D5398(ctx, base);
loc_825D0EB4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d0e6c
	if (ctx.cr6.gt) goto loc_825D0E6C;
loc_825D0EC4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d0f00
	if (!ctx.cr0.lt) goto loc_825D0F00;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0F00;
	sub_825D5398(ctx, base);
loc_825D0F00:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,3980(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3980, ctx.r28.u32);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d0f78
	if (!ctx.cr6.lt) goto loc_825D0F78;
loc_825D0F20:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d0f78
	if (ctx.cr6.eq) goto loc_825D0F78;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d0f68
	if (!ctx.cr0.lt) goto loc_825D0F68;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0F68;
	sub_825D5398(ctx, base);
loc_825D0F68:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d0f20
	if (ctx.cr6.gt) goto loc_825D0F20;
loc_825D0F78:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d0fb4
	if (!ctx.cr0.lt) goto loc_825D0FB4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D0FB4;
	sub_825D5398(ctx, base);
loc_825D0FB4:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,436(r27)
	PPC_STORE_U32(ctx.r27.u32 + 436, ctx.r28.u32);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d102c
	if (!ctx.cr6.lt) goto loc_825D102C;
loc_825D0FD4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d102c
	if (ctx.cr6.eq) goto loc_825D102C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d101c
	if (!ctx.cr0.lt) goto loc_825D101C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D101C;
	sub_825D5398(ctx, base);
loc_825D101C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d0fd4
	if (ctx.cr6.gt) goto loc_825D0FD4;
loc_825D102C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d1068
	if (!ctx.cr0.lt) goto loc_825D1068;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1068;
	sub_825D5398(ctx, base);
loc_825D1068:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,2972(r27)
	PPC_STORE_U32(ctx.r27.u32 + 2972, ctx.r28.u32);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d10e0
	if (!ctx.cr6.lt) goto loc_825D10E0;
loc_825D1088:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d10e0
	if (ctx.cr6.eq) goto loc_825D10E0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d10d0
	if (!ctx.cr0.lt) goto loc_825D10D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D10D0;
	sub_825D5398(ctx, base);
loc_825D10D0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d1088
	if (ctx.cr6.gt) goto loc_825D1088;
loc_825D10E0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d111c
	if (!ctx.cr0.lt) goto loc_825D111C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D111C;
	sub_825D5398(ctx, base);
loc_825D111C:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// stw r30,3436(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3436, ctx.r30.u32);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// li r30,1
	ctx.r30.s64 = 1;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825d11e8
	if (ctx.cr6.eq) goto loc_825D11E8;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d119c
	if (!ctx.cr6.lt) goto loc_825D119C;
loc_825D1144:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d119c
	if (ctx.cr6.eq) goto loc_825D119C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d118c
	if (!ctx.cr0.lt) goto loc_825D118C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D118C;
	sub_825D5398(ctx, base);
loc_825D118C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d1144
	if (ctx.cr6.gt) goto loc_825D1144;
loc_825D119C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d11d8
	if (!ctx.cr0.lt) goto loc_825D11D8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D11D8;
	sub_825D5398(ctx, base);
loc_825D11D8:
	// addi r11,r30,2
	ctx.r11.s64 = ctx.r30.s64 + 2;
	// stw r30,3428(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3428, ctx.r30.u32);
	// stw r11,21524(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21524, ctx.r11.u32);
	// b 0x825d128c
	goto loc_825D128C;
loc_825D11E8:
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d1248
	if (!ctx.cr6.lt) goto loc_825D1248;
loc_825D11F0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d1248
	if (ctx.cr6.eq) goto loc_825D1248;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d1238
	if (!ctx.cr0.lt) goto loc_825D1238;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1238;
	sub_825D5398(ctx, base);
loc_825D1238:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d11f0
	if (ctx.cr6.gt) goto loc_825D11F0;
loc_825D1248:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d1284
	if (!ctx.cr0.lt) goto loc_825D1284;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1284;
	sub_825D5398(ctx, base);
loc_825D1284:
	// stw r30,3440(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3440, ctx.r30.u32);
	// stw r30,21524(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21524, ctx.r30.u32);
loc_825D128C:
	// lwz r11,21204(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21204);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d1374
	if (ctx.cr6.eq) goto loc_825D1374;
	// lwz r11,21208(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21208);
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d1374
	if (!ctx.cr6.gt) goto loc_825D1374;
loc_825D12A8:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825d1320
	if (!ctx.cr6.lt) goto loc_825D1320;
loc_825D12C4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d1320
	if (ctx.cr6.eq) goto loc_825D1320;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d1310
	if (!ctx.cr0.lt) goto loc_825D1310;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1310;
	sub_825D5398(ctx, base);
loc_825D1310:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d12c4
	if (ctx.cr6.gt) goto loc_825D12C4;
loc_825D1320:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d135c
	if (!ctx.cr0.lt) goto loc_825D135C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D135C;
	sub_825D5398(ctx, base);
loc_825D135C:
	// addi r9,r1,80
	ctx.r9.s64 = ctx.r1.s64 + 80;
	// lwz r10,21208(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21208);
	// stbx r30,r28,r9
	PPC_STORE_U8(ctx.r28.u32 + ctx.r9.u32, ctx.r30.u8);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// cmpw cr6,r28,r10
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825d12a8
	if (ctx.cr6.lt) goto loc_825D12A8;
loc_825D1374:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d13e8
	if (!ctx.cr6.lt) goto loc_825D13E8;
loc_825D1390:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d13e8
	if (ctx.cr6.eq) goto loc_825D13E8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d13d8
	if (!ctx.cr0.lt) goto loc_825D13D8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D13D8;
	sub_825D5398(ctx, base);
loc_825D13D8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d1390
	if (ctx.cr6.gt) goto loc_825D1390;
loc_825D13E8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d1424
	if (!ctx.cr0.lt) goto loc_825D1424;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1424;
	sub_825D5398(ctx, base);
loc_825D1424:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d15e4
	if (ctx.cr6.eq) goto loc_825D15E4;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,12
	ctx.r30.s64 = 12;
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,12
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 12, ctx.xer);
	// bge cr6,0x825d14a0
	if (!ctx.cr6.lt) goto loc_825D14A0;
loc_825D1448:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d14a0
	if (ctx.cr6.eq) goto loc_825D14A0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d1490
	if (!ctx.cr0.lt) goto loc_825D1490;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1490;
	sub_825D5398(ctx, base);
loc_825D1490:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d1448
	if (ctx.cr6.gt) goto loc_825D1448;
loc_825D14A0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d14dc
	if (!ctx.cr0.lt) goto loc_825D14DC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D14DC;
	sub_825D5398(ctx, base);
loc_825D14DC:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// li r30,12
	ctx.r30.s64 = 12;
	// rlwinm r28,r11,1,0,30
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,12
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 12, ctx.xer);
	// bge cr6,0x825d1558
	if (!ctx.cr6.lt) goto loc_825D1558;
loc_825D1500:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d1558
	if (ctx.cr6.eq) goto loc_825D1558;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d1548
	if (!ctx.cr0.lt) goto loc_825D1548;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1548;
	sub_825D5398(ctx, base);
loc_825D1548:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d1500
	if (ctx.cr6.gt) goto loc_825D1500;
loc_825D1558:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d1594
	if (!ctx.cr0.lt) goto loc_825D1594;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1594;
	sub_825D5398(ctx, base);
loc_825D1594:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// beq cr6,0x825d15b4
	if (ctx.cr6.eq) goto loc_825D15B4;
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// beq cr6,0x825d15b4
	if (ctx.cr6.eq) goto loc_825D15B4;
	// stw r28,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r28.u32);
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
loc_825D15B4:
	// lwz r10,21352(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21352);
	// cmpw cr6,r28,r10
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r10.s32, ctx.xer);
	// bgt cr6,0x825d15d8
	if (ctx.cr6.gt) goto loc_825D15D8;
	// lwz r10,21356(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21356);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bgt cr6,0x825d15d8
	if (ctx.cr6.gt) goto loc_825D15D8;
	// stw r28,156(r27)
	PPC_STORE_U32(ctx.r27.u32 + 156, ctx.r28.u32);
	// stw r11,160(r27)
	PPC_STORE_U32(ctx.r27.u32 + 160, ctx.r11.u32);
	// b 0x825d15f4
	goto loc_825D15F4;
loc_825D15D8:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D15E4:
	// lwz r11,21352(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21352);
	// lwz r10,21356(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21356);
	// stw r11,156(r27)
	PPC_STORE_U32(ctx.r27.u32 + 156, ctx.r11.u32);
	// stw r10,160(r27)
	PPC_STORE_U32(ctx.r27.u32 + 160, ctx.r10.u32);
loc_825D15F4:
	// lwz r11,20864(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 20864);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d16b4
	if (ctx.cr6.eq) goto loc_825D16B4;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d1674
	if (!ctx.cr6.lt) goto loc_825D1674;
loc_825D161C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d1674
	if (ctx.cr6.eq) goto loc_825D1674;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d1664
	if (!ctx.cr0.lt) goto loc_825D1664;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1664;
	sub_825D5398(ctx, base);
loc_825D1664:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d161c
	if (ctx.cr6.gt) goto loc_825D161C;
loc_825D1674:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d16b0
	if (!ctx.cr0.lt) goto loc_825D16B0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D16B0;
	sub_825D5398(ctx, base);
loc_825D16B0:
	// stw r30,20956(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20956, ctx.r30.u32);
loc_825D16B4:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d1728
	if (!ctx.cr6.lt) goto loc_825D1728;
loc_825D16D0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d1728
	if (ctx.cr6.eq) goto loc_825D1728;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d1718
	if (!ctx.cr0.lt) goto loc_825D1718;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1718;
	sub_825D5398(ctx, base);
loc_825D1718:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d16d0
	if (ctx.cr6.gt) goto loc_825D16D0;
loc_825D1728:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d1764
	if (!ctx.cr0.lt) goto loc_825D1764;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1764;
	sub_825D5398(ctx, base);
loc_825D1764:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// stw r30,21212(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21212, ctx.r30.u32);
	// beq cr6,0x825d182c
	if (ctx.cr6.eq) goto loc_825D182C;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825d17e4
	if (!ctx.cr6.lt) goto loc_825D17E4;
loc_825D178C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d17e4
	if (ctx.cr6.eq) goto loc_825D17E4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d17d4
	if (!ctx.cr0.lt) goto loc_825D17D4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D17D4;
	sub_825D5398(ctx, base);
loc_825D17D4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d178c
	if (ctx.cr6.gt) goto loc_825D178C;
loc_825D17E4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d1820
	if (!ctx.cr0.lt) goto loc_825D1820;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1820;
	sub_825D5398(ctx, base);
loc_825D1820:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// stw r11,21220(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21220, ctx.r11.u32);
	// b 0x825d1830
	goto loc_825D1830;
loc_825D182C:
	// stw r24,21220(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21220, ctx.r24.u32);
loc_825D1830:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d18a4
	if (!ctx.cr6.lt) goto loc_825D18A4;
loc_825D184C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d18a4
	if (ctx.cr6.eq) goto loc_825D18A4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d1894
	if (!ctx.cr0.lt) goto loc_825D1894;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1894;
	sub_825D5398(ctx, base);
loc_825D1894:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d184c
	if (ctx.cr6.gt) goto loc_825D184C;
loc_825D18A4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d18e0
	if (!ctx.cr0.lt) goto loc_825D18E0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D18E0;
	sub_825D5398(ctx, base);
loc_825D18E0:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// stw r30,21216(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21216, ctx.r30.u32);
	// beq cr6,0x825d19a8
	if (ctx.cr6.eq) goto loc_825D19A8;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825d1960
	if (!ctx.cr6.lt) goto loc_825D1960;
loc_825D1908:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d1960
	if (ctx.cr6.eq) goto loc_825D1960;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d1950
	if (!ctx.cr0.lt) goto loc_825D1950;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D1950;
	sub_825D5398(ctx, base);
loc_825D1950:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d1908
	if (ctx.cr6.gt) goto loc_825D1908;
loc_825D1960:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d199c
	if (!ctx.cr0.lt) goto loc_825D199C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D199C;
	sub_825D5398(ctx, base);
loc_825D199C:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// stw r11,21224(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21224, ctx.r11.u32);
	// b 0x825d19ac
	goto loc_825D19AC;
loc_825D19A8:
	// stw r24,21224(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21224, ctx.r24.u32);
loc_825D19AC:
	// lwz r11,84(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d15d8
	if (!ctx.cr6.eq) goto loc_825D15D8;
	// lwz r10,14772(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 14772);
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r9,3436(r27)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3436);
	// addi r11,r11,-22904
	ctx.r11.s64 = ctx.r11.s64 + -22904;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// lwzx r11,r10,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// stw r11,14776(r27)
	PPC_STORE_U32(ctx.r27.u32 + 14776, ctx.r11.u32);
	// bne cr6,0x825d19f0
	if (!ctx.cr6.eq) goto loc_825D19F0;
	// lwz r11,3440(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3440);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// mr r11,r24
	ctx.r11.u64 = ctx.r24.u64;
	// beq cr6,0x825d19f4
	if (ctx.cr6.eq) goto loc_825D19F4;
loc_825D19F0:
	// li r11,1
	ctx.r11.s64 = 1;
loc_825D19F4:
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r11,3432(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3432, ctx.r11.u32);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// stw r24,284(r27)
	PPC_STORE_U32(ctx.r27.u32 + 284, ctx.r24.u32);
	// bl 0x825ed4d0
	ctx.lr = 0x825D1A08;
	sub_825ED4D0(ctx, base);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x825cfef0
	ctx.lr = 0x825D1A14;
	sub_825CFEF0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825d1a38
	if (ctx.cr6.eq) goto loc_825D1A38;
	// cmpwi cr6,r3,12
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 12, ctx.xer);
	// bne cr6,0x825d1a40
	if (!ctx.cr6.eq) goto loc_825D1A40;
	// li r11,1
	ctx.r11.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,21336(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21336, ctx.r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D1A38:
	// stw r24,21336(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21336, ctx.r24.u32);
	// li r3,0
	ctx.r3.s64 = 0;
loc_825D1A40:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825D1A48"))) PPC_WEAK_FUNC(sub_825D1A48);
PPC_FUNC_IMPL(__imp__sub_825D1A48) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9e8
	ctx.lr = 0x825D1A50;
	sub_8239B9E8(ctx, base);
	// stwu r1,-1392(r1)
	ea = -1392 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r20,r4
	ctx.r20.u64 = ctx.r4.u64;
	// mr r21,r5
	ctx.r21.u64 = ctx.r5.u64;
	// mr r27,r6
	ctx.r27.u64 = ctx.r6.u64;
	// mr r30,r7
	ctx.r30.u64 = ctx.r7.u64;
	// mr r17,r8
	ctx.r17.u64 = ctx.r8.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825d1a80
	if (!ctx.cr6.eq) goto loc_825D1A80;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239ba38
	// ERROR 8239BA38
	return;
loc_825D1A80:
	// lwz r11,3668(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3668);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d1a98
	if (!ctx.cr6.eq) goto loc_825D1A98;
	// li r3,3
	ctx.r3.s64 = 3;
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239ba38
	// ERROR 8239BA38
	return;
loc_825D1A98:
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// li r19,0
	ctx.r19.s64 = 0;
	// stw r19,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r19.u32);
	// lwz r11,3924(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d1b24
	if (ctx.cr6.eq) goto loc_825D1B24;
	// lis r11,12889
	ctx.r11.s64 = 844693504;
	// ori r11,r11,21849
	ctx.r11.u64 = ctx.r11.u64 | 21849;
	// cmplw cr6,r20,r11
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825d1afc
	if (ctx.cr6.eq) goto loc_825D1AFC;
	// lis r11,22870
	ctx.r11.s64 = 1498808320;
	// ori r11,r11,22869
	ctx.r11.u64 = ctx.r11.u64 | 22869;
	// cmplw cr6,r20,r11
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825d1afc
	if (ctx.cr6.eq) goto loc_825D1AFC;
	// lis r11,21849
	ctx.r11.s64 = 1431896064;
	// ori r11,r11,22105
	ctx.r11.u64 = ctx.r11.u64 | 22105;
	// cmplw cr6,r20,r11
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825d1afc
	if (ctx.cr6.eq) goto loc_825D1AFC;
	// cmplwi cr6,r20,0
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, 0, ctx.xer);
	// beq cr6,0x825d1b04
	if (ctx.cr6.eq) goto loc_825D1B04;
	// cmplwi cr6,r20,3
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, 3, ctx.xer);
	// beq cr6,0x825d1afc
	if (ctx.cr6.eq) goto loc_825D1AFC;
loc_825D1AF0:
	// li r3,5
	ctx.r3.s64 = 5;
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239ba38
	// ERROR 8239BA38
	return;
loc_825D1AFC:
	// cmplwi cr6,r20,0
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, 0, ctx.xer);
	// bne cr6,0x825d1b10
	if (!ctx.cr6.eq) goto loc_825D1B10;
loc_825D1B04:
	// clrlwi r11,r21,16
	ctx.r11.u64 = ctx.r21.u32 & 0xFFFF;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// beq cr6,0x825d1af0
	if (ctx.cr6.eq) goto loc_825D1AF0;
loc_825D1B10:
	// cmplwi cr6,r20,3
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, 3, ctx.xer);
	// bne cr6,0x825d1b24
	if (!ctx.cr6.eq) goto loc_825D1B24;
	// clrlwi r11,r21,16
	ctx.r11.u64 = ctx.r21.u32 & 0xFFFF;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// beq cr6,0x825d1af0
	if (ctx.cr6.eq) goto loc_825D1AF0;
loc_825D1B24:
	// lwz r11,21428(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21428);
	// li r18,1
	ctx.r18.s64 = 1;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d1b40
	if (!ctx.cr6.eq) goto loc_825D1B40;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ceab0
	ctx.lr = 0x825D1B3C;
	sub_825CEAB0(ctx, base);
	// b 0x825d2480
	goto loc_825D2480;
loc_825D1B40:
	// lwz r11,3424(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3424);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d1b68
	if (!ctx.cr6.eq) goto loc_825D1B68;
	// lwz r11,21388(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21388);
	// stw r19,3424(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3424, ctx.r19.u32);
	// sth r18,3684(r31)
	PPC_STORE_U16(ctx.r31.u32 + 3684, ctx.r18.u16);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d2480
	if (!ctx.cr6.eq) goto loc_825D2480;
	// stw r19,21388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21388, ctx.r19.u32);
	// b 0x825d2070
	goto loc_825D2070;
loc_825D1B68:
	// lhz r11,3684(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 3684);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825d1b80
	if (!ctx.cr6.eq) goto loc_825D1B80;
	// li r3,9
	ctx.r3.s64 = 9;
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239ba38
	// ERROR 8239BA38
	return;
loc_825D1B80:
	// lwz r11,15300(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15300);
	// cmplwi cr6,r17,0
	ctx.cr6.compare<uint32_t>(ctx.r17.u32, 0, ctx.xer);
	// bne cr6,0x825d1dd4
	if (!ctx.cr6.eq) goto loc_825D1DD4;
	// li r18,1
	ctx.r18.s64 = 1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d1ba4
	if (!ctx.cr6.eq) goto loc_825D1BA4;
	// lwz r11,15368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15368);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d1bac
	if (ctx.cr6.eq) goto loc_825D1BAC;
loc_825D1BA4:
	// stw r18,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r18.u32);
	// b 0x825d1bb0
	goto loc_825D1BB0;
loc_825D1BAC:
	// stw r19,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r19.u32);
loc_825D1BB0:
	// lwz r11,14792(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14792);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d1c44
	if (ctx.cr6.eq) goto loc_825D1C44;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d1bd0
	if (ctx.cr6.eq) goto loc_825D1BD0;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bne cr6,0x825d1c18
	if (!ctx.cr6.eq) goto loc_825D1C18;
loc_825D1BD0:
	// lwz r10,14772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825d1be8
	if (!ctx.cr6.eq) goto loc_825D1BE8;
	// lwz r11,14796(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14796);
	// stw r11,14800(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14800, ctx.r11.u32);
	// b 0x825d1c18
	goto loc_825D1C18;
loc_825D1BE8:
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825d1c18
	if (ctx.cr6.eq) goto loc_825D1C18;
	// lwz r11,14800(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14800);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d1c10
	if (!ctx.cr6.eq) goto loc_825D1C10;
	// lwz r11,14796(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14796);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d1c10
	if (!ctx.cr6.eq) goto loc_825D1C10;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825dd508
	ctx.lr = 0x825D1C10;
	sub_825DD508(ctx, base);
loc_825D1C10:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82604610
	ctx.lr = 0x825D1C18;
	sub_82604610(ctx, base);
loc_825D1C18:
	// lwz r11,14796(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14796);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d1c44
	if (ctx.cr6.eq) goto loc_825D1C44;
	// lwz r11,15564(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d1c3c
	if (!ctx.cr6.eq) goto loc_825D1C3C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f6498
	ctx.lr = 0x825D1C38;
	sub_825F6498(ctx, base);
	// stw r18,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r18.u32);
loc_825D1C3C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82604750
	ctx.lr = 0x825D1C44;
	sub_82604750(ctx, base);
loc_825D1C44:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825d1c74
	if (!ctx.cr6.eq) goto loc_825D1C74;
	// lwz r11,21212(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21212);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d1c68
	if (!ctx.cr6.eq) goto loc_825D1C68;
	// lwz r11,21216(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21216);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d1c74
	if (ctx.cr6.eq) goto loc_825D1C74;
loc_825D1C68:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82604b60
	ctx.lr = 0x825D1C70;
	sub_82604B60(ctx, base);
	// stw r18,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r18.u32);
loc_825D1C74:
	// lwz r11,14820(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14820);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d1d64
	if (ctx.cr6.eq) goto loc_825D1D64;
	// lwz r11,14824(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d1d64
	if (ctx.cr6.eq) goto loc_825D1D64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f1cd8
	ctx.lr = 0x825D1C98;
	sub_825F1CD8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f17a0
	ctx.lr = 0x825D1CA0;
	sub_825F17A0(ctx, base);
	// lwz r29,14824(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// lwz r11,15564(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rotlwi r10,r29,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r29.u32, 0);
	// lwz r28,14828(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r30,220(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// stw r19,14824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14824, ctx.r19.u32);
	// stw r29,14828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14828, ctx.r29.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// beq cr6,0x825d1d1c
	if (ctx.cr6.eq) goto loc_825D1D1C;
	// lwz r9,3752(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3752);
	// lwz r8,3748(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3748);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// lwz r7,3744(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3744);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// lwz r11,14904(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// lwz r6,3784(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3784);
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + ctx.r30.u64;
	// lwz r5,3780(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3780);
	// lwz r4,3776(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3776);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + ctx.r11.u64;
	// add r4,r10,r4
	ctx.r4.u64 = ctx.r10.u64 + ctx.r4.u64;
	// bl 0x825f28f8
	ctx.lr = 0x825D1D10;
	sub_825F28F8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825dd410
	ctx.lr = 0x825D1D18;
	sub_825DD410(ctx, base);
	// b 0x825d1d58
	goto loc_825D1D58;
loc_825D1D1C:
	// lwz r9,3784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3784);
	// lwz r8,3780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3780);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// lwz r7,3776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3776);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// lwz r11,14904(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + ctx.r30.u64;
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + ctx.r11.u64;
	// add r4,r10,r4
	ctx.r4.u64 = ctx.r10.u64 + ctx.r4.u64;
	// bl 0x825f28f8
	ctx.lr = 0x825D1D58;
	sub_825F28F8(ctx, base);
loc_825D1D58:
	// stw r28,14828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14828, ctx.r28.u32);
	// stw r29,14824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14824, ctx.r29.u32);
	// stw r18,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r18.u32);
loc_825D1D64:
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d1dc4
	if (!ctx.cr6.gt) goto loc_825D1DC4;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825d1d84
	if (ctx.cr6.eq) goto loc_825D1D84;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bne cr6,0x825d1da4
	if (!ctx.cr6.eq) goto loc_825D1DA4;
loc_825D1D84:
	// lwz r11,3452(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3452);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d1dbc
	if (ctx.cr6.eq) goto loc_825D1DBC;
	// lwz r11,292(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 292);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825d1dbc
	if (ctx.cr6.eq) goto loc_825D1DBC;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d1dbc
	if (ctx.cr6.eq) goto loc_825D1DBC;
loc_825D1DA4:
	// mr r11,r18
	ctx.r11.u64 = ctx.r18.u64;
	// stw r18,15500(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15500, ctx.r18.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,3396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3396, ctx.r11.u32);
	// bl 0x825ceb10
	ctx.lr = 0x825D1DB8;
	sub_825CEB10(ctx, base);
	// b 0x825d28d8
	goto loc_825D28D8;
loc_825D1DBC:
	// mr r11,r19
	ctx.r11.u64 = ctx.r19.u64;
	// stw r11,3396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3396, ctx.r11.u32);
loc_825D1DC4:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r18,15500(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15500, ctx.r18.u32);
	// bl 0x825ceb10
	ctx.lr = 0x825D1DD0;
	sub_825CEB10(ctx, base);
	// b 0x825d28d8
	goto loc_825D28D8;
loc_825D1DD4:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r11,2
	ctx.r11.s64 = 2;
	// stw r11,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r11.u32);
	// bne cr6,0x825d1df0
	if (!ctx.cr6.eq) goto loc_825D1DF0;
	// lwz r11,15368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15368);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d1f88
	if (ctx.cr6.eq) goto loc_825D1F88;
loc_825D1DF0:
	// lwz r11,15308(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15308);
	// lwz r10,15312(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15312);
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// lwz r4,3356(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3356);
	// srawi r8,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r11.s32 >> 4;
	// stw r19,220(r31)
	PPC_STORE_U32(ctx.r31.u32 + 220, ctx.r19.u32);
	// srawi r3,r10,4
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r3.s64 = ctx.r10.s32 >> 4;
	// stw r19,224(r31)
	PPC_STORE_U32(ctx.r31.u32 + 224, ctx.r19.u32);
	// stw r11,88(r31)
	PPC_STORE_U32(ctx.r31.u32 + 88, ctx.r11.u32);
	// stw r10,92(r31)
	PPC_STORE_U32(ctx.r31.u32 + 92, ctx.r10.u32);
	// stw r9,208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 208, ctx.r9.u32);
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r8,136(r31)
	PPC_STORE_U32(ctx.r31.u32 + 136, ctx.r8.u32);
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,204(r31)
	PPC_STORE_U32(ctx.r31.u32 + 204, ctx.r11.u32);
	// stw r3,140(r31)
	PPC_STORE_U32(ctx.r31.u32 + 140, ctx.r3.u32);
	// stw r9,228(r31)
	PPC_STORE_U32(ctx.r31.u32 + 228, ctx.r9.u32);
	// stw r8,232(r31)
	PPC_STORE_U32(ctx.r31.u32 + 232, ctx.r8.u32);
	// bl 0x82624490
	ctx.lr = 0x825D1E3C;
	sub_82624490(ctx, base);
	// lwz r6,216(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 216);
	// li r18,1
	ctx.r18.s64 = 1;
	// lwz r7,3356(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3356);
	// lwz r11,204(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// twllei r7,0
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// stw r6,116(r31)
	PPC_STORE_U32(ctx.r31.u32 + 116, ctx.r6.u32);
	// rlwinm r4,r10,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,212(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// divwu r7,r8,r7
	ctx.r7.u32 = ctx.r8.u32 / ctx.r7.u32;
	// rlwinm r6,r11,4,0,27
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r5,88(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
	// stw r3,3812(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3812, ctx.r3.u32);
	// stw r3,3848(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3848, ctx.r3.u32);
	// cmpw cr6,r5,r11
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r11.s32, ctx.xer);
	// stw r11,96(r31)
	PPC_STORE_U32(ctx.r31.u32 + 96, ctx.r11.u32);
	// stw r7,3816(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3816, ctx.r7.u32);
	// stw r10,108(r31)
	PPC_STORE_U32(ctx.r31.u32 + 108, ctx.r10.u32);
	// stw r9,104(r31)
	PPC_STORE_U32(ctx.r31.u32 + 104, ctx.r9.u32);
	// stw r6,100(r31)
	PPC_STORE_U32(ctx.r31.u32 + 100, ctx.r6.u32);
	// stw r4,112(r31)
	PPC_STORE_U32(ctx.r31.u32 + 112, ctx.r4.u32);
	// bne cr6,0x825d1ea8
	if (!ctx.cr6.eq) goto loc_825D1EA8;
	// lwz r11,92(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 92);
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// mr r11,r18
	ctx.r11.u64 = ctx.r18.u64;
	// beq cr6,0x825d1eac
	if (ctx.cr6.eq) goto loc_825D1EAC;
loc_825D1EA8:
	// mr r11,r19
	ctx.r11.u64 = ctx.r19.u64;
loc_825D1EAC:
	// lwz r9,15308(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15308);
	// rlwinm r8,r8,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 4) & 0xFFFFFFF0;
	// lwz r7,15312(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15312);
	// stw r11,120(r31)
	PPC_STORE_U32(ctx.r31.u32 + 120, ctx.r11.u32);
	// addi r11,r9,15
	ctx.r11.s64 = ctx.r9.s64 + 15;
	// addi r10,r7,15
	ctx.r10.s64 = ctx.r7.s64 + 15;
	// srawi r11,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 4;
	// srawi r10,r10,4
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 4;
	// cmplw cr6,r9,r8
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r8.u32, ctx.xer);
	// mullw r8,r10,r11
	ctx.r8.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// stw r11,128(r31)
	PPC_STORE_U32(ctx.r31.u32 + 128, ctx.r11.u32);
	// stw r10,132(r31)
	PPC_STORE_U32(ctx.r31.u32 + 132, ctx.r10.u32);
	// stw r8,124(r31)
	PPC_STORE_U32(ctx.r31.u32 + 124, ctx.r8.u32);
	// bne cr6,0x825d1ef8
	if (!ctx.cr6.eq) goto loc_825D1EF8;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// cmplw cr6,r7,r11
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r11.u32, ctx.xer);
	// mr r11,r18
	ctx.r11.u64 = ctx.r18.u64;
	// beq cr6,0x825d1efc
	if (ctx.cr6.eq) goto loc_825D1EFC;
loc_825D1EF8:
	// mr r11,r19
	ctx.r11.u64 = ctx.r19.u64;
loc_825D1EFC:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// stw r11,152(r31)
	PPC_STORE_U32(ctx.r31.u32 + 152, ctx.r11.u32);
	// stw r20,15488(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15488, ctx.r20.u32);
	// sth r21,15492(r31)
	PPC_STORE_U16(ctx.r31.u32 + 15492, ctx.r21.u16);
	// stw r30,15504(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15504, ctx.r30.u32);
	// beq cr6,0x825d1f20
	if (ctx.cr6.eq) goto loc_825D1F20;
	// cmpwi cr6,r30,2
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 2, ctx.xer);
	// beq cr6,0x825d1f20
	if (ctx.cr6.eq) goto loc_825D1F20;
	// stw r19,15504(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15504, ctx.r19.u32);
loc_825D1F20:
	// lwz r11,15504(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15504);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x825d1f48
	if (ctx.cr6.eq) goto loc_825D1F48;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// beq cr6,0x825d1f48
	if (ctx.cr6.eq) goto loc_825D1F48;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// bne cr6,0x825d1f58
	if (!ctx.cr6.eq) goto loc_825D1F58;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
	// b 0x825d1f58
	goto loc_825D1F58;
loc_825D1F48:
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// bne cr6,0x825d1f58
	if (!ctx.cr6.eq) goto loc_825D1F58;
	// mr r11,r7
	ctx.r11.u64 = ctx.r7.u64;
loc_825D1F58:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,15496(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15496, ctx.r11.u32);
	// bl 0x826228e0
	ctx.lr = 0x825D1F64;
	sub_826228E0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d28ec
	if (!ctx.cr6.eq) goto loc_825D28EC;
	// mr r4,r17
	ctx.r4.u64 = ctx.r17.u64;
	// stw r18,15484(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15484, ctx.r18.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82622778
	ctx.lr = 0x825D1F7C;
	sub_82622778(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d28ec
	if (!ctx.cr6.eq) goto loc_825D28EC;
	// b 0x825d28d8
	goto loc_825D28D8;
loc_825D1F88:
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// stw r20,15488(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15488, ctx.r20.u32);
	// sth r21,15492(r31)
	PPC_STORE_U16(ctx.r31.u32 + 15492, ctx.r21.u16);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// stw r30,15504(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15504, ctx.r30.u32);
	// bne cr6,0x825d1fa4
	if (!ctx.cr6.eq) goto loc_825D1FA4;
	// lwz r11,88(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 88);
loc_825D1FA4:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// stw r11,15496(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15496, ctx.r11.u32);
	// beq cr6,0x825d1fbc
	if (ctx.cr6.eq) goto loc_825D1FBC;
	// cmpwi cr6,r30,2
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 2, ctx.xer);
	// beq cr6,0x825d1fbc
	if (ctx.cr6.eq) goto loc_825D1FBC;
	// stw r19,15504(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15504, ctx.r19.u32);
loc_825D1FBC:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x826228e0
	ctx.lr = 0x825D1FC4;
	sub_826228E0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d28ec
	if (!ctx.cr6.eq) goto loc_825D28EC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r18,15484(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15484, ctx.r18.u32);
	// bl 0x825cf7f0
	ctx.lr = 0x825D1FD8;
	sub_825CF7F0(ctx, base);
	// lwz r11,3644(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3644);
	// cmpwi cr6,r11,-1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, -1, ctx.xer);
	// bne cr6,0x825d2004
	if (!ctx.cr6.eq) goto loc_825D2004;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d2020
	if (!ctx.cr6.eq) goto loc_825D2020;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f6a28
	ctx.lr = 0x825D1FF8;
	sub_825F6A28(ctx, base);
	// lwz r11,15548(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15548);
	// stw r11,3644(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3644, ctx.r11.u32);
	// b 0x825d2020
	goto loc_825D2020;
loc_825D2004:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// blt cr6,0x825d2018
	if (ctx.cr6.lt) goto loc_825D2018;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bgt cr6,0x825d2018
	if (ctx.cr6.gt) goto loc_825D2018;
	// stw r11,15548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15548, ctx.r11.u32);
loc_825D2018:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f6a28
	ctx.lr = 0x825D2020;
	sub_825F6A28(ctx, base);
loc_825D2020:
	// lwz r11,15508(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15508);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2070
	if (ctx.cr6.eq) goto loc_825D2070;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc830
	ctx.lr = 0x825D2034;
	sub_825CC830(ctx, base);
	// lwz r11,3924(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// lwz r5,140(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r4,0
	ctx.r4.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// beq cr6,0x825d205c
	if (ctx.cr6.eq) goto loc_825D205C;
	// lwz r11,15860(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15860);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x825D2058;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// b 0x825d2064
	goto loc_825D2064;
loc_825D205C:
	// li r6,0
	ctx.r6.s64 = 0;
	// bl 0x825f65f8
	ctx.lr = 0x825D2064;
	sub_825F65F8(ctx, base);
loc_825D2064:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r18,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r18.u32);
	// bl 0x825cc890
	ctx.lr = 0x825D2070;
	sub_825CC890(ctx, base);
loc_825D2070:
	// lwz r11,14792(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14792);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2104
	if (ctx.cr6.eq) goto loc_825D2104;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2090
	if (ctx.cr6.eq) goto loc_825D2090;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bne cr6,0x825d20d8
	if (!ctx.cr6.eq) goto loc_825D20D8;
loc_825D2090:
	// lwz r10,14772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825d20a8
	if (!ctx.cr6.eq) goto loc_825D20A8;
	// lwz r11,14796(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14796);
	// stw r11,14800(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14800, ctx.r11.u32);
	// b 0x825d20d8
	goto loc_825D20D8;
loc_825D20A8:
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825d20d8
	if (ctx.cr6.eq) goto loc_825D20D8;
	// lwz r11,14800(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14800);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d20d0
	if (!ctx.cr6.eq) goto loc_825D20D0;
	// lwz r11,14796(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14796);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d20d0
	if (!ctx.cr6.eq) goto loc_825D20D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825dd508
	ctx.lr = 0x825D20D0;
	sub_825DD508(ctx, base);
loc_825D20D0:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82604610
	ctx.lr = 0x825D20D8;
	sub_82604610(ctx, base);
loc_825D20D8:
	// lwz r11,14796(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14796);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2104
	if (ctx.cr6.eq) goto loc_825D2104;
	// lwz r11,15564(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d20fc
	if (!ctx.cr6.eq) goto loc_825D20FC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f6498
	ctx.lr = 0x825D20F8;
	sub_825F6498(ctx, base);
	// stw r18,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r18.u32);
loc_825D20FC:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825fa1c0
	ctx.lr = 0x825D2104;
	sub_825FA1C0(ctx, base);
loc_825D2104:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825d2134
	if (!ctx.cr6.eq) goto loc_825D2134;
	// lwz r11,21212(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21212);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d2128
	if (!ctx.cr6.eq) goto loc_825D2128;
	// lwz r11,21216(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21216);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2134
	if (ctx.cr6.eq) goto loc_825D2134;
loc_825D2128:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82604b60
	ctx.lr = 0x825D2130;
	sub_82604B60(ctx, base);
	// stw r18,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r18.u32);
loc_825D2134:
	// lwz r11,14824(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2424
	if (ctx.cr6.eq) goto loc_825D2424;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f1cd8
	ctx.lr = 0x825D214C;
	sub_825F1CD8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f17a0
	ctx.lr = 0x825D2154;
	sub_825F17A0(ctx, base);
	// lwz r29,14824(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// lwz r11,15564(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// lwz r28,14828(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r11,15900(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15900);
	// stw r19,14824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14824, ctx.r19.u32);
	// stw r29,14828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14828, ctx.r29.u32);
	// beq cr6,0x825d22c8
	if (ctx.cr6.eq) goto loc_825D22C8;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2268
	if (ctx.cr6.eq) goto loc_825D2268;
	// lwz r11,19712(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19712);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d2268
	if (!ctx.cr6.eq) goto loc_825D2268;
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// li r8,8
	ctx.r8.s64 = 8;
	// lwz r9,584(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 584);
	// mulli r11,r9,68
	ctx.r11.s64 = ctx.r9.s64 * 68;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,584(r10)
	PPC_STORE_U32(ctx.r10.u32 + 584, ctx.r9.u32);
	// stw r8,40(r11)
	PPC_STORE_U32(ctx.r11.u32 + 40, ctx.r8.u32);
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// stw r10,44(r11)
	PPC_STORE_U32(ctx.r11.u32 + 44, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// stw r10,64(r11)
	PPC_STORE_U32(ctx.r11.u32 + 64, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// lwz r10,14904(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// stw r10,68(r11)
	PPC_STORE_U32(ctx.r11.u32 + 68, ctx.r10.u32);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// stw r10,72(r11)
	PPC_STORE_U32(ctx.r11.u32 + 72, ctx.r10.u32);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// stw r10,76(r11)
	PPC_STORE_U32(ctx.r11.u32 + 76, ctx.r10.u32);
	// lwz r10,14824(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// stw r10,80(r11)
	PPC_STORE_U32(ctx.r11.u32 + 80, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// stw r10,84(r11)
	PPC_STORE_U32(ctx.r11.u32 + 84, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// lwz r10,14884(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14884);
	// stw r10,88(r11)
	PPC_STORE_U32(ctx.r11.u32 + 88, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// lwz r10,14888(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14888);
	// stw r10,92(r11)
	PPC_STORE_U32(ctx.r11.u32 + 92, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// lwz r10,14872(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14872);
	// stw r10,104(r11)
	PPC_STORE_U32(ctx.r11.u32 + 104, ctx.r10.u32);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// stw r10,96(r11)
	PPC_STORE_U32(ctx.r11.u32 + 96, ctx.r10.u32);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// stw r10,100(r11)
	PPC_STORE_U32(ctx.r11.u32 + 100, ctx.r10.u32);
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// stw r10,52(r11)
	PPC_STORE_U32(ctx.r11.u32 + 52, ctx.r10.u32);
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// stw r10,60(r11)
	PPC_STORE_U32(ctx.r11.u32 + 60, ctx.r10.u32);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// stw r10,48(r11)
	PPC_STORE_U32(ctx.r11.u32 + 48, ctx.r10.u32);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// stw r10,56(r11)
	PPC_STORE_U32(ctx.r11.u32 + 56, ctx.r10.u32);
	// b 0x825d2418
	goto loc_825D2418;
loc_825D2268:
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// lwz r9,3752(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3752);
	// lwz r8,3748(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3748);
	// lwz r7,3744(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3744);
	// lwz r30,220(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// lwz r6,3784(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3784);
	// lwz r5,3780(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3780);
	// lwz r4,3776(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3776);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + ctx.r30.u64;
	// lwz r11,14904(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + ctx.r11.u64;
	// add r4,r10,r4
	ctx.r4.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + ctx.r11.u64;
	// bl 0x825f28f8
	ctx.lr = 0x825D22BC;
	sub_825F28F8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825dd410
	ctx.lr = 0x825D22C4;
	sub_825DD410(ctx, base);
	// b 0x825d2418
	goto loc_825D2418;
loc_825D22C8:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d23c4
	if (ctx.cr6.eq) goto loc_825D23C4;
	// lwz r11,19712(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19712);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d23c4
	if (!ctx.cr6.eq) goto loc_825D23C4;
	// lwz r11,3704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// li r8,8
	ctx.r8.s64 = 8;
	// stw r19,584(r11)
	PPC_STORE_U32(ctx.r11.u32 + 584, ctx.r19.u32);
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// lwz r9,584(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 584);
	// mulli r11,r9,68
	ctx.r11.s64 = ctx.r9.s64 * 68;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// stw r9,584(r10)
	PPC_STORE_U32(ctx.r10.u32 + 584, ctx.r9.u32);
	// stw r8,40(r11)
	PPC_STORE_U32(ctx.r11.u32 + 40, ctx.r8.u32);
	// lwz r10,3688(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// stw r10,44(r11)
	PPC_STORE_U32(ctx.r11.u32 + 44, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// stw r10,64(r11)
	PPC_STORE_U32(ctx.r11.u32 + 64, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// lwz r10,14904(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// stw r10,68(r11)
	PPC_STORE_U32(ctx.r11.u32 + 68, ctx.r10.u32);
	// lwz r10,220(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// stw r10,72(r11)
	PPC_STORE_U32(ctx.r11.u32 + 72, ctx.r10.u32);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// stw r10,76(r11)
	PPC_STORE_U32(ctx.r11.u32 + 76, ctx.r10.u32);
	// lwz r10,14824(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// stw r10,80(r11)
	PPC_STORE_U32(ctx.r11.u32 + 80, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// stw r10,84(r11)
	PPC_STORE_U32(ctx.r11.u32 + 84, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// lwz r10,14884(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14884);
	// stw r10,88(r11)
	PPC_STORE_U32(ctx.r11.u32 + 88, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// lwz r10,14888(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14888);
	// stw r10,92(r11)
	PPC_STORE_U32(ctx.r11.u32 + 92, ctx.r10.u32);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// lwz r10,14872(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14872);
	// stw r10,104(r11)
	PPC_STORE_U32(ctx.r11.u32 + 104, ctx.r10.u32);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// stw r10,96(r11)
	PPC_STORE_U32(ctx.r11.u32 + 96, ctx.r10.u32);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// stw r10,100(r11)
	PPC_STORE_U32(ctx.r11.u32 + 100, ctx.r10.u32);
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// stw r10,52(r11)
	PPC_STORE_U32(ctx.r11.u32 + 52, ctx.r10.u32);
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// stw r10,60(r11)
	PPC_STORE_U32(ctx.r11.u32 + 60, ctx.r10.u32);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// stw r10,48(r11)
	PPC_STORE_U32(ctx.r11.u32 + 48, ctx.r10.u32);
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// stw r10,56(r11)
	PPC_STORE_U32(ctx.r11.u32 + 56, ctx.r10.u32);
	// b 0x825d2418
	goto loc_825D2418;
loc_825D23C4:
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// mulli r10,r10,84
	ctx.r10.s64 = ctx.r10.s64 * 84;
	// lwz r9,3784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3784);
	// lwz r8,3780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3780);
	// lwz r7,3776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3776);
	// lwz r30,220(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// add r7,r7,r30
	ctx.r7.u64 = ctx.r7.u64 + ctx.r30.u64;
	// lwz r11,14904(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14904);
	// lwz r10,14900(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 14900);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + ctx.r11.u64;
	// add r4,r10,r4
	ctx.r4.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + ctx.r11.u64;
	// bl 0x825f28f8
	ctx.lr = 0x825D2418;
	sub_825F28F8(ctx, base);
loc_825D2418:
	// stw r28,14828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14828, ctx.r28.u32);
	// stw r29,14824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14824, ctx.r29.u32);
	// stw r18,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r18.u32);
loc_825D2424:
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// stw r19,21432(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21432, ctx.r19.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d2478
	if (!ctx.cr6.gt) goto loc_825D2478;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825d2448
	if (ctx.cr6.eq) goto loc_825D2448;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bne cr6,0x825d2468
	if (!ctx.cr6.eq) goto loc_825D2468;
loc_825D2448:
	// lwz r11,3452(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3452);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2470
	if (ctx.cr6.eq) goto loc_825D2470;
	// lwz r11,292(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 292);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825d2470
	if (ctx.cr6.eq) goto loc_825D2470;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d2470
	if (ctx.cr6.eq) goto loc_825D2470;
loc_825D2468:
	// mr r11,r18
	ctx.r11.u64 = ctx.r18.u64;
	// b 0x825d2474
	goto loc_825D2474;
loc_825D2470:
	// mr r11,r19
	ctx.r11.u64 = ctx.r19.u64;
loc_825D2474:
	// stw r11,3396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3396, ctx.r11.u32);
loc_825D2478:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ceb10
	ctx.lr = 0x825D2480;
	sub_825CEB10(ctx, base);
loc_825D2480:
	// addi r5,r1,128
	ctx.r5.s64 = ctx.r1.s64 + 128;
	// addi r4,r1,184
	ctx.r4.s64 = ctx.r1.s64 + 184;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cf060
	ctx.lr = 0x825D2490;
	sub_825CF060(ctx, base);
	// lwz r25,15472(r31)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r25,7
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 7, ctx.xer);
	// beq cr6,0x825d24a4
	if (ctx.cr6.eq) goto loc_825D24A4;
	// cmpwi cr6,r25,6
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 6, ctx.xer);
	// bne cr6,0x825d28c0
	if (!ctx.cr6.eq) goto loc_825D28C0;
loc_825D24A4:
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d28bc
	if (!ctx.cr6.eq) goto loc_825D28BC;
	// lwz r11,3716(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3716);
	// li r29,64
	ctx.r29.s64 = 64;
	// lwz r10,21184(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21184);
	// stw r19,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r19.u32);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// lwz r24,0(r11)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r23,4(r11)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r22,8(r11)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// bne cr6,0x825d2524
	if (!ctx.cr6.eq) goto loc_825D2524;
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d2524
	if (!ctx.cr6.gt) goto loc_825D2524;
	// ld r11,3576(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 3576);
	// cmpdi cr6,r11,1
	ctx.cr6.compare<int64_t>(ctx.r11.s64, 1, ctx.xer);
	// ble cr6,0x825d2524
	if (!ctx.cr6.gt) goto loc_825D2524;
	// lwz r10,21352(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21352);
	// lwz r11,21380(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21380);
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bgt cr6,0x825d2504
	if (ctx.cr6.gt) goto loc_825D2504;
	// mr r8,r11
	ctx.r8.u64 = ctx.r11.u64;
loc_825D2504:
	// lwz r11,21384(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21384);
	// lwz r10,21356(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21356);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// ble cr6,0x825d251c
	if (!ctx.cr6.gt) goto loc_825D251C;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// b 0x825d252c
	goto loc_825D252C;
loc_825D251C:
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
	// b 0x825d252c
	goto loc_825D252C;
loc_825D2524:
	// lwz r8,156(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// lwz r4,160(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
loc_825D252C:
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// bne cr6,0x825d253c
	if (!ctx.cr6.eq) goto loc_825D253C;
	// mr r5,r8
	ctx.r5.u64 = ctx.r8.u64;
loc_825D253C:
	// lwz r9,15300(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15300);
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// stw r5,15496(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15496, ctx.r5.u32);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x825d2560
	if (!ctx.cr6.eq) goto loc_825D2560;
	// addi r11,r8,15
	ctx.r11.s64 = ctx.r8.s64 + 15;
	// srawi r11,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 4;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// rlwinm r8,r11,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
loc_825D2560:
	// lwz r30,3924(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825d2590
	if (ctx.cr6.eq) goto loc_825D2590;
	// srawi r11,r8,2
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r8.s32 >> 2;
	// addi r28,r8,64
	ctx.r28.s64 = ctx.r8.s64 + 64;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// mr r6,r19
	ctx.r6.u64 = ctx.r19.u64;
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// mr r7,r19
	ctx.r7.u64 = ctx.r19.u64;
	// mr r27,r11
	ctx.r27.u64 = ctx.r11.u64;
	// mr r26,r11
	ctx.r26.u64 = ctx.r11.u64;
	// b 0x825d25a4
	goto loc_825D25A4;
loc_825D2590:
	// mr r28,r19
	ctx.r28.u64 = ctx.r19.u64;
	// mr r27,r19
	ctx.r27.u64 = ctx.r19.u64;
	// mr r26,r19
	ctx.r26.u64 = ctx.r19.u64;
	// li r6,32
	ctx.r6.s64 = 32;
	// li r7,32
	ctx.r7.s64 = 32;
loc_825D25A4:
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x825d25b8
	if (!ctx.cr6.eq) goto loc_825D25B8;
	// lwz r11,15368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15368);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d25c4
	if (ctx.cr6.eq) goto loc_825D25C4;
loc_825D25B8:
	// mr r6,r19
	ctx.r6.u64 = ctx.r19.u64;
	// mr r7,r19
	ctx.r7.u64 = ctx.r19.u64;
	// mr r29,r19
	ctx.r29.u64 = ctx.r19.u64;
loc_825D25C4:
	// cmplwi cr6,r20,3
	ctx.cr6.compare<uint32_t>(ctx.r20.u32, 3, ctx.xer);
	// bne cr6,0x825d263c
	if (!ctx.cr6.eq) goto loc_825D263C;
	// clrlwi r11,r21,16
	ctx.r11.u64 = ctx.r21.u32 & 0xFFFF;
	// li r9,31
	ctx.r9.s64 = 31;
	// cmplwi cr6,r11,16
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 16, ctx.xer);
	// bne cr6,0x825d25f4
	if (!ctx.cr6.eq) goto loc_825D25F4;
	// lis r3,0
	ctx.r3.s64 = 0;
	// stw r9,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r9.u32);
	// ori r3,r3,63488
	ctx.r3.u64 = ctx.r3.u64 | 63488;
	// stw r3,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r3.u32);
	// li r3,2016
	ctx.r3.s64 = 2016;
	// stw r3,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r3.u32);
loc_825D25F4:
	// cmplwi cr6,r11,15
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 15, ctx.xer);
	// bne cr6,0x825d2614
	if (!ctx.cr6.eq) goto loc_825D2614;
	// li r11,31744
	ctx.r11.s64 = 31744;
	// stw r9,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r9.u32);
	// li r21,16
	ctx.r21.s64 = 16;
	// stw r11,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r11.u32);
	// li r11,992
	ctx.r11.s64 = 992;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r11.u32);
loc_825D2614:
	// clrlwi r11,r21,16
	ctx.r11.u64 = ctx.r21.u32 & 0xFFFF;
	// cmplwi cr6,r11,32
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 32, ctx.xer);
	// bne cr6,0x825d263c
	if (!ctx.cr6.eq) goto loc_825D263C;
	// lis r11,255
	ctx.r11.s64 = 16711680;
	// stw r11,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r11.u32);
	// lis r11,0
	ctx.r11.s64 = 0;
	// ori r11,r11,65280
	ctx.r11.u64 = ctx.r11.u64 | 65280;
	// stw r11,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r11.u32);
	// li r11,255
	ctx.r11.s64 = 255;
	// stw r11,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r11.u32);
loc_825D263C:
	// li r9,40
	ctx.r9.s64 = 40;
	// lwz r11,21336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21336);
	// stw r5,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r5.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// stw r9,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r9.u32);
	// bne cr6,0x825d265c
	if (!ctx.cr6.eq) goto loc_825D265C;
	// lwz r11,21416(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21416);
loc_825D265C:
	// clrlwi r3,r21,16
	ctx.r3.u64 = ctx.r21.u32 & 0xFFFF;
	// lwz r16,15504(r31)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15504);
	// stw r11,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r11.u32);
	// mullw r5,r3,r5
	ctx.r5.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r5.s32);
	// sth r18,204(r1)
	PPC_STORE_U16(ctx.r1.u32 + 204, ctx.r18.u16);
	// sth r21,206(r1)
	PPC_STORE_U16(ctx.r1.u32 + 206, ctx.r21.u16);
	// stw r20,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r20.u32);
	// stw r19,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r19.u32);
	// stw r19,220(r1)
	PPC_STORE_U32(ctx.r1.u32 + 220, ctx.r19.u32);
	// stw r19,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r19.u32);
	// stw r19,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r19.u32);
	// mullw r5,r5,r4
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r4.s32);
	// rlwinm r5,r5,29,3,31
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 29) & 0x1FFFFFFF;
	// cmpwi cr6,r16,2
	ctx.cr6.compare<int32_t>(ctx.r16.s32, 2, ctx.xer);
	// stw r5,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r5.u32);
	// bne cr6,0x825d26a4
	if (!ctx.cr6.eq) goto loc_825D26A4;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
	// stw r11,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r11.u32);
loc_825D26A4:
	// add r11,r8,r29
	ctx.r11.u64 = ctx.r8.u64 + ctx.r29.u64;
	// stw r9,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r9.u32);
	// add r9,r4,r29
	ctx.r9.u64 = ctx.r4.u64 + ctx.r29.u64;
	// lwz r3,20952(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20952);
	// li r8,12
	ctx.r8.s64 = 12;
	// sth r18,156(r1)
	PPC_STORE_U16(ctx.r1.u32 + 156, ctx.r18.u16);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r11.u32);
	// stw r9,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r9.u32);
	// sth r8,158(r1)
	PPC_STORE_U16(ctx.r1.u32 + 158, ctx.r8.u16);
	// bne cr6,0x825d2780
	if (!ctx.cr6.eq) goto loc_825D2780;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825d26e4
	if (ctx.cr6.eq) goto loc_825D26E4;
	// lis r8,12593
	ctx.r8.s64 = 825294848;
	// ori r8,r8,13392
	ctx.r8.u64 = ctx.r8.u64 | 13392;
	// b 0x825d26ec
	goto loc_825D26EC;
loc_825D26E4:
	// lis r8,12338
	ctx.r8.s64 = 808583168;
	// ori r8,r8,13385
	ctx.r8.u64 = ctx.r8.u64 | 13385;
loc_825D26EC:
	// mullw r11,r9,r11
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// stw r8,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r8.u32);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r25,7
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 7, ctx.xer);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r11.u32);
	// bne cr6,0x825d2734
	if (!ctx.cr6.eq) goto loc_825D2734;
	// ld r11,3576(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 3576);
	// cmpdi cr6,r11,1
	ctx.cr6.compare<int64_t>(ctx.r11.s64, 1, ctx.xer);
	// ble cr6,0x825d2724
	if (!ctx.cr6.gt) goto loc_825D2724;
	// lwz r11,20976(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20976);
	// b 0x825d2728
	goto loc_825D2728;
loc_825D2724:
	// lwz r11,20972(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20972);
loc_825D2728:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r11,2
	ctx.r11.s64 = 2;
	// beq cr6,0x825d2738
	if (ctx.cr6.eq) goto loc_825D2738;
loc_825D2734:
	// mr r11,r19
	ctx.r11.u64 = ctx.r19.u64;
loc_825D2738:
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r26,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r26.u32);
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// stw r27,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r27.u32);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r28.u32);
	// addi r3,r1,128
	ctx.r3.s64 = ctx.r1.s64 + 128;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// bl 0x8261e830
	ctx.lr = 0x825D2764;
	sub_8261E830(ctx, base);
	// lwz r11,128(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// stw r3,20952(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20952, ctx.r3.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d281c
	if (ctx.cr6.eq) goto loc_825D281C;
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239ba38
	// ERROR 8239BA38
	return;
loc_825D2780:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825d2794
	if (ctx.cr6.eq) goto loc_825D2794;
	// lis r8,12593
	ctx.r8.s64 = 825294848;
	// ori r8,r8,13392
	ctx.r8.u64 = ctx.r8.u64 | 13392;
	// b 0x825d279c
	goto loc_825D279C;
loc_825D2794:
	// lis r8,12338
	ctx.r8.s64 = 808583168;
	// ori r8,r8,13385
	ctx.r8.u64 = ctx.r8.u64 | 13385;
loc_825D279C:
	// mullw r11,r9,r11
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// stw r8,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r8.u32);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r25,7
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 7, ctx.xer);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r11.u32);
	// bne cr6,0x825d27e4
	if (!ctx.cr6.eq) goto loc_825D27E4;
	// ld r11,3576(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 3576);
	// cmpdi cr6,r11,1
	ctx.cr6.compare<int64_t>(ctx.r11.s64, 1, ctx.xer);
	// ble cr6,0x825d27d4
	if (!ctx.cr6.gt) goto loc_825D27D4;
	// lwz r11,20976(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20976);
	// b 0x825d27d8
	goto loc_825D27D8;
loc_825D27D4:
	// lwz r11,20972(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20972);
loc_825D27D8:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r11,2
	ctx.r11.s64 = 2;
	// beq cr6,0x825d27e8
	if (ctx.cr6.eq) goto loc_825D27E8;
loc_825D27E4:
	// mr r11,r19
	ctx.r11.u64 = ctx.r19.u64;
loc_825D27E8:
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r26,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r26.u32);
	// addi r5,r1,192
	ctx.r5.s64 = ctx.r1.s64 + 192;
	// stw r27,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r27.u32);
	// addi r4,r1,144
	ctx.r4.s64 = ctx.r1.s64 + 144;
	// stw r28,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r28.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// bl 0x8261eb70
	ctx.lr = 0x825D2810;
	sub_8261EB70(ctx, base);
	// stw r3,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r3.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d28ec
	if (!ctx.cr6.eq) goto loc_825D28EC;
loc_825D281C:
	// lwz r8,20952(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20952);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x825d1af0
	if (ctx.cr6.eq) goto loc_825D1AF0;
	// lwz r11,3924(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2858
	if (ctx.cr6.eq) goto loc_825D2858;
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// addi r10,r27,1
	ctx.r10.s64 = ctx.r27.s64 + 1;
	// addi r7,r26,1
	ctx.r7.s64 = ctx.r26.s64 + 1;
	// rlwinm r9,r11,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r11,r7,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 4) & 0xFFFFFFF0;
	// add r24,r9,r24
	ctx.r24.u64 = ctx.r9.u64 + ctx.r24.u64;
	// add r23,r10,r23
	ctx.r23.u64 = ctx.r10.u64 + ctx.r23.u64;
	// add r22,r11,r22
	ctx.r22.u64 = ctx.r11.u64 + ctx.r22.u64;
loc_825D2858:
	// lwz r11,21440(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21440);
	// stw r11,52(r8)
	PPC_STORE_U32(ctx.r8.u32 + 52, ctx.r11.u32);
	// lwz r11,21440(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21440);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d289c
	if (!ctx.cr6.eq) goto loc_825D289C;
	// lwz r11,21444(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21444);
	// stw r11,40(r8)
	PPC_STORE_U32(ctx.r8.u32 + 40, ctx.r11.u32);
	// lwz r11,21448(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21448);
	// stw r11,44(r8)
	PPC_STORE_U32(ctx.r8.u32 + 44, ctx.r11.u32);
	// lwz r11,21452(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21452);
	// stw r11,48(r8)
	PPC_STORE_U32(ctx.r8.u32 + 48, ctx.r11.u32);
	// lwz r11,21456(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21456);
	// stw r11,14624(r8)
	PPC_STORE_U32(ctx.r8.u32 + 14624, ctx.r11.u32);
	// lwz r11,21460(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21460);
	// stw r11,14628(r8)
	PPC_STORE_U32(ctx.r8.u32 + 14628, ctx.r11.u32);
	// lwz r11,21464(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21464);
	// stw r11,14632(r8)
	PPC_STORE_U32(ctx.r8.u32 + 14632, ctx.r11.u32);
loc_825D289C:
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
	// lwz r3,20952(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20952);
	// mr r6,r23
	ctx.r6.u64 = ctx.r23.u64;
	// mr r5,r17
	ctx.r5.u64 = ctx.r17.u64;
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// bl 0x8261eb40
	ctx.lr = 0x825D28B4;
	sub_8261EB40(ctx, base);
	// stw r19,21184(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21184, ctx.r19.u32);
	// b 0x825d28dc
	goto loc_825D28DC;
loc_825D28BC:
	// cmpwi cr6,r25,6
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 6, ctx.xer);
loc_825D28C0:
	// mr r4,r17
	ctx.r4.u64 = ctx.r17.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82622778
	ctx.lr = 0x825D28CC;
	sub_82622778(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d28ec
	if (!ctx.cr6.eq) goto loc_825D28EC;
	// stw r19,21184(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21184, ctx.r19.u32);
loc_825D28D8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825D28DC:
	// lhz r11,3684(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 3684);
	// addis r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 65536;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// sth r11,3684(r31)
	PPC_STORE_U16(ctx.r31.u32 + 3684, ctx.r11.u16);
loc_825D28EC:
	// addi r1,r1,1392
	ctx.r1.s64 = ctx.r1.s64 + 1392;
	// b 0x8239ba38
	// ERROR 8239BA38
	return;
}

__attribute__((alias("__imp__sub_825D28F4"))) PPC_WEAK_FUNC(sub_825D28F4);
PPC_FUNC_IMPL(__imp__sub_825D28F4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D28F8"))) PPC_WEAK_FUNC(sub_825D28F8);
PPC_FUNC_IMPL(__imp__sub_825D28F8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba00
	ctx.lr = 0x825D2900;
	sub_8239BA00(ctx, base);
	// stfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	PPC_STORE_U64(ctx.r1.u32 + -104, ctx.f30.u64);
	// stfd f31,-96(r1)
	PPC_STORE_U64(ctx.r1.u32 + -96, ctx.f31.u64);
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r27,0
	ctx.r27.s64 = 0;
	// li r23,1
	ctx.r23.s64 = 1;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// mr r24,r5
	ctx.r24.u64 = ctx.r5.u64;
	// mr r22,r6
	ctx.r22.u64 = ctx.r6.u64;
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r27.u32);
	// stw r27,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r27.u32);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// stw r23,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r23.u32);
	// bne cr6,0x825d294c
	if (!ctx.cr6.eq) goto loc_825D294C;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D294C:
	// lwz r11,15300(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15300);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d2978
	if (!ctx.cr6.eq) goto loc_825D2978;
	// lwz r11,15368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15368);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d2978
	if (!ctx.cr6.eq) goto loc_825D2978;
loc_825D2964:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D2978:
	// lwz r11,15552(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15552);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// ble cr6,0x825d2990
	if (!ctx.cr6.gt) goto loc_825D2990;
	// lwz r11,3420(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3420);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,3420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3420, ctx.r11.u32);
loc_825D2990:
	// lhz r11,3684(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 3684);
	// stw r27,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r27.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d29c0
	if (ctx.cr6.eq) goto loc_825D29C0;
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r7,15504(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15504);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r6,15496(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15496);
	// lhz r5,15492(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 15492);
	// lwz r4,15488(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15488);
	// bl 0x825d1a48
	ctx.lr = 0x825D29BC;
	sub_825D1A48(ctx, base);
	// sth r27,3684(r31)
	PPC_STORE_U16(ctx.r31.u32 + 3684, ctx.r27.u16);
loc_825D29C0:
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// lwz r3,3340(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r7,r1,132
	ctx.r7.s64 = ctx.r1.s64 + 132;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,136
	ctx.r5.s64 = ctx.r1.s64 + 136;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8248c788
	ctx.lr = 0x825D29DC;
	sub_8248C788(ctx, base);
	// lwz r6,128(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r5,132(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r4,136(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x825d2a14
	if (ctx.cr6.eq) goto loc_825D2A14;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x825d2a00
	if (ctx.cr6.eq) goto loc_825D2A00;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// bne cr6,0x825d2a14
	if (!ctx.cr6.eq) goto loc_825D2A14;
loc_825D2A00:
	// li r3,11
	ctx.r3.s64 = 11;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D2A14:
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// bne cr6,0x825d2a30
	if (!ctx.cr6.eq) goto loc_825D2A30;
loc_825D2A1C:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D2A30:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825d2a84
	if (!ctx.cr6.eq) goto loc_825D2A84;
	// addi r8,r1,128
	ctx.r8.s64 = ctx.r1.s64 + 128;
	// addi r7,r1,132
	ctx.r7.s64 = ctx.r1.s64 + 132;
	// addi r6,r1,136
	ctx.r6.s64 = ctx.r1.s64 + 136;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e7f70
	ctx.lr = 0x825D2A50;
	sub_825E7F70(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d314c
	if (!ctx.cr6.eq) goto loc_825D314C;
	// lwz r11,3672(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3672);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d2a78
	if (!ctx.cr6.eq) goto loc_825D2A78;
	// li r3,3
	ctx.r3.s64 = 3;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D2A78:
	// lwz r5,132(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// lwz r4,136(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r6,128(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
loc_825D2A84:
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// bne cr6,0x825d2a94
	if (!ctx.cr6.eq) goto loc_825D2A94;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x825d2a1c
	if (ctx.cr6.eq) goto loc_825D2A1C;
loc_825D2A94:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// addi r11,r11,-7
	ctx.r11.s64 = ctx.r11.s64 + -7;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// bl 0x825d5100
	ctx.lr = 0x825D2AAC;
	sub_825D5100(ctx, base);
	// lwz r11,80(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// lwz r10,128(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// mr r28,r27
	ctx.r28.u64 = ctx.r27.u64;
	// stw r10,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r10.u32);
	// ld r10,3576(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 3576);
	// ld r11,3584(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 3584);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r30,84(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r27,3424(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3424, ctx.r27.u32);
	// stw r23,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r23.u32);
	// std r10,3576(r31)
	PPC_STORE_U64(ctx.r31.u32 + 3576, ctx.r10.u64);
	// std r11,3584(r31)
	PPC_STORE_U64(ctx.r31.u32 + 3584, ctx.r11.u64);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d2b4c
	if (!ctx.cr6.lt) goto loc_825D2B4C;
loc_825D2AF4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d2b4c
	if (ctx.cr6.eq) goto loc_825D2B4C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825d2b3c
	if (!ctx.cr0.lt) goto loc_825D2B3C;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D2B3C;
	sub_825D5398(ctx, base);
loc_825D2B3C:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d2af4
	if (ctx.cr6.gt) goto loc_825D2AF4;
loc_825D2B4C:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r26,r11,r28
	ctx.r26.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r8.u64);
	// bge 0x825d2b88
	if (!ctx.cr0.lt) goto loc_825D2B88;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D2B88;
	sub_825D5398(ctx, base);
loc_825D2B88:
	// lwz r30,84(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// stw r26,288(r31)
	PPC_STORE_U32(ctx.r31.u32 + 288, ctx.r26.u32);
	// mr r28,r27
	ctx.r28.u64 = ctx.r27.u64;
	// lwz r26,15304(r31)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15304);
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d2c04
	if (!ctx.cr6.lt) goto loc_825D2C04;
loc_825D2BAC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d2c04
	if (ctx.cr6.eq) goto loc_825D2C04;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825d2bf4
	if (!ctx.cr0.lt) goto loc_825D2BF4;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D2BF4;
	sub_825D5398(ctx, base);
loc_825D2BF4:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d2bac
	if (ctx.cr6.gt) goto loc_825D2BAC;
loc_825D2C04:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r8.u64);
	// bge 0x825d2c40
	if (!ctx.cr0.lt) goto loc_825D2C40;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D2C40;
	sub_825D5398(ctx, base);
loc_825D2C40:
	// addi r11,r29,1
	ctx.r11.s64 = ctx.r29.s64 + 1;
	// lwz r10,288(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 288);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r11,15304(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15304, ctx.r11.u32);
	// bne cr6,0x825d2f40
	if (!ctx.cr6.eq) goto loc_825D2F40;
	// lwz r4,180(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
	// lwz r11,188(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// lwz r9,200(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// srawi r8,r4,4
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0xF) != 0);
	ctx.r8.s64 = ctx.r4.s32 >> 4;
	// srawi r30,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r11.s32 >> 4;
	// lwz r10,192(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,160(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	// stw r23,1944(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1944, ctx.r23.u32);
	// stw r27,220(r31)
	PPC_STORE_U32(ctx.r31.u32 + 220, ctx.r27.u32);
	// stw r9,216(r31)
	PPC_STORE_U32(ctx.r31.u32 + 216, ctx.r9.u32);
	// rlwinm r9,r4,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r8,136(r31)
	PPC_STORE_U32(ctx.r31.u32 + 136, ctx.r8.u32);
	// rlwinm r8,r4,4,0,27
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r30,140(r31)
	PPC_STORE_U32(ctx.r31.u32 + 140, ctx.r30.u32);
	// addi r9,r9,-8
	ctx.r9.s64 = ctx.r9.s64 + -8;
	// rlwinm r30,r10,3,0,28
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r27,224(r31)
	PPC_STORE_U32(ctx.r31.u32 + 224, ctx.r27.u32);
	// stw r4,204(r31)
	PPC_STORE_U32(ctx.r31.u32 + 204, ctx.r4.u32);
	// stw r11,212(r31)
	PPC_STORE_U32(ctx.r31.u32 + 212, ctx.r11.u32);
	// stw r10,208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 208, ctx.r10.u32);
	// stw r9,236(r31)
	PPC_STORE_U32(ctx.r31.u32 + 236, ctx.r9.u32);
	// stw r8,228(r31)
	PPC_STORE_U32(ctx.r31.u32 + 228, ctx.r8.u32);
	// stw r30,232(r31)
	PPC_STORE_U32(ctx.r31.u32 + 232, ctx.r30.u32);
	// bl 0x8262bfc0
	ctx.lr = 0x825D2CC0;
	sub_8262BFC0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d314c
	if (!ctx.cr6.eq) goto loc_825D314C;
	// addi r11,r25,15
	ctx.r11.s64 = ctx.r25.s64 + 15;
	// stw r25,15308(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15308, ctx.r25.u32);
	// addi r10,r24,15
	ctx.r10.s64 = ctx.r24.s64 + 15;
	// stw r24,15312(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15312, ctx.r24.u32);
	// srawi r11,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 4;
	// stw r23,3356(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3356, ctx.r23.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// srawi r10,r10,4
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 4;
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r11,15316(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15316, ctx.r11.u32);
	// stw r10,15320(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15320, ctx.r10.u32);
	// bl 0x82632600
	ctx.lr = 0x825D2D04;
	sub_82632600(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d314c
	if (!ctx.cr6.eq) goto loc_825D314C;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// blt cr6,0x825d2d44
	if (ctx.cr6.lt) goto loc_825D2D44;
	// li r11,8
	ctx.r11.s64 = 8;
	// stw r27,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r27.u32);
	// addi r10,r31,2640
	ctx.r10.s64 = ctx.r31.s64 + 2640;
	// addi r9,r31,2680
	ctx.r9.s64 = ctx.r31.s64 + 2680;
	// stw r11,344(r31)
	PPC_STORE_U32(ctx.r31.u32 + 344, ctx.r11.u32);
	// stw r10,2904(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2904, ctx.r10.u32);
	// stw r9,2916(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2916, ctx.r9.u32);
	// stw r11,348(r31)
	PPC_STORE_U32(ctx.r31.u32 + 348, ctx.r11.u32);
	// stw r11,20004(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20004, ctx.r11.u32);
	// stw r11,14804(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14804, ctx.r11.u32);
	// stw r11,20940(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20940, ctx.r11.u32);
loc_825D2D44:
	// lwz r11,15396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15396);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d2d64
	if (!ctx.cr6.eq) goto loc_825D2D64;
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r4,r11,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// bl 0x825deb78
	ctx.lr = 0x825D2D64;
	sub_825DEB78(ctx, base);
loc_825D2D64:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// beq cr6,0x825d2d90
	if (ctx.cr6.eq) goto loc_825D2D90;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e1840
	ctx.lr = 0x825D2D78;
	sub_825E1840(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825d2e38
	if (ctx.cr6.eq) goto loc_825D2E38;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D2D90:
	// lwz r11,21160(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21160);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d2a1c
	if (!ctx.cr6.eq) goto loc_825D2A1C;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// stw r27,19980(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19980, ctx.r27.u32);
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// stw r27,19976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19976, ctx.r27.u32);
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// lis r7,-32139
	ctx.r7.s64 = -2106261504;
	// lis r6,-32139
	ctx.r6.s64 = -2106261504;
	// lis r5,-32139
	ctx.r5.s64 = -2106261504;
	// lis r4,-32139
	ctx.r4.s64 = -2106261504;
	// addi r11,r11,5080
	ctx.r11.s64 = ctx.r11.s64 + 5080;
	// addi r10,r10,5008
	ctx.r10.s64 = ctx.r10.s64 + 5008;
	// addi r9,r9,4436
	ctx.r9.s64 = ctx.r9.s64 + 4436;
	// addi r8,r8,5180
	ctx.r8.s64 = ctx.r8.s64 + 5180;
	// addi r7,r7,5144
	ctx.r7.s64 = ctx.r7.s64 + 5144;
	// addi r6,r6,5216
	ctx.r6.s64 = ctx.r6.s64 + 5216;
	// stw r11,1824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1824, ctx.r11.u32);
	// addi r5,r5,5252
	ctx.r5.s64 = ctx.r5.s64 + 5252;
	// stw r10,1828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1828, ctx.r10.u32);
	// addi r4,r4,5272
	ctx.r4.s64 = ctx.r4.s64 + 5272;
	// stw r9,1836(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1836, ctx.r9.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r8,1840(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1840, ctx.r8.u32);
	// stw r7,1844(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1844, ctx.r7.u32);
	// stw r6,1848(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1848, ctx.r6.u32);
	// stw r5,1864(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1864, ctx.r5.u32);
	// stw r4,1868(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1868, ctx.r4.u32);
	// bl 0x825dcb80
	ctx.lr = 0x825D2E0C;
	sub_825DCB80(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r27,19976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19976, ctx.r27.u32);
	// stw r27,19984(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19984, ctx.r27.u32);
	// bl 0x825ed198
	ctx.lr = 0x825D2E1C;
	sub_825ED198(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825dfb48
	ctx.lr = 0x825D2E24;
	sub_825DFB48(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d314c
	if (!ctx.cr6.eq) goto loc_825D314C;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// beq cr6,0x825d2a1c
	if (ctx.cr6.eq) goto loc_825D2A1C;
loc_825D2E38:
	// clrlwi r11,r22,28
	ctx.r11.u64 = ctx.r22.u32 & 0xF;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2e68
	if (ctx.cr6.eq) goto loc_825D2E68;
	// rlwinm r11,r22,16,0,15
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r22.u32 | (ctx.r22.u64 << 32), 16) & 0xFFFF0000;
	// srawi r11,r11,28
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xFFFFFFF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 28;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// beq cr6,0x825d2e68
	if (ctx.cr6.eq) goto loc_825D2E68;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d2e68
	if (ctx.cr6.eq) goto loc_825D2E68;
	// li r11,2
	ctx.r11.s64 = 2;
	// stw r11,15548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15548, ctx.r11.u32);
	// b 0x825d2e6c
	goto loc_825D2E6C;
loc_825D2E68:
	// stw r27,15548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15548, ctx.r27.u32);
loc_825D2E6C:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// beq cr6,0x825d2e8c
	if (ctx.cr6.eq) goto loc_825D2E8C;
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825d2edc
	if (!ctx.cr6.eq) goto loc_825D2EDC;
	// lwz r11,19976(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d2edc
	if (!ctx.cr6.eq) goto loc_825D2EDC;
loc_825D2E8C:
	// lwz r11,3372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3372);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2ed0
	if (ctx.cr6.eq) goto loc_825D2ED0;
	// lis r11,-32158
	ctx.r11.s64 = -2107506688;
	// lwz r10,3164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3164);
	// addi r11,r11,-20584
	ctx.r11.s64 = ctx.r11.s64 + -20584;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825d2ed0
	if (ctx.cr6.eq) goto loc_825D2ED0;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// beq cr6,0x825d2ed0
	if (ctx.cr6.eq) goto loc_825D2ED0;
	// lwz r11,3364(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3364);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x825d2ed0
	if (ctx.cr6.gt) goto loc_825D2ED0;
	// lis r11,-32144
	ctx.r11.s64 = -2106589184;
	// addi r11,r11,16464
	ctx.r11.s64 = ctx.r11.s64 + 16464;
	// b 0x825d2ed8
	goto loc_825D2ED8;
loc_825D2ED0:
	// lis r11,-32162
	ctx.r11.s64 = -2107768832;
	// addi r11,r11,-10456
	ctx.r11.s64 = ctx.r11.s64 + -10456;
loc_825D2ED8:
	// stw r11,15772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15772, ctx.r11.u32);
loc_825D2EDC:
	// lwz r11,15772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15772);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x825D2EEC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,21552(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21552);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// lwz r10,3688(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// stw r11,608(r10)
	PPC_STORE_U32(ctx.r10.u32 + 608, ctx.r11.u32);
	// bne cr6,0x825d314c
	if (!ctx.cr6.eq) goto loc_825D314C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f6a28
	ctx.lr = 0x825D2F08;
	sub_825F6A28(ctx, base);
	// lwz r11,15508(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15508);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2f38
	if (ctx.cr6.eq) goto loc_825D2F38;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc830
	ctx.lr = 0x825D2F1C;
	sub_825CC830(ctx, base);
	// li r6,1
	ctx.r6.s64 = 1;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r5,3812(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3812);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f65f8
	ctx.lr = 0x825D2F30;
	sub_825F65F8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc890
	ctx.lr = 0x825D2F38;
	sub_825CC890(ctx, base);
loc_825D2F38:
	// stw r23,3668(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3668, ctx.r23.u32);
	// b 0x825d2f78
	goto loc_825D2F78;
loc_825D2F40:
	// lwz r11,3668(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3668);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2964
	if (ctx.cr6.eq) goto loc_825D2964;
	// li r9,2
	ctx.r9.s64 = 2;
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21552);
	// rlwinm r11,r22,16,0,15
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r22.u32 | (ctx.r22.u64 << 32), 16) & 0xFFFF0000;
	// srawi r11,r11,28
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xFFFFFFF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 28;
	// stw r9,284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 284, ctx.r9.u32);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// lwz r9,3688(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// stw r10,608(r9)
	PPC_STORE_U32(ctx.r9.u32 + 608, ctx.r10.u32);
	// beq cr6,0x825d3140
	if (ctx.cr6.eq) goto loc_825D3140;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d3140
	if (ctx.cr6.eq) goto loc_825D3140;
loc_825D2F78:
	// lwz r11,15304(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15304);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x825d2f8c
	if (ctx.cr6.eq) goto loc_825D2F8C;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825d2964
	if (!ctx.cr6.eq) goto loc_825D2964;
loc_825D2F8C:
	// lwz r11,15368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15368);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d2fcc
	if (ctx.cr6.eq) goto loc_825D2FCC;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x82631aa8
	ctx.lr = 0x825D2FA4;
	sub_82631AA8(ctx, base);
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// lwz r10,3704(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// lwz r11,608(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 608);
	// stw r11,608(r10)
	PPC_STORE_U32(ctx.r10.u32 + 608, ctx.r11.u32);
	// beq cr6,0x825d312c
	if (ctx.cr6.eq) goto loc_825D312C;
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D2FCC:
	// addi r10,r1,140
	ctx.r10.s64 = ctx.r1.s64 + 140;
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// addi r8,r1,156
	ctx.r8.s64 = ctx.r1.s64 + 156;
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// addi r6,r1,148
	ctx.r6.s64 = ctx.r1.s64 + 148;
	// addi r5,r1,164
	ctx.r5.s64 = ctx.r1.s64 + 164;
	// addi r4,r1,152
	ctx.r4.s64 = ctx.r1.s64 + 152;
	// bl 0x825cc9d0
	ctx.lr = 0x825D2FEC;
	sub_825CC9D0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d314c
	if (!ctx.cr6.eq) goto loc_825D314C;
	// lis r9,-32249
	ctx.r9.s64 = -2113470464;
	// lwz r11,15364(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15364);
	// lis r10,-32249
	ctx.r10.s64 = -2113470464;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lfd f31,-31520(r9)
	ctx.fpscr.disableFlushMode();
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r9.u32 + -31520);
	// lfd f30,-31512(r10)
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r10.u32 + -31512);
	// bne cr6,0x825d3024
	if (!ctx.cr6.eq) goto loc_825D3024;
	// fmr f5,f30
	ctx.f5.f64 = ctx.f30.f64;
	// fmr f4,f31
	ctx.f4.f64 = ctx.f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = ctx.f31.f64;
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// b 0x825d3050
	goto loc_825D3050;
loc_825D3024:
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d3040
	if (!ctx.cr6.eq) goto loc_825D3040;
	// lfs f5,152(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f5.f64 = double(temp.f32);
	// fmr f4,f31
	ctx.f4.f64 = ctx.f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = ctx.f31.f64;
	// fmr f1,f5
	ctx.f1.f64 = ctx.f5.f64;
	// b 0x825d3050
	goto loc_825D3050;
loc_825D3040:
	// lfs f5,156(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,164(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f1.f64 = double(temp.f32);
loc_825D3050:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lfs f7,140(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f6.f64 = double(temp.f32);
	// lwz r9,3784(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3784);
	// lfs f3,148(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f3.f64 = double(temp.f32);
	// lwz r8,3780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3780);
	// lwz r7,3776(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3776);
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// bl 0x8262c4a0
	ctx.lr = 0x825D307C;
	sub_8262C4A0(ctx, base);
	// lwz r11,15304(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15304);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825d312c
	if (!ctx.cr6.eq) goto loc_825D312C;
	// addi r10,r1,148
	ctx.r10.s64 = ctx.r1.s64 + 148;
	// addi r9,r1,144
	ctx.r9.s64 = ctx.r1.s64 + 144;
	// addi r8,r1,164
	ctx.r8.s64 = ctx.r1.s64 + 164;
	// addi r7,r1,160
	ctx.r7.s64 = ctx.r1.s64 + 160;
	// addi r6,r1,140
	ctx.r6.s64 = ctx.r1.s64 + 140;
	// addi r5,r1,156
	ctx.r5.s64 = ctx.r1.s64 + 156;
	// addi r4,r1,152
	ctx.r4.s64 = ctx.r1.s64 + 152;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc9d0
	ctx.lr = 0x825D30AC;
	sub_825CC9D0(ctx, base);
	// lwz r11,15364(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15364);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d30cc
	if (!ctx.cr6.eq) goto loc_825D30CC;
	// fmr f5,f30
	ctx.fpscr.disableFlushMode();
	ctx.f5.f64 = ctx.f30.f64;
	// fmr f4,f31
	ctx.f4.f64 = ctx.f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = ctx.f31.f64;
	// fmr f1,f30
	ctx.f1.f64 = ctx.f30.f64;
	// b 0x825d30f8
	goto loc_825D30F8;
loc_825D30CC:
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d30e8
	if (!ctx.cr6.eq) goto loc_825D30E8;
	// lfs f5,152(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f5.f64 = double(temp.f32);
	// fmr f4,f31
	ctx.f4.f64 = ctx.f31.f64;
	// fmr f2,f31
	ctx.f2.f64 = ctx.f31.f64;
	// fmr f1,f5
	ctx.f1.f64 = ctx.f5.f64;
	// b 0x825d30f8
	goto loc_825D30F8;
loc_825D30E8:
	// lfs f5,164(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 164);
	ctx.f5.f64 = double(temp.f32);
	// lfs f4,160(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	ctx.f4.f64 = double(temp.f32);
	// lfs f2,156(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	ctx.f2.f64 = double(temp.f32);
	// lfs f1,152(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	ctx.f1.f64 = double(temp.f32);
loc_825D30F8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lfs f7,148(r1)
	ctx.fpscr.disableFlushMode();
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 148);
	ctx.f7.f64 = double(temp.f32);
	// lfs f6,144(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	ctx.f6.f64 = double(temp.f32);
	// lwz r9,3796(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3796);
	// lfs f3,140(r1)
	temp.u32 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	ctx.f3.f64 = double(temp.f32);
	// lwz r8,3792(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3792);
	// lwz r7,3788(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3788);
	// lwz r6,3740(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// lwz r5,3736(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// bl 0x8262c4a0
	ctx.lr = 0x825D3124;
	sub_8262C4A0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8262b6c8
	ctx.lr = 0x825D312C;
	sub_8262B6C8(ctx, base);
loc_825D312C:
	// lwz r11,3704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// li r10,3
	ctx.r10.s64 = 3;
	// stw r11,3716(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3716, ctx.r11.u32);
	// stw r10,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r10.u32);
	// b 0x825d3144
	goto loc_825D3144;
loc_825D3140:
	// stw r23,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r23.u32);
loc_825D3144:
	// sth r23,3684(r31)
	PPC_STORE_U16(ctx.r31.u32 + 3684, ctx.r23.u16);
	// li r3,0
	ctx.r3.s64 = 0;
loc_825D314C:
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// lfd f30,-104(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -104);
	// lfd f31,-96(r1)
	ctx.f31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -96);
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
}

__attribute__((alias("__imp__sub_825D315C"))) PPC_WEAK_FUNC(sub_825D315C);
PPC_FUNC_IMPL(__imp__sub_825D315C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D3160"))) PPC_WEAK_FUNC(sub_825D3160);
PPC_FUNC_IMPL(__imp__sub_825D3160) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba14
	ctx.lr = 0x825D3168;
	sub_8239BA14(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r28,1
	ctx.r28.s64 = 1;
	// stw r28,21360(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21360, ctx.r28.u32);
	// bl 0x825ed050
	ctx.lr = 0x825D317C;
	sub_825ED050(ctx, base);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r30,156(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r29,160(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d0948
	ctx.lr = 0x825D3194;
	sub_825D0948(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d3254
	if (!ctx.cr6.eq) goto loc_825D3254;
	// lwz r4,156(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// lwz r11,21352(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21352);
	// cmpw cr6,r4,r11
	ctx.cr6.compare<int32_t>(ctx.r4.s32, ctx.r11.s32, ctx.xer);
	// bgt cr6,0x825d3250
	if (ctx.cr6.gt) goto loc_825D3250;
	// lwz r5,160(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	// lwz r11,21356(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21356);
	// cmpw cr6,r5,r11
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r11.s32, ctx.xer);
	// bgt cr6,0x825d3250
	if (ctx.cr6.gt) goto loc_825D3250;
	// li r27,0
	ctx.r27.s64 = 0;
	// cmpw cr6,r4,r30
	ctx.cr6.compare<int32_t>(ctx.r4.s32, ctx.r30.s32, ctx.xer);
	// bne cr6,0x825d31d8
	if (!ctx.cr6.eq) goto loc_825D31D8;
	// cmpw cr6,r5,r29
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r29.s32, ctx.xer);
	// bne cr6,0x825d31d8
	if (!ctx.cr6.eq) goto loc_825D31D8;
	// stw r27,21184(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21184, ctx.r27.u32);
	// b 0x825d31f0
	goto loc_825D31F0;
loc_825D31D8:
	// addis r11,r31,2
	ctx.r11.s64 = ctx.r31.s64 + 131072;
	// stw r28,21184(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21184, ctx.r28.u32);
	// addi r11,r11,-31828
	ctx.r11.s64 = ctx.r11.s64 + -31828;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
loc_825D31F0:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f0508
	ctx.lr = 0x825D31F8;
	sub_825F0508(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d325c
	if (!ctx.cr6.eq) goto loc_825D325C;
	// lwz r11,21288(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21288);
	// stw r28,21360(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21360, ctx.r28.u32);
	// stw r28,3672(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3672, ctx.r28.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d3234
	if (!ctx.cr6.gt) goto loc_825D3234;
	// lwz r10,21280(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21280);
	// twllei r11,0
	// stw r27,21288(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21288, ctx.r27.u32);
	// divwu r11,r10,r11
	ctx.r11.u32 = ctx.r10.u32 / ctx.r11.u32;
	// stw r27,21280(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21280, ctx.r27.u32);
	// stw r11,21284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21284, ctx.r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
loc_825D3234:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// stw r27,21288(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21288, ctx.r27.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r27,21280(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21280, ctx.r27.u32);
	// stw r11,21284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21284, ctx.r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
loc_825D3250:
	// li r3,4
	ctx.r3.s64 = 4;
loc_825D3254:
	// stw r29,160(r31)
	PPC_STORE_U32(ctx.r31.u32 + 160, ctx.r29.u32);
	// stw r30,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r30.u32);
loc_825D325C:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
}

__attribute__((alias("__imp__sub_825D3264"))) PPC_WEAK_FUNC(sub_825D3264);
PPC_FUNC_IMPL(__imp__sub_825D3264) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D3268"))) PPC_WEAK_FUNC(sub_825D3268);
PPC_FUNC_IMPL(__imp__sub_825D3268) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba00
	ctx.lr = 0x825D3270;
	sub_8239BA00(ctx, base);
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r27,0
	ctx.r27.s64 = 0;
	// mr r23,r4
	ctx.r23.u64 = ctx.r4.u64;
	// li r24,1
	ctx.r24.s64 = 1;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r25,r5
	ctx.r25.u64 = ctx.r5.u64;
	// mr r22,r27
	ctx.r22.u64 = ctx.r27.u64;
	// stw r27,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r27.u32);
	// stw r27,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r27.u32);
	// mr r28,r27
	ctx.r28.u64 = ctx.r27.u64;
	// stw r24,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r24.u32);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// sth r27,0(r23)
	PPC_STORE_U16(ctx.r23.u32 + 0, ctx.r27.u16);
	// bne cr6,0x825d32b4
	if (!ctx.cr6.eq) goto loc_825D32B4;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D32B4:
	// lwz r11,15552(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15552);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// ble cr6,0x825d32cc
	if (!ctx.cr6.gt) goto loc_825D32CC;
	// lwz r11,3420(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3420);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,3420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3420, ctx.r11.u32);
loc_825D32CC:
	// lhz r11,3684(r31)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r31.u32 + 3684);
	// stw r27,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r27.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d32fc
	if (ctx.cr6.eq) goto loc_825D32FC;
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r7,15504(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15504);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r6,15496(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15496);
	// lhz r5,15492(r31)
	ctx.r5.u64 = PPC_LOAD_U16(ctx.r31.u32 + 15492);
	// lwz r4,15488(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15488);
	// bl 0x825d1a48
	ctx.lr = 0x825D32F8;
	sub_825D1A48(ctx, base);
	// sth r27,3684(r31)
	PPC_STORE_U16(ctx.r31.u32 + 3684, ctx.r27.u16);
loc_825D32FC:
	// lwz r11,15556(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15556);
	// rlwinm r11,r11,28,0,3
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 28) & 0xF0000000;
	// srawi. r11,r11,28
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xFFFFFFF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 28;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// blt 0x825d3318
	if (ctx.cr0.lt) goto loc_825D3318;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bgt cr6,0x825d3318
	if (ctx.cr6.gt) goto loc_825D3318;
	// stw r11,3644(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3644, ctx.r11.u32);
loc_825D3318:
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// lwz r3,3340(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// bl 0x8248c788
	ctx.lr = 0x825D3338;
	sub_8248C788(ctx, base);
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x825d3460
	if (ctx.cr6.eq) goto loc_825D3460;
loc_825D3344:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825d339c
	if (!ctx.cr6.eq) goto loc_825D339C;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r29,r30,r5
	ctx.r29.u64 = ctx.r30.u64 + ctx.r5.u64;
	// cmplwi cr6,r29,64
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 64, ctx.xer);
	// bge cr6,0x825d33a0
	if (!ctx.cr6.lt) goto loc_825D33A0;
	// addi r11,r1,128
	ctx.r11.s64 = ctx.r1.s64 + 128;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// add r3,r30,r11
	ctx.r3.u64 = ctx.r30.u64 + ctx.r11.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825D3370;
	sub_8239CB70(ctx, base);
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// lwz r3,3340(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,116
	ctx.r5.s64 = ctx.r1.s64 + 116;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r30,r29
	ctx.r30.u64 = ctx.r29.u64;
	// bl 0x8248c788
	ctx.lr = 0x825D3390;
	sub_8248C788(ctx, base);
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// bne cr6,0x825d3344
	if (!ctx.cr6.eq) goto loc_825D3344;
loc_825D339C:
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
loc_825D33A0:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825d3430
	if (ctx.cr6.eq) goto loc_825D3430;
	// lwz r11,23248(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23248);
	// add r29,r30,r5
	ctx.r29.u64 = ctx.r30.u64 + ctx.r5.u64;
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x825d33f4
	if (!ctx.cr6.gt) goto loc_825D33F4;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d33c8
	if (ctx.cr6.eq) goto loc_825D33C8;
	// lwz r3,23252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23252);
	// bl 0x825edb28
	ctx.lr = 0x825D33C8;
	sub_825EDB28(ctx, base);
loc_825D33C8:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x825edb18
	ctx.lr = 0x825D33D4;
	sub_825EDB18(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,23252(r31)
	PPC_STORE_U32(ctx.r31.u32 + 23252, ctx.r3.u32);
	// bne cr6,0x825d33f0
	if (!ctx.cr6.eq) goto loc_825D33F0;
	// li r3,2
	ctx.r3.s64 = 2;
	// stw r27,23248(r31)
	PPC_STORE_U32(ctx.r31.u32 + 23248, ctx.r27.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D33F0:
	// stw r29,23248(r31)
	PPC_STORE_U32(ctx.r31.u32 + 23248, ctx.r29.u32);
loc_825D33F4:
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r3,23252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23252);
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// bl 0x8239cb70
	ctx.lr = 0x825D3404;
	sub_8239CB70(ctx, base);
	// lwz r11,23252(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23252);
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r3,r11,r30
	ctx.r3.u64 = ctx.r11.u64 + ctx.r30.u64;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// bl 0x8239cb70
	ctx.lr = 0x825D3418;
	sub_8239CB70(ctx, base);
	// lwz r11,23252(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23252);
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r11.u32);
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// add r5,r30,r11
	ctx.r5.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stw r5,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r5.u32);
loc_825D3430:
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x825d3464
	if (ctx.cr6.eq) goto loc_825D3464;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// beq cr6,0x825d344c
	if (ctx.cr6.eq) goto loc_825D344C;
	// lwz r11,116(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825d3464
	if (!ctx.cr6.eq) goto loc_825D3464;
loc_825D344C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ce570
	ctx.lr = 0x825D3454;
	sub_825CE570(ctx, base);
	// li r3,11
	ctx.r3.s64 = 11;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D3460:
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
loc_825D3464:
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// bne cr6,0x825d3498
	if (!ctx.cr6.eq) goto loc_825D3498;
loc_825D346C:
	// lwz r11,3668(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3668);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d3484
	if (!ctx.cr6.eq) goto loc_825D3484;
loc_825D3478:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D3484:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r24,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r24.u32);
	// stw r24,3424(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3424, ctx.r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D3498:
	// cmplwi cr6,r5,1
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 1, ctx.xer);
	// bne cr6,0x825d34f8
	if (!ctx.cr6.eq) goto loc_825D34F8;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// bne cr6,0x825d34f8
	if (!ctx.cr6.eq) goto loc_825D34F8;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// blt cr6,0x825d3478
	if (ctx.cr6.lt) goto loc_825D3478;
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// beq cr6,0x825d34f8
	if (ctx.cr6.eq) goto loc_825D34F8;
	// lwz r11,3668(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3668);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3478
	if (ctx.cr6.eq) goto loc_825D3478;
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d34e4
	if (ctx.cr6.eq) goto loc_825D34E4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825dd4d8
	ctx.lr = 0x825D34DC;
	sub_825DD4D8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d4278
	if (!ctx.cr6.eq) goto loc_825D4278;
loc_825D34E4:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r24,3424(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3424, ctx.r24.u32);
	// stw r24,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D34F8:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825d3548
	if (!ctx.cr6.eq) goto loc_825D3548;
	// addi r8,r1,120
	ctx.r8.s64 = ctx.r1.s64 + 120;
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// addi r6,r1,116
	ctx.r6.s64 = ctx.r1.s64 + 116;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e7f70
	ctx.lr = 0x825D351C;
	sub_825E7F70(ctx, base);
	// cmpwi cr6,r3,4
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 4, ctx.xer);
	// bne cr6,0x825d352c
	if (!ctx.cr6.eq) goto loc_825D352C;
	// li r22,4
	ctx.r22.s64 = 4;
	// b 0x825d3534
	goto loc_825D3534;
loc_825D352C:
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d4278
	if (!ctx.cr6.eq) goto loc_825D4278;
loc_825D3534:
	// lwz r11,3672(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3672);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3ce8
	if (ctx.cr6.eq) goto loc_825D3CE8;
	// lwz r5,112(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r6,120(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
loc_825D3548:
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// bne cr6,0x825d3558
	if (!ctx.cr6.eq) goto loc_825D3558;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x825d346c
	if (ctx.cr6.eq) goto loc_825D346C;
loc_825D3558:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// lwz r4,116(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r11,r11,-7
	ctx.r11.s64 = ctx.r11.s64 + -7;
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// bl 0x825d5100
	ctx.lr = 0x825D3574;
	sub_825D5100(ctx, base);
	// lwz r11,80(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r10,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r10.u32);
	// lwz r11,3676(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3676);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d3ce8
	if (!ctx.cr6.eq) goto loc_825D3CE8;
	// ld r10,3576(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 3576);
	// ld r11,3584(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 3584);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r9,3452(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3452);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r27,21272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21272, ctx.r27.u32);
	// stw r27,3424(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3424, ctx.r27.u32);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stw r27,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r27.u32);
	// stw r27,3408(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3408, ctx.r27.u32);
	// std r10,3576(r31)
	PPC_STORE_U64(ctx.r31.u32 + 3576, ctx.r10.u64);
	// std r11,3584(r31)
	PPC_STORE_U64(ctx.r31.u32 + 3584, ctx.r11.u64);
	// bne cr6,0x825d35cc
	if (!ctx.cr6.eq) goto loc_825D35CC;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// stw r11,292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 292, ctx.r11.u32);
	// b 0x825d35d0
	goto loc_825D35D0;
loc_825D35CC:
	// stw r27,3452(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3452, ctx.r27.u32);
loc_825D35D0:
	// lwz r8,15472(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r8,6
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 6, ctx.xer);
	// beq cr6,0x825d35f0
	if (ctx.cr6.eq) goto loc_825D35F0;
	// cmpwi cr6,r8,7
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 7, ctx.xer);
	// bne cr6,0x825d3674
	if (!ctx.cr6.eq) goto loc_825D3674;
	// lwz r11,19976(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d3674
	if (!ctx.cr6.eq) goto loc_825D3674;
loc_825D35F0:
	// lwz r11,3372(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3372);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3650
	if (ctx.cr6.eq) goto loc_825D3650;
	// lis r11,-32158
	ctx.r11.s64 = -2107506688;
	// lwz r10,3164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3164);
	// addi r11,r11,-20584
	ctx.r11.s64 = ctx.r11.s64 + -20584;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825d3650
	if (ctx.cr6.eq) goto loc_825D3650;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// beq cr6,0x825d3650
	if (ctx.cr6.eq) goto loc_825D3650;
	// lwz r11,3364(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3364);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x825d3650
	if (ctx.cr6.gt) goto loc_825D3650;
	// lis r9,-32160
	ctx.r9.s64 = -2107637760;
	// lis r10,-32143
	ctx.r10.s64 = -2106523648;
	// lis r11,-32144
	ctx.r11.s64 = -2106589184;
	// addi r9,r9,-1032
	ctx.r9.s64 = ctx.r9.s64 + -1032;
	// addi r10,r10,-29264
	ctx.r10.s64 = ctx.r10.s64 + -29264;
	// addi r11,r11,16464
	ctx.r11.s64 = ctx.r11.s64 + 16464;
	// stw r9,15776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15776, ctx.r9.u32);
	// stw r10,3056(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3056, ctx.r10.u32);
	// stw r11,15772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15772, ctx.r11.u32);
	// b 0x825d3674
	goto loc_825D3674;
loc_825D3650:
	// lis r9,-32162
	ctx.r9.s64 = -2107768832;
	// lis r10,-32158
	ctx.r10.s64 = -2107506688;
	// lis r11,-32159
	ctx.r11.s64 = -2107572224;
	// addi r9,r9,-10456
	ctx.r9.s64 = ctx.r9.s64 + -10456;
	// addi r10,r10,-25288
	ctx.r10.s64 = ctx.r10.s64 + -25288;
	// addi r11,r11,5008
	ctx.r11.s64 = ctx.r11.s64 + 5008;
	// stw r9,15772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15772, ctx.r9.u32);
	// stw r10,15776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15776, ctx.r10.u32);
	// stw r11,3056(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3056, ctx.r11.u32);
loc_825D3674:
	// cmpwi cr6,r8,6
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 6, ctx.xer);
	// blt cr6,0x825d36a8
	if (ctx.cr6.lt) goto loc_825D36A8;
	// li r11,8
	ctx.r11.s64 = 8;
	// stw r27,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r27.u32);
	// addi r10,r31,2640
	ctx.r10.s64 = ctx.r31.s64 + 2640;
	// addi r9,r31,2680
	ctx.r9.s64 = ctx.r31.s64 + 2680;
	// stw r11,344(r31)
	PPC_STORE_U32(ctx.r31.u32 + 344, ctx.r11.u32);
	// stw r10,2904(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2904, ctx.r10.u32);
	// stw r9,2916(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2916, ctx.r9.u32);
	// stw r11,348(r31)
	PPC_STORE_U32(ctx.r31.u32 + 348, ctx.r11.u32);
	// stw r11,20004(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20004, ctx.r11.u32);
	// stw r11,14804(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14804, ctx.r11.u32);
	// stw r11,20940(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20940, ctx.r11.u32);
loc_825D36A8:
	// cmpwi cr6,r8,7
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 7, ctx.xer);
	// beq cr6,0x825d36d8
	if (ctx.cr6.eq) goto loc_825D36D8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e1840
	ctx.lr = 0x825D36B8;
	sub_825E1840(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825d3c94
	if (ctx.cr6.eq) goto loc_825D3C94;
loc_825D36C4:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ce570
	ctx.lr = 0x825D36CC;
	sub_825CE570(ctx, base);
loc_825D36CC:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D36D8:
	// lwz r11,21160(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21160);
	// li r26,2
	ctx.r26.s64 = 2;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3868
	if (ctx.cr6.eq) goto loc_825D3868;
	// lwz r30,84(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// mr r28,r27
	ctx.r28.u64 = ctx.r27.u64;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d375c
	if (!ctx.cr6.lt) goto loc_825D375C;
loc_825D3704:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d375c
	if (ctx.cr6.eq) goto loc_825D375C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825d374c
	if (!ctx.cr0.lt) goto loc_825D374C;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D374C;
	sub_825D5398(ctx, base);
loc_825D374C:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d3704
	if (ctx.cr6.gt) goto loc_825D3704;
loc_825D375C:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r8.u64);
	// bge 0x825d3798
	if (!ctx.cr0.lt) goto loc_825D3798;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D3798;
	sub_825D5398(ctx, base);
loc_825D3798:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x825d3868
	if (ctx.cr6.eq) goto loc_825D3868;
	// lwz r30,84(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// mr r28,r27
	ctx.r28.u64 = ctx.r27.u64;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d3814
	if (!ctx.cr6.lt) goto loc_825D3814;
loc_825D37BC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d3814
	if (ctx.cr6.eq) goto loc_825D3814;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825d3804
	if (!ctx.cr0.lt) goto loc_825D3804;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D3804;
	sub_825D5398(ctx, base);
loc_825D3804:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d37bc
	if (ctx.cr6.gt) goto loc_825D37BC;
loc_825D3814:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r8.u64);
	// bge 0x825d3850
	if (!ctx.cr0.lt) goto loc_825D3850;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D3850;
	sub_825D5398(ctx, base);
loc_825D3850:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825d3860
	if (!ctx.cr6.eq) goto loc_825D3860;
	// stw r26,21436(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21436, ctx.r26.u32);
	// b 0x825d386c
	goto loc_825D386C;
loc_825D3860:
	// stw r24,21436(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21436, ctx.r24.u32);
	// b 0x825d386c
	goto loc_825D386C;
loc_825D3868:
	// stw r27,21436(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21436, ctx.r27.u32);
loc_825D386C:
	// lwz r11,21436(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21436);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r27,19980(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19980, ctx.r27.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r27,19976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19976, ctx.r27.u32);
	// bne cr6,0x825d3aa4
	if (!ctx.cr6.eq) goto loc_825D3AA4;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// lis r7,-32139
	ctx.r7.s64 = -2106261504;
	// lis r6,-32139
	ctx.r6.s64 = -2106261504;
	// lis r5,-32139
	ctx.r5.s64 = -2106261504;
	// lis r4,-32139
	ctx.r4.s64 = -2106261504;
	// addi r11,r11,5080
	ctx.r11.s64 = ctx.r11.s64 + 5080;
	// addi r10,r10,5008
	ctx.r10.s64 = ctx.r10.s64 + 5008;
	// addi r9,r9,4436
	ctx.r9.s64 = ctx.r9.s64 + 4436;
	// addi r8,r8,5180
	ctx.r8.s64 = ctx.r8.s64 + 5180;
	// addi r7,r7,5144
	ctx.r7.s64 = ctx.r7.s64 + 5144;
	// addi r6,r6,5216
	ctx.r6.s64 = ctx.r6.s64 + 5216;
	// stw r11,1824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1824, ctx.r11.u32);
	// addi r5,r5,5252
	ctx.r5.s64 = ctx.r5.s64 + 5252;
	// stw r10,1828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1828, ctx.r10.u32);
	// addi r4,r4,5272
	ctx.r4.s64 = ctx.r4.s64 + 5272;
	// stw r9,1836(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1836, ctx.r9.u32);
	// stw r8,1840(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1840, ctx.r8.u32);
	// stw r7,1844(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1844, ctx.r7.u32);
	// stw r6,1848(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1848, ctx.r6.u32);
	// stw r5,1864(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1864, ctx.r5.u32);
	// stw r4,1868(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1868, ctx.r4.u32);
	// bl 0x825dcb80
	ctx.lr = 0x825D38E8;
	sub_825DCB80(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r27,19976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19976, ctx.r27.u32);
	// mr r28,r27
	ctx.r28.u64 = ctx.r27.u64;
	// stw r27,19980(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19980, ctx.r27.u32);
	// stw r27,19984(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19984, ctx.r27.u32);
	// bl 0x825ed198
	ctx.lr = 0x825D3900;
	sub_825ED198(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825dfb48
	ctx.lr = 0x825D3908;
	sub_825DFB48(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825d36c4
	if (!ctx.cr6.eq) goto loc_825D36C4;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825d3934
	if (ctx.cr6.eq) goto loc_825D3934;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d3934
	if (ctx.cr6.eq) goto loc_825D3934;
	// lwz r10,21516(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21516);
	// stw r27,21516(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21516, ctx.r27.u32);
	// stw r10,21520(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21520, ctx.r10.u32);
loc_825D3934:
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// bne cr6,0x825d3c94
	if (!ctx.cr6.eq) goto loc_825D3C94;
	// lwz r11,21480(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21480);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3a80
	if (ctx.cr6.eq) goto loc_825D3A80;
	// lwz r11,14788(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14788);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3a80
	if (ctx.cr6.eq) goto loc_825D3A80;
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d3a80
	if (!ctx.cr6.gt) goto loc_825D3A80;
	// lwz r11,3376(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3376);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d3970
	if (!ctx.cr6.eq) goto loc_825D3970;
	// stw r27,3376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3376, ctx.r27.u32);
loc_825D3970:
	// lwz r11,3396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3396);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3988
	if (ctx.cr6.eq) goto loc_825D3988;
	// bl 0x82611280
	ctx.lr = 0x825D3984;
	sub_82611280(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
loc_825D3988:
	// bl 0x82611318
	ctx.lr = 0x825D398C;
	sub_82611318(ctx, base);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r11,212(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// bl 0x8239cb70
	ctx.lr = 0x825D39A4;
	sub_8239CB70(ctx, base);
	// lwz r11,216(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// mullw r5,r10,r11
	ctx.r5.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// bl 0x8239cb70
	ctx.lr = 0x825D39BC;
	sub_8239CB70(ctx, base);
	// lwz r10,216(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 216);
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239cb70
	ctx.lr = 0x825D39D4;
	sub_8239CB70(ctx, base);
	// lwz r11,144(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// lwz r3,1772(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r5,r11,4,0,27
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x8239ca70
	ctx.lr = 0x825D39E8;
	sub_8239CA70(ctx, base);
	// lwz r11,144(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,1780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1780);
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239ca70
	ctx.lr = 0x825D39FC;
	sub_8239CA70(ctx, base);
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// lwz r10,276(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 276);
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d3a80
	if (!ctx.cr6.gt) goto loc_825D3A80;
loc_825D3A10:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x825d3a70
	if (!ctx.cr6.gt) goto loc_825D3A70;
loc_825D3A20:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r6,3048(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3048);
	// mullw r9,r9,r7
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r7.s32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r5,r9,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// sthx r27,r8,r6
	PPC_STORE_U16(ctx.r8.u32 + ctx.r6.u32, ctx.r27.u16);
	// lwz r9,3048(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3048);
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// sth r27,2(r9)
	PPC_STORE_U16(ctx.r9.u32 + 2, ctx.r27.u16);
	// lwz r9,3052(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3052);
	// stwx r27,r5,r9
	PPC_STORE_U32(ctx.r5.u32 + ctx.r9.u32, ctx.r27.u32);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// oris r9,r9,2
	ctx.r9.u64 = ctx.r9.u64 | 131072;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x825d3a20
	if (ctx.cr6.lt) goto loc_825D3A20;
loc_825D3A70:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// cmpw cr6,r7,r11
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825d3a10
	if (ctx.cr6.lt) goto loc_825D3A10;
loc_825D3A80:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,19984(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19984);
	// bl 0x82604db0
	ctx.lr = 0x825D3A8C;
	sub_82604DB0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ce4b8
	ctx.lr = 0x825D3A94;
	sub_825CE4B8(ctx, base);
	// stw r24,3424(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3424, ctx.r24.u32);
	// stw r24,21388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21388, ctx.r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D3AA4:
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// lis r7,-32139
	ctx.r7.s64 = -2106261504;
	// lis r6,-32139
	ctx.r6.s64 = -2106261504;
	// lis r5,-32139
	ctx.r5.s64 = -2106261504;
	// lis r4,-32139
	ctx.r4.s64 = -2106261504;
	// addi r11,r11,5368
	ctx.r11.s64 = ctx.r11.s64 + 5368;
	// addi r10,r10,5296
	ctx.r10.s64 = ctx.r10.s64 + 5296;
	// addi r9,r9,5432
	ctx.r9.s64 = ctx.r9.s64 + 5432;
	// addi r8,r8,5504
	ctx.r8.s64 = ctx.r8.s64 + 5504;
	// addi r7,r7,5468
	ctx.r7.s64 = ctx.r7.s64 + 5468;
	// addi r6,r6,5540
	ctx.r6.s64 = ctx.r6.s64 + 5540;
	// stw r11,1824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1824, ctx.r11.u32);
	// addi r5,r5,5576
	ctx.r5.s64 = ctx.r5.s64 + 5576;
	// stw r10,1828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1828, ctx.r10.u32);
	// addi r4,r4,5596
	ctx.r4.s64 = ctx.r4.s64 + 5596;
	// stw r9,1836(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1836, ctx.r9.u32);
	// stw r8,1840(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1840, ctx.r8.u32);
	// stw r7,1844(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1844, ctx.r7.u32);
	// stw r6,1848(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1848, ctx.r6.u32);
	// stw r5,1864(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1864, ctx.r5.u32);
	// stw r4,1868(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1868, ctx.r4.u32);
	// bne cr6,0x825d44ac
	if (!ctx.cr6.eq) goto loc_825D44AC;
	// bl 0x825dcb80
	ctx.lr = 0x825D3B10;
	sub_825DCB80(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// stw r24,19976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19976, ctx.r24.u32);
	// stw r27,19984(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19984, ctx.r27.u32);
	// bl 0x825ed198
	ctx.lr = 0x825D3B24;
	sub_825ED198(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82607c68
	ctx.lr = 0x825D3B2C;
	sub_82607C68(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825d36c4
	if (!ctx.cr6.eq) goto loc_825D36C4;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825d3b58
	if (ctx.cr6.eq) goto loc_825D3B58;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d3b58
	if (ctx.cr6.eq) goto loc_825D3B58;
	// lwz r10,21516(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21516);
	// stw r26,21516(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21516, ctx.r26.u32);
	// stw r10,21520(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21520, ctx.r10.u32);
loc_825D3B58:
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// bne cr6,0x825d3c94
	if (!ctx.cr6.eq) goto loc_825D3C94;
	// lwz r11,21480(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21480);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3bf8
	if (ctx.cr6.eq) goto loc_825D3BF8;
	// lwz r11,14788(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14788);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3bf8
	if (ctx.cr6.eq) goto loc_825D3BF8;
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d3bf8
	if (!ctx.cr6.gt) goto loc_825D3BF8;
	// lwz r11,3376(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3376);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d3b94
	if (!ctx.cr6.eq) goto loc_825D3B94;
	// stw r27,3376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3376, ctx.r27.u32);
loc_825D3B94:
	// lwz r11,3396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3396);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3bac
	if (ctx.cr6.eq) goto loc_825D3BAC;
	// bl 0x82611280
	ctx.lr = 0x825D3BA8;
	sub_82611280(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
loc_825D3BAC:
	// bl 0x82611318
	ctx.lr = 0x825D3BB0;
	sub_82611318(ctx, base);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r11,212(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// bl 0x8239cb70
	ctx.lr = 0x825D3BC8;
	sub_8239CB70(ctx, base);
	// lwz r11,216(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 216);
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// mullw r5,r10,r11
	ctx.r5.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// bl 0x8239cb70
	ctx.lr = 0x825D3BE0;
	sub_8239CB70(ctx, base);
	// lwz r10,216(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 216);
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x8239cb70
	ctx.lr = 0x825D3BF8;
	sub_8239CB70(ctx, base);
loc_825D3BF8:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// mr r9,r27
	ctx.r9.u64 = ctx.r27.u64;
	// lwz r10,276(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 276);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x825d3c4c
	if (!ctx.cr6.gt) goto loc_825D3C4C;
loc_825D3C0C:
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// ble cr6,0x825d3c3c
	if (!ctx.cr6.gt) goto loc_825D3C3C;
loc_825D3C1C:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// oris r8,r8,2
	ctx.r8.u64 = ctx.r8.u64 | 131072;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,136(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmplw cr6,r11,r8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r8.u32, ctx.xer);
	// blt cr6,0x825d3c1c
	if (ctx.cr6.lt) goto loc_825D3C1C;
loc_825D3C3C:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// cmplw cr6,r9,r11
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x825d3c0c
	if (ctx.cr6.lt) goto loc_825D3C0C;
loc_825D3C4C:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r3,3052(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3052);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239ca70
	ctx.lr = 0x825D3C70;
	sub_8239CA70(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,19984(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19984);
	// bl 0x82604db0
	ctx.lr = 0x825D3C7C;
	sub_82604DB0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ce4b8
	ctx.lr = 0x825D3C84;
	sub_825CE4B8(ctx, base);
	// stw r24,3424(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3424, ctx.r24.u32);
	// stw r24,21388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21388, ctx.r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D3C94:
	// lwz r11,21580(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21580);
	// stw r24,1944(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1944, ctx.r24.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r27,20060(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20060, ctx.r27.u32);
	// beq cr6,0x825d3cb8
	if (ctx.cr6.eq) goto loc_825D3CB8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e91b0
	ctx.lr = 0x825D3CB0;
	sub_825E91B0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d4278
	if (!ctx.cr6.eq) goto loc_825D4278;
loc_825D3CB8:
	// lwz r10,284(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825d3cf4
	if (ctx.cr6.eq) goto loc_825D3CF4;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// beq cr6,0x825d3cf4
	if (ctx.cr6.eq) goto loc_825D3CF4;
	// lwz r11,3668(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3668);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d3df0
	if (!ctx.cr6.eq) goto loc_825D3DF0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ce4b8
	ctx.lr = 0x825D3CE0;
	sub_825CE4B8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d4278
	if (!ctx.cr6.eq) goto loc_825D4278;
loc_825D3CE8:
	// li r3,3
	ctx.r3.s64 = 3;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D3CF4:
	// lwz r11,14788(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14788);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3df0
	if (ctx.cr6.eq) goto loc_825D3DF0;
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d3de0
	if (!ctx.cr6.gt) goto loc_825D3DE0;
	// lwz r11,3376(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3376);
	// cmpwi cr6,r11,-3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, -3, ctx.xer);
	// bne cr6,0x825d3d50
	if (!ctx.cr6.eq) goto loc_825D3D50;
	// cmpwi cr6,r10,2
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 2, ctx.xer);
	// beq cr6,0x825d3478
	if (ctx.cr6.eq) goto loc_825D3478;
	// lwz r11,21368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21368);
	// lwz r10,3396(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3396);
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// stw r11,3376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3376, ctx.r11.u32);
	// beq cr6,0x825d3d44
	if (ctx.cr6.eq) goto loc_825D3D44;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82611280
	ctx.lr = 0x825D3D44;
	sub_82611280(ctx, base);
loc_825D3D44:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82611318
	ctx.lr = 0x825D3D4C;
	sub_82611318(ctx, base);
	// b 0x825d3df0
	goto loc_825D3DF0;
loc_825D3D50:
	// cmpwi cr6,r10,2
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 2, ctx.xer);
	// beq cr6,0x825d3d94
	if (ctx.cr6.eq) goto loc_825D3D94;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// beq cr6,0x825d3d94
	if (ctx.cr6.eq) goto loc_825D3D94;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d3d70
	if (!ctx.cr6.eq) goto loc_825D3D70;
	// stw r27,3376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3376, ctx.r27.u32);
	// b 0x825d3d7c
	goto loc_825D3D7C;
loc_825D3D70:
	// lwz r11,3396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3396);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3d84
	if (ctx.cr6.eq) goto loc_825D3D84;
loc_825D3D7C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82611280
	ctx.lr = 0x825D3D84;
	sub_82611280(ctx, base);
loc_825D3D84:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82611318
	ctx.lr = 0x825D3D8C;
	sub_82611318(ctx, base);
	// stw r27,3384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3384, ctx.r27.u32);
	// b 0x825d3df0
	goto loc_825D3DF0;
loc_825D3D94:
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d3dc8
	if (!ctx.cr6.eq) goto loc_825D3DC8;
	// lwz r11,21368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21368);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d3dc8
	if (!ctx.cr6.eq) goto loc_825D3DC8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r24,3408(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3408, ctx.r24.u32);
	// sth r24,3684(r31)
	PPC_STORE_U16(ctx.r31.u32 + 3684, ctx.r24.u16);
	// bl 0x825ce4b8
	ctx.lr = 0x825D3DB8;
	sub_825CE4B8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r24,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D3DC8:
	// lwz r11,3396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3396);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3df0
	if (ctx.cr6.eq) goto loc_825D3DF0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82611280
	ctx.lr = 0x825D3DDC;
	sub_82611280(ctx, base);
	// b 0x825d3df0
	goto loc_825D3DF0;
loc_825D3DE0:
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rlwinm r4,r11,27,31,31
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// bl 0x825deb78
	ctx.lr = 0x825D3DF0;
	sub_825DEB78(ctx, base);
loc_825D3DF0:
	// lwz r11,248(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 248);
	// lis r10,-32128
	ctx.r10.s64 = -2105540608;
	// stw r11,22288(r10)
	PPC_STORE_U32(ctx.r10.u32 + 22288, ctx.r11.u32);
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d4280
	if (ctx.cr6.eq) goto loc_825D4280;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d4280
	if (ctx.cr6.eq) goto loc_825D4280;
	// lis r10,-32128
	ctx.r10.s64 = -2105540608;
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,22284(r10)
	PPC_STORE_U32(ctx.r10.u32 + 22284, ctx.r11.u32);
	// bl 0x825d55d8
	ctx.lr = 0x825D3E24;
	sub_825D55D8(ctx, base);
	// lwz r11,14788(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14788);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3fe8
	if (ctx.cr6.eq) goto loc_825D3FE8;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x825d3fe8
	if (ctx.cr6.eq) goto loc_825D3FE8;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825d41a0
	if (!ctx.cr6.eq) goto loc_825D41A0;
	// lwz r11,3376(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3376);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d3e78
	if (!ctx.cr6.eq) goto loc_825D3E78;
	// lwz r11,21368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21368);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d3e78
	if (!ctx.cr6.eq) goto loc_825D3E78;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r24,3408(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3408, ctx.r24.u32);
	// sth r24,3684(r31)
	PPC_STORE_U16(ctx.r31.u32 + 3684, ctx.r24.u16);
	// bl 0x825ce4b8
	ctx.lr = 0x825D3E6C;
	sub_825CE4B8(ctx, base);
	// stw r24,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r24.u32);
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D3E78:
	// lwz r11,3384(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3384);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3ef8
	if (ctx.cr6.eq) goto loc_825D3EF8;
	// lwz r11,3396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3396);
	// stw r27,3384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3384, ctx.r27.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3e9c
	if (ctx.cr6.eq) goto loc_825D3E9C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82611280
	ctx.lr = 0x825D3E9C;
	sub_82611280(ctx, base);
loc_825D3E9C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82611318
	ctx.lr = 0x825D3EA4;
	sub_82611318(ctx, base);
	// lwz r11,212(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// mullw r5,r11,r10
	ctx.r5.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// lwz r3,3720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// bl 0x8239cb70
	ctx.lr = 0x825D3EBC;
	sub_8239CB70(ctx, base);
	// lwz r11,212(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// srawi r5,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r11.s32 >> 2;
	// bl 0x8239cb70
	ctx.lr = 0x825D3ED8;
	sub_8239CB70(ctx, base);
	// lwz r11,212(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 212);
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// srawi r5,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r11.s32 >> 2;
	// bl 0x8239cb70
	ctx.lr = 0x825D3EF4;
	sub_8239CB70(ctx, base);
	// stw r24,3396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3396, ctx.r24.u32);
loc_825D3EF8:
	// rlwinm r11,r25,16,0,15
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r25.u32 | (ctx.r25.u64 << 32), 16) & 0xFFFF0000;
	// srawi r11,r11,28
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xFFFFFFF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 28;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// beq cr6,0x825d3fb4
	if (ctx.cr6.eq) goto loc_825D3FB4;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d3fb4
	if (ctx.cr6.eq) goto loc_825D3FB4;
	// lwz r11,3396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3396);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3f24
	if (ctx.cr6.eq) goto loc_825D3F24;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82611280
	ctx.lr = 0x825D3F24;
	sub_82611280(ctx, base);
loc_825D3F24:
	// lwz r11,3924(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d3f68
	if (!ctx.cr6.eq) goto loc_825D3F68;
	// lwz r11,19976(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d3f68
	if (!ctx.cr6.eq) goto loc_825D3F68;
	// lwz r11,3056(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3056);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x825D3F4C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21552);
	// stw r10,608(r11)
	PPC_STORE_U32(ctx.r11.u32 + 608, ctx.r10.u32);
	// stw r24,15560(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15560, ctx.r24.u32);
	// stw r24,15536(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15536, ctx.r24.u32);
	// b 0x825d41a0
	goto loc_825D41A0;
loc_825D3F68:
	// lwz r11,19976(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3f98
	if (ctx.cr6.eq) goto loc_825D3F98;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82706958
	ctx.lr = 0x825D3F7C;
	sub_82706958(ctx, base);
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21552);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// stw r10,608(r11)
	PPC_STORE_U32(ctx.r11.u32 + 608, ctx.r10.u32);
	// stw r24,15560(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15560, ctx.r24.u32);
	// stw r24,15536(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15536, ctx.r24.u32);
	// b 0x825d41a0
	goto loc_825D41A0;
loc_825D3F98:
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// li r30,6
	ctx.r30.s64 = 6;
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21552);
	// stw r10,608(r11)
	PPC_STORE_U32(ctx.r11.u32 + 608, ctx.r10.u32);
	// stw r24,15560(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15560, ctx.r24.u32);
	// stw r24,15536(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15536, ctx.r24.u32);
	// b 0x825d41a0
	goto loc_825D41A0;
loc_825D3FB4:
	// lwz r11,3924(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d3fd0
	if (ctx.cr6.eq) goto loc_825D3FD0;
	// stw r24,15560(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15560, ctx.r24.u32);
	// li r30,6
	ctx.r30.s64 = 6;
	// stw r24,15536(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15536, ctx.r24.u32);
	// b 0x825d41a0
	goto loc_825D41A0;
loc_825D3FD0:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8260f810
	ctx.lr = 0x825D3FD8;
	sub_8260F810(ctx, base);
	// stw r24,15560(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15560, ctx.r24.u32);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// stw r24,15536(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15536, ctx.r24.u32);
	// b 0x825d41a0
	goto loc_825D41A0;
loc_825D3FE8:
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d40b0
	if (!ctx.cr6.eq) goto loc_825D40B0;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825deb78
	ctx.lr = 0x825D4000;
	sub_825DEB78(ctx, base);
	// lwz r11,14820(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14820);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d401c
	if (ctx.cr6.eq) goto loc_825D401C;
	// lwz r11,14824(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bne cr6,0x825d40e0
	if (!ctx.cr6.eq) goto loc_825D40E0;
loc_825D401C:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// bge cr6,0x825d40e0
	if (!ctx.cr6.lt) goto loc_825D40E0;
	// lwz r11,19696(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19696);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d40e0
	if (ctx.cr6.eq) goto loc_825D40E0;
	// lwz r11,204(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 184);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r9,164(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r6,220(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// lwz r5,172(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	// lwz r3,3732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// lwz r30,15856(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15856);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r24.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// mtctr r30
	ctx.ctr.u64 = ctx.r30.u64;
	// bctrl 
	ctx.lr = 0x825D406C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// lwz r30,196(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 196);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r10,168(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r7,224(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r6,176(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// lwz r3,3736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// lwz r29,15852(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15852);
	// stw r24,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r24.u32);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r30.u32);
	// mtctr r29
	ctx.ctr.u64 = ctx.r29.u64;
	// bctrl 
	ctx.lr = 0x825D40AC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// b 0x825d40e0
	goto loc_825D40E0;
loc_825D40B0:
	// lwz r11,3376(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3376);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d40c4
	if (!ctx.cr6.eq) goto loc_825D40C4;
	// stw r27,3376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3376, ctx.r27.u32);
	// b 0x825d40d0
	goto loc_825D40D0;
loc_825D40C4:
	// lwz r11,3396(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3396);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d40d8
	if (ctx.cr6.eq) goto loc_825D40D8;
loc_825D40D0:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82611280
	ctx.lr = 0x825D40D8;
	sub_82611280(ctx, base);
loc_825D40D8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82611318
	ctx.lr = 0x825D40E0;
	sub_82611318(ctx, base);
loc_825D40E0:
	// lwz r11,15900(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15900);
	// stw r27,3384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3384, ctx.r27.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d40f4
	if (ctx.cr6.eq) goto loc_825D40F4;
	// stw r27,15908(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15908, ctx.r27.u32);
loc_825D40F4:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// blt cr6,0x825d4164
	if (ctx.cr6.lt) goto loc_825D4164;
	// lwz r11,14820(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14820);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d4114
	if (ctx.cr6.eq) goto loc_825D4114;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e1550
	ctx.lr = 0x825D4114;
	sub_825E1550(ctx, base);
loc_825D4114:
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d4128
	if (ctx.cr6.eq) goto loc_825D4128;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825dd508
	ctx.lr = 0x825D4128;
	sub_825DD508(ctx, base);
loc_825D4128:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825fa130
	ctx.lr = 0x825D4130;
	sub_825FA130(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8260ecd0
	ctx.lr = 0x825D4138;
	sub_8260ECD0(ctx, base);
	// lwz r11,19976(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d4150
	if (ctx.cr6.eq) goto loc_825D4150;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82706850
	ctx.lr = 0x825D414C;
	sub_82706850(ctx, base);
	// b 0x825d4174
	goto loc_825D4174;
loc_825D4150:
	// lwz r11,3924(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d4164
	if (ctx.cr6.eq) goto loc_825D4164;
	// li r30,6
	ctx.r30.s64 = 6;
	// b 0x825d4178
	goto loc_825D4178;
loc_825D4164:
	// lwz r11,15776(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15776);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x825D4174;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_825D4174:
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
loc_825D4178:
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// cmpwi cr6,r30,4
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 4, ctx.xer);
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21552);
	// stw r10,608(r11)
	PPC_STORE_U32(ctx.r11.u32 + 608, ctx.r10.u32);
	// bne cr6,0x825d4198
	if (!ctx.cr6.eq) goto loc_825D4198;
	// li r22,4
	ctx.r22.s64 = 4;
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// b 0x825d41a0
	goto loc_825D41A0;
loc_825D4198:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825d36c4
	if (!ctx.cr6.eq) goto loc_825D36C4;
loc_825D41A0:
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d41b0
	if (!ctx.cr6.eq) goto loc_825D41B0;
	// stw r27,3380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3380, ctx.r27.u32);
loc_825D41B0:
	// cmpwi cr6,r30,4
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 4, ctx.xer);
	// bne cr6,0x825d4548
	if (!ctx.cr6.eq) goto loc_825D4548;
loc_825D41B8:
	// li r22,4
	ctx.r22.s64 = 4;
loc_825D41BC:
	// sth r24,3684(r31)
	PPC_STORE_U16(ctx.r31.u32 + 3684, ctx.r24.u16);
	// sth r24,0(r23)
	PPC_STORE_U16(ctx.r23.u32 + 0, ctx.r24.u16);
	// lwz r11,3452(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3452);
	// lwz r10,19976(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// subfic r11,r11,0
	ctx.xer.ca = ctx.r11.u32 <= 0;
	ctx.r11.s64 = 0 - ctx.r11.s64;
	// cmpw cr6,r10,r28
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r28.s32, ctx.xer);
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// stw r11,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r11.u32);
	// bne cr6,0x825d4260
	if (!ctx.cr6.eq) goto loc_825D4260;
	// lwz r9,19980(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19980);
	// cmpw cr6,r9,r27
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r27.s32, ctx.xer);
	// bne cr6,0x825d4260
	if (!ctx.cr6.eq) goto loc_825D4260;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x825d4208
	if (ctx.cr6.eq) goto loc_825D4208;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d4224
	if (!ctx.cr6.eq) goto loc_825D4224;
loc_825D4208:
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825d4218
	if (ctx.cr6.eq) goto loc_825D4218;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x825d4224
	if (!ctx.cr6.eq) goto loc_825D4224;
loc_825D4218:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,19984(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19984);
	// bl 0x82604db0
	ctx.lr = 0x825D4224;
	sub_82604DB0(ctx, base);
loc_825D4224:
	// lwz r11,21436(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21436);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d4260
	if (!ctx.cr6.eq) goto loc_825D4260;
	// lwz r11,21072(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21072);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d4260
	if (!ctx.cr6.eq) goto loc_825D4260;
	// lwz r11,21076(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21076);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d4260
	if (!ctx.cr6.eq) goto loc_825D4260;
	// lwz r11,21432(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21432);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d4260
	if (ctx.cr6.eq) goto loc_825D4260;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ce6b0
	ctx.lr = 0x825D425C;
	sub_825CE6B0(ctx, base);
	// stw r24,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r24.u32);
loc_825D4260:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ce4b8
	ctx.lr = 0x825D4268;
	sub_825CE4B8(ctx, base);
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// bne cr6,0x825d4274
	if (!ctx.cr6.eq) goto loc_825D4274;
	// mr r22,r3
	ctx.r22.u64 = ctx.r3.u64;
loc_825D4274:
	// mr r3,r22
	ctx.r3.u64 = ctx.r22.u64;
loc_825D4278:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D4280:
	// lwz r10,15900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15900);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825d4298
	if (ctx.cr6.eq) goto loc_825D4298;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d4298
	if (ctx.cr6.eq) goto loc_825D4298;
	// stw r27,15908(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15908, ctx.r27.u32);
loc_825D4298:
	// lwz r11,14820(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14820);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d42ac
	if (ctx.cr6.eq) goto loc_825D42AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e1550
	ctx.lr = 0x825D42AC;
	sub_825E1550(ctx, base);
loc_825D42AC:
	// mr r11,r24
	ctx.r11.u64 = ctx.r24.u64;
	// lis r10,-32128
	ctx.r10.s64 = -2105540608;
	// stw r11,22284(r10)
	PPC_STORE_U32(ctx.r10.u32 + 22284, ctx.r11.u32);
	// lwz r11,19976(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d42d0
	if (ctx.cr6.eq) goto loc_825D42D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x827066b0
	ctx.lr = 0x825D42CC;
	sub_827066B0(ctx, base);
	// b 0x825d430c
	goto loc_825D430C;
loc_825D42D0:
	// lwz r11,3948(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3948);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d42e8
	if (ctx.cr6.eq) goto loc_825D42E8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8260d0c8
	ctx.lr = 0x825D42E4;
	sub_8260D0C8(ctx, base);
	// b 0x825d430c
	goto loc_825D430C;
loc_825D42E8:
	// lwz r11,3924(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d42fc
	if (ctx.cr6.eq) goto loc_825D42FC;
	// li r30,6
	ctx.r30.s64 = 6;
	// b 0x825d4310
	goto loc_825D4310;
loc_825D42FC:
	// lwz r11,15772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15772);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x825D430C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_825D430C:
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
loc_825D4310:
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// cmpwi cr6,r30,4
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 4, ctx.xer);
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21552);
	// stw r10,608(r11)
	PPC_STORE_U32(ctx.r11.u32 + 608, ctx.r10.u32);
	// bne cr6,0x825d4330
	if (!ctx.cr6.eq) goto loc_825D4330;
	// li r22,4
	ctx.r22.s64 = 4;
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// b 0x825d4338
	goto loc_825D4338;
loc_825D4330:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825d36c4
	if (!ctx.cr6.eq) goto loc_825D36C4;
loc_825D4338:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d44a4
	if (ctx.cr6.eq) goto loc_825D44A4;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825d4354
	if (ctx.cr6.eq) goto loc_825D4354;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// bne cr6,0x825d4360
	if (!ctx.cr6.eq) goto loc_825D4360;
loc_825D4354:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d58a8
	ctx.lr = 0x825D435C;
	sub_825D58A8(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
loc_825D4360:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825d36cc
	if (!ctx.cr6.eq) goto loc_825D36CC;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// blt cr6,0x825d4424
	if (ctx.cr6.lt) goto loc_825D4424;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d4424
	if (ctx.cr6.eq) goto loc_825D4424;
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d4424
	if (!ctx.cr6.gt) goto loc_825D4424;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// mr r10,r27
	ctx.r10.u64 = ctx.r27.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x825d4400
	if (!ctx.cr6.gt) goto loc_825D4400;
loc_825D439C:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// ble cr6,0x825d43f0
	if (!ctx.cr6.gt) goto loc_825D43F0;
loc_825D43AC:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r8,3048(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3048);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// sthx r27,r9,r8
	PPC_STORE_U16(ctx.r9.u32 + ctx.r8.u32, ctx.r27.u16);
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r8,3048(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3048);
	// mullw r9,r9,r10
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r9,r9,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// sth r27,2(r9)
	PPC_STORE_U16(ctx.r9.u32 + 2, ctx.r27.u16);
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// blt cr6,0x825d43ac
	if (ctx.cr6.lt) goto loc_825D43AC;
loc_825D43F0:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r10,r11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x825d439c
	if (ctx.cr6.lt) goto loc_825D439C;
loc_825D4400:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r3,3052(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3052);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r5,r11,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x8239ca70
	ctx.lr = 0x825D4424;
	sub_8239CA70(ctx, base);
loc_825D4424:
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d44a4
	if (!ctx.cr6.gt) goto loc_825D44A4;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d44a4
	if (ctx.cr6.eq) goto loc_825D44A4;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// mr r7,r27
	ctx.r7.u64 = ctx.r27.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x825d44a4
	if (!ctx.cr6.gt) goto loc_825D44A4;
loc_825D444C:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// ble cr6,0x825d4494
	if (!ctx.cr6.gt) goto loc_825D4494;
loc_825D445C:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r9,276(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 276);
	// mullw r10,r10,r7
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r8,r10,r9
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// rlwinm r8,r8,0,15,13
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825d445c
	if (ctx.cr6.lt) goto loc_825D445C;
loc_825D4494:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// cmplw cr6,r7,r11
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x825d444c
	if (ctx.cr6.lt) goto loc_825D444C;
loc_825D44A4:
	// stw r24,3668(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3668, ctx.r24.u32);
	// b 0x825d41bc
	goto loc_825D41BC;
loc_825D44AC:
	// bl 0x825dcb80
	ctx.lr = 0x825D44B0;
	sub_825DCB80(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// stw r24,19976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19976, ctx.r24.u32);
	// mr r27,r24
	ctx.r27.u64 = ctx.r24.u64;
	// stw r24,19980(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19980, ctx.r24.u32);
	// bl 0x825ed198
	ctx.lr = 0x825D44C8;
	sub_825ED198(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8260a220
	ctx.lr = 0x825D44D0;
	sub_8260A220(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825d36c4
	if (!ctx.cr6.eq) goto loc_825D36C4;
	// lwz r11,21072(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21072);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825d4500
	if (ctx.cr6.eq) goto loc_825D4500;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d4500
	if (ctx.cr6.eq) goto loc_825D4500;
	// lwz r11,21516(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21516);
	// stw r24,21516(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21516, ctx.r24.u32);
	// stw r11,21520(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21520, ctx.r11.u32);
	// b 0x825d4504
	goto loc_825D4504;
loc_825D4500:
	// stw r26,284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 284, ctx.r26.u32);
loc_825D4504:
	// rlwinm r11,r25,16,0,15
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r25.u32 | (ctx.r25.u64 << 32), 16) & 0xFFFF0000;
	// srawi r11,r11,28
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xFFFFFFF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 28;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// beq cr6,0x825d4528
	if (ctx.cr6.eq) goto loc_825D4528;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825d4528
	if (ctx.cr6.eq) goto loc_825D4528;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82606d10
	ctx.lr = 0x825D4524;
	sub_82606D10(ctx, base);
	// b 0x825d4530
	goto loc_825D4530;
loc_825D4528:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8260f810
	ctx.lr = 0x825D4530;
	sub_8260F810(ctx, base);
loc_825D4530:
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// lwz r10,21552(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21552);
	// cmpwi cr6,r30,4
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 4, ctx.xer);
	// stw r10,608(r11)
	PPC_STORE_U32(ctx.r11.u32 + 608, ctx.r10.u32);
	// beq cr6,0x825d41b8
	if (ctx.cr6.eq) goto loc_825D41B8;
loc_825D4548:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825d41bc
	if (ctx.cr6.eq) goto loc_825D41BC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ce570
	ctx.lr = 0x825D4558;
	sub_825CE570(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
}

__attribute__((alias("__imp__sub_825D4564"))) PPC_WEAK_FUNC(sub_825D4564);
PPC_FUNC_IMPL(__imp__sub_825D4564) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D4568"))) PPC_WEAK_FUNC(sub_825D4568);
PPC_FUNC_IMPL(__imp__sub_825D4568) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba00
	ctx.lr = 0x825D4570;
	sub_8239BA00(ctx, base);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// mr r26,r8
	ctx.r26.u64 = ctx.r8.u64;
	// mr r25,r9
	ctx.r25.u64 = ctx.r9.u64;
	// mr r24,r10
	ctx.r24.u64 = ctx.r10.u64;
	// li r22,1
	ctx.r22.s64 = 1;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825d45c8
	if (ctx.cr6.eq) goto loc_825D45C8;
	// lwz r11,19768(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19768);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d45c8
	if (!ctx.cr6.eq) goto loc_825D45C8;
	// addi r3,r1,104
	ctx.r3.s64 = ctx.r1.s64 + 104;
	// bl 0x826926a8
	ctx.lr = 0x825D45B4;
	sub_826926A8(ctx, base);
	// ld r11,19744(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 19744);
	// ld r10,104(r1)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r1.u32 + 104);
	// stw r22,19768(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19768, ctx.r22.u32);
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// std r11,19744(r31)
	PPC_STORE_U64(ctx.r31.u32 + 19744, ctx.r11.u64);
loc_825D45C8:
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d45e0
	if (ctx.cr6.eq) goto loc_825D45E0;
	// li r3,13
	ctx.r3.s64 = 13;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D45E0:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d47e4
	if (ctx.cr6.eq) goto loc_825D47E4;
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// beq cr6,0x825d47e4
	if (ctx.cr6.eq) goto loc_825D47E4;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// beq cr6,0x825d47e4
	if (ctx.cr6.eq) goto loc_825D47E4;
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// beq cr6,0x825d47e4
	if (ctx.cr6.eq) goto loc_825D47E4;
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// beq cr6,0x825d47e4
	if (ctx.cr6.eq) goto loc_825D47E4;
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x825d47e4
	if (ctx.cr6.eq) goto loc_825D47E4;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x825d47e4
	if (ctx.cr6.eq) goto loc_825D47E4;
	// lwz r11,21336(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21336);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d4678
	if (!ctx.cr6.eq) goto loc_825D4678;
	// lis r4,12338
	ctx.r4.s64 = 808583168;
	// stw r22,21440(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21440, ctx.r22.u32);
	// mr r8,r30
	ctx.r8.u64 = ctx.r30.u64;
	// stw r30,21444(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21444, ctx.r30.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r28,21448(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21448, ctx.r28.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r27,21452(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21452, ctx.r27.u32);
	// li r5,12
	ctx.r5.s64 = 12;
	// stw r26,21456(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21456, ctx.r26.u32);
	// ori r4,r4,13385
	ctx.r4.u64 = ctx.r4.u64 | 13385;
	// stw r25,21460(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21460, ctx.r25.u32);
	// stw r24,21464(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21464, ctx.r24.u32);
	// bl 0x825d1a48
	ctx.lr = 0x825D4660;
	sub_825D1A48(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc7d0
	ctx.lr = 0x825D466C;
	sub_825CC7D0(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D4678:
	// addi r5,r1,104
	ctx.r5.s64 = ctx.r1.s64 + 104;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// bl 0x825cf060
	ctx.lr = 0x825D4684;
	sub_825CF060(ctx, base);
	// lwz r11,21412(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21412);
	// lwz r23,96(r1)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// lwz r29,104(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 104);
	// cmpw cr6,r23,r11
	ctx.cr6.compare<int32_t>(ctx.r23.s32, ctx.r11.s32, ctx.xer);
	// bne cr6,0x825d46f8
	if (!ctx.cr6.eq) goto loc_825D46F8;
	// lwz r10,21416(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21416);
	// cmpw cr6,r29,r10
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r10.s32, ctx.xer);
	// bne cr6,0x825d46f8
	if (!ctx.cr6.eq) goto loc_825D46F8;
	// lis r4,12338
	ctx.r4.s64 = 808583168;
	// stw r22,21440(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21440, ctx.r22.u32);
	// mr r8,r30
	ctx.r8.u64 = ctx.r30.u64;
	// stw r30,21444(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21444, ctx.r30.u32);
	// li r7,0
	ctx.r7.s64 = 0;
	// stw r28,21448(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21448, ctx.r28.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r27,21452(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21452, ctx.r27.u32);
	// li r5,12
	ctx.r5.s64 = 12;
	// stw r26,21456(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21456, ctx.r26.u32);
	// ori r4,r4,13385
	ctx.r4.u64 = ctx.r4.u64 | 13385;
	// stw r25,21460(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21460, ctx.r25.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r24,21464(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21464, ctx.r24.u32);
	// bl 0x825d1a48
	ctx.lr = 0x825D46E0;
	sub_825D1A48(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc7d0
	ctx.lr = 0x825D46EC;
	sub_825CC7D0(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D46F8:
	// lwz r10,21420(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21420);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825d475c
	if (!ctx.cr6.eq) goto loc_825D475C;
	// cmpw cr6,r23,r11
	ctx.cr6.compare<int32_t>(ctx.r23.s32, ctx.r11.s32, ctx.xer);
	// mr r10,r23
	ctx.r10.u64 = ctx.r23.u64;
	// bgt cr6,0x825d4714
	if (ctx.cr6.gt) goto loc_825D4714;
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
loc_825D4714:
	// lwz r11,21416(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21416);
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x825d4724
	if (!ctx.cr6.gt) goto loc_825D4724;
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_825D4724:
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// addi r11,r11,32
	ctx.r11.s64 = ctx.r11.s64 + 32;
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x825edb18
	ctx.lr = 0x825D4738;
	sub_825EDB18(ctx, base);
	// addi r11,r3,127
	ctx.r11.s64 = ctx.r3.s64 + 127;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,21420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21420, ctx.r3.u32);
	// rlwinm r11,r11,0,0,24
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFF80;
	// stw r11,21424(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21424, ctx.r11.u32);
	// bne cr6,0x825d475c
	if (!ctx.cr6.eq) goto loc_825D475C;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D475C:
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r8,21424(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21424);
	// lis r4,12338
	ctx.r4.s64 = 808583168;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r5,12
	ctx.r5.s64 = 12;
	// ori r4,r4,13385
	ctx.r4.u64 = ctx.r4.u64 | 13385;
	// stw r11,21440(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21440, ctx.r11.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d1a48
	ctx.lr = 0x825D4784;
	sub_825D1A48(ctx, base);
	// lis r4,12338
	ctx.r4.s64 = 808583168;
	// mr r7,r29
	ctx.r7.u64 = ctx.r29.u64;
	// lwz r10,21424(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21424);
	// mr r6,r23
	ctx.r6.u64 = ctx.r23.u64;
	// lwz r9,21416(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21416);
	// li r5,12
	ctx.r5.s64 = 12;
	// lwz r8,21412(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21412);
	// ori r4,r4,13385
	ctx.r4.u64 = ctx.r4.u64 | 13385;
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r30.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r22,21440(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21440, ctx.r22.u32);
	// stw r30,21444(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21444, ctx.r30.u32);
	// stw r28,21448(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21448, ctx.r28.u32);
	// stw r27,21452(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21452, ctx.r27.u32);
	// stw r26,21456(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21456, ctx.r26.u32);
	// stw r25,21460(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21460, ctx.r25.u32);
	// stw r24,21464(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21464, ctx.r24.u32);
	// bl 0x825cf128
	ctx.lr = 0x825D47CC;
	sub_825CF128(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc7d0
	ctx.lr = 0x825D47D8;
	sub_825CC7D0(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825D47E4:
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
}

__attribute__((alias("__imp__sub_825D47F0"))) PPC_WEAK_FUNC(sub_825D47F0);
PPC_FUNC_IMPL(__imp__sub_825D47F0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba18
	ctx.lr = 0x825D47F8;
	sub_8239BA18(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825d481c
	if (!ctx.cr6.eq) goto loc_825D481C;
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
loc_825D481C:
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d4834
	if (ctx.cr6.eq) goto loc_825D4834;
	// li r3,13
	ctx.r3.s64 = 13;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
loc_825D4834:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc770
	ctx.lr = 0x825D483C;
	sub_825CC770(ctx, base);
	// cmpwi cr6,r30,-1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, -1, ctx.xer);
	// beq cr6,0x825d484c
	if (ctx.cr6.eq) goto loc_825D484C;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825d4854
	if (!ctx.cr6.eq) goto loc_825D4854;
loc_825D484C:
	// rlwinm r11,r30,0,20,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 0) & 0xFF0;
	// ori r30,r11,15
	ctx.r30.u64 = ctx.r11.u64 | 15;
loc_825D4854:
	// lwz r11,15300(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15300);
	// li r28,1
	ctx.r28.s64 = 1;
	// stw r30,15556(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15556, ctx.r30.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d4874
	if (!ctx.cr6.eq) goto loc_825D4874;
	// lwz r11,15368(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15368);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d4890
	if (ctx.cr6.eq) goto loc_825D4890;
loc_825D4874:
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// lwz r5,15312(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15312);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,15308(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15308);
	// bl 0x825d28f8
	ctx.lr = 0x825D4888;
	sub_825D28F8(ctx, base);
	// sth r28,0(r29)
	PPC_STORE_U16(ctx.r29.u32 + 0, ctx.r28.u16);
	// b 0x825d48a0
	goto loc_825D48A0;
loc_825D4890:
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d3268
	ctx.lr = 0x825D48A0;
	sub_825D3268(ctx, base);
loc_825D48A0:
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmpwi cr6,r30,2
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 2, ctx.xer);
	// bne cr6,0x825d48b0
	if (!ctx.cr6.eq) goto loc_825D48B0;
	// stw r28,3676(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3676, ctx.r28.u32);
loc_825D48B0:
	// lwz r11,3680(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3680);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d48c8
	if (ctx.cr6.eq) goto loc_825D48C8;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r28,3676(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3676, ctx.r28.u32);
	// stw r11,3680(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3680, ctx.r11.u32);
loc_825D48C8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc7d0
	ctx.lr = 0x825D48D0;
	sub_825CC7D0(ctx, base);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
}

__attribute__((alias("__imp__sub_825D48DC"))) PPC_WEAK_FUNC(sub_825D48DC);
PPC_FUNC_IMPL(__imp__sub_825D48DC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D48E0"))) PPC_WEAK_FUNC(sub_825D48E0);
PPC_FUNC_IMPL(__imp__sub_825D48E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9fc
	ctx.lr = 0x825D48E8;
	sub_8239B9FC(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r21,0
	ctx.r21.s64 = 0;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// mr r27,r6
	ctx.r27.u64 = ctx.r6.u64;
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mr r25,r7
	ctx.r25.u64 = ctx.r7.u64;
	// lwz r28,28(r11)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// mr r24,r8
	ctx.r24.u64 = ctx.r8.u64;
	// lwz r31,8(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lis r11,9
	ctx.r11.s64 = 589824;
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// ori r23,r11,128
	ctx.r23.u64 = ctx.r11.u64 | 128;
	// lis r11,-32688
	ctx.r11.s64 = -2142240768;
	// mr r26,r9
	ctx.r26.u64 = ctx.r9.u64;
	// mr r3,r21
	ctx.r3.u64 = ctx.r21.u64;
	// ori r22,r11,183
	ctx.r22.u64 = ctx.r11.u64 | 183;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825d496c
	if (ctx.cr6.eq) goto loc_825D496C;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// beq cr6,0x825d496c
	if (ctx.cr6.eq) goto loc_825D496C;
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// mr r5,r23
	ctx.r5.u64 = ctx.r23.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82593b70
	ctx.lr = 0x825D4954;
	sub_82593B70(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bge cr6,0x825d49b4
	if (!ctx.cr6.lt) goto loc_825D49B4;
	// cmplw cr6,r3,r22
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r22.u32, ctx.xer);
	// bne cr6,0x825d4b38
	if (!ctx.cr6.eq) goto loc_825D4B38;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r11.u32);
loc_825D496C:
	// extsb r26,r27
	ctx.r26.s64 = ctx.r27.s8;
	// mr r27,r21
	ctx.r27.u64 = ctx.r21.u64;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// ble cr6,0x825d49b4
	if (!ctx.cr6.gt) goto loc_825D49B4;
loc_825D497C:
	// lwz r31,0(r30)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r5,r23
	ctx.r5.u64 = ctx.r23.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// bl 0x82593b70
	ctx.lr = 0x825D4994;
	sub_82593B70(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bge cr6,0x825d49b4
	if (!ctx.cr6.lt) goto loc_825D49B4;
	// cmplw cr6,r3,r22
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r22.u32, ctx.xer);
	// bne cr6,0x825d4b38
	if (!ctx.cr6.eq) goto loc_825D4B38;
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// addi r30,r30,4
	ctx.r30.s64 = ctx.r30.s64 + 4;
	// cmpw cr6,r27,r26
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r26.s32, ctx.xer);
	// blt cr6,0x825d497c
	if (ctx.cr6.lt) goto loc_825D497C;
loc_825D49B4:
	// addi r11,r31,-1
	ctx.r11.s64 = ctx.r31.s64 + -1;
	// cmplwi cr6,r11,11
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 11, ctx.xer);
	// bgt cr6,0x825d4b30
	if (ctx.cr6.gt) goto loc_825D4B30;
	// lis r12,-32163
	ctx.r12.s64 = -2107834368;
	// addi r12,r12,18904
	ctx.r12.s64 = ctx.r12.s64 + 18904;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr 
	switch (ctx.r11.u64) {
	case 0:
		goto loc_825D4A5C;
	case 1:
		goto loc_825D4A78;
	case 2:
		goto loc_825D4A94;
	case 3:
		goto loc_825D4AB0;
	case 4:
		goto loc_825D4AC4;
	case 5:
		goto loc_825D4AD8;
	case 6:
		goto loc_825D4B08;
	case 7:
		goto loc_825D4B1C;
	case 8:
		goto loc_825D4A24;
	case 9:
		goto loc_825D4A40;
	case 10:
		goto loc_825D4AF0;
	case 11:
		goto loc_825D4A08;
	default:
		__builtin_unreachable();
	}
	// lwz r18,19036(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 19036);
	// lwz r18,19064(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 19064);
	// lwz r18,19092(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 19092);
	// lwz r18,19120(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 19120);
	// lwz r18,19140(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 19140);
	// lwz r18,19160(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 19160);
	// lwz r18,19208(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 19208);
	// lwz r18,19228(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 19228);
	// lwz r18,18980(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 18980);
	// lwz r18,19008(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 19008);
	// lwz r18,19184(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 19184);
	// lwz r18,18952(r29)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r29.u32 + 18952);
loc_825D4A08:
	// lis r11,12849
	ctx.r11.s64 = 842072064;
	// li r10,12
	ctx.r10.s64 = 12;
	// ori r11,r11,22105
	ctx.r11.u64 = ctx.r11.u64 | 22105;
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4A24:
	// lis r11,22101
	ctx.r11.s64 = 1448411136;
	// li r10,12
	ctx.r10.s64 = 12;
	// ori r11,r11,22857
	ctx.r11.u64 = ctx.r11.u64 | 22857;
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4A40:
	// lis r11,12338
	ctx.r11.s64 = 808583168;
	// li r10,12
	ctx.r10.s64 = 12;
	// ori r11,r11,13385
	ctx.r11.u64 = ctx.r11.u64 | 13385;
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4A5C:
	// lis r11,12889
	ctx.r11.s64 = 844693504;
	// li r10,16
	ctx.r10.s64 = 16;
	// ori r11,r11,21849
	ctx.r11.u64 = ctx.r11.u64 | 21849;
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4A78:
	// lis r11,22870
	ctx.r11.s64 = 1498808320;
	// li r10,16
	ctx.r10.s64 = 16;
	// ori r11,r11,22869
	ctx.r11.u64 = ctx.r11.u64 | 22869;
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4A94:
	// lis r11,21849
	ctx.r11.s64 = 1431896064;
	// li r10,16
	ctx.r10.s64 = 16;
	// ori r11,r11,22105
	ctx.r11.u64 = ctx.r11.u64 | 22105;
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4AB0:
	// li r11,24
	ctx.r11.s64 = 24;
	// stw r21,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r21.u32);
	// stw r11,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r11.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4AC4:
	// li r11,16
	ctx.r11.s64 = 16;
	// stw r21,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r21.u32);
	// stw r11,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r11.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4AD8:
	// li r11,3
	ctx.r11.s64 = 3;
	// li r10,16
	ctx.r10.s64 = 16;
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4AF0:
	// li r11,3
	ctx.r11.s64 = 3;
	// li r10,12
	ctx.r10.s64 = 12;
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
	// stw r10,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r10.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4B08:
	// li r11,32
	ctx.r11.s64 = 32;
	// stw r21,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r21.u32);
	// stw r11,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r11.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4B1C:
	// li r11,8
	ctx.r11.s64 = 8;
	// stw r21,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r21.u32);
	// stw r11,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r11.u32);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
loc_825D4B30:
	// lis r3,-32688
	ctx.r3.s64 = -2142240768;
	// ori r3,r3,183
	ctx.r3.u64 = ctx.r3.u64 | 183;
loc_825D4B38:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
}

__attribute__((alias("__imp__sub_825D4B40"))) PPC_WEAK_FUNC(sub_825D4B40);
PPC_FUNC_IMPL(__imp__sub_825D4B40) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9f8
	ctx.lr = 0x825D4B48;
	sub_8239B9F8(ctx, base);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// lwz r10,4(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// mr r21,r5
	ctx.r21.u64 = ctx.r5.u64;
	// lwz r27,28(r3)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mr r23,r6
	ctx.r23.u64 = ctx.r6.u64;
	// rlwinm r9,r10,0,30,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x2;
	// mr r22,r7
	ctx.r22.u64 = ctx.r7.u64;
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mr r20,r8
	ctx.r20.u64 = ctx.r8.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// lwz r31,8(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r29,12(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// beq cr6,0x825d4b8c
	if (ctx.cr6.eq) goto loc_825D4B8C;
	// lwz r30,12(r3)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// b 0x825d4b90
	goto loc_825D4B90;
loc_825D4B8C:
	// mr r30,r31
	ctx.r30.u64 = ctx.r31.u64;
loc_825D4B90:
	// rlwinm r11,r10,0,29,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x4;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d4ba4
	if (ctx.cr6.eq) goto loc_825D4BA4;
	// lwz r26,16(r3)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// b 0x825d4ba8
	goto loc_825D4BA8;
loc_825D4BA4:
	// mr r26,r29
	ctx.r26.u64 = ctx.r29.u64;
loc_825D4BA8:
	// lis r11,-32688
	ctx.r11.s64 = -2142240768;
	// cmplw cr6,r30,r31
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r31.u32, ctx.xer);
	// ori r25,r11,3
	ctx.r25.u64 = ctx.r11.u64 | 3;
	// lis r11,-32688
	ctx.r11.s64 = -2142240768;
	// ori r24,r11,182
	ctx.r24.u64 = ctx.r11.u64 | 182;
	// ble cr6,0x825d4bfc
	if (!ctx.cr6.gt) goto loc_825D4BFC;
	// lis r5,9
	ctx.r5.s64 = 589824;
	// stw r30,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r30.u32);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// stw r31,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r31.u32);
	// ori r5,r5,144
	ctx.r5.u64 = ctx.r5.u64 | 144;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82593b70
	ctx.lr = 0x825D4BE0;
	sub_82593B70(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bge cr6,0x825d4c00
	if (!ctx.cr6.lt) goto loc_825D4C00;
	// cmplw cr6,r3,r25
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r25.u32, ctx.xer);
	// beq cr6,0x825d4bf8
	if (ctx.cr6.eq) goto loc_825D4BF8;
	// cmplw cr6,r3,r24
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r24.u32, ctx.xer);
	// bne cr6,0x825d4d10
	if (!ctx.cr6.eq) goto loc_825D4D10;
loc_825D4BF8:
	// cmplw cr6,r30,r31
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r31.u32, ctx.xer);
loc_825D4BFC:
	// bne cr6,0x825d4d08
	if (!ctx.cr6.eq) goto loc_825D4D08;
loc_825D4C00:
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r30.u32);
	// cmplw cr6,r26,r29
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r29.u32, ctx.xer);
	// ble cr6,0x825d4c44
	if (!ctx.cr6.gt) goto loc_825D4C44;
	// lis r5,9
	ctx.r5.s64 = 589824;
	// stw r26,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r26.u32);
	// addi r6,r1,88
	ctx.r6.s64 = ctx.r1.s64 + 88;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r29.u32);
	// ori r5,r5,160
	ctx.r5.u64 = ctx.r5.u64 | 160;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82593b70
	ctx.lr = 0x825D4C2C;
	sub_82593B70(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bge cr6,0x825d4c4c
	if (!ctx.cr6.lt) goto loc_825D4C4C;
	// cmplw cr6,r3,r25
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r25.u32, ctx.xer);
	// beq cr6,0x825d4c44
	if (ctx.cr6.eq) goto loc_825D4C44;
	// cmplw cr6,r3,r24
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r24.u32, ctx.xer);
	// bne cr6,0x825d4d10
	if (!ctx.cr6.eq) goto loc_825D4D10;
loc_825D4C44:
	// cmplw cr6,r30,r31
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r31.u32, ctx.xer);
	// bne cr6,0x825d4d08
	if (!ctx.cr6.eq) goto loc_825D4D08;
loc_825D4C4C:
	// lis r5,9
	ctx.r5.s64 = 589824;
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r26.u32);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// ori r5,r5,176
	ctx.r5.u64 = ctx.r5.u64 | 176;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82593b70
	ctx.lr = 0x825D4C68;
	sub_82593B70(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x825d4d10
	if (ctx.cr6.lt) goto loc_825D4D10;
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r9,r11,31
	ctx.r9.u64 = ctx.r11.u32 & 0x1;
	// stw r10,0(r22)
	PPC_STORE_U32(ctx.r22.u32 + 0, ctx.r10.u32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// stw r11,0(r23)
	PPC_STORE_U32(ctx.r23.u32 + 0, ctx.r11.u32);
	// bne cr6,0x825d4d08
	if (!ctx.cr6.eq) goto loc_825D4D08;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825d4d08
	if (!ctx.cr6.eq) goto loc_825D4D08;
	// cmplwi cr6,r21,12
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 12, ctx.xer);
	// bne cr6,0x825d4d08
	if (!ctx.cr6.eq) goto loc_825D4D08;
	// stw r10,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r10.u32);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// li r9,1
	ctx.r9.s64 = 1;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lis r5,9
	ctx.r5.s64 = 589824;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r6,r1,96
	ctx.r6.s64 = ctx.r1.s64 + 96;
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// stw r9,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r9.u32);
	// ori r5,r5,112
	ctx.r5.u64 = ctx.r5.u64 | 112;
	// rlwinm r11,r11,29,3,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 29) & 0x1FFFFFFF;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r11.u32);
	// bl 0x82593b70
	ctx.lr = 0x825D4CDC;
	sub_82593B70(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// blt cr6,0x825d4d10
	if (ctx.cr6.lt) goto loc_825D4D10;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,31,3,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x1FFFFFFF;
	// stw r11,0(r20)
	PPC_STORE_U32(ctx.r20.u32 + 0, ctx.r11.u32);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825D4D08:
	// lis r3,-32688
	ctx.r3.s64 = -2142240768;
	// ori r3,r3,2
	ctx.r3.u64 = ctx.r3.u64 | 2;
loc_825D4D10:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
}

__attribute__((alias("__imp__sub_825D4D18"))) PPC_WEAK_FUNC(sub_825D4D18);
PPC_FUNC_IMPL(__imp__sub_825D4D18) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,7
	ctx.r10.s64 = 7;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_825D4D28:
	// std r9,0(r11)
	PPC_STORE_U64(ctx.r11.u32 + 0, ctx.r9.u64);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bdnz 0x825d4d28
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_825D4D28;
	// stw r4,44(r3)
	PPC_STORE_U32(ctx.r3.u32 + 44, ctx.r4.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D4D3C"))) PPC_WEAK_FUNC(sub_825D4D3C);
PPC_FUNC_IMPL(__imp__sub_825D4D3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D4D40"))) PPC_WEAK_FUNC(sub_825D4D40);
PPC_FUNC_IMPL(__imp__sub_825D4D40) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,29
	ctx.r11.u64 = ctx.r11.u32 & 0x7;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d4d54
	if (!ctx.cr6.eq) goto loc_825D4D54;
	// li r11,8
	ctx.r11.s64 = 8;
loc_825D4D54:
	// subfic r10,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r10.s64 = 64 - ctx.r11.s64;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// clrldi r10,r10,32
	ctx.r10.u64 = ctx.r10.u64 & 0xFFFFFFFF;
	// srd r11,r11,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rotlwi r3,r11,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D4D70"))) PPC_WEAK_FUNC(sub_825D4D70);
PPC_FUNC_IMPL(__imp__sub_825D4D70) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825D4D78;
	sub_8239BA0C(ctx, base);
	// lwz r11,40(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// li r10,-1
	ctx.r10.s64 = -1;
	// lwz r9,36(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 36);
	// lwz r26,0(r5)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// mr r27,r9
	ctx.r27.u64 = ctx.r9.u64;
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// lwz r29,21320(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21320);
	// bne cr6,0x825d4da0
	if (!ctx.cr6.eq) goto loc_825D4DA0;
	// li r10,-256
	ctx.r10.s64 = -256;
	// b 0x825d4dac
	goto loc_825D4DAC;
loc_825D4DA0:
	// cmpwi cr6,r9,2
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 2, ctx.xer);
	// bne cr6,0x825d4dac
	if (!ctx.cr6.eq) goto loc_825D4DAC;
	// li r10,0
	ctx.r10.s64 = 0;
loc_825D4DAC:
	// clrlwi r11,r4,31
	ctx.r11.u64 = ctx.r4.u32 & 0x1;
	// li r31,0
	ctx.r31.s64 = 0;
	// mr r6,r4
	ctx.r6.u64 = ctx.r4.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d4e04
	if (ctx.cr6.eq) goto loc_825D4E04;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// ble cr6,0x825d4e04
	if (!ctx.cr6.gt) goto loc_825D4E04;
	// clrlwi r11,r10,16
	ctx.r11.u64 = ctx.r10.u32 & 0xFFFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825d4de0
	if (!ctx.cr6.eq) goto loc_825D4DE0;
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// cmplwi cr6,r10,3
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 3, ctx.xer);
	// beq cr6,0x825d4dec
	if (ctx.cr6.eq) goto loc_825D4DEC;
loc_825D4DE0:
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// stb r10,0(r29)
	PPC_STORE_U8(ctx.r29.u32 + 0, ctx.r10.u8);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
loc_825D4DEC:
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rlwinm r11,r11,8,0,23
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFFFFFF00;
	// li r31,1
	ctx.r31.s64 = 1;
	// or r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 | ctx.r10.u64;
	// addi r6,r4,1
	ctx.r6.s64 = ctx.r4.s64 + 1;
	// clrlwi r10,r11,16
	ctx.r10.u64 = ctx.r11.u32 & 0xFFFF;
loc_825D4E04:
	// addi r28,r26,-1
	ctx.r28.s64 = ctx.r26.s64 + -1;
	// cmpw cr6,r31,r28
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r28.s32, ctx.xer);
	// bge cr6,0x825d4eb0
	if (!ctx.cr6.lt) goto loc_825D4EB0;
	// subf r11,r31,r28
	ctx.r11.s64 = ctx.r28.s64 - ctx.r31.s64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	ctx.r30.s64 = ctx.r11.s64 + 1;
	// rlwinm r11,r30,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r11,r31
	ctx.r31.u64 = ctx.r11.u64 + ctx.r31.u64;
loc_825D4E28:
	// lhz r11,0(r6)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r6.u32 + 0);
	// clrlwi r8,r10,16
	ctx.r8.u64 = ctx.r10.u32 & 0xFFFF;
	// mr r7,r11
	ctx.r7.u64 = ctx.r11.u64;
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// and r25,r7,r8
	ctx.r25.u64 = ctx.r7.u64 & ctx.r8.u64;
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// bne cr6,0x825d4e88
	if (!ctx.cr6.eq) goto loc_825D4E88;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// bne cr6,0x825d4e64
	if (!ctx.cr6.eq) goto loc_825D4E64;
	// rlwinm r25,r7,0,16,23
	ctx.r25.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFF00;
	// cmplwi cr6,r25,768
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 768, ctx.xer);
	// bne cr6,0x825d4e64
	if (!ctx.cr6.eq) goto loc_825D4E64;
	// stb r11,0(r29)
	PPC_STORE_U8(ctx.r29.u32 + 0, ctx.r11.u8);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// b 0x825d4e9c
	goto loc_825D4E9C;
loc_825D4E64:
	// clrlwi r8,r8,24
	ctx.r8.u64 = ctx.r8.u32 & 0xFF;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// bne cr6,0x825d4e88
	if (!ctx.cr6.eq) goto loc_825D4E88;
	// cmplwi cr6,r7,3
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 3, ctx.xer);
	// bne cr6,0x825d4e88
	if (!ctx.cr6.eq) goto loc_825D4E88;
	// rlwinm r11,r11,24,24,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 24) & 0xFF;
	// stb r11,0(r29)
	PPC_STORE_U8(ctx.r29.u32 + 0, ctx.r11.u8);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// b 0x825d4e9c
	goto loc_825D4E9C;
loc_825D4E88:
	// rlwinm r11,r10,24,24,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 24) & 0xFF;
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// stb r11,0(r29)
	PPC_STORE_U8(ctx.r29.u32 + 0, ctx.r11.u8);
	// stb r8,1(r29)
	PPC_STORE_U8(ctx.r29.u32 + 1, ctx.r8.u8);
	// addi r29,r29,2
	ctx.r29.s64 = ctx.r29.s64 + 2;
loc_825D4E9C:
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825d4e28
	if (!ctx.cr6.eq) goto loc_825D4E28;
	// cmpw cr6,r31,r28
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r28.s32, ctx.xer);
loc_825D4EB0:
	// bne cr6,0x825d4f70
	if (!ctx.cr6.eq) goto loc_825D4F70;
	// clrlwi r10,r10,16
	ctx.r10.u64 = ctx.r10.u32 & 0xFFFF;
	// lbz r11,0(r6)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825d4f44
	if (!ctx.cr6.eq) goto loc_825D4F44;
	// clrlwi r10,r11,24
	ctx.r10.u64 = ctx.r11.u32 & 0xFF;
	// cmplwi cr6,r10,3
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 3, ctx.xer);
	// beq cr6,0x825d4ed8
	if (ctx.cr6.eq) goto loc_825D4ED8;
	// stb r11,0(r29)
	PPC_STORE_U8(ctx.r29.u32 + 0, ctx.r11.u8);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
loc_825D4ED8:
	// li r9,0
	ctx.r9.s64 = 0;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825d4ff8
	if (!ctx.cr6.eq) goto loc_825D4FF8;
	// add r31,r27,r26
	ctx.r31.u64 = ctx.r27.u64 + ctx.r26.u64;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// ble cr6,0x825d4ff8
	if (!ctx.cr6.gt) goto loc_825D4FF8;
	// add r11,r26,r4
	ctx.r11.u64 = ctx.r26.u64 + ctx.r4.u64;
	// addi r7,r11,-1
	ctx.r7.s64 = ctx.r11.s64 + -1;
	// lis r11,21845
	ctx.r11.s64 = 1431633920;
	// ori r11,r11,21846
	ctx.r11.u64 = ctx.r11.u64 | 21846;
loc_825D4F00:
	// cmpw cr6,r10,r26
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r26.s32, ctx.xer);
	// bge cr6,0x825d4f14
	if (!ctx.cr6.lt) goto loc_825D4F14;
	// lbz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// bne cr6,0x825d4ff8
	if (!ctx.cr6.eq) goto loc_825D4FF8;
loc_825D4F14:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// mulhw r8,r9,r11
	ctx.r8.s64 = (int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32)) >> 32;
	// rlwinm r6,r8,1,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0x1;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// cmpw cr6,r10,r31
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r31.s32, ctx.xer);
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// blt cr6,0x825d4f00
	if (ctx.cr6.lt) goto loc_825D4F00;
	// b 0x825d4ff8
	goto loc_825D4FF8;
loc_825D4F44:
	// clrlwi r10,r10,24
	ctx.r10.u64 = ctx.r10.u32 & 0xFF;
	// stb r11,0(r29)
	PPC_STORE_U8(ctx.r29.u32 + 0, ctx.r11.u8);
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// clrlwi r10,r11,24
	ctx.r10.u64 = ctx.r11.u32 & 0xFF;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// bne cr6,0x825d4f68
	if (!ctx.cr6.eq) goto loc_825D4F68;
	// rlwinm r9,r10,28,30,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0x2;
	// b 0x825d4ff8
	goto loc_825D4FF8;
loc_825D4F68:
	// rlwinm r9,r10,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// b 0x825d4ff8
	goto loc_825D4FF8;
loc_825D4F70:
	// cmpw cr6,r31,r26
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r26.s32, ctx.xer);
	// bne cr6,0x825d4ff8
	if (!ctx.cr6.eq) goto loc_825D4FF8;
	// clrlwi r11,r10,16
	ctx.r11.u64 = ctx.r10.u32 & 0xFFFF;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825d4fec
	if (!ctx.cr6.eq) goto loc_825D4FEC;
	// add r31,r27,r26
	ctx.r31.u64 = ctx.r27.u64 + ctx.r26.u64;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// ble cr6,0x825d4ff8
	if (!ctx.cr6.gt) goto loc_825D4FF8;
	// add r11,r26,r4
	ctx.r11.u64 = ctx.r26.u64 + ctx.r4.u64;
	// addi r7,r11,-1
	ctx.r7.s64 = ctx.r11.s64 + -1;
	// lis r11,21845
	ctx.r11.s64 = 1431633920;
	// ori r11,r11,21846
	ctx.r11.u64 = ctx.r11.u64 | 21846;
loc_825D4FA8:
	// cmpw cr6,r10,r26
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r26.s32, ctx.xer);
	// bge cr6,0x825d4fbc
	if (!ctx.cr6.lt) goto loc_825D4FBC;
	// lbz r8,0(r7)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r7.u32 + 0);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// bne cr6,0x825d4ff8
	if (!ctx.cr6.eq) goto loc_825D4FF8;
loc_825D4FBC:
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// mulhw r8,r9,r11
	ctx.r8.s64 = (int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32)) >> 32;
	// rlwinm r6,r8,1,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0x1;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// cmpw cr6,r10,r31
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r31.s32, ctx.xer);
	// rlwinm r6,r8,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// blt cr6,0x825d4fa8
	if (ctx.cr6.lt) goto loc_825D4FA8;
	// b 0x825d4ff8
	goto loc_825D4FF8;
loc_825D4FEC:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r9,r11,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
loc_825D4FF8:
	// lwz r11,40(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 40);
	// lwz r10,21320(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21320);
	// stw r9,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r9.u32);
	// lwz r11,21320(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 21320);
	// subf r10,r10,r29
	ctx.r10.s64 = ctx.r29.s64 - ctx.r10.s64;
	// add r9,r11,r10
	ctx.r9.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r11.u32);
	// stw r9,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r9.u32);
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825D5024"))) PPC_WEAK_FUNC(sub_825D5024);
PPC_FUNC_IMPL(__imp__sub_825D5024) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D5028"))) PPC_WEAK_FUNC(sub_825D5028);
PPC_FUNC_IMPL(__imp__sub_825D5028) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r30,r31,24
	ctx.r30.s64 = ctx.r31.s64 + 24;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,88
	ctx.r5.s64 = ctx.r1.s64 + 88;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,44(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// mr r8,r30
	ctx.r8.u64 = ctx.r30.u64;
	// bl 0x8248c788
	ctx.lr = 0x825D5060;
	sub_8248C788(ctx, base);
	// lwz r3,40(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// lwz r11,15472(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825d50cc
	if (!ctx.cr6.eq) goto loc_825D50CC;
	// lwz r11,32(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d50cc
	if (ctx.cr6.eq) goto loc_825D50CC;
	// lwz r4,88(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// addi r7,r1,80
	ctx.r7.s64 = ctx.r1.s64 + 80;
	// addi r6,r1,84
	ctx.r6.s64 = ctx.r1.s64 + 84;
	// lwz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lwz r5,80(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r4,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r4.u32);
	// bl 0x825e8930
	ctx.lr = 0x825D5098;
	sub_825E8930(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825d50a8
	if (ctx.cr6.eq) goto loc_825D50A8;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r11.u32);
loc_825D50A8:
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d50e4
	if (ctx.cr6.eq) goto loc_825D50E4;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// lwz r4,84(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r6,0(r30)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// bl 0x825d4d70
	ctx.lr = 0x825D50C8;
	sub_825D4D70(ctx, base);
	// b 0x825d50e4
	goto loc_825D50E4;
loc_825D50CC:
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stw r11,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r11.u32);
	// stw r10,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r10.u32);
loc_825D50E4:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D50FC"))) PPC_WEAK_FUNC(sub_825D50FC);
PPC_FUNC_IMPL(__imp__sub_825D50FC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D5100"))) PPC_WEAK_FUNC(sub_825D5100);
PPC_FUNC_IMPL(__imp__sub_825D5100) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// add r10,r4,r5
	ctx.r10.u64 = ctx.r4.u64 + ctx.r5.u64;
	// stw r5,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r5.u32);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r7,28(r3)
	PPC_STORE_U32(ctx.r3.u32 + 28, ctx.r7.u32);
	// li r9,-16
	ctx.r9.s64 = -16;
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// mr r31,r6
	ctx.r31.u64 = ctx.r6.u64;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r11.u64);
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r11.u32);
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// stw r10,16(r3)
	PPC_STORE_U32(ctx.r3.u32 + 16, ctx.r10.u32);
	// beq cr6,0x825d5154
	if (ctx.cr6.eq) goto loc_825D5154;
	// addi r5,r1,132
	ctx.r5.s64 = ctx.r1.s64 + 132;
	// stw r11,36(r3)
	PPC_STORE_U32(ctx.r3.u32 + 36, ctx.r11.u32);
	// bl 0x825d4d70
	ctx.lr = 0x825D5150;
	sub_825D4D70(ctx, base);
	// lwz r4,12(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
loc_825D5154:
	// lwz r11,16(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplw cr6,r4,r11
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d51a0
	if (ctx.cr6.gt) goto loc_825D51A0;
loc_825D5160:
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmpwi cr6,r11,40
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 40, ctx.xer);
	// bgt cr6,0x825d51a0
	if (ctx.cr6.gt) goto loc_825D51A0;
	// subfic r8,r11,40
	ctx.xer.ca = ctx.r11.u32 <= 40;
	ctx.r8.s64 = 40 - ctx.r11.s64;
	// lbz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// ld r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r11.u64);
	// lwz r11,16(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplw cr6,r4,r11
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825d5160
	if (!ctx.cr6.gt) goto loc_825D5160;
loc_825D51A0:
	// stw r4,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r4.u32);
	// stw r31,24(r3)
	PPC_STORE_U32(ctx.r3.u32 + 24, ctx.r31.u32);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D51BC"))) PPC_WEAK_FUNC(sub_825D51BC);
PPC_FUNC_IMPL(__imp__sub_825D51BC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D51C0"))) PPC_WEAK_FUNC(sub_825D51C0);
PPC_FUNC_IMPL(__imp__sub_825D51C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825D51C8;
	sub_8239BA1C(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// addi r30,r29,8
	ctx.r30.s64 = ctx.r29.s64 + 8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrlwi r11,r10,29
	ctx.r11.u64 = ctx.r10.u32 & 0x7;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d51ec
	if (ctx.cr6.eq) goto loc_825D51EC;
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
loc_825D51EC:
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// ble cr6,0x825d526c
	if (!ctx.cr6.gt) goto loc_825D526C;
loc_825D51FC:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bgt cr6,0x825d5244
	if (ctx.cr6.gt) goto loc_825D5244;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpwi cr6,r10,40
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 40, ctx.xer);
	// bgt cr6,0x825d5294
	if (ctx.cr6.gt) goto loc_825D5294;
	// subfic r7,r10,40
	ctx.xer.ca = ctx.r10.u32 <= 40;
	ctx.r7.s64 = 40 - ctx.r10.s64;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r11.u32);
	// sld r10,r8,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r7.u8 & 0x7F));
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// b 0x825d525c
	goto loc_825D525C;
loc_825D5244:
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d526c
	if (!ctx.cr6.eq) goto loc_825D526C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5028
	ctx.lr = 0x825D5258;
	sub_825D5028(ctx, base);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
loc_825D525C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r10,r10,16
	ctx.r10.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r10
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r10.u32, ctx.xer);
	// bgt cr6,0x825d51fc
	if (ctx.cr6.gt) goto loc_825D51FC;
loc_825D526C:
	// subfic r11,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r30.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
loc_825D527C:
	// li r10,-1
	ctx.r10.s64 = -1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// srw r10,r10,r29
	ctx.r10.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 >> (ctx.r29.u8 & 0x3F));
	// and r3,r10,r11
	ctx.r3.u64 = ctx.r10.u64 & ctx.r11.u64;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
loc_825D5294:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r9,r10,16
	ctx.r9.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r9
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r9.u32, ctx.xer);
	// ble cr6,0x825d526c
	if (!ctx.cr6.gt) goto loc_825D526C;
	// lwz r9,16(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// bgt cr6,0x825d526c
	if (ctx.cr6.gt) goto loc_825D526C;
	// addi r8,r10,248
	ctx.r8.s64 = ctx.r10.s64 + 248;
	// lbz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subfic r9,r30,32
	ctx.xer.ca = ctx.r30.u32 <= 32;
	ctx.r9.s64 = 32 - ctx.r30.s64;
	// clrlwi r8,r8,24
	ctx.r8.u64 = ctx.r8.u32 & 0xFF;
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// srd r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r8.u8 & 0x7F));
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// srd r11,r11,r9
	ctx.r11.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// b 0x825d527c
	goto loc_825D527C;
}

__attribute__((alias("__imp__sub_825D52D8"))) PPC_WEAK_FUNC(sub_825D52D8);
PPC_FUNC_IMPL(__imp__sub_825D52D8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,12(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 12);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bgt cr6,0x825d5338
	if (ctx.cr6.gt) goto loc_825D5338;
loc_825D52F4:
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r9,r10,8
	ctx.r9.s64 = ctx.r10.s64 + 8;
	// ld r8,0(r3)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// subfic r10,r10,40
	ctx.xer.ca = ctx.r10.u32 <= 40;
	ctx.r10.s64 = 40 - ctx.r10.s64;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// stw r9,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r9.u32);
	// sld r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r7.u64 << (ctx.r10.u8 & 0x7F));
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// std r10,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r10.u64);
	// lwz r10,16(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 16);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// ble cr6,0x825d52f4
	if (!ctx.cr6.gt) goto loc_825D52F4;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stw r11,12(r3)
	PPC_STORE_U32(ctx.r3.u32 + 12, ctx.r11.u32);
	// bge cr6,0x825d5384
	if (!ctx.cr6.lt) goto loc_825D5384;
loc_825D5338:
	// lwz r11,24(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 24);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d535c
	if (!ctx.cr6.eq) goto loc_825D535C;
	// bl 0x825d5028
	ctx.lr = 0x825D5348;
	sub_825D5028(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_825D535C:
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// cmpwi cr6,r11,-16
	ctx.cr6.compare<int32_t>(ctx.r11.s32, -16, ctx.xer);
	// bge cr6,0x825d5384
	if (!ctx.cr6.lt) goto loc_825D5384;
	// lwz r11,20(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d537c
	if (!ctx.cr6.eq) goto loc_825D537C;
	// li r11,2
	ctx.r11.s64 = 2;
	// stw r11,20(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20, ctx.r11.u32);
loc_825D537C:
	// li r11,127
	ctx.r11.s64 = 127;
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
loc_825D5384:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D5398"))) PPC_WEAK_FUNC(sub_825D5398);
PPC_FUNC_IMPL(__imp__sub_825D5398) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
loc_825D53AC:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825d53e4
	if (ctx.cr6.lt) goto loc_825D53E4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825D53C8;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825d53ac
	if (ctx.cr6.eq) goto loc_825D53AC;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825D53E4:
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r5,r11,6
	ctx.r5.s64 = ctx.r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r10,r10,8,63
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// lbz r6,2(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lbz r9,5(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,3(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// rldicr r11,r10,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// stw r5,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r5.u32);
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r9,r11,r9
	ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// neg r8,r11
	ctx.r8.s64 = -ctx.r11.s64;
	// addi r11,r11,48
	ctx.r11.s64 = ctx.r11.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D5464"))) PPC_WEAK_FUNC(sub_825D5464);
PPC_FUNC_IMPL(__imp__sub_825D5464) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D5468"))) PPC_WEAK_FUNC(sub_825D5468);
PPC_FUNC_IMPL(__imp__sub_825D5468) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// clrldi r10,r4,32
	ctx.r10.u64 = ctx.r4.u64 & 0xFFFFFFFF;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// sld r11,r11,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r4,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r4.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// bge 0x825d55c0
	if (!ctx.cr0.lt) goto loc_825D55C0;
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825d5528
	if (ctx.cr6.lt) goto loc_825D5528;
loc_825D54B0:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bgt cr6,0x825d5500
	if (ctx.cr6.gt) goto loc_825D5500;
loc_825D54BC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r9,r10,8
	ctx.r9.s64 = ctx.r10.s64 + 8;
	// ld r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subfic r10,r10,40
	ctx.xer.ca = ctx.r10.u32 <= 40;
	ctx.r10.s64 = 40 - ctx.r10.s64;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// sld r10,r7,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r7.u64 << (ctx.r10.u8 & 0x7F));
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// ble cr6,0x825d54bc
	if (!ctx.cr6.gt) goto loc_825D54BC;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stw r11,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r11.u32);
	// bge cr6,0x825d55c0
	if (!ctx.cr6.lt) goto loc_825D55C0;
loc_825D5500:
	// lwz r10,24(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x825d5594
	if (!ctx.cr6.eq) goto loc_825D5594;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5028
	ctx.lr = 0x825D5514;
	sub_825D5028(ctx, base);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bge cr6,0x825d54b0
	if (!ctx.cr6.lt) goto loc_825D54B0;
loc_825D5528:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r4,2(r11)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r5,3(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// lbz r7,5(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// rldicr r8,r9,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r8,r8,r4
	ctx.r8.u64 = ctx.r8.u64 + ctx.r4.u64;
	// rldicr r8,r8,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// rldicr r8,r8,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// rldicr r8,r8,8,55
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// neg r7,r10
	ctx.r7.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r7,r7
	ctx.r7.s64 = ctx.r7.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r10,r8,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x40 ? 0 : (ctx.r8.u64 << (ctx.r7.u8 & 0x7F));
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// b 0x825d55bc
	goto loc_825D55BC;
loc_825D5594:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpwi cr6,r10,-16
	ctx.cr6.compare<int32_t>(ctx.r10.s32, -16, ctx.xer);
	// bge cr6,0x825d55bc
	if (!ctx.cr6.lt) goto loc_825D55BC;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825d55b4
	if (!ctx.cr6.eq) goto loc_825D55B4;
	// li r10,2
	ctx.r10.s64 = 2;
	// stw r10,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r10.u32);
loc_825D55B4:
	// li r10,127
	ctx.r10.s64 = 127;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
loc_825D55BC:
	// stw r11,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r11.u32);
loc_825D55C0:
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D55D4"))) PPC_WEAK_FUNC(sub_825D55D4);
PPC_FUNC_IMPL(__imp__sub_825D55D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D55D8"))) PPC_WEAK_FUNC(sub_825D55D8);
PPC_FUNC_IMPL(__imp__sub_825D55D8) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,452(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 452);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d55f0
	if (!ctx.cr6.eq) goto loc_825D55F0;
	// lwz r11,3112(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3112);
	// lwz r10,3116(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3116);
	// b 0x825d561c
	goto loc_825D561C;
loc_825D55F0:
	// lwz r11,3900(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3900);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d5624
	if (ctx.cr6.eq) goto loc_825D5624;
	// lwz r11,3904(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3904);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d5614
	if (ctx.cr6.eq) goto loc_825D5614;
	// lwz r11,3092(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3092);
	// lwz r10,3088(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3088);
	// b 0x825d561c
	goto loc_825D561C;
loc_825D5614:
	// lwz r11,3100(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3100);
	// lwz r10,3096(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3096);
loc_825D561C:
	// stw r11,3084(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3084, ctx.r11.u32);
	// stw r10,3080(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3080, ctx.r10.u32);
loc_825D5624:
	// lwz r11,3900(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3900);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// lwz r11,3904(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3904);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d5650
	if (ctx.cr6.eq) goto loc_825D5650;
	// lwz r11,3092(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3092);
	// lwz r10,3088(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3088);
	// stw r11,3104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3104, ctx.r11.u32);
	// stw r10,3108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3108, ctx.r10.u32);
	// blr 
	return;
loc_825D5650:
	// lwz r11,3100(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3100);
	// lwz r10,3096(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3096);
	// stw r11,3104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3104, ctx.r11.u32);
	// stw r10,3108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3108, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D5664"))) PPC_WEAK_FUNC(sub_825D5664);
PPC_FUNC_IMPL(__imp__sub_825D5664) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D5668"))) PPC_WEAK_FUNC(sub_825D5668);
PPC_FUNC_IMPL(__imp__sub_825D5668) {
	PPC_FUNC_PROLOGUE();
	// lbz r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// lbz r11,1(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// lbz r9,5(r3)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r3.u32 + 5);
	// lbz r8,4(r3)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r3.u32 + 4);
	// lbz r7,3(r3)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r3.u32 + 3);
	// stb r10,5(r3)
	PPC_STORE_U8(ctx.r3.u32 + 5, ctx.r10.u8);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// lbz r11,2(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 2);
	// stb r9,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r9.u8);
	// stb r8,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, ctx.r8.u8);
	// stb r7,2(r3)
	PPC_STORE_U8(ctx.r3.u32 + 2, ctx.r7.u8);
	// stb r10,4(r3)
	PPC_STORE_U8(ctx.r3.u32 + 4, ctx.r10.u8);
	// stb r11,3(r3)
	PPC_STORE_U8(ctx.r3.u32 + 3, ctx.r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D56A0"))) PPC_WEAK_FUNC(sub_825D56A0);
PPC_FUNC_IMPL(__imp__sub_825D56A0) {
	PPC_FUNC_PROLOGUE();
	// lwz r8,15472(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15472);
	// rlwinm r9,r4,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r4,248(r3)
	PPC_STORE_U32(ctx.r3.u32 + 248, ctx.r4.u32);
	// cmpwi cr6,r8,6
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 6, ctx.xer);
	// blt cr6,0x825d575c
	if (ctx.cr6.lt) goto loc_825D575C;
	// lwz r11,252(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 252);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r10,6548(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 6548);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r8,320(r3)
	PPC_STORE_U32(ctx.r3.u32 + 320, ctx.r8.u32);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,-16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + -16);
	// stw r10,316(r3)
	PPC_STORE_U32(ctx.r3.u32 + 316, ctx.r10.u32);
	// lwz r10,-20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + -20);
	// stw r10,312(r3)
	PPC_STORE_U32(ctx.r3.u32 + 312, ctx.r10.u32);
	// lwz r11,-4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// lwz r10,2968(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2968);
	// stw r11,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r11.u32);
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// stw r11,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r11.u32);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825d5730
	if (ctx.cr6.eq) goto loc_825D5730;
	// lwz r11,1900(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// sth r8,16(r11)
	PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r8.u16);
	// lwz r11,1900(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// sth r8,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r8.u16);
	// lwz r11,1904(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// sth r8,16(r11)
	PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r8.u16);
	// lwz r11,1904(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// sth r8,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r8.u16);
	// blr 
	return;
loc_825D5730:
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// twllei r11,0
	// addi r10,r10,1024
	ctx.r10.s64 = ctx.r10.s64 + 1024;
	// divw r9,r10,r11
	ctx.r9.s32 = ctx.r10.s32 / ctx.r11.s32;
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// andc r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 & ~ctx.r10.u64;
	// twlgei r11,-1
	// lwz r11,1900(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// sth r9,16(r11)
	PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r9.u16);
	// b 0x825d5860
	goto loc_825D5860;
loc_825D575C:
	// not r11,r4
	ctx.r11.u64 = ~ctx.r4.u64;
	// stw r9,312(r3)
	PPC_STORE_U32(ctx.r3.u32 + 312, ctx.r9.u32);
	// li r10,8
	ctx.r10.s64 = 8;
	// clrlwi r11,r11,31
	ctx.r11.u64 = ctx.r11.u32 & 0x1;
	// cmpwi cr6,r8,3
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 3, ctx.xer);
	// subf r7,r11,r4
	ctx.r7.s64 = ctx.r4.s64 - ctx.r11.s64;
	// stw r10,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r10.u32);
	// stw r10,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r10.u32);
	// stw r11,320(r3)
	PPC_STORE_U32(ctx.r3.u32 + 320, ctx.r11.u32);
	// stw r7,316(r3)
	PPC_STORE_U32(ctx.r3.u32 + 316, ctx.r7.u32);
	// bge cr6,0x825d5790
	if (!ctx.cr6.lt) goto loc_825D5790;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// bnelr cr6
	if (!ctx.cr6.eq) return;
loc_825D5790:
	// cmpwi cr6,r4,4
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 4, ctx.xer);
	// bgt cr6,0x825d57c0
	if (ctx.cr6.gt) goto loc_825D57C0;
	// lwz r11,14756(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14756);
	// stw r10,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r10.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r10,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r10.u32);
	// beq cr6,0x825d582c
	if (ctx.cr6.eq) goto loc_825D582C;
	// cmpwi cr6,r4,2
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 2, ctx.xer);
	// bgt cr6,0x825d582c
	if (ctx.cr6.gt) goto loc_825D582C;
	// stw r9,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r9.u32);
	// stw r9,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r9.u32);
	// b 0x825d582c
	goto loc_825D582C;
loc_825D57C0:
	// cmpwi cr6,r8,4
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 4, ctx.xer);
	// blt cr6,0x825d57d8
	if (ctx.cr6.lt) goto loc_825D57D8;
	// srawi r11,r4,1
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r4.s32 >> 1;
	// addi r11,r11,6
	ctx.r11.s64 = ctx.r11.s64 + 6;
	// stw r11,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r11.u32);
	// b 0x825d5828
	goto loc_825D5828;
loc_825D57D8:
	// cmpwi cr6,r4,8
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 8, ctx.xer);
	// bgt cr6,0x825d57f8
	if (ctx.cr6.gt) goto loc_825D57F8;
	// addi r11,r4,13
	ctx.r11.s64 = ctx.r4.s64 + 13;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// stw r10,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r10.u32);
	// stw r11,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r11.u32);
	// b 0x825d582c
	goto loc_825D582C;
loc_825D57F8:
	// cmpwi cr6,r4,24
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 24, ctx.xer);
	// bgt cr6,0x825d5818
	if (ctx.cr6.gt) goto loc_825D5818;
	// addi r11,r4,13
	ctx.r11.s64 = ctx.r4.s64 + 13;
	// addi r10,r4,8
	ctx.r10.s64 = ctx.r4.s64 + 8;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// stw r10,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r10.u32);
	// stw r11,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r11.u32);
	// b 0x825d582c
	goto loc_825D582C;
loc_825D5818:
	// addi r10,r4,-6
	ctx.r10.s64 = ctx.r4.s64 + -6;
	// addi r11,r4,-8
	ctx.r11.s64 = ctx.r4.s64 + -8;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,300(r3)
	PPC_STORE_U32(ctx.r3.u32 + 300, ctx.r10.u32);
loc_825D5828:
	// stw r11,296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 296, ctx.r11.u32);
loc_825D582C:
	// cmpwi cr6,r8,3
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 3, ctx.xer);
	// bltlr cr6
	if (ctx.cr6.lt) return;
	// lwz r11,296(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 296);
	// lwz r9,1900(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// twllei r11,0
	// addi r10,r10,1024
	ctx.r10.s64 = ctx.r10.s64 + 1024;
	// divw r8,r10,r11
	ctx.r8.s32 = ctx.r10.s32 / ctx.r11.s32;
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// andc r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 & ~ctx.r10.u64;
	// sth r8,16(r9)
	PPC_STORE_U16(ctx.r9.u32 + 16, ctx.r8.u16);
	// twlgei r11,-1
loc_825D5860:
	// lwz r11,1900(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// lhz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 16);
	// sth r10,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r10.u16);
	// lwz r11,300(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 300);
	// lwz r9,1904(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// twllei r11,0
	// addi r10,r10,1024
	ctx.r10.s64 = ctx.r10.s64 + 1024;
	// divw r8,r10,r11
	ctx.r8.s32 = ctx.r10.s32 / ctx.r11.s32;
	// rotlwi r10,r10,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// andc r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 & ~ctx.r10.u64;
	// sth r8,16(r9)
	PPC_STORE_U16(ctx.r9.u32 + 16, ctx.r8.u16);
	// twlgei r11,-1
	// lwz r11,1904(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// lhz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + 16);
	// sth r10,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r10.u16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D58A8"))) PPC_WEAK_FUNC(sub_825D58A8);
PPC_FUNC_IMPL(__imp__sub_825D58A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba14
	ctx.lr = 0x825D58B0;
	sub_8239BA14(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// lwz r11,15472(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 15472);
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// blt cr6,0x825d5990
	if (ctx.cr6.lt) goto loc_825D5990;
	// lwz r11,3884(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3884);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d5c14
	if (ctx.cr6.eq) goto loc_825D5C14;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d5944
	if (!ctx.cr6.lt) goto loc_825D5944;
loc_825D58EC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d5944
	if (ctx.cr6.eq) goto loc_825D5944;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d5934
	if (!ctx.cr0.lt) goto loc_825D5934;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5934;
	sub_825D5398(ctx, base);
loc_825D5934:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d58ec
	if (ctx.cr6.gt) goto loc_825D58EC;
loc_825D5944:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d5980
	if (!ctx.cr0.lt) goto loc_825D5980;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5980;
	sub_825D5398(ctx, base);
loc_825D5980:
	// stw r30,3948(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3948, ctx.r30.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
loc_825D5990:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r28,0
	ctx.r28.s64 = 0;
	// li r30,5
	ctx.r30.s64 = 5;
	// mr r29,r28
	ctx.r29.u64 = ctx.r28.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825d5a08
	if (!ctx.cr6.lt) goto loc_825D5A08;
loc_825D59B0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d5a08
	if (ctx.cr6.eq) goto loc_825D5A08;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d59f8
	if (!ctx.cr0.lt) goto loc_825D59F8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D59F8;
	sub_825D5398(ctx, base);
loc_825D59F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d59b0
	if (ctx.cr6.gt) goto loc_825D59B0;
loc_825D5A08:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d5a44
	if (!ctx.cr0.lt) goto loc_825D5A44;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5A44;
	sub_825D5398(ctx, base);
loc_825D5A44:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d5a74
	if (ctx.cr6.eq) goto loc_825D5A74;
loc_825D5A54:
	// li r11,30
	ctx.r11.s64 = 30;
	// stw r28,3900(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3900, ctx.r28.u32);
	// li r10,500
	ctx.r10.s64 = 500;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,3656(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3656, ctx.r11.u32);
	// stw r10,3660(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3660, ctx.r10.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
loc_825D5A74:
	// lwz r11,3656(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3656);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d5a84
	if (!ctx.cr6.eq) goto loc_825D5A84;
	// stw r30,3656(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3656, ctx.r30.u32);
loc_825D5A84:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// li r30,11
	ctx.r30.s64 = 11;
	// mr r29,r28
	ctx.r29.u64 = ctx.r28.u64;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,11
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 11, ctx.xer);
	// bge cr6,0x825d5af4
	if (!ctx.cr6.lt) goto loc_825D5AF4;
loc_825D5A9C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d5af4
	if (ctx.cr6.eq) goto loc_825D5AF4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d5ae4
	if (!ctx.cr0.lt) goto loc_825D5AE4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5AE4;
	sub_825D5398(ctx, base);
loc_825D5AE4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d5a9c
	if (ctx.cr6.gt) goto loc_825D5A9C;
loc_825D5AF4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d5b30
	if (!ctx.cr0.lt) goto loc_825D5B30;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5B30;
	sub_825D5398(ctx, base);
loc_825D5B30:
	// lis r11,-32128
	ctx.r11.s64 = -2105540608;
	// stw r30,3660(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3660, ctx.r30.u32);
	// lis r10,-32128
	ctx.r10.s64 = -2105540608;
	// stw r30,22292(r11)
	PPC_STORE_U32(ctx.r11.u32 + 22292, ctx.r30.u32);
	// lwz r11,3656(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3656);
	// stw r11,22296(r10)
	PPC_STORE_U32(ctx.r10.u32 + 22296, ctx.r11.u32);
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d5a54
	if (!ctx.cr6.eq) goto loc_825D5A54;
	// lwz r11,15472(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 15472);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825d5c14
	if (ctx.cr6.eq) goto loc_825D5C14;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// li r30,1
	ctx.r30.s64 = 1;
	// mr r29,r28
	ctx.r29.u64 = ctx.r28.u64;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d5bd4
	if (!ctx.cr6.lt) goto loc_825D5BD4;
loc_825D5B7C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d5bd4
	if (ctx.cr6.eq) goto loc_825D5BD4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d5bc4
	if (!ctx.cr0.lt) goto loc_825D5BC4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5BC4;
	sub_825D5398(ctx, base);
loc_825D5BC4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d5b7c
	if (ctx.cr6.gt) goto loc_825D5B7C;
loc_825D5BD4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d5c10
	if (!ctx.cr0.lt) goto loc_825D5C10;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5C10;
	sub_825D5398(ctx, base);
loc_825D5C10:
	// stw r30,3900(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3900, ctx.r30.u32);
loc_825D5C14:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
}

__attribute__((alias("__imp__sub_825D5C20"))) PPC_WEAK_FUNC(sub_825D5C20);
PPC_FUNC_IMPL(__imp__sub_825D5C20) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba14
	ctx.lr = 0x825D5C28;
	sub_8239BA14(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// li r30,5
	ctx.r30.s64 = 5;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825d5ca4
	if (!ctx.cr6.lt) goto loc_825D5CA4;
loc_825D5C4C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d5ca4
	if (ctx.cr6.eq) goto loc_825D5CA4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d5c94
	if (!ctx.cr0.lt) goto loc_825D5C94;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5C94;
	sub_825D5398(ctx, base);
loc_825D5C94:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d5c4c
	if (ctx.cr6.gt) goto loc_825D5C4C;
loc_825D5CA4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d5ce0
	if (!ctx.cr0.lt) goto loc_825D5CE0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5CE0;
	sub_825D5398(ctx, base);
loc_825D5CE0:
	// lis r11,-32128
	ctx.r11.s64 = -2105540608;
	// stw r28,3656(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3656, ctx.r28.u32);
	// li r30,11
	ctx.r30.s64 = 11;
	// li r29,0
	ctx.r29.s64 = 0;
	// stw r28,22296(r11)
	PPC_STORE_U32(ctx.r11.u32 + 22296, ctx.r28.u32);
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,11
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 11, ctx.xer);
	// bge cr6,0x825d5d60
	if (!ctx.cr6.lt) goto loc_825D5D60;
loc_825D5D08:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d5d60
	if (ctx.cr6.eq) goto loc_825D5D60;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d5d50
	if (!ctx.cr0.lt) goto loc_825D5D50;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5D50;
	sub_825D5398(ctx, base);
loc_825D5D50:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d5d08
	if (ctx.cr6.gt) goto loc_825D5D08;
loc_825D5D60:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d5d9c
	if (!ctx.cr0.lt) goto loc_825D5D9C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5D9C;
	sub_825D5398(ctx, base);
loc_825D5D9C:
	// lis r11,-32128
	ctx.r11.s64 = -2105540608;
	// stw r28,3660(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3660, ctx.r28.u32);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// stw r28,22292(r11)
	PPC_STORE_U32(ctx.r11.u32 + 22292, ctx.r28.u32);
	// li r11,1
	ctx.r11.s64 = 1;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// stw r11,3900(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3900, ctx.r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d5e24
	if (!ctx.cr6.lt) goto loc_825D5E24;
loc_825D5DCC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d5e24
	if (ctx.cr6.eq) goto loc_825D5E24;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d5e14
	if (!ctx.cr0.lt) goto loc_825D5E14;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5E14;
	sub_825D5398(ctx, base);
loc_825D5E14:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d5dcc
	if (ctx.cr6.gt) goto loc_825D5DCC;
loc_825D5E24:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d5e60
	if (!ctx.cr0.lt) goto loc_825D5E60;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5E60;
	sub_825D5398(ctx, base);
loc_825D5E60:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,3888(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3888, ctx.r28.u32);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d5ed8
	if (!ctx.cr6.lt) goto loc_825D5ED8;
loc_825D5E80:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d5ed8
	if (ctx.cr6.eq) goto loc_825D5ED8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d5ec8
	if (!ctx.cr0.lt) goto loc_825D5EC8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5EC8;
	sub_825D5398(ctx, base);
loc_825D5EC8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d5e80
	if (ctx.cr6.gt) goto loc_825D5E80;
loc_825D5ED8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d5f14
	if (!ctx.cr0.lt) goto loc_825D5F14;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5F14;
	sub_825D5398(ctx, base);
loc_825D5F14:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,3892(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3892, ctx.r28.u32);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d5f8c
	if (!ctx.cr6.lt) goto loc_825D5F8C;
loc_825D5F34:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d5f8c
	if (ctx.cr6.eq) goto loc_825D5F8C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d5f7c
	if (!ctx.cr0.lt) goto loc_825D5F7C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5F7C;
	sub_825D5398(ctx, base);
loc_825D5F7C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d5f34
	if (ctx.cr6.gt) goto loc_825D5F34;
loc_825D5F8C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d5fc8
	if (!ctx.cr0.lt) goto loc_825D5FC8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D5FC8;
	sub_825D5398(ctx, base);
loc_825D5FC8:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,436(r27)
	PPC_STORE_U32(ctx.r27.u32 + 436, ctx.r28.u32);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d6040
	if (!ctx.cr6.lt) goto loc_825D6040;
loc_825D5FE8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d6040
	if (ctx.cr6.eq) goto loc_825D6040;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d6030
	if (!ctx.cr0.lt) goto loc_825D6030;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6030;
	sub_825D5398(ctx, base);
loc_825D6030:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d5fe8
	if (ctx.cr6.gt) goto loc_825D5FE8;
loc_825D6040:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d607c
	if (!ctx.cr0.lt) goto loc_825D607C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D607C;
	sub_825D5398(ctx, base);
loc_825D607C:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,3884(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3884, ctx.r28.u32);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d60f4
	if (!ctx.cr6.lt) goto loc_825D60F4;
loc_825D609C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d60f4
	if (ctx.cr6.eq) goto loc_825D60F4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d60e4
	if (!ctx.cr0.lt) goto loc_825D60E4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D60E4;
	sub_825D5398(ctx, base);
loc_825D60E4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d609c
	if (ctx.cr6.gt) goto loc_825D609C;
loc_825D60F4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d6130
	if (!ctx.cr0.lt) goto loc_825D6130;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6130;
	sub_825D5398(ctx, base);
loc_825D6130:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,444(r27)
	PPC_STORE_U32(ctx.r27.u32 + 444, ctx.r28.u32);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d61a8
	if (!ctx.cr6.lt) goto loc_825D61A8;
loc_825D6150:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d61a8
	if (ctx.cr6.eq) goto loc_825D61A8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d6198
	if (!ctx.cr0.lt) goto loc_825D6198;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6198;
	sub_825D5398(ctx, base);
loc_825D6198:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d6150
	if (ctx.cr6.gt) goto loc_825D6150;
loc_825D61A8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d61e4
	if (!ctx.cr0.lt) goto loc_825D61E4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D61E4;
	sub_825D5398(ctx, base);
loc_825D61E4:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// stw r28,396(r27)
	PPC_STORE_U32(ctx.r27.u32 + 396, ctx.r28.u32);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825d625c
	if (!ctx.cr6.lt) goto loc_825D625C;
loc_825D6204:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d625c
	if (ctx.cr6.eq) goto loc_825D625C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d624c
	if (!ctx.cr0.lt) goto loc_825D624C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D624C;
	sub_825D5398(ctx, base);
loc_825D624C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d6204
	if (ctx.cr6.gt) goto loc_825D6204;
loc_825D625C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d6298
	if (!ctx.cr0.lt) goto loc_825D6298;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6298;
	sub_825D5398(ctx, base);
loc_825D6298:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r30,15464(r27)
	PPC_STORE_U32(ctx.r27.u32 + 15464, ctx.r30.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
}

__attribute__((alias("__imp__sub_825D62A8"))) PPC_WEAK_FUNC(sub_825D62A8);
PPC_FUNC_IMPL(__imp__sub_825D62A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825D62B0;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// li r30,24
	ctx.r30.s64 = 24;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,24
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 24, ctx.xer);
	// bge cr6,0x825d632c
	if (!ctx.cr6.lt) goto loc_825D632C;
loc_825D62D4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d632c
	if (ctx.cr6.eq) goto loc_825D632C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d631c
	if (!ctx.cr0.lt) goto loc_825D631C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D631C;
	sub_825D5398(ctx, base);
loc_825D631C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d62d4
	if (ctx.cr6.gt) goto loc_825D62D4;
loc_825D632C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d6368
	if (!ctx.cr0.lt) goto loc_825D6368;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6368;
	sub_825D5398(ctx, base);
loc_825D6368:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// beq cr6,0x825d637c
	if (ctx.cr6.eq) goto loc_825D637C;
loc_825D6370:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825D637C:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825d63f0
	if (!ctx.cr6.lt) goto loc_825D63F0;
loc_825D6398:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d63f0
	if (ctx.cr6.eq) goto loc_825D63F0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d63e0
	if (!ctx.cr0.lt) goto loc_825D63E0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D63E0;
	sub_825D5398(ctx, base);
loc_825D63E0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d6398
	if (ctx.cr6.gt) goto loc_825D6398;
loc_825D63F0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d642c
	if (!ctx.cr0.lt) goto loc_825D642C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D642C;
	sub_825D5398(ctx, base);
loc_825D642C:
	// cmplwi cr6,r30,182
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 182, ctx.xer);
	// bne cr6,0x825d6370
	if (!ctx.cr6.eq) goto loc_825D6370;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825d64a8
	if (!ctx.cr6.lt) goto loc_825D64A8;
loc_825D6450:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d64a8
	if (ctx.cr6.eq) goto loc_825D64A8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d6498
	if (!ctx.cr0.lt) goto loc_825D6498;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6498;
	sub_825D5398(ctx, base);
loc_825D6498:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d6450
	if (ctx.cr6.gt) goto loc_825D6450;
loc_825D64A8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d64e4
	if (!ctx.cr0.lt) goto loc_825D64E4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D64E4;
	sub_825D5398(ctx, base);
loc_825D64E4:
	// li r28,0
	ctx.r28.s64 = 0;
	// stw r30,284(r26)
	PPC_STORE_U32(ctx.r26.u32 + 284, ctx.r30.u32);
	// li r25,1
	ctx.r25.s64 = 1;
loc_825D64F0:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d6568
	if (!ctx.cr6.lt) goto loc_825D6568;
loc_825D650C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d6568
	if (ctx.cr6.eq) goto loc_825D6568;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d6558
	if (!ctx.cr0.lt) goto loc_825D6558;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6558;
	sub_825D5398(ctx, base);
loc_825D6558:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d650c
	if (ctx.cr6.gt) goto loc_825D650C;
loc_825D6568:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d65a4
	if (!ctx.cr0.lt) goto loc_825D65A4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D65A4;
	sub_825D5398(ctx, base);
loc_825D65A4:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d65b4
	if (ctx.cr6.eq) goto loc_825D65B4;
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// b 0x825d64f0
	goto loc_825D64F0;
loc_825D65B4:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// clrldi r11,r28,32
	ctx.r11.u64 = ctx.r28.u64 & 0xFFFFFFFF;
	// ld r10,3600(r26)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r26.u32 + 3600);
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// add r27,r11,r10
	ctx.r27.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d6618
	if (!ctx.cr6.lt) goto loc_825D6618;
loc_825D65D8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d6618
	if (ctx.cr6.eq) goto loc_825D6618;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825d6608
	if (!ctx.cr0.lt) goto loc_825D6608;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6608;
	sub_825D5398(ctx, base);
loc_825D6608:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d65d8
	if (ctx.cr6.gt) goto loc_825D65D8;
loc_825D6618:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825d6640
	if (!ctx.cr0.lt) goto loc_825D6640;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6640;
	sub_825D5398(ctx, base);
loc_825D6640:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r30,3632(r26)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3632);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825d6664
	if (!ctx.cr6.eq) goto loc_825D6664;
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x825d6704
	goto loc_825D6704;
loc_825D6664:
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825d66c4
	if (!ctx.cr6.gt) goto loc_825D66C4;
loc_825D666C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d66c4
	if (ctx.cr6.eq) goto loc_825D66C4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d66b4
	if (!ctx.cr0.lt) goto loc_825D66B4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D66B4;
	sub_825D5398(ctx, base);
loc_825D66B4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d666c
	if (ctx.cr6.gt) goto loc_825D666C;
loc_825D66C4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d6700
	if (!ctx.cr0.lt) goto loc_825D6700;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6700;
	sub_825D5398(ctx, base);
loc_825D6700:
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_825D6704:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// clrldi r28,r11,32
	ctx.r28.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d6760
	if (!ctx.cr6.lt) goto loc_825D6760;
loc_825D6720:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d6760
	if (ctx.cr6.eq) goto loc_825D6760;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825d6750
	if (!ctx.cr0.lt) goto loc_825D6750;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6750;
	sub_825D5398(ctx, base);
loc_825D6750:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d6720
	if (ctx.cr6.gt) goto loc_825D6720;
loc_825D6760:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825d6788
	if (!ctx.cr0.lt) goto loc_825D6788;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6788;
	sub_825D5398(ctx, base);
loc_825D6788:
	// lwz r10,3568(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3568);
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// ld r11,3600(r26)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r26.u32 + 3600);
	// li r29,0
	ctx.r29.s64 = 0;
	// extsw r10,r10
	ctx.r10.s64 = ctx.r10.s32;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// std r27,3600(r26)
	PPC_STORE_U64(ctx.r26.u32 + 3600, ctx.r27.u64);
	// mulld r10,r10,r27
	ctx.r10.s64 = ctx.r10.s64 * ctx.r27.s64;
	// std r11,3616(r26)
	PPC_STORE_U64(ctx.r26.u32 + 3616, ctx.r11.u64);
	// add r10,r10,r28
	ctx.r10.u64 = ctx.r10.u64 + ctx.r28.u64;
	// std r10,3576(r26)
	PPC_STORE_U64(ctx.r26.u32 + 3576, ctx.r10.u64);
	// ld r10,3608(r26)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r26.u32 + 3608);
	// std r11,3608(r26)
	PPC_STORE_U64(ctx.r26.u32 + 3608, ctx.r11.u64);
	// std r10,3624(r26)
	PPC_STORE_U64(ctx.r26.u32 + 3624, ctx.r10.u64);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d6828
	if (!ctx.cr6.lt) goto loc_825D6828;
loc_825D67D0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d6828
	if (ctx.cr6.eq) goto loc_825D6828;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d6818
	if (!ctx.cr0.lt) goto loc_825D6818;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6818;
	sub_825D5398(ctx, base);
loc_825D6818:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d67d0
	if (ctx.cr6.gt) goto loc_825D67D0;
loc_825D6828:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d6864
	if (!ctx.cr0.lt) goto loc_825D6864;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6864;
	sub_825D5398(ctx, base);
loc_825D6864:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d6370
	if (ctx.cr6.eq) goto loc_825D6370;
	// lwz r11,284(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 284);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d6930
	if (!ctx.cr6.eq) goto loc_825D6930;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d68ec
	if (!ctx.cr6.lt) goto loc_825D68EC;
loc_825D6894:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d68ec
	if (ctx.cr6.eq) goto loc_825D68EC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d68dc
	if (!ctx.cr0.lt) goto loc_825D68DC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D68DC;
	sub_825D5398(ctx, base);
loc_825D68DC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d6894
	if (ctx.cr6.gt) goto loc_825D6894;
loc_825D68EC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d6928
	if (!ctx.cr0.lt) goto loc_825D6928;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6928;
	sub_825D5398(ctx, base);
loc_825D6928:
	// stw r30,3904(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3904, ctx.r30.u32);
	// b 0x825d6938
	goto loc_825D6938;
loc_825D6930:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,3904(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3904, ctx.r11.u32);
loc_825D6938:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825d6990
	if (!ctx.cr6.lt) goto loc_825D6990;
loc_825D6950:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d6990
	if (ctx.cr6.eq) goto loc_825D6990;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825d6980
	if (!ctx.cr0.lt) goto loc_825D6980;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6980;
	sub_825D5398(ctx, base);
loc_825D6980:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d6950
	if (ctx.cr6.gt) goto loc_825D6950;
loc_825D6990:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825d69b8
	if (!ctx.cr0.lt) goto loc_825D69B8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D69B8;
	sub_825D5398(ctx, base);
loc_825D69B8:
	// lwz r11,284(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d6a88
	if (!ctx.cr6.eq) goto loc_825D6A88;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,5
	ctx.r30.s64 = 5;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825d6a38
	if (!ctx.cr6.lt) goto loc_825D6A38;
loc_825D69E0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d6a38
	if (ctx.cr6.eq) goto loc_825D6A38;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d6a28
	if (!ctx.cr0.lt) goto loc_825D6A28;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6A28;
	sub_825D5398(ctx, base);
loc_825D6A28:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d69e0
	if (ctx.cr6.gt) goto loc_825D69E0;
loc_825D6A38:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d6a74
	if (!ctx.cr0.lt) goto loc_825D6A74;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6A74;
	sub_825D5398(ctx, base);
loc_825D6A74:
	// stw r30,248(r26)
	PPC_STORE_U32(ctx.r26.u32 + 248, ctx.r30.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r25,3556(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3556, ctx.r25.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825D6A88:
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825d6c10
	if (!ctx.cr6.eq) goto loc_825D6C10;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,5
	ctx.r30.s64 = 5;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825d6b04
	if (!ctx.cr6.lt) goto loc_825D6B04;
loc_825D6AAC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d6b04
	if (ctx.cr6.eq) goto loc_825D6B04;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d6af4
	if (!ctx.cr0.lt) goto loc_825D6AF4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6AF4;
	sub_825D5398(ctx, base);
loc_825D6AF4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d6aac
	if (ctx.cr6.gt) goto loc_825D6AAC;
loc_825D6B04:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d6b40
	if (!ctx.cr0.lt) goto loc_825D6B40;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6B40;
	sub_825D5398(ctx, base);
loc_825D6B40:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// stw r28,248(r26)
	PPC_STORE_U32(ctx.r26.u32 + 248, ctx.r28.u32);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825d6bb8
	if (!ctx.cr6.lt) goto loc_825D6BB8;
loc_825D6B60:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d6bb8
	if (ctx.cr6.eq) goto loc_825D6BB8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d6ba8
	if (!ctx.cr0.lt) goto loc_825D6BA8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6BA8;
	sub_825D5398(ctx, base);
loc_825D6BA8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d6b60
	if (ctx.cr6.gt) goto loc_825D6B60;
loc_825D6BB8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d6bf4
	if (!ctx.cr0.lt) goto loc_825D6BF4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6BF4;
	sub_825D5398(ctx, base);
loc_825D6BF4:
	// addi r11,r30,-1
	ctx.r11.s64 = ctx.r30.s64 + -1;
	// stw r30,3556(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3556, ctx.r30.u32);
	// li r10,16
	ctx.r10.s64 = 16;
	// slw r11,r25,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r25.u32 << (ctx.r11.u8 & 0x3F));
	// slw r10,r10,r30
	ctx.r10.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// stw r11,3564(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3564, ctx.r11.u32);
	// stw r10,3560(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3560, ctx.r10.u32);
loc_825D6C10:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825D6C1C"))) PPC_WEAK_FUNC(sub_825D6C1C);
PPC_FUNC_IMPL(__imp__sub_825D6C1C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D6C20"))) PPC_WEAK_FUNC(sub_825D6C20);
PPC_FUNC_IMPL(__imp__sub_825D6C20) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba14
	ctx.lr = 0x825D6C28;
	sub_8239BA14(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// clrlwi r29,r30,31
	ctx.r29.u64 = ctx.r30.u32 & 0x1;
	// mr r31,r5
	ctx.r31.u64 = ctx.r5.u64;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x825d6c80
	if (ctx.cr6.eq) goto loc_825D6C80;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r28,r11,0
	ctx.r28.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge 0x825d6c70
	if (!ctx.cr0.lt) goto loc_825D6C70;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6C70;
	sub_825D5398(ctx, base);
loc_825D6C70:
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// rlwimi r11,r28,31,0,0
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r28.u32, 31) & 0x80000000) | (ctx.r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r11.u32);
	// addi r27,r27,20
	ctx.r27.s64 = ctx.r27.s64 + 20;
loc_825D6C80:
	// cmpw cr6,r29,r30
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r30.s32, ctx.xer);
	// bge cr6,0x825d6d94
	if (!ctx.cr6.lt) goto loc_825D6D94;
	// subf r11,r29,r30
	ctx.r11.s64 = ctx.r30.s64 - ctx.r29.s64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r29,r11,1
	ctx.r29.s64 = ctx.r11.s64 + 1;
loc_825D6C98:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge 0x825d6cc4
	if (!ctx.cr0.lt) goto loc_825D6CC4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6CC4;
	sub_825D5398(ctx, base);
loc_825D6CC4:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d6d6c
	if (ctx.cr6.eq) goto loc_825D6D6C;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge 0x825d6cf8
	if (!ctx.cr0.lt) goto loc_825D6CF8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6CF8;
	sub_825D5398(ctx, base);
loc_825D6CF8:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d6d18
	if (ctx.cr6.eq) goto loc_825D6D18;
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// oris r11,r11,32768
	ctx.r11.u64 = ctx.r11.u64 | 2147483648;
	// stw r11,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r11.u32);
	// lwz r11,20(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 20);
	// oris r11,r11,32768
	ctx.r11.u64 = ctx.r11.u64 | 2147483648;
	// b 0x825d6d80
	goto loc_825D6D80;
loc_825D6D18:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge 0x825d6d44
	if (!ctx.cr0.lt) goto loc_825D6D44;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6D44;
	sub_825D5398(ctx, base);
loc_825D6D44:
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d6d64
	if (ctx.cr6.eq) goto loc_825D6D64;
	// clrlwi r11,r11,1
	ctx.r11.u64 = ctx.r11.u32 & 0x7FFFFFFF;
	// stw r11,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r11.u32);
	// lwz r11,20(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 20);
	// oris r11,r11,32768
	ctx.r11.u64 = ctx.r11.u64 | 2147483648;
	// b 0x825d6d80
	goto loc_825D6D80;
loc_825D6D64:
	// oris r11,r11,32768
	ctx.r11.u64 = ctx.r11.u64 | 2147483648;
	// b 0x825d6d74
	goto loc_825D6D74;
loc_825D6D6C:
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// clrlwi r11,r11,1
	ctx.r11.u64 = ctx.r11.u32 & 0x7FFFFFFF;
loc_825D6D74:
	// stw r11,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r11.u32);
	// lwz r11,20(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 20);
	// clrlwi r11,r11,1
	ctx.r11.u64 = ctx.r11.u32 & 0x7FFFFFFF;
loc_825D6D80:
	// addi r29,r29,-1
	ctx.r29.s64 = ctx.r29.s64 + -1;
	// stw r11,20(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20, ctx.r11.u32);
	// addi r27,r27,40
	ctx.r27.s64 = ctx.r27.s64 + 40;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825d6c98
	if (!ctx.cr6.eq) goto loc_825D6C98;
loc_825D6D94:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
}

__attribute__((alias("__imp__sub_825D6D9C"))) PPC_WEAK_FUNC(sub_825D6D9C);
PPC_FUNC_IMPL(__imp__sub_825D6D9C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D6DA0"))) PPC_WEAK_FUNC(sub_825D6DA0);
PPC_FUNC_IMPL(__imp__sub_825D6DA0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825D6DA8;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r6
	ctx.r24.u64 = ctx.r6.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r26,r4
	ctx.r26.u64 = ctx.r4.u64;
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// li r25,0
	ctx.r25.s64 = 0;
	// lwz r27,0(r24)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
loc_825D6DC4:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge 0x825d6df0
	if (!ctx.cr0.lt) goto loc_825D6DF0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6DF0;
	sub_825D5398(ctx, base);
loc_825D6DF0:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825d6ef4
	if (!ctx.cr6.eq) goto loc_825D6EF4;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// li r30,3
	ctx.r30.s64 = 3;
	// li r29,0
	ctx.r29.s64 = 0;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825d6e68
	if (!ctx.cr6.lt) goto loc_825D6E68;
loc_825D6E10:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d6e68
	if (ctx.cr6.eq) goto loc_825D6E68;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d6e58
	if (!ctx.cr0.lt) goto loc_825D6E58;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6E58;
	sub_825D5398(ctx, base);
loc_825D6E58:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d6e10
	if (ctx.cr6.gt) goto loc_825D6E10;
loc_825D6E68:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d6ea4
	if (!ctx.cr0.lt) goto loc_825D6EA4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6EA4;
	sub_825D5398(ctx, base);
loc_825D6EA4:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bgt cr6,0x825d6efc
	if (ctx.cr6.gt) goto loc_825D6EFC;
	// bne cr6,0x825d6ffc
	if (!ctx.cr6.eq) goto loc_825D6FFC;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r30,r11,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// std r9,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r9.u64);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge 0x825d6edc
	if (!ctx.cr0.lt) goto loc_825D6EDC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6EDC;
	sub_825D5398(ctx, base);
loc_825D6EDC:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d6f1c
	if (ctx.cr6.eq) goto loc_825D6F1C;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// bne cr6,0x825d6f0c
	if (!ctx.cr6.eq) goto loc_825D6F0C;
	// li r25,1
	ctx.r25.s64 = 1;
	// b 0x825d6dc4
	goto loc_825D6DC4;
loc_825D6EF4:
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x825d70c8
	goto loc_825D70C8;
loc_825D6EFC:
	// addi r11,r30,-2
	ctx.r11.s64 = ctx.r30.s64 + -2;
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r11.u8 & 0x3F));
	// b 0x825d70c8
	goto loc_825D70C8;
loc_825D6F0C:
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r27,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r27.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D6F1C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// li r30,5
	ctx.r30.s64 = 5;
	// li r29,0
	ctx.r29.s64 = 0;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825d6f8c
	if (!ctx.cr6.lt) goto loc_825D6F8C;
loc_825D6F34:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d6f8c
	if (ctx.cr6.eq) goto loc_825D6F8C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d6f7c
	if (!ctx.cr0.lt) goto loc_825D6F7C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6F7C;
	sub_825D5398(ctx, base);
loc_825D6F7C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d6f34
	if (ctx.cr6.gt) goto loc_825D6F34;
loc_825D6F8C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d6fc8
	if (!ctx.cr0.lt) goto loc_825D6FC8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D6FC8;
	sub_825D5398(ctx, base);
loc_825D6FC8:
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// lwzx r10,r10,r28
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r28.u32);
	// cmpwi cr6,r10,2
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 2, ctx.xer);
	// bne cr6,0x825d6fe4
	if (!ctx.cr6.eq) goto loc_825D6FE4;
	// ori r11,r30,32
	ctx.r11.u64 = ctx.r30.u64 | 32;
	// b 0x825d70c8
	goto loc_825D70C8;
loc_825D6FE4:
	// cmpwi cr6,r10,3
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 3, ctx.xer);
	// beq cr6,0x825d70c8
	if (ctx.cr6.eq) goto loc_825D70C8;
	// li r3,4
	ctx.r3.s64 = 4;
	// stw r30,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r30.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D6FFC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// li r30,4
	ctx.r30.s64 = 4;
	// li r29,0
	ctx.r29.s64 = 0;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bge cr6,0x825d706c
	if (!ctx.cr6.lt) goto loc_825D706C;
loc_825D7014:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d706c
	if (ctx.cr6.eq) goto loc_825D706C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d705c
	if (!ctx.cr0.lt) goto loc_825D705C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D705C;
	sub_825D5398(ctx, base);
loc_825D705C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d7014
	if (ctx.cr6.gt) goto loc_825D7014;
loc_825D706C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d70a8
	if (!ctx.cr0.lt) goto loc_825D70A8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D70A8;
	sub_825D5398(ctx, base);
loc_825D70A8:
	// cmpwi cr6,r30,15
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 15, ctx.xer);
	// bne cr6,0x825d70c0
	if (!ctx.cr6.eq) goto loc_825D70C0;
	// li r3,4
	ctx.r3.s64 = 4;
	// stw r27,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r27.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D70C0:
	// rlwinm r11,r30,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r26
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r26.u32);
loc_825D70C8:
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// beq cr6,0x825d70d4
	if (ctx.cr6.eq) goto loc_825D70D4;
	// xori r11,r11,63
	ctx.r11.u64 = ctx.r11.u64 ^ 63;
loc_825D70D4:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825D70E4"))) PPC_WEAK_FUNC(sub_825D70E4);
PPC_FUNC_IMPL(__imp__sub_825D70E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D70E8"))) PPC_WEAK_FUNC(sub_825D70E8);
PPC_FUNC_IMPL(__imp__sub_825D70E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825D70F0;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// li r4,3
	ctx.r4.s64 = 3;
	// li r24,0
	ctx.r24.s64 = 0;
	// lwz r9,144(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 144);
	// lwz r11,19984(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 19984);
	// lwz r10,268(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 268);
	// mullw r11,r11,r9
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r9.s32);
	// lwz r3,140(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r10
	ctx.r31.u64 = ctx.r11.u64 + ctx.r10.u64;
	// bl 0x82624478
	ctx.lr = 0x825D7128;
	sub_82624478(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d7270
	if (!ctx.cr6.eq) goto loc_825D7270;
	// li r4,3
	ctx.r4.s64 = 3;
	// lwz r3,136(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// bl 0x82624478
	ctx.lr = 0x825D713C;
	sub_82624478(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825d7270
	if (ctx.cr6.eq) goto loc_825D7270;
	// lwz r11,136(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// li r26,0
	ctx.r26.s64 = 0;
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// clrlwi r25,r11,31
	ctx.r25.u64 = ctx.r11.u32 & 0x1;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825d73b4
	if (!ctx.cr6.gt) goto loc_825D73B4;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r27,r10,3804
	ctx.r27.s64 = ctx.r10.s64 + 3804;
loc_825D7164:
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// cmpw cr6,r25,r11
	ctx.cr6.compare<int32_t>(ctx.r25.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x825d725c
	if (!ctx.cr6.lt) goto loc_825D725C;
loc_825D7170:
	// mullw r11,r11,r26
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r26.s32);
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 84);
	// addi r5,r27,60
	ctx.r5.s64 = ctx.r27.s64 + 60;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bl 0x825d6da0
	ctx.lr = 0x825D718C;
	sub_825D6DA0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d75ac
	if (!ctx.cr6.eq) goto loc_825D75AC;
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r29,r29,2
	ctx.r29.s64 = ctx.r29.s64 + 2;
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// mr r8,r11
	ctx.r8.u64 = ctx.r11.u64;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// mr r7,r11
	ctx.r7.u64 = ctx.r11.u64;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// mr r6,r11
	ctx.r6.u64 = ctx.r11.u64;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// mr r5,r11
	ctx.r5.u64 = ctx.r11.u64;
	// rlwinm r11,r28,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r28,r11
	ctx.r11.u64 = ctx.r28.u64 + ctx.r11.u64;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + ctx.r31.u64;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwimi r4,r10,31,0,0
	ctx.r4.u64 = (__builtin_rotateleft32(ctx.r10.u32, 31) & 0x80000000) | (ctx.r4.u64 & 0xFFFFFFFF7FFFFFFF);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// rlwimi r10,r8,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r8.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r4,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r4.u32);
	// stw r10,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r10.u32);
	// lwz r11,136(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// add r10,r28,r11
	ctx.r10.u64 = ctx.r28.u64 + ctx.r11.u64;
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + ctx.r31.u64;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwimi r8,r7,31,0,0
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r7.u32, 31) & 0x80000000) | (ctx.r8.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r8,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
	// lwz r8,20(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// rlwimi r8,r6,31,0,0
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r6.u32, 31) & 0x80000000) | (ctx.r8.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r8,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r8.u32);
	// lwz r11,136(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + ctx.r31.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwimi r10,r5,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r5.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// rlwimi r10,r9,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r10,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r10.u32);
	// lwz r11,136(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825d7170
	if (ctx.cr6.lt) goto loc_825D7170;
loc_825D725C:
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// addi r26,r26,3
	ctx.r26.s64 = ctx.r26.s64 + 3;
	// cmpw cr6,r26,r10
	ctx.cr6.compare<int32_t>(ctx.r26.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825d7164
	if (ctx.cr6.lt) goto loc_825D7164;
	// b 0x825d73b4
	goto loc_825D73B4;
loc_825D7270:
	// lwz r11,140(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// li r4,3
	ctx.r4.s64 = 3;
	// lwz r3,136(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// clrlwi r24,r11,31
	ctx.r24.u64 = ctx.r11.u32 & 0x1;
	// bl 0x82624478
	ctx.lr = 0x825D7284;
	sub_82624478(ctx, base);
	// lwz r11,140(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// mr r25,r3
	ctx.r25.u64 = ctx.r3.u64;
	// mr r26,r24
	ctx.r26.u64 = ctx.r24.u64;
	// cmpw cr6,r24,r11
	ctx.cr6.compare<int32_t>(ctx.r24.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x825d73b4
	if (!ctx.cr6.lt) goto loc_825D73B4;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// addi r27,r11,3804
	ctx.r27.s64 = ctx.r11.s64 + 3804;
loc_825D72A0:
	// lwz r11,136(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// mr r28,r25
	ctx.r28.u64 = ctx.r25.u64;
	// cmpw cr6,r25,r11
	ctx.cr6.compare<int32_t>(ctx.r25.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x825d73a4
	if (!ctx.cr6.lt) goto loc_825D73A4;
loc_825D72B0:
	// mullw r11,r11,r26
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r26.s32);
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 84);
	// addi r5,r27,60
	ctx.r5.s64 = ctx.r27.s64 + 60;
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bl 0x825d6da0
	ctx.lr = 0x825D72CC;
	sub_825D6DA0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825d75ac
	if (!ctx.cr6.eq) goto loc_825D75AC;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r11,r29,2
	ctx.r11.s64 = ctx.r29.s64 + 2;
	// addi r28,r28,3
	ctx.r28.s64 = ctx.r28.s64 + 3;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r6,r10
	ctx.r6.u64 = ctx.r10.u64;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r8,r11,r8
	ctx.r8.u64 = ctx.r11.u64 + ctx.r8.u64;
	// mr r5,r10
	ctx.r5.u64 = ctx.r10.u64;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// srawi r9,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 1;
	// mr r3,r10
	ctx.r3.u64 = ctx.r10.u64;
	// rlwinm r10,r29,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r29,r10
	ctx.r10.u64 = ctx.r29.u64 + ctx.r10.u64;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r31
	ctx.r11.u64 = ctx.r10.u64 + ctx.r31.u64;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwimi r8,r7,31,0,0
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r7.u32, 31) & 0x80000000) | (ctx.r8.u64 & 0xFFFFFFFF7FFFFFFF);
	// lwzx r7,r10,r31
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r31.u32);
	// rlwimi r7,r5,31,0,0
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r5.u32, 31) & 0x80000000) | (ctx.r7.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r8,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
	// stwx r7,r10,r31
	PPC_STORE_U32(ctx.r10.u32 + ctx.r31.u32, ctx.r7.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// rlwimi r10,r6,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r6.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r10,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r10.u32);
	// lwz r11,136(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// add r11,r29,r11
	ctx.r11.u64 = ctx.r29.u64 + ctx.r11.u64;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r11,2
	ctx.r10.s64 = ctx.r11.s64 + 2;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + ctx.r31.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwzx r7,r10,r31
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r31.u32);
	// rlwimi r8,r4,31,0,0
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r4.u32, 31) & 0x80000000) | (ctx.r8.u64 & 0xFFFFFFFF7FFFFFFF);
	// rlwimi r7,r9,31,0,0
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r9.u32, 31) & 0x80000000) | (ctx.r7.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r8,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
	// stwx r7,r10,r31
	PPC_STORE_U32(ctx.r10.u32 + ctx.r31.u32, ctx.r7.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// rlwimi r10,r3,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r3.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r10,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r10.u32);
	// lwz r11,136(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// cmpw cr6,r28,r11
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825d72b0
	if (ctx.cr6.lt) goto loc_825D72B0;
loc_825D73A4:
	// lwz r11,140(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// addi r26,r26,2
	ctx.r26.s64 = ctx.r26.s64 + 2;
	// cmpw cr6,r26,r11
	ctx.cr6.compare<int32_t>(ctx.r26.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825d72a0
	if (ctx.cr6.lt) goto loc_825D72A0;
loc_825D73B4:
	// li r27,0
	ctx.r27.s64 = 0;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// ble cr6,0x825d74b8
	if (!ctx.cr6.gt) goto loc_825D74B8;
loc_825D73C0:
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r29,r8,0
	ctx.r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d73ec
	if (!ctx.cr0.lt) goto loc_825D73EC;
	// bl 0x825d5398
	ctx.lr = 0x825D73EC;
	sub_825D5398(ctx, base);
loc_825D73EC:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x825d7468
	if (ctx.cr6.eq) goto loc_825D7468;
	// lwz r11,140(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// li r29,0
	ctx.r29.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d74ac
	if (!ctx.cr6.gt) goto loc_825D74AC;
loc_825D7404:
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r28,r8,0
	ctx.r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d7430
	if (!ctx.cr0.lt) goto loc_825D7430;
	// bl 0x825d5398
	ctx.lr = 0x825D7430;
	sub_825D5398(ctx, base);
loc_825D7430:
	// lwz r11,136(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// mullw r11,r29,r11
	ctx.r11.s64 = int64_t(ctx.r29.s32) * int64_t(ctx.r11.s32);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	// rlwimi r10,r28,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r28.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r31
	PPC_STORE_U32(ctx.r11.u32 + ctx.r31.u32, ctx.r10.u32);
	// lwz r11,140(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// cmpw cr6,r29,r11
	ctx.cr6.compare<int32_t>(ctx.r29.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825d7404
	if (ctx.cr6.lt) goto loc_825D7404;
	// b 0x825d74ac
	goto loc_825D74AC;
loc_825D7468:
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825d74ac
	if (!ctx.cr6.gt) goto loc_825D74AC;
loc_825D7478:
	// lwz r10,136(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// mullw r10,r11,r10
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + ctx.r27.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r31
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r31.u32);
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stwx r9,r10,r31
	PPC_STORE_U32(ctx.r10.u32 + ctx.r31.u32, ctx.r9.u32);
	// lwz r10,140(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 140);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825d7478
	if (ctx.cr6.lt) goto loc_825D7478;
loc_825D74AC:
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// cmpw cr6,r27,r25
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r25.s32, ctx.xer);
	// blt cr6,0x825d73c0
	if (ctx.cr6.lt) goto loc_825D73C0;
loc_825D74B8:
	// cmpwi cr6,r24,0
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// beq cr6,0x825d75a8
	if (ctx.cr6.eq) goto loc_825D75A8;
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r29,r8,0
	ctx.r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d74ec
	if (!ctx.cr0.lt) goto loc_825D74EC;
	// bl 0x825d5398
	ctx.lr = 0x825D74EC;
	sub_825D5398(ctx, base);
loc_825D74EC:
	// lwz r11,136(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x825d756c
	if (ctx.cr6.eq) goto loc_825D756C;
	// mr r28,r25
	ctx.r28.u64 = ctx.r25.u64;
	// cmpw cr6,r25,r11
	ctx.cr6.compare<int32_t>(ctx.r25.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x825d75a8
	if (!ctx.cr6.lt) goto loc_825D75A8;
	// rlwinm r11,r25,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r25,r11
	ctx.r11.u64 = ctx.r25.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r31
	ctx.r31.u64 = ctx.r11.u64 + ctx.r31.u64;
loc_825D7514:
	// lwz r3,84(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r29,r8,0
	ctx.r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d7540
	if (!ctx.cr0.lt) goto loc_825D7540;
	// bl 0x825d5398
	ctx.lr = 0x825D7540;
	sub_825D5398(ctx, base);
loc_825D7540:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// rlwimi r11,r29,31,0,0
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r29.u32, 31) & 0x80000000) | (ctx.r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// addi r31,r31,20
	ctx.r31.s64 = ctx.r31.s64 + 20;
	// lwz r11,136(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// cmpw cr6,r28,r11
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825d7514
	if (ctx.cr6.lt) goto loc_825D7514;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D756C:
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
	// cmpw cr6,r25,r11
	ctx.cr6.compare<int32_t>(ctx.r25.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x825d75a8
	if (!ctx.cr6.lt) goto loc_825D75A8;
	// rlwinm r11,r25,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r25,r11
	ctx.r11.u64 = ctx.r25.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + ctx.r31.u64;
loc_825D7588:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,20
	ctx.r11.s64 = ctx.r11.s64 + 20;
	// lwz r9,136(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 136);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x825d7588
	if (ctx.cr6.lt) goto loc_825D7588;
loc_825D75A8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825D75AC:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825D75B4"))) PPC_WEAK_FUNC(sub_825D75B4);
PPC_FUNC_IMPL(__imp__sub_825D75B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D75B8"))) PPC_WEAK_FUNC(sub_825D75B8);
PPC_FUNC_IMPL(__imp__sub_825D75B8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825D75C0;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// lwz r10,144(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// lwz r11,19984(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19984);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// mullw r11,r10,r11
	ctx.r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// lwz r9,268(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// ld r10,0(r3)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r11,r9
	ctx.r29.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r10,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rldicl r10,r10,1,63
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u64, 1) & 0x1;
	// rotlwi r25,r10,0
	ctx.r25.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d7618
	if (!ctx.cr0.lt) goto loc_825D7618;
	// bl 0x825d5398
	ctx.lr = 0x825D7618;
	sub_825D5398(ctx, base);
loc_825D7618:
	// lwz r30,84(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// li r28,2
	ctx.r28.s64 = 2;
	// li r27,0
	ctx.r27.s64 = 0;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825d768c
	if (!ctx.cr6.lt) goto loc_825D768C;
loc_825D7634:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d768c
	if (ctx.cr6.eq) goto loc_825D768C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r28,r11,r28
	ctx.r28.s64 = ctx.r28.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r28
	ctx.r11.u64 = ctx.r28.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r28.u8 & 0x3F));
	// add r27,r11,r27
	ctx.r27.u64 = ctx.r11.u64 + ctx.r27.u64;
	// bge 0x825d767c
	if (!ctx.cr0.lt) goto loc_825D767C;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D767C;
	sub_825D5398(ctx, base);
loc_825D767C:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d7634
	if (ctx.cr6.gt) goto loc_825D7634;
loc_825D768C:
	// subfic r9,r28,64
	ctx.xer.ca = ctx.r28.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r28.s64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// clrldi r8,r28,32
	ctx.r8.u64 = ctx.r28.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r28,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r28.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r27
	ctx.r28.u64 = ctx.r11.u64 + ctx.r27.u64;
	// std r8,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r8.u64);
	// bge 0x825d76c8
	if (!ctx.cr0.lt) goto loc_825D76C8;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D76C8;
	sub_825D5398(ctx, base);
loc_825D76C8:
	// cmplwi cr6,r28,1
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 1, ctx.xer);
	// beq cr6,0x825d7860
	if (ctx.cr6.eq) goto loc_825D7860;
	// cmplwi cr6,r28,2
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 2, ctx.xer);
	// beq cr6,0x825d7848
	if (ctx.cr6.eq) goto loc_825D7848;
	// cmplwi cr6,r28,3
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 3, ctx.xer);
	// beq cr6,0x825d782c
	if (ctx.cr6.eq) goto loc_825D782C;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d770c
	if (!ctx.cr0.lt) goto loc_825D770C;
	// bl 0x825d5398
	ctx.lr = 0x825D770C;
	sub_825D5398(ctx, base);
loc_825D770C:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d7784
	if (ctx.cr6.eq) goto loc_825D7784;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r5,84(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// lwz r4,144(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// li r26,2
	ctx.r26.s64 = 2;
	// bl 0x825d6c20
	ctx.lr = 0x825D7728;
	sub_825D6C20(ctx, base);
loc_825D7728:
	// lwz r9,19984(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19984);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,144(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// lwz r10,268(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// mullw r11,r11,r9
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r9.s32);
	// lwz r9,140(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// ble cr6,0x825d7814
	if (!ctx.cr6.gt) goto loc_825D7814;
loc_825D7758:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825d7b38
	if (!ctx.cr6.gt) goto loc_825D7B38;
loc_825D7768:
	// add. r10,r6,r7
	ctx.r10.u64 = ctx.r6.u64 + ctx.r7.u64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x825d7b0c
	if (ctx.cr0.eq) goto loc_825D7B0C;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// bne cr6,0x825d7ab8
	if (!ctx.cr6.eq) goto loc_825D7AB8;
	// lwz r10,-20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + -20);
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// b 0x825d7b10
	goto loc_825D7B10;
loc_825D7784:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d77b0
	if (!ctx.cr0.lt) goto loc_825D77B0;
	// bl 0x825d5398
	ctx.lr = 0x825D77B0;
	sub_825D5398(ctx, base);
loc_825D77B0:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d77d4
	if (ctx.cr6.eq) goto loc_825D77D4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r26,4
	ctx.r26.s64 = 4;
	// bl 0x825d70e8
	ctx.lr = 0x825D77C4;
	sub_825D70E8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825d7728
	if (ctx.cr6.eq) goto loc_825D7728;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D77D4:
	// li r26,0
	ctx.r26.s64 = 0;
loc_825D77D8:
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// beq cr6,0x825d7814
	if (ctx.cr6.eq) goto loc_825D7814;
	// lwz r10,144(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825d7814
	if (!ctx.cr6.gt) goto loc_825D7814;
loc_825D77F0:
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// not r9,r10
	ctx.r9.u64 = ~ctx.r10.u64;
	// rlwimi r9,r10,0,1,31
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x7FFFFFFF) | (ctx.r9.u64 & 0xFFFFFFFF80000000);
	// stw r9,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r9.u32);
	// addi r29,r29,20
	ctx.r29.s64 = ctx.r29.s64 + 20;
	// lwz r10,144(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825d77f0
	if (ctx.cr6.lt) goto loc_825D77F0;
loc_825D7814:
	// cmpwi cr6,r24,0
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// bne cr6,0x825d7b4c
	if (!ctx.cr6.eq) goto loc_825D7B4C;
	// stw r26,344(r31)
	PPC_STORE_U32(ctx.r31.u32 + 344, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D782C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r26,3
	ctx.r26.s64 = 3;
	// bl 0x825d70e8
	ctx.lr = 0x825D7838;
	sub_825D70E8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825d77d8
	if (ctx.cr6.eq) goto loc_825D77D8;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D7848:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r5,84(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// lwz r4,144(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// li r26,1
	ctx.r26.s64 = 1;
	// bl 0x825d6c20
	ctx.lr = 0x825D785C;
	sub_825D6C20(ctx, base);
	// b 0x825d77d8
	goto loc_825D77D8;
loc_825D7860:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d788c
	if (!ctx.cr0.lt) goto loc_825D788C;
	// bl 0x825d5398
	ctx.lr = 0x825D788C;
	sub_825D5398(ctx, base);
loc_825D788C:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// li r27,0
	ctx.r27.s64 = 0;
	// beq cr6,0x825d79a8
	if (ctx.cr6.eq) goto loc_825D79A8;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// li r26,6
	ctx.r26.s64 = 6;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d77d8
	if (!ctx.cr6.gt) goto loc_825D77D8;
loc_825D78A8:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d78d4
	if (!ctx.cr0.lt) goto loc_825D78D4;
	// bl 0x825d5398
	ctx.lr = 0x825D78D4;
	sub_825D5398(ctx, base);
loc_825D78D4:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d7950
	if (ctx.cr6.eq) goto loc_825D7950;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r30,0
	ctx.r30.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d7994
	if (!ctx.cr6.gt) goto loc_825D7994;
loc_825D78EC:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r28,r8,0
	ctx.r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d7918
	if (!ctx.cr0.lt) goto loc_825D7918;
	// bl 0x825d5398
	ctx.lr = 0x825D7918;
	sub_825D5398(ctx, base);
loc_825D7918:
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mullw r11,r11,r30
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r30.s32);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r29
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r29.u32);
	// rlwimi r10,r28,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r28.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r29
	PPC_STORE_U32(ctx.r11.u32 + ctx.r29.u32, ctx.r10.u32);
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825d78ec
	if (ctx.cr6.lt) goto loc_825D78EC;
	// b 0x825d7994
	goto loc_825D7994;
loc_825D7950:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825d7994
	if (!ctx.cr6.gt) goto loc_825D7994;
loc_825D7960:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + ctx.r27.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r29
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r29.u32);
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stwx r9,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + ctx.r29.u32, ctx.r9.u32);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825d7960
	if (ctx.cr6.lt) goto loc_825D7960;
loc_825D7994:
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// cmpw cr6,r27,r11
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825d78a8
	if (ctx.cr6.lt) goto loc_825D78A8;
	// b 0x825d77d8
	goto loc_825D77D8;
loc_825D79A8:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r26,5
	ctx.r26.s64 = 5;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d77d8
	if (!ctx.cr6.gt) goto loc_825D77D8;
loc_825D79B8:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d79e4
	if (!ctx.cr0.lt) goto loc_825D79E4;
	// bl 0x825d5398
	ctx.lr = 0x825D79E4;
	sub_825D5398(ctx, base);
loc_825D79E4:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d7a60
	if (ctx.cr6.eq) goto loc_825D7A60;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// li r30,0
	ctx.r30.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825d7aa4
	if (!ctx.cr6.gt) goto loc_825D7AA4;
loc_825D79FC:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r28,r8,0
	ctx.r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d7a28
	if (!ctx.cr0.lt) goto loc_825D7A28;
	// bl 0x825d5398
	ctx.lr = 0x825D7A28;
	sub_825D5398(ctx, base);
loc_825D7A28:
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mullw r11,r11,r27
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r27.s32);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r29
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r29.u32);
	// rlwimi r10,r28,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r28.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r29
	PPC_STORE_U32(ctx.r11.u32 + ctx.r29.u32, ctx.r10.u32);
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825d79fc
	if (ctx.cr6.lt) goto loc_825D79FC;
	// b 0x825d7aa4
	goto loc_825D7AA4;
loc_825D7A60:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825d7aa4
	if (!ctx.cr6.gt) goto loc_825D7AA4;
loc_825D7A70:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mullw r10,r10,r27
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r27.s32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r29
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r29.u32);
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stwx r9,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + ctx.r29.u32, ctx.r9.u32);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825d7a70
	if (ctx.cr6.lt) goto loc_825D7A70;
loc_825D7AA4:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// cmpw cr6,r27,r11
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825d79b8
	if (ctx.cr6.lt) goto loc_825D79B8;
	// b 0x825d77d8
	goto loc_825D77D8;
loc_825D7AB8:
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// bne cr6,0x825d7ae0
	if (!ctx.cr6.eq) goto loc_825D7AE0;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r10.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// b 0x825d7b10
	goto loc_825D7B10;
loc_825D7AE0:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r10,-20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + -20);
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r9,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r9.s64;
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r9,r9,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x825d7b10
	if (ctx.cr6.eq) goto loc_825D7B10;
loc_825D7B0C:
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
loc_825D7B10:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r10,r10,31,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x80000000;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// rlwimi r10,r9,0,1,31
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0x7FFFFFFF) | (ctx.r10.u64 & 0xFFFFFFFF80000000);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// addi r11,r11,20
	ctx.r11.s64 = ctx.r11.s64 + 20;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmpw cr6,r7,r10
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825d7768
	if (ctx.cr6.lt) goto loc_825D7768;
loc_825D7B38:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmpw cr6,r6,r10
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825d7758
	if (ctx.cr6.lt) goto loc_825D7758;
	// b 0x825d7814
	goto loc_825D7814;
loc_825D7B4C:
	// cmpwi cr6,r24,5
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 5, ctx.xer);
	// bne cr6,0x825d7b64
	if (!ctx.cr6.eq) goto loc_825D7B64;
	// stw r26,20940(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20940, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D7B64:
	// cmpwi cr6,r24,4
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 4, ctx.xer);
	// bne cr6,0x825d7b7c
	if (!ctx.cr6.eq) goto loc_825D7B7C;
	// stw r26,20004(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20004, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D7B7C:
	// cmpwi cr6,r24,3
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 3, ctx.xer);
	// bne cr6,0x825d7b94
	if (!ctx.cr6.eq) goto loc_825D7B94;
	// stw r26,14804(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14804, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D7B94:
	// cmpwi cr6,r24,2
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 2, ctx.xer);
	// bne cr6,0x825d7bac
	if (!ctx.cr6.eq) goto loc_825D7BAC;
	// stw r26,19988(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19988, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D7BAC:
	// stw r26,348(r31)
	PPC_STORE_U32(ctx.r31.u32 + 348, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825D7BBC"))) PPC_WEAK_FUNC(sub_825D7BBC);
PPC_FUNC_IMPL(__imp__sub_825D7BBC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D7BC0"))) PPC_WEAK_FUNC(sub_825D7BC0);
PPC_FUNC_IMPL(__imp__sub_825D7BC0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825D7BC8;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r3
	ctx.r25.u64 = ctx.r3.u64;
	// lwz r11,268(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 268);
	// lwz r10,3944(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 3944);
	// mr r24,r11
	ctx.r24.u64 = ctx.r11.u64;
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// blt cr6,0x825d8014
	if (ctx.cr6.lt) goto loc_825D8014;
	// beq cr6,0x825d7e18
	if (ctx.cr6.eq) goto loc_825D7E18;
	// cmplwi cr6,r10,3
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 3, ctx.xer);
	// bge cr6,0x825d8130
	if (!ctx.cr6.lt) goto loc_825D8130;
	// mr r26,r11
	ctx.r26.u64 = ctx.r11.u64;
	// lwz r11,136(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 136);
	// li r24,0
	ctx.r24.s64 = 0;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x825d8130
	if (!ctx.cr6.gt) goto loc_825D8130;
loc_825D7C04:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r28,r26
	ctx.r28.u64 = ctx.r26.u64;
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d7c80
	if (!ctx.cr6.lt) goto loc_825D7C80;
loc_825D7C24:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d7c80
	if (ctx.cr6.eq) goto loc_825D7C80;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d7c70
	if (!ctx.cr0.lt) goto loc_825D7C70;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D7C70;
	sub_825D5398(ctx, base);
loc_825D7C70:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d7c24
	if (ctx.cr6.gt) goto loc_825D7C24;
loc_825D7C80:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d7cbc
	if (!ctx.cr0.lt) goto loc_825D7CBC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D7CBC;
	sub_825D5398(ctx, base);
loc_825D7CBC:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d7d08
	if (ctx.cr6.eq) goto loc_825D7D08;
	// lwz r10,140(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 140);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// ble cr6,0x825d7dfc
	if (!ctx.cr6.gt) goto loc_825D7DFC;
loc_825D7CD4:
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stw r10,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r10.u32);
	// lwz r9,140(r25)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r25.u32 + 140);
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 136);
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r10,r28
	ctx.r28.u64 = ctx.r10.u64 + ctx.r28.u64;
	// blt cr6,0x825d7cd4
	if (ctx.cr6.lt) goto loc_825D7CD4;
	// b 0x825d7dfc
	goto loc_825D7DFC;
loc_825D7D08:
	// lwz r11,140(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 140);
	// li r27,0
	ctx.r27.s64 = 0;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x825d7dfc
	if (!ctx.cr6.gt) goto loc_825D7DFC;
loc_825D7D18:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d7d90
	if (!ctx.cr6.lt) goto loc_825D7D90;
loc_825D7D34:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d7d90
	if (ctx.cr6.eq) goto loc_825D7D90;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d7d80
	if (!ctx.cr0.lt) goto loc_825D7D80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D7D80;
	sub_825D5398(ctx, base);
loc_825D7D80:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d7d34
	if (ctx.cr6.gt) goto loc_825D7D34;
loc_825D7D90:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d7dcc
	if (!ctx.cr0.lt) goto loc_825D7DCC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D7DCC;
	sub_825D5398(ctx, base);
loc_825D7DCC:
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// rlwimi r11,r30,31,0,0
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r30.u32, 31) & 0x80000000) | (ctx.r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r11.u32);
	// lwz r10,140(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 140);
	// lwz r11,136(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 136);
	// cmplw cr6,r27,r10
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, ctx.r10.u32, ctx.xer);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// blt cr6,0x825d7d18
	if (ctx.cr6.lt) goto loc_825D7D18;
loc_825D7DFC:
	// lwz r11,136(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 136);
	// addi r24,r24,1
	ctx.r24.s64 = ctx.r24.s64 + 1;
	// addi r26,r26,20
	ctx.r26.s64 = ctx.r26.s64 + 20;
	// cmplw cr6,r24,r11
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x825d7c04
	if (ctx.cr6.lt) goto loc_825D7C04;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D7E18:
	// lwz r11,140(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 140);
	// li r27,0
	ctx.r27.s64 = 0;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x825d8130
	if (!ctx.cr6.gt) goto loc_825D8130;
loc_825D7E28:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d7ea0
	if (!ctx.cr6.lt) goto loc_825D7EA0;
loc_825D7E44:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d7ea0
	if (ctx.cr6.eq) goto loc_825D7EA0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d7e90
	if (!ctx.cr0.lt) goto loc_825D7E90;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D7E90;
	sub_825D5398(ctx, base);
loc_825D7E90:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d7e44
	if (ctx.cr6.gt) goto loc_825D7E44;
loc_825D7EA0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d7edc
	if (!ctx.cr0.lt) goto loc_825D7EDC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D7EDC;
	sub_825D5398(ctx, base);
loc_825D7EDC:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d7f18
	if (ctx.cr6.eq) goto loc_825D7F18;
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 136);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// ble cr6,0x825d7ffc
	if (!ctx.cr6.gt) goto loc_825D7FFC;
loc_825D7EF4:
	// lwz r10,0(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stw r10,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r10.u32);
	// addi r24,r24,20
	ctx.r24.s64 = ctx.r24.s64 + 20;
	// lwz r10,136(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 136);
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825d7ef4
	if (ctx.cr6.lt) goto loc_825D7EF4;
	// b 0x825d7ffc
	goto loc_825D7FFC;
loc_825D7F18:
	// lwz r11,136(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 136);
	// li r28,0
	ctx.r28.s64 = 0;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x825d7ffc
	if (!ctx.cr6.gt) goto loc_825D7FFC;
loc_825D7F28:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d7fa0
	if (!ctx.cr6.lt) goto loc_825D7FA0;
loc_825D7F44:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d7fa0
	if (ctx.cr6.eq) goto loc_825D7FA0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d7f90
	if (!ctx.cr0.lt) goto loc_825D7F90;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D7F90;
	sub_825D5398(ctx, base);
loc_825D7F90:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d7f44
	if (ctx.cr6.gt) goto loc_825D7F44;
loc_825D7FA0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d7fdc
	if (!ctx.cr0.lt) goto loc_825D7FDC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D7FDC;
	sub_825D5398(ctx, base);
loc_825D7FDC:
	// lwz r11,0(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// rlwimi r11,r30,31,0,0
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r30.u32, 31) & 0x80000000) | (ctx.r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r11.u32);
	// addi r24,r24,20
	ctx.r24.s64 = ctx.r24.s64 + 20;
	// lwz r11,136(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 136);
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x825d7f28
	if (ctx.cr6.lt) goto loc_825D7F28;
loc_825D7FFC:
	// lwz r11,140(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 140);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// cmplw cr6,r27,r11
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x825d7e28
	if (ctx.cr6.lt) goto loc_825D7E28;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825D8014:
	// lwz r11,140(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 140);
	// li r26,0
	ctx.r26.s64 = 0;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x825d8130
	if (!ctx.cr6.gt) goto loc_825D8130;
loc_825D8024:
	// lwz r11,136(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 136);
	// li r27,0
	ctx.r27.s64 = 0;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x825d810c
	if (!ctx.cr6.gt) goto loc_825D810C;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
loc_825D8038:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d80b0
	if (!ctx.cr6.lt) goto loc_825D80B0;
loc_825D8054:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d80b0
	if (ctx.cr6.eq) goto loc_825D80B0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d80a0
	if (!ctx.cr0.lt) goto loc_825D80A0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D80A0;
	sub_825D5398(ctx, base);
loc_825D80A0:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d8054
	if (ctx.cr6.gt) goto loc_825D8054;
loc_825D80B0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d80ec
	if (!ctx.cr0.lt) goto loc_825D80EC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D80EC;
	sub_825D5398(ctx, base);
loc_825D80EC:
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// rlwimi r11,r30,31,0,0
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r30.u32, 31) & 0x80000000) | (ctx.r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r11.u32);
	// addi r28,r28,20
	ctx.r28.s64 = ctx.r28.s64 + 20;
	// lwz r11,136(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 136);
	// cmplw cr6,r27,r11
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x825d8038
	if (ctx.cr6.lt) goto loc_825D8038;
loc_825D810C:
	// lwz r11,136(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 136);
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// lwz r9,140(r25)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r25.u32 + 140);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplw cr6,r26,r9
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r9.u32, ctx.xer);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r24,r11,r24
	ctx.r24.u64 = ctx.r11.u64 + ctx.r24.u64;
	// blt cr6,0x825d8024
	if (ctx.cr6.lt) goto loc_825D8024;
loc_825D8130:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825D8138"))) PPC_WEAK_FUNC(sub_825D8138);
PPC_FUNC_IMPL(__imp__sub_825D8138) {
	PPC_FUNC_PROLOGUE();
	// lwz r10,3708(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3708);
	// lwz r11,3704(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3704);
	// stw r10,3704(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3704, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r11,3708(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3708, ctx.r11.u32);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r9,3776(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3776, ctx.r9.u32);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r9,3780(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3780, ctx.r9.u32);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r10,3784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3784, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r10,3788(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3788, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r10,3792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3792, ctx.r10.u32);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r11,3796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3796, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825D8180"))) PPC_WEAK_FUNC(sub_825D8180);
PPC_FUNC_IMPL(__imp__sub_825D8180) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba14
	ctx.lr = 0x825D8188;
	sub_8239BA14(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// lwz r11,3980(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3980);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825d81b0
	if (!ctx.cr6.eq) goto loc_825D81B0;
	// li r11,1
	ctx.r11.s64 = 1;
	// li r10,15
	ctx.r10.s64 = 15;
	// stw r11,280(r27)
	PPC_STORE_U32(ctx.r27.u32 + 280, ctx.r11.u32);
	// stw r10,3976(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3976, ctx.r10.u32);
	// b 0x825d84a0
	goto loc_825D84A0;
loc_825D81B0:
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// stw r29,472(r27)
	PPC_STORE_U32(ctx.r27.u32 + 472, ctx.r29.u32);
	// stw r29,3976(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3976, ctx.r29.u32);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d81e8
	if (!ctx.cr0.lt) goto loc_825D81E8;
	// bl 0x825d5398
	ctx.lr = 0x825D81E8;
	sub_825D5398(ctx, base);
loc_825D81E8:
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// stw r31,280(r27)
	PPC_STORE_U32(ctx.r27.u32 + 280, ctx.r31.u32);
	// beq cr6,0x825d84b0
	if (ctx.cr6.eq) goto loc_825D84B0;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// mr r28,r29
	ctx.r28.u64 = ctx.r29.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825d8268
	if (!ctx.cr6.lt) goto loc_825D8268;
loc_825D8210:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8268
	if (ctx.cr6.eq) goto loc_825D8268;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825d8258
	if (!ctx.cr0.lt) goto loc_825D8258;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8258;
	sub_825D5398(ctx, base);
loc_825D8258:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d8210
	if (ctx.cr6.gt) goto loc_825D8210;
loc_825D8268:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r28
	ctx.r30.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d82a4
	if (!ctx.cr0.lt) goto loc_825D82A4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D82A4;
	sub_825D5398(ctx, base);
loc_825D82A4:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825d82b8
	if (!ctx.cr6.eq) goto loc_825D82B8;
	// li r11,15
	ctx.r11.s64 = 15;
	// stw r11,3976(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3976, ctx.r11.u32);
	// b 0x825d8488
	goto loc_825D8488;
loc_825D82B8:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// bne cr6,0x825d8390
	if (!ctx.cr6.eq) goto loc_825D8390;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825d8330
	if (!ctx.cr6.lt) goto loc_825D8330;
loc_825D82D8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8330
	if (ctx.cr6.eq) goto loc_825D8330;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d8320
	if (!ctx.cr0.lt) goto loc_825D8320;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8320;
	sub_825D5398(ctx, base);
loc_825D8320:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d82d8
	if (ctx.cr6.gt) goto loc_825D82D8;
loc_825D8330:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d836c
	if (!ctx.cr0.lt) goto loc_825D836C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D836C;
	sub_825D5398(ctx, base);
loc_825D836C:
	// cmplwi cr6,r30,3
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 3, ctx.xer);
	// beq cr6,0x825d8384
	if (ctx.cr6.eq) goto loc_825D8384;
	// li r11,3
	ctx.r11.s64 = 3;
	// slw r11,r11,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r11.u32 << (ctx.r30.u8 & 0x3F));
	// stw r11,3976(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3976, ctx.r11.u32);
	// b 0x825d8488
	goto loc_825D8488;
loc_825D8384:
	// li r11,9
	ctx.r11.s64 = 9;
	// stw r11,3976(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3976, ctx.r11.u32);
	// b 0x825d8488
	goto loc_825D8488;
loc_825D8390:
	// cmplwi cr6,r30,2
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 2, ctx.xer);
	// bne cr6,0x825d8450
	if (!ctx.cr6.eq) goto loc_825D8450;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825d8404
	if (!ctx.cr6.lt) goto loc_825D8404;
loc_825D83AC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8404
	if (ctx.cr6.eq) goto loc_825D8404;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d83f4
	if (!ctx.cr0.lt) goto loc_825D83F4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D83F4;
	sub_825D5398(ctx, base);
loc_825D83F4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d83ac
	if (ctx.cr6.gt) goto loc_825D83AC;
loc_825D8404:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d8440
	if (!ctx.cr0.lt) goto loc_825D8440;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8440;
	sub_825D5398(ctx, base);
loc_825D8440:
	// li r11,1
	ctx.r11.s64 = 1;
	// slw r11,r11,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r11.u32 << (ctx.r30.u8 & 0x3F));
	// stw r11,3976(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3976, ctx.r11.u32);
	// b 0x825d8488
	goto loc_825D8488;
loc_825D8450:
	// cmplwi cr6,r30,3
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 3, ctx.xer);
	// bne cr6,0x825d8488
	if (!ctx.cr6.eq) goto loc_825D8488;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825d8484
	if (!ctx.cr0.lt) goto loc_825D8484;
	// bl 0x825d5398
	ctx.lr = 0x825D8484;
	sub_825D5398(ctx, base);
loc_825D8484:
	// stw r31,472(r27)
	PPC_STORE_U32(ctx.r27.u32 + 472, ctx.r31.u32);
loc_825D8488:
	// lwz r11,472(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 472);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825d84a0
	if (!ctx.cr6.eq) goto loc_825D84A0;
	// lwz r11,3976(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3976);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825d84b0
	if (ctx.cr6.eq) goto loc_825D84B0;
loc_825D84A0:
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x82622d30
	ctx.lr = 0x825D84AC;
	sub_82622D30(ctx, base);
	// stw r3,3984(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3984, ctx.r3.u32);
loc_825D84B0:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
}

__attribute__((alias("__imp__sub_825D84B8"))) PPC_WEAK_FUNC(sub_825D84B8);
PPC_FUNC_IMPL(__imp__sub_825D84B8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba10
	ctx.lr = 0x825D84C0;
	sub_8239BA10(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// li r26,0
	ctx.r26.s64 = 0;
	// li r30,11
	ctx.r30.s64 = 11;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,11
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 11, ctx.xer);
	// bge cr6,0x825d8540
	if (!ctx.cr6.lt) goto loc_825D8540;
loc_825D84E8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8540
	if (ctx.cr6.eq) goto loc_825D8540;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d8530
	if (!ctx.cr0.lt) goto loc_825D8530;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8530;
	sub_825D5398(ctx, base);
loc_825D8530:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d84e8
	if (ctx.cr6.gt) goto loc_825D84E8;
loc_825D8540:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d857c
	if (!ctx.cr0.lt) goto loc_825D857C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D857C;
	sub_825D5398(ctx, base);
loc_825D857C:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,11
	ctx.r30.s64 = 11;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,11
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 11, ctx.xer);
	// bge cr6,0x825d85f0
	if (!ctx.cr6.lt) goto loc_825D85F0;
loc_825D8598:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d85f0
	if (ctx.cr6.eq) goto loc_825D85F0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d85e0
	if (!ctx.cr0.lt) goto loc_825D85E0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D85E0;
	sub_825D5398(ctx, base);
loc_825D85E0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d8598
	if (ctx.cr6.gt) goto loc_825D8598;
loc_825D85F0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d862c
	if (!ctx.cr0.lt) goto loc_825D862C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D862C;
	sub_825D5398(ctx, base);
loc_825D862C:
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq cr6,0x825d8900
	if (ctx.cr6.eq) goto loc_825D8900;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x825d8900
	if (ctx.cr6.eq) goto loc_825D8900;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,5
	ctx.r30.s64 = 5;
	// stw r28,156(r27)
	PPC_STORE_U32(ctx.r27.u32 + 156, ctx.r28.u32);
	// stw r29,160(r27)
	PPC_STORE_U32(ctx.r27.u32 + 160, ctx.r29.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825d869c
	if (!ctx.cr6.lt) goto loc_825D869C;
loc_825D865C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d869c
	if (ctx.cr6.eq) goto loc_825D869C;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825d868c
	if (!ctx.cr0.lt) goto loc_825D868C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D868C;
	sub_825D5398(ctx, base);
loc_825D868C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d865c
	if (ctx.cr6.gt) goto loc_825D865C;
loc_825D869C:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825d86c4
	if (!ctx.cr0.lt) goto loc_825D86C4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D86C4;
	sub_825D5398(ctx, base);
loc_825D86C4:
	// li r11,1
	ctx.r11.s64 = 1;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// stw r26,3888(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3888, ctx.r26.u32);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r26,436(r27)
	PPC_STORE_U32(ctx.r27.u32 + 436, ctx.r26.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// stw r26,3892(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3892, ctx.r26.u32);
	// stw r11,3900(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3900, ctx.r11.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d874c
	if (!ctx.cr6.lt) goto loc_825D874C;
loc_825D86F4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d874c
	if (ctx.cr6.eq) goto loc_825D874C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d873c
	if (!ctx.cr0.lt) goto loc_825D873C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D873C;
	sub_825D5398(ctx, base);
loc_825D873C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d86f4
	if (ctx.cr6.gt) goto loc_825D86F4;
loc_825D874C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d8788
	if (!ctx.cr0.lt) goto loc_825D8788;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8788;
	sub_825D5398(ctx, base);
loc_825D8788:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,3884(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3884, ctx.r28.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d8800
	if (!ctx.cr6.lt) goto loc_825D8800;
loc_825D87A8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8800
	if (ctx.cr6.eq) goto loc_825D8800;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d87f0
	if (!ctx.cr0.lt) goto loc_825D87F0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D87F0;
	sub_825D5398(ctx, base);
loc_825D87F0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d87a8
	if (ctx.cr6.gt) goto loc_825D87A8;
loc_825D8800:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d883c
	if (!ctx.cr0.lt) goto loc_825D883C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D883C;
	sub_825D5398(ctx, base);
loc_825D883C:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// stw r28,396(r27)
	PPC_STORE_U32(ctx.r27.u32 + 396, ctx.r28.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825d88b4
	if (!ctx.cr6.lt) goto loc_825D88B4;
loc_825D885C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d88b4
	if (ctx.cr6.eq) goto loc_825D88B4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d88a4
	if (!ctx.cr0.lt) goto loc_825D88A4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D88A4;
	sub_825D5398(ctx, base);
loc_825D88A4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d885c
	if (ctx.cr6.gt) goto loc_825D885C;
loc_825D88B4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d88f0
	if (!ctx.cr0.lt) goto loc_825D88F0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D88F0;
	sub_825D5398(ctx, base);
loc_825D88F0:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r30,15464(r27)
	PPC_STORE_U32(ctx.r27.u32 + 15464, ctx.r30.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
loc_825D8900:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
}

__attribute__((alias("__imp__sub_825D890C"))) PPC_WEAK_FUNC(sub_825D890C);
PPC_FUNC_IMPL(__imp__sub_825D890C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825D8910"))) PPC_WEAK_FUNC(sub_825D8910);
PPC_FUNC_IMPL(__imp__sub_825D8910) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9e8
	ctx.lr = 0x825D8918;
	sub_8239B9E8(ctx, base);
	// stwu r1,-496(r1)
	ea = -496 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r22,0
	ctx.r22.s64 = 0;
	// mr r25,r3
	ctx.r25.u64 = ctx.r3.u64;
	// mr r27,r5
	ctx.r27.u64 = ctx.r5.u64;
	// li r5,88
	ctx.r5.s64 = 88;
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,132
	ctx.r3.s64 = ctx.r1.s64 + 132;
	// stw r22,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r22.u32);
	// mr r26,r6
	ctx.r26.u64 = ctx.r6.u64;
	// mr r24,r7
	ctx.r24.u64 = ctx.r7.u64;
	// mr r23,r8
	ctx.r23.u64 = ctx.r8.u64;
	// bl 0x8239ca70
	ctx.lr = 0x825D8948;
	sub_8239CA70(ctx, base);
	// li r5,126
	ctx.r5.s64 = 126;
	// sth r22,224(r1)
	PPC_STORE_U16(ctx.r1.u32 + 224, ctx.r22.u16);
	// li r4,0
	ctx.r4.s64 = 0;
	// addi r3,r1,226
	ctx.r3.s64 = ctx.r1.s64 + 226;
	// bl 0x8239ca70
	ctx.lr = 0x825D895C;
	sub_8239CA70(ctx, base);
	// li r11,24
	ctx.r11.s64 = 24;
	// li r21,1
	ctx.r21.s64 = 1;
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// stw r22,21212(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21212, ctx.r22.u32);
	// mr r16,r22
	ctx.r16.u64 = ctx.r22.u64;
	// stw r22,21216(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21216, ctx.r22.u32);
	// mr r19,r22
	ctx.r19.u64 = ctx.r22.u64;
	// mr r20,r21
	ctx.r20.u64 = ctx.r21.u64;
	// stw r11,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r11.u32);
	// li r11,25
	ctx.r11.s64 = 25;
	// stw r21,3900(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3900, ctx.r21.u32);
	// mr r18,r21
	ctx.r18.u64 = ctx.r21.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// mr r17,r21
	ctx.r17.u64 = ctx.r21.u64;
	// li r30,2
	ctx.r30.s64 = 2;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r11.u32);
	// li r11,30
	ctx.r11.s64 = 30;
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r11.u32);
	// li r11,50
	ctx.r11.s64 = 50;
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r11.u32);
	// li r11,60
	ctx.r11.s64 = 60;
	// stw r11,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r11.u32);
	// li r11,48
	ctx.r11.s64 = 48;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r11.u32);
	// li r11,72
	ctx.r11.s64 = 72;
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r11.u32);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825d8a2c
	if (!ctx.cr6.lt) goto loc_825D8A2C;
loc_825D89D4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8a2c
	if (ctx.cr6.eq) goto loc_825D8A2C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d8a1c
	if (!ctx.cr0.lt) goto loc_825D8A1C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8A1C;
	sub_825D5398(ctx, base);
loc_825D8A1C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d89d4
	if (ctx.cr6.gt) goto loc_825D89D4;
loc_825D8A2C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d8a68
	if (!ctx.cr0.lt) goto loc_825D8A68;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8A68;
	sub_825D5398(ctx, base);
loc_825D8A68:
	// cmpwi cr6,r30,3
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 3, ctx.xer);
	// bne cr6,0x825da6dc
	if (!ctx.cr6.eq) goto loc_825DA6DC;
	// li r11,3
	ctx.r11.s64 = 3;
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// stw r11,3908(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3908, ctx.r11.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825d8ae8
	if (!ctx.cr6.lt) goto loc_825D8AE8;
loc_825D8A90:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8ae8
	if (ctx.cr6.eq) goto loc_825D8AE8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d8ad8
	if (!ctx.cr0.lt) goto loc_825D8AD8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8AD8;
	sub_825D5398(ctx, base);
loc_825D8AD8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d8a90
	if (ctx.cr6.gt) goto loc_825D8A90;
loc_825D8AE8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d8b24
	if (!ctx.cr0.lt) goto loc_825D8B24;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8B24;
	sub_825D5398(ctx, base);
loc_825D8B24:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// stw r28,21228(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21228, ctx.r28.u32);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825d8b9c
	if (!ctx.cr6.lt) goto loc_825D8B9C;
loc_825D8B44:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8b9c
	if (ctx.cr6.eq) goto loc_825D8B9C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d8b8c
	if (!ctx.cr0.lt) goto loc_825D8B8C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8B8C;
	sub_825D5398(ctx, base);
loc_825D8B8C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d8b44
	if (ctx.cr6.gt) goto loc_825D8B44;
loc_825D8B9C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d8bd8
	if (!ctx.cr0.lt) goto loc_825D8BD8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8BD8;
	sub_825D5398(ctx, base);
loc_825D8BD8:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// stw r28,21200(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21200, ctx.r28.u32);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825d8c50
	if (!ctx.cr6.lt) goto loc_825D8C50;
loc_825D8BF8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8c50
	if (ctx.cr6.eq) goto loc_825D8C50;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d8c40
	if (!ctx.cr0.lt) goto loc_825D8C40;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8C40;
	sub_825D5398(ctx, base);
loc_825D8C40:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d8bf8
	if (ctx.cr6.gt) goto loc_825D8BF8;
loc_825D8C50:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d8c8c
	if (!ctx.cr0.lt) goto loc_825D8C8C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8C8C;
	sub_825D5398(ctx, base);
loc_825D8C8C:
	// clrldi r11,r30,32
	ctx.r11.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,5
	ctx.r30.s64 = 5;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// stfd f0,20856(r25)
	PPC_STORE_U64(ctx.r25.u32 + 20856, ctx.f0.u64);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825d8d14
	if (!ctx.cr6.lt) goto loc_825D8D14;
loc_825D8CBC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8d14
	if (ctx.cr6.eq) goto loc_825D8D14;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d8d04
	if (!ctx.cr0.lt) goto loc_825D8D04;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8D04;
	sub_825D5398(ctx, base);
loc_825D8D04:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d8cbc
	if (ctx.cr6.gt) goto loc_825D8CBC;
loc_825D8D14:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d8d50
	if (!ctx.cr0.lt) goto loc_825D8D50;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8D50;
	sub_825D5398(ctx, base);
loc_825D8D50:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// stw r28,3660(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3660, ctx.r28.u32);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d8dc8
	if (!ctx.cr6.lt) goto loc_825D8DC8;
loc_825D8D70:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8dc8
	if (ctx.cr6.eq) goto loc_825D8DC8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d8db8
	if (!ctx.cr0.lt) goto loc_825D8DB8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8DB8;
	sub_825D5398(ctx, base);
loc_825D8DB8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d8d70
	if (ctx.cr6.gt) goto loc_825D8D70;
loc_825D8DC8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d8e04
	if (!ctx.cr0.lt) goto loc_825D8E04;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8E04;
	sub_825D5398(ctx, base);
loc_825D8E04:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,12
	ctx.r30.s64 = 12;
	// stw r28,20868(r25)
	PPC_STORE_U32(ctx.r25.u32 + 20868, ctx.r28.u32);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,12
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 12, ctx.xer);
	// bge cr6,0x825d8e7c
	if (!ctx.cr6.lt) goto loc_825D8E7C;
loc_825D8E24:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8e7c
	if (ctx.cr6.eq) goto loc_825D8E7C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d8e6c
	if (!ctx.cr0.lt) goto loc_825D8E6C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8E6C;
	sub_825D5398(ctx, base);
loc_825D8E6C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d8e24
	if (ctx.cr6.gt) goto loc_825D8E24;
loc_825D8E7C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d8eb8
	if (!ctx.cr0.lt) goto loc_825D8EB8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8EB8;
	sub_825D5398(ctx, base);
loc_825D8EB8:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,12
	ctx.r30.s64 = 12;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// stw r11,21352(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21352, ctx.r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,12
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 12, ctx.xer);
	// bge cr6,0x825d8f38
	if (!ctx.cr6.lt) goto loc_825D8F38;
loc_825D8EE0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d8f38
	if (ctx.cr6.eq) goto loc_825D8F38;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d8f28
	if (!ctx.cr0.lt) goto loc_825D8F28;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8F28;
	sub_825D5398(ctx, base);
loc_825D8F28:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d8ee0
	if (ctx.cr6.gt) goto loc_825D8EE0;
loc_825D8F38:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d8f74
	if (!ctx.cr0.lt) goto loc_825D8F74;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D8F74;
	sub_825D5398(ctx, base);
loc_825D8F74:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,21356(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21356, ctx.r11.u32);
	// beq cr6,0x825d8fa0
	if (ctx.cr6.eq) goto loc_825D8FA0;
	// cmplwi cr6,r26,0
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 0, ctx.xer);
	// beq cr6,0x825d8fa0
	if (ctx.cr6.eq) goto loc_825D8FA0;
	// lwz r11,21352(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 21352);
	// stw r11,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r11.u32);
	// lwz r11,21356(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 21356);
	// stw r11,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r11.u32);
loc_825D8FA0:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d9014
	if (!ctx.cr6.lt) goto loc_825D9014;
loc_825D8FBC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9014
	if (ctx.cr6.eq) goto loc_825D9014;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9004
	if (!ctx.cr0.lt) goto loc_825D9004;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9004;
	sub_825D5398(ctx, base);
loc_825D9004:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d8fbc
	if (ctx.cr6.gt) goto loc_825D8FBC;
loc_825D9014:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9050
	if (!ctx.cr0.lt) goto loc_825D9050;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9050;
	sub_825D5398(ctx, base);
loc_825D9050:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// stw r28,20832(r25)
	PPC_STORE_U32(ctx.r25.u32 + 20832, ctx.r28.u32);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d90c8
	if (!ctx.cr6.lt) goto loc_825D90C8;
loc_825D9070:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d90c8
	if (ctx.cr6.eq) goto loc_825D90C8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d90b8
	if (!ctx.cr0.lt) goto loc_825D90B8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D90B8;
	sub_825D5398(ctx, base);
loc_825D90B8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9070
	if (ctx.cr6.gt) goto loc_825D9070;
loc_825D90C8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9104
	if (!ctx.cr0.lt) goto loc_825D9104;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9104;
	sub_825D5398(ctx, base);
loc_825D9104:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// stw r28,21160(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21160, ctx.r28.u32);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d917c
	if (!ctx.cr6.lt) goto loc_825D917C;
loc_825D9124:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d917c
	if (ctx.cr6.eq) goto loc_825D917C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d916c
	if (!ctx.cr0.lt) goto loc_825D916C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D916C;
	sub_825D5398(ctx, base);
loc_825D916C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9124
	if (ctx.cr6.gt) goto loc_825D9124;
loc_825D917C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d91b8
	if (!ctx.cr0.lt) goto loc_825D91B8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D91B8;
	sub_825D5398(ctx, base);
loc_825D91B8:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// stw r28,20848(r25)
	PPC_STORE_U32(ctx.r25.u32 + 20848, ctx.r28.u32);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d9230
	if (!ctx.cr6.lt) goto loc_825D9230;
loc_825D91D8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9230
	if (ctx.cr6.eq) goto loc_825D9230;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9220
	if (!ctx.cr0.lt) goto loc_825D9220;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9220;
	sub_825D5398(ctx, base);
loc_825D9220:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d91d8
	if (ctx.cr6.gt) goto loc_825D91D8;
loc_825D9230:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d926c
	if (!ctx.cr0.lt) goto loc_825D926C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D926C;
	sub_825D5398(ctx, base);
loc_825D926C:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// stw r29,3444(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3444, ctx.r29.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d92c8
	if (!ctx.cr6.lt) goto loc_825D92C8;
loc_825D9288:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d92c8
	if (ctx.cr6.eq) goto loc_825D92C8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825d92b8
	if (!ctx.cr0.lt) goto loc_825D92B8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D92B8;
	sub_825D5398(ctx, base);
loc_825D92B8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9288
	if (ctx.cr6.gt) goto loc_825D9288;
loc_825D92C8:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825d92f0
	if (!ctx.cr0.lt) goto loc_825D92F0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D92F0;
	sub_825D5398(ctx, base);
loc_825D92F0:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d9364
	if (!ctx.cr6.lt) goto loc_825D9364;
loc_825D930C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9364
	if (ctx.cr6.eq) goto loc_825D9364;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9354
	if (!ctx.cr0.lt) goto loc_825D9354;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9354;
	sub_825D5398(ctx, base);
loc_825D9354:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d930c
	if (ctx.cr6.gt) goto loc_825D930C;
loc_825D9364:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d93a0
	if (!ctx.cr0.lt) goto loc_825D93A0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D93A0;
	sub_825D5398(ctx, base);
loc_825D93A0:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// stw r28,21548(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21548, ctx.r28.u32);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d9418
	if (!ctx.cr6.lt) goto loc_825D9418;
loc_825D93C0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9418
	if (ctx.cr6.eq) goto loc_825D9418;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9408
	if (!ctx.cr0.lt) goto loc_825D9408;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9408;
	sub_825D5398(ctx, base);
loc_825D9408:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d93c0
	if (ctx.cr6.gt) goto loc_825D93C0;
loc_825D9418:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9454
	if (!ctx.cr0.lt) goto loc_825D9454;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9454;
	sub_825D5398(ctx, base);
loc_825D9454:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d9f54
	if (ctx.cr6.eq) goto loc_825D9F54;
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,14
	ctx.r30.s64 = 14;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,14
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 14, ctx.xer);
	// bge cr6,0x825d94d0
	if (!ctx.cr6.lt) goto loc_825D94D0;
loc_825D9478:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d94d0
	if (ctx.cr6.eq) goto loc_825D94D0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d94c0
	if (!ctx.cr0.lt) goto loc_825D94C0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D94C0;
	sub_825D5398(ctx, base);
loc_825D94C0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9478
	if (ctx.cr6.gt) goto loc_825D9478;
loc_825D94D0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d950c
	if (!ctx.cr0.lt) goto loc_825D950C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D950C;
	sub_825D5398(ctx, base);
loc_825D950C:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// addi r28,r30,1
	ctx.r28.s64 = ctx.r30.s64 + 1;
	// li r30,14
	ctx.r30.s64 = 14;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,14
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 14, ctx.xer);
	// bge cr6,0x825d9584
	if (!ctx.cr6.lt) goto loc_825D9584;
loc_825D952C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9584
	if (ctx.cr6.eq) goto loc_825D9584;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9574
	if (!ctx.cr0.lt) goto loc_825D9574;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9574;
	sub_825D5398(ctx, base);
loc_825D9574:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d952c
	if (ctx.cr6.gt) goto loc_825D952C;
loc_825D9584:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d95c0
	if (!ctx.cr0.lt) goto loc_825D95C0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D95C0;
	sub_825D5398(ctx, base);
loc_825D95C0:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x825d95dc
	if (ctx.cr6.eq) goto loc_825D95DC;
	// cmplwi cr6,r23,0
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 0, ctx.xer);
	// beq cr6,0x825d95dc
	if (ctx.cr6.eq) goto loc_825D95DC;
	// stw r28,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r28.u32);
	// stw r11,0(r23)
	PPC_STORE_U32(ctx.r23.u32 + 0, ctx.r11.u32);
loc_825D95DC:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// stw r11,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r11.u32);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// stw r21,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r21.u32);
	// stw r28,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r28.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d965c
	if (!ctx.cr6.lt) goto loc_825D965C;
loc_825D9604:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d965c
	if (ctx.cr6.eq) goto loc_825D965C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d964c
	if (!ctx.cr0.lt) goto loc_825D964C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D964C;
	sub_825D5398(ctx, base);
loc_825D964C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9604
	if (ctx.cr6.gt) goto loc_825D9604;
loc_825D965C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9698
	if (!ctx.cr0.lt) goto loc_825D9698;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9698;
	sub_825D5398(ctx, base);
loc_825D9698:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d98dc
	if (ctx.cr6.eq) goto loc_825D98DC;
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,4
	ctx.r30.s64 = 4;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bge cr6,0x825d9714
	if (!ctx.cr6.lt) goto loc_825D9714;
loc_825D96BC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9714
	if (ctx.cr6.eq) goto loc_825D9714;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9704
	if (!ctx.cr0.lt) goto loc_825D9704;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9704;
	sub_825D5398(ctx, base);
loc_825D9704:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d96bc
	if (ctx.cr6.gt) goto loc_825D96BC;
loc_825D9714:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r27,r11,r29
	ctx.r27.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9750
	if (!ctx.cr0.lt) goto loc_825D9750;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9750;
	sub_825D5398(ctx, base);
loc_825D9750:
	// cmpwi cr6,r27,15
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 15, ctx.xer);
	// bne cr6,0x825d98c4
	if (!ctx.cr6.eq) goto loc_825D98C4;
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825d97cc
	if (!ctx.cr6.lt) goto loc_825D97CC;
loc_825D9774:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d97cc
	if (ctx.cr6.eq) goto loc_825D97CC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d97bc
	if (!ctx.cr0.lt) goto loc_825D97BC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D97BC;
	sub_825D5398(ctx, base);
loc_825D97BC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9774
	if (ctx.cr6.gt) goto loc_825D9774;
loc_825D97CC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9808
	if (!ctx.cr0.lt) goto loc_825D9808;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9808;
	sub_825D5398(ctx, base);
loc_825D9808:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// addi r28,r30,1
	ctx.r28.s64 = ctx.r30.s64 + 1;
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825d9880
	if (!ctx.cr6.lt) goto loc_825D9880;
loc_825D9828:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9880
	if (ctx.cr6.eq) goto loc_825D9880;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9870
	if (!ctx.cr0.lt) goto loc_825D9870;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9870;
	sub_825D5398(ctx, base);
loc_825D9870:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9828
	if (ctx.cr6.gt) goto loc_825D9828;
loc_825D9880:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d98bc
	if (!ctx.cr0.lt) goto loc_825D98BC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D98BC;
	sub_825D5398(ctx, base);
loc_825D98BC:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// b 0x825d98cc
	goto loc_825D98CC;
loc_825D98C4:
	// mr r11,r22
	ctx.r11.u64 = ctx.r22.u64;
	// mr r28,r22
	ctx.r28.u64 = ctx.r22.u64;
loc_825D98CC:
	// stw r21,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r21.u32);
	// stw r27,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r27.u32);
	// stw r28,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r28.u32);
	// stw r11,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r11.u32);
loc_825D98DC:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d9950
	if (!ctx.cr6.lt) goto loc_825D9950;
loc_825D98F8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9950
	if (ctx.cr6.eq) goto loc_825D9950;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9940
	if (!ctx.cr0.lt) goto loc_825D9940;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9940;
	sub_825D5398(ctx, base);
loc_825D9940:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d98f8
	if (ctx.cr6.gt) goto loc_825D98F8;
loc_825D9950:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d998c
	if (!ctx.cr0.lt) goto loc_825D998C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D998C;
	sub_825D5398(ctx, base);
loc_825D998C:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d9c7c
	if (ctx.cr6.eq) goto loc_825D9C7C;
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// stw r21,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r21.u32);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d9a0c
	if (!ctx.cr6.lt) goto loc_825D9A0C;
loc_825D99B4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9a0c
	if (ctx.cr6.eq) goto loc_825D9A0C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d99fc
	if (!ctx.cr0.lt) goto loc_825D99FC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D99FC;
	sub_825D5398(ctx, base);
loc_825D99FC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d99b4
	if (ctx.cr6.gt) goto loc_825D99B4;
loc_825D9A0C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9a48
	if (!ctx.cr0.lt) goto loc_825D9A48;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9A48;
	sub_825D5398(ctx, base);
loc_825D9A48:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825d9b18
	if (ctx.cr6.eq) goto loc_825D9B18;
	// li r30,16
	ctx.r30.s64 = 16;
	// cmplwi cr6,r11,16
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 16, ctx.xer);
	// bge cr6,0x825d9ac4
	if (!ctx.cr6.lt) goto loc_825D9AC4;
loc_825D9A6C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9ac4
	if (ctx.cr6.eq) goto loc_825D9AC4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9ab4
	if (!ctx.cr0.lt) goto loc_825D9AB4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9AB4;
	sub_825D5398(ctx, base);
loc_825D9AB4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9a6c
	if (ctx.cr6.gt) goto loc_825D9A6C;
loc_825D9AC4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9b00
	if (!ctx.cr0.lt) goto loc_825D9B00;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9B00;
	sub_825D5398(ctx, base);
loc_825D9B00:
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// stw r21,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r21.u32);
	// mr r16,r21
	ctx.r16.u64 = ctx.r21.u64;
	// mr r17,r11
	ctx.r17.u64 = ctx.r11.u64;
	// stw r11,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r11.u32);
	// b 0x825d9c7c
	goto loc_825D9C7C;
loc_825D9B18:
	// li r30,8
	ctx.r30.s64 = 8;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825d9b7c
	if (!ctx.cr6.lt) goto loc_825D9B7C;
loc_825D9B24:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9b7c
	if (ctx.cr6.eq) goto loc_825D9B7C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9b6c
	if (!ctx.cr0.lt) goto loc_825D9B6C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9B6C;
	sub_825D5398(ctx, base);
loc_825D9B6C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9b24
	if (ctx.cr6.gt) goto loc_825D9B24;
loc_825D9B7C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9bb8
	if (!ctx.cr0.lt) goto loc_825D9BB8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9BB8;
	sub_825D5398(ctx, base);
loc_825D9BB8:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r18,r28
	ctx.r18.u64 = ctx.r28.u64;
	// li r30,4
	ctx.r30.s64 = 4;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bge cr6,0x825d9c30
	if (!ctx.cr6.lt) goto loc_825D9C30;
loc_825D9BD8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9c30
	if (ctx.cr6.eq) goto loc_825D9C30;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9c20
	if (!ctx.cr0.lt) goto loc_825D9C20;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9C20;
	sub_825D5398(ctx, base);
loc_825D9C20:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9bd8
	if (ctx.cr6.gt) goto loc_825D9BD8;
loc_825D9C30:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9c6c
	if (!ctx.cr0.lt) goto loc_825D9C6C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9C6C;
	sub_825D5398(ctx, base);
loc_825D9C6C:
	// mr r20,r30
	ctx.r20.u64 = ctx.r30.u64;
	// stw r28,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r28.u32);
	// mr r19,r21
	ctx.r19.u64 = ctx.r21.u64;
	// stw r30,180(r1)
	PPC_STORE_U32(ctx.r1.u32 + 180, ctx.r30.u32);
loc_825D9C7C:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d9cf0
	if (!ctx.cr6.lt) goto loc_825D9CF0;
loc_825D9C98:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9cf0
	if (ctx.cr6.eq) goto loc_825D9CF0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9ce0
	if (!ctx.cr0.lt) goto loc_825D9CE0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9CE0;
	sub_825D5398(ctx, base);
loc_825D9CE0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9c98
	if (ctx.cr6.gt) goto loc_825D9C98;
loc_825D9CF0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9d2c
	if (!ctx.cr0.lt) goto loc_825D9D2C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9D2C;
	sub_825D5398(ctx, base);
loc_825D9D2C:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825d9f54
	if (ctx.cr6.eq) goto loc_825D9F54;
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825d9da8
	if (!ctx.cr6.lt) goto loc_825D9DA8;
loc_825D9D50:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9da8
	if (ctx.cr6.eq) goto loc_825D9DA8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9d98
	if (!ctx.cr0.lt) goto loc_825D9D98;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9D98;
	sub_825D5398(ctx, base);
loc_825D9D98:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9d50
	if (ctx.cr6.gt) goto loc_825D9D50;
loc_825D9DA8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r27,r11,r29
	ctx.r27.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9de4
	if (!ctx.cr0.lt) goto loc_825D9DE4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9DE4;
	sub_825D5398(ctx, base);
loc_825D9DE4:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825d9e58
	if (!ctx.cr6.lt) goto loc_825D9E58;
loc_825D9E00:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9e58
	if (ctx.cr6.eq) goto loc_825D9E58;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9e48
	if (!ctx.cr0.lt) goto loc_825D9E48;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9E48;
	sub_825D5398(ctx, base);
loc_825D9E48:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9e00
	if (ctx.cr6.gt) goto loc_825D9E00;
loc_825D9E58:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9e94
	if (!ctx.cr0.lt) goto loc_825D9E94;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9E94;
	sub_825D5398(ctx, base);
loc_825D9E94:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825d9f08
	if (!ctx.cr6.lt) goto loc_825D9F08;
loc_825D9EB0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9f08
	if (ctx.cr6.eq) goto loc_825D9F08;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9ef8
	if (!ctx.cr0.lt) goto loc_825D9EF8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9EF8;
	sub_825D5398(ctx, base);
loc_825D9EF8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9eb0
	if (ctx.cr6.gt) goto loc_825D9EB0;
loc_825D9F08:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825d9f44
	if (!ctx.cr0.lt) goto loc_825D9F44;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9F44;
	sub_825D5398(ctx, base);
loc_825D9F44:
	// stw r21,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r21.u32);
	// stw r27,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r27.u32);
	// stw r28,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r28.u32);
	// stw r30,200(r1)
	PPC_STORE_U32(ctx.r1.u32 + 200, ctx.r30.u32);
loc_825D9F54:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825d9fc8
	if (!ctx.cr6.lt) goto loc_825D9FC8;
loc_825D9F70:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825d9fc8
	if (ctx.cr6.eq) goto loc_825D9FC8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825d9fb8
	if (!ctx.cr0.lt) goto loc_825D9FB8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825D9FB8;
	sub_825D5398(ctx, base);
loc_825D9FB8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825d9f70
	if (ctx.cr6.gt) goto loc_825D9F70;
loc_825D9FC8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825da004
	if (!ctx.cr0.lt) goto loc_825DA004;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA004;
	sub_825D5398(ctx, base);
loc_825DA004:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// stw r30,21204(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21204, ctx.r30.u32);
	// beq cr6,0x825da3e8
	if (ctx.cr6.eq) goto loc_825DA3E8;
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,5
	ctx.r30.s64 = 5;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825da084
	if (!ctx.cr6.lt) goto loc_825DA084;
loc_825DA02C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825da084
	if (ctx.cr6.eq) goto loc_825DA084;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825da074
	if (!ctx.cr0.lt) goto loc_825DA074;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA074;
	sub_825D5398(ctx, base);
loc_825DA074:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825da02c
	if (ctx.cr6.gt) goto loc_825DA02C;
loc_825DA084:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825da0c0
	if (!ctx.cr0.lt) goto loc_825DA0C0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA0C0;
	sub_825D5398(ctx, base);
loc_825DA0C0:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,4
	ctx.r30.s64 = 4;
	// stw r28,21208(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21208, ctx.r28.u32);
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bge cr6,0x825da138
	if (!ctx.cr6.lt) goto loc_825DA138;
loc_825DA0E0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825da138
	if (ctx.cr6.eq) goto loc_825DA138;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825da128
	if (!ctx.cr0.lt) goto loc_825DA128;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA128;
	sub_825D5398(ctx, base);
loc_825DA128:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825da0e0
	if (ctx.cr6.gt) goto loc_825DA0E0;
loc_825DA138:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825da174
	if (!ctx.cr0.lt) goto loc_825DA174;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA174;
	sub_825D5398(ctx, base);
loc_825DA174:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// addi r28,r30,6
	ctx.r28.s64 = ctx.r30.s64 + 6;
	// li r30,4
	ctx.r30.s64 = 4;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bge cr6,0x825da1ec
	if (!ctx.cr6.lt) goto loc_825DA1EC;
loc_825DA194:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825da1ec
	if (ctx.cr6.eq) goto loc_825DA1EC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825da1dc
	if (!ctx.cr0.lt) goto loc_825DA1DC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA1DC;
	sub_825D5398(ctx, base);
loc_825DA1DC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825da194
	if (ctx.cr6.gt) goto loc_825DA194;
loc_825DA1EC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825da228
	if (!ctx.cr0.lt) goto loc_825DA228;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA228;
	sub_825D5398(ctx, base);
loc_825DA228:
	// lwz r11,21208(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 21208);
	// addi r10,r30,4
	ctx.r10.s64 = ctx.r30.s64 + 4;
	// stw r28,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r28.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r10,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r10.u32);
	// stw r11,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r11.u32);
	// ble cr6,0x825da250
	if (!ctx.cr6.gt) goto loc_825DA250;
	// addi r10,r1,224
	ctx.r10.s64 = ctx.r1.s64 + 224;
	// stw r10,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r10.u32);
	// b 0x825da254
	goto loc_825DA254;
loc_825DA250:
	// stw r22,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r22.u32);
loc_825DA254:
	// mr r26,r22
	ctx.r26.u64 = ctx.r22.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825da3e8
	if (!ctx.cr6.gt) goto loc_825DA3E8;
	// addi r27,r1,226
	ctx.r27.s64 = ctx.r1.s64 + 226;
loc_825DA264:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,16
	ctx.r30.s64 = 16;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,16
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 16, ctx.xer);
	// bge cr6,0x825da2dc
	if (!ctx.cr6.lt) goto loc_825DA2DC;
loc_825DA280:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825da2dc
	if (ctx.cr6.eq) goto loc_825DA2DC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825da2cc
	if (!ctx.cr0.lt) goto loc_825DA2CC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA2CC;
	sub_825D5398(ctx, base);
loc_825DA2CC:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825da280
	if (ctx.cr6.gt) goto loc_825DA280;
loc_825DA2DC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825da318
	if (!ctx.cr0.lt) goto loc_825DA318;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA318;
	sub_825D5398(ctx, base);
loc_825DA318:
	// lwz r31,84(r25)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// li r30,16
	ctx.r30.s64 = 16;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,16
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 16, ctx.xer);
	// bge cr6,0x825da390
	if (!ctx.cr6.lt) goto loc_825DA390;
loc_825DA334:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825da390
	if (ctx.cr6.eq) goto loc_825DA390;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825da380
	if (!ctx.cr0.lt) goto loc_825DA380;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA380;
	sub_825D5398(ctx, base);
loc_825DA380:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825da334
	if (ctx.cr6.gt) goto loc_825DA334;
loc_825DA390:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825da3cc
	if (!ctx.cr0.lt) goto loc_825DA3CC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA3CC;
	sub_825D5398(ctx, base);
loc_825DA3CC:
	// lwz r11,21208(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 21208);
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// sth r28,-2(r27)
	PPC_STORE_U16(ctx.r27.u32 + -2, ctx.r28.u16);
	// sth r30,0(r27)
	PPC_STORE_U16(ctx.r27.u32 + 0, ctx.r30.u16);
	// cmpw cr6,r26,r11
	ctx.cr6.compare<int32_t>(ctx.r26.s32, ctx.r11.s32, ctx.xer);
	// addi r27,r27,4
	ctx.r27.s64 = ctx.r27.s64 + 4;
	// blt cr6,0x825da264
	if (ctx.cr6.lt) goto loc_825DA264;
loc_825DA3E8:
	// lwz r11,84(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825da6dc
	if (!ctx.cr6.eq) goto loc_825DA6DC;
	// lwz r10,21480(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 21480);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x825da508
	if (!ctx.cr6.eq) goto loc_825DA508;
	// lis r11,-32249
	ctx.r11.s64 = -2113470464;
	// lfd f13,20856(r25)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r25.u32 + 20856);
	// lfd f0,-31520(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -31520);
	// fcmpu cr6,f13,f0
	ctx.cr6.compare(ctx.f13.f64, ctx.f0.f64);
	// bne cr6,0x825da508
	if (!ctx.cr6.eq) goto loc_825DA508;
	// lwz r11,3660(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 3660);
	// cmpwi cr6,r11,31
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 31, ctx.xer);
	// bne cr6,0x825da508
	if (!ctx.cr6.eq) goto loc_825DA508;
	// lwz r11,21160(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 21160);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r11,21356(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 21356);
	// beq cr6,0x825da45c
	if (ctx.cr6.eq) goto loc_825DA45C;
	// cmpwi cr6,r11,486
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 486, ctx.xer);
	// ble cr6,0x825da448
	if (!ctx.cr6.gt) goto loc_825DA448;
	// cmpwi cr6,r11,576
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 576, ctx.xer);
	// li r11,25
	ctx.r11.s64 = 25;
	// ble cr6,0x825da44c
	if (!ctx.cr6.gt) goto loc_825DA44C;
loc_825DA448:
	// li r11,30
	ctx.r11.s64 = 30;
loc_825DA44C:
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// b 0x825da480
	goto loc_825DA480;
loc_825DA45C:
	// cmpwi cr6,r11,576
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 576, ctx.xer);
	// ble cr6,0x825da470
	if (!ctx.cr6.gt) goto loc_825DA470;
	// cmpwi cr6,r11,720
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 720, ctx.xer);
	// li r11,60
	ctx.r11.s64 = 60;
	// ble cr6,0x825da474
	if (!ctx.cr6.gt) goto loc_825DA474;
loc_825DA470:
	// li r11,30
	ctx.r11.s64 = 30;
loc_825DA474:
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_825DA480:
	// fcfid f0,f0
	ctx.fpscr.disableFlushMode();
	ctx.f0.f64 = double(ctx.f0.s64);
	// cmpwi cr6,r16,0
	ctx.cr6.compare<int32_t>(ctx.r16.s32, 0, ctx.xer);
	// stfd f0,20856(r25)
	PPC_STORE_U64(ctx.r25.u32 + 20856, ctx.f0.u64);
	// beq cr6,0x825da4b8
	if (ctx.cr6.eq) goto loc_825DA4B8;
	// addi r11,r17,17
	ctx.r11.s64 = ctx.r17.s64 + 17;
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfd f0,80(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(ctx.f0.f64));
	// lfs f0,-11752(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -11752);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfd f0,20856(r25)
	PPC_STORE_U64(ctx.r25.u32 + 20856, ctx.f0.u64);
loc_825DA4B8:
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// beq cr6,0x825da5cc
	if (ctx.cr6.eq) goto loc_825DA5CC;
	// addi r11,r18,-1
	ctx.r11.s64 = ctx.r18.s64 + -1;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bgt cr6,0x825da5cc
	if (ctx.cr6.gt) goto loc_825DA5CC;
	// addi r11,r20,-1
	ctx.r11.s64 = ctx.r20.s64 + -1;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bgt cr6,0x825da5cc
	if (ctx.cr6.gt) goto loc_825DA5CC;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// rlwinm r11,r18,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r20,999
	ctx.r9.s64 = ctx.r20.s64 + 999;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// lwz r11,-4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// std r10,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r10.u64);
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lfd f0,80(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
	// lfd f13,88(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// b 0x825da5a8
	goto loc_825DA5A8;
loc_825DA508:
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lfd f12,20856(r25)
	ctx.fpscr.disableFlushMode();
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r25.u32 + 20856);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// lfd f0,-22872(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -22872);
	// lis r11,-32251
	ctx.r11.s64 = -2113601536;
	// lfd f13,112(r11)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r11.u32 + 112);
	// fmadd f0,f12,f0,f13
	ctx.f0.f64 = ctx.f12.f64 * ctx.f0.f64 + ctx.f13.f64;
	// stfd f0,20856(r25)
	PPC_STORE_U64(ctx.r25.u32 + 20856, ctx.f0.u64);
	// bne cr6,0x825da5cc
	if (!ctx.cr6.eq) goto loc_825DA5CC;
	// cmpwi cr6,r16,0
	ctx.cr6.compare<int32_t>(ctx.r16.s32, 0, ctx.xer);
	// beq cr6,0x825da55c
	if (ctx.cr6.eq) goto loc_825DA55C;
	// addi r11,r17,17
	ctx.r11.s64 = ctx.r17.s64 + 17;
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// std r11,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r11.u64);
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfd f0,88(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f13,f0
	ctx.f13.f64 = double(float(ctx.f0.f64));
	// lfs f0,-11752(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + -11752);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// stfd f0,20856(r25)
	PPC_STORE_U64(ctx.r25.u32 + 20856, ctx.f0.u64);
loc_825DA55C:
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// beq cr6,0x825da5cc
	if (ctx.cr6.eq) goto loc_825DA5CC;
	// addi r11,r18,-1
	ctx.r11.s64 = ctx.r18.s64 + -1;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bgt cr6,0x825da5cc
	if (ctx.cr6.gt) goto loc_825DA5CC;
	// addi r11,r20,-1
	ctx.r11.s64 = ctx.r20.s64 + -1;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bgt cr6,0x825da5cc
	if (ctx.cr6.gt) goto loc_825DA5CC;
	// addi r10,r1,96
	ctx.r10.s64 = ctx.r1.s64 + 96;
	// rlwinm r11,r18,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r18.u32 | (ctx.r18.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r20,999
	ctx.r9.s64 = ctx.r20.s64 + 999;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// extsw r10,r9
	ctx.r10.s64 = ctx.r9.s32;
	// lwz r11,-4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// std r10,88(r1)
	PPC_STORE_U64(ctx.r1.u32 + 88, ctx.r10.u64);
	// extsw r11,r11
	ctx.r11.s64 = ctx.r11.s32;
	// std r11,80(r1)
	PPC_STORE_U64(ctx.r1.u32 + 80, ctx.r11.u64);
	// lfd f0,88(r1)
	ctx.fpscr.disableFlushMode();
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 88);
	// lfd f13,80(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 80);
loc_825DA5A8:
	// fcfid f13,f13
	ctx.fpscr.disableFlushMode();
	ctx.f13.f64 = double(ctx.f13.s64);
	// lis r11,-32256
	ctx.r11.s64 = -2113929216;
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// frsp f13,f13
	ctx.f13.f64 = double(float(ctx.f13.f64));
	// frsp f12,f0
	ctx.f12.f64 = double(float(ctx.f0.f64));
	// lfs f0,17324(r11)
	temp.u32 = PPC_LOAD_U32(ctx.r11.u32 + 17324);
	ctx.f0.f64 = double(temp.f32);
	// fmuls f0,f13,f0
	ctx.f0.f64 = double(float(ctx.f13.f64 * ctx.f0.f64));
	// fdivs f0,f0,f12
	ctx.f0.f64 = double(float(ctx.f0.f64 / ctx.f12.f64));
	// stfd f0,20856(r25)
	PPC_STORE_U64(ctx.r25.u32 + 20856, ctx.f0.u64);
loc_825DA5CC:
	// lis r11,-32254
	ctx.r11.s64 = -2113798144;
	// lfd f13,20856(r25)
	ctx.fpscr.disableFlushMode();
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r25.u32 + 20856);
	// lwz r10,3660(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 3660);
	// addi r8,r25,3656
	ctx.r8.s64 = ctx.r25.s64 + 3656;
	// lwz r9,14772(r25)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r25.u32 + 14772);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r10,r10,6,0,25
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 6) & 0xFFFFFFC0;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lfd f0,-28640(r11)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r11.u32 + -28640);
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// fadd f0,f13,f0
	ctx.f0.f64 = ctx.f13.f64 + ctx.f0.f64;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// addi r11,r11,-22904
	ctx.r11.s64 = ctx.r11.s64 + -22904;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// stw r10,3660(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3660, ctx.r10.u32);
	// fctiwz f0,f0
	ctx.f0.s64 = (ctx.f0.f64 > double(INT_MAX)) ? INT_MAX : simde_mm_cvttsd_si32(simde_mm_load_sd(&ctx.f0.f64));
	// stfiwx f0,0,r8
	PPC_STORE_U32(ctx.r8.u32, ctx.f0.u32);
	// lwzx r11,r9,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// stw r11,14776(r25)
	PPC_STORE_U32(ctx.r25.u32 + 14776, ctx.r11.u32);
	// bl 0x825ed4d0
	ctx.lr = 0x825DA61C;
	sub_825ED4D0(ctx, base);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x825cfd80
	ctx.lr = 0x825DA624;
	sub_825CFD80(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825da6e0
	if (!ctx.cr6.eq) goto loc_825DA6E0;
	// addi r4,r1,128
	ctx.r4.s64 = ctx.r1.s64 + 128;
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x825cfe30
	ctx.lr = 0x825DA638;
	sub_825CFE30(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825da65c
	if (ctx.cr6.eq) goto loc_825DA65C;
	// cmpwi cr6,r3,12
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 12, ctx.xer);
	// beq cr6,0x825da654
	if (ctx.cr6.eq) goto loc_825DA654;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,496
	ctx.r1.s64 = ctx.r1.s64 + 496;
	// b 0x8239ba38
	// ERROR 8239BA38
	return;
loc_825DA654:
	// stw r21,21336(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21336, ctx.r21.u32);
	// b 0x825da660
	goto loc_825DA660;
loc_825DA65C:
	// stw r22,21336(r25)
	PPC_STORE_U32(ctx.r25.u32 + 21336, ctx.r22.u32);
loc_825DA660:
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// stw r22,444(r25)
	PPC_STORE_U32(ctx.r25.u32 + 444, ctx.r22.u32);
	// stw r22,3888(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3888, ctx.r22.u32);
	// stw r22,3948(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3948, ctx.r22.u32);
	// stw r22,3884(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3884, ctx.r22.u32);
	// stw r22,14820(r25)
	PPC_STORE_U32(ctx.r25.u32 + 14820, ctx.r22.u32);
	// stw r21,1788(r25)
	PPC_STORE_U32(ctx.r25.u32 + 1788, ctx.r21.u32);
	// stw r22,396(r25)
	PPC_STORE_U32(ctx.r25.u32 + 396, ctx.r22.u32);
	// stw r22,3924(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3924, ctx.r22.u32);
	// stw r22,15300(r25)
	PPC_STORE_U32(ctx.r25.u32 + 15300, ctx.r22.u32);
	// bl 0x825ed678
	ctx.lr = 0x825DA68C;
	sub_825ED678(ctx, base);
	// mr r3,r25
	ctx.r3.u64 = ctx.r25.u64;
	// bl 0x825ed7f8
	ctx.lr = 0x825DA694;
	sub_825ED7F8(ctx, base);
	// lwz r10,1964(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 1964);
	// lwz r9,3180(r25)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r25.u32 + 3180);
	// lis r11,-32157
	ctx.r11.s64 = -2107441152;
	// addi r11,r11,28040
	ctx.r11.s64 = ctx.r11.s64 + 28040;
	// stw r9,52(r10)
	PPC_STORE_U32(ctx.r10.u32 + 52, ctx.r9.u32);
	// lwz r10,3924(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 3924);
	// lwz r9,3936(r25)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r25.u32 + 3936);
	// stw r11,3156(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3156, ctx.r11.u32);
	// stw r21,3372(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3372, ctx.r21.u32);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// stw r22,3368(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3368, ctx.r22.u32);
	// stw r22,3364(r25)
	PPC_STORE_U32(ctx.r25.u32 + 3364, ctx.r22.u32);
	// bne cr6,0x825da6dc
	if (!ctx.cr6.eq) goto loc_825DA6DC;
	// lwz r11,15300(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 15300);
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r10,3940(r25)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r25.u32 + 3940);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// beq cr6,0x825da6e0
	if (ctx.cr6.eq) goto loc_825DA6E0;
loc_825DA6DC:
	// li r3,4
	ctx.r3.s64 = 4;
loc_825DA6E0:
	// addi r1,r1,496
	ctx.r1.s64 = ctx.r1.s64 + 496;
	// b 0x8239ba38
	// ERROR 8239BA38
	return;
}

__attribute__((alias("__imp__sub_825DA6E8"))) PPC_WEAK_FUNC(sub_825DA6E8);
PPC_FUNC_IMPL(__imp__sub_825DA6E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba00
	ctx.lr = 0x825DA6F0;
	sub_8239BA00(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r3
	ctx.r24.u64 = ctx.r3.u64;
	// li r23,0
	ctx.r23.s64 = 0;
	// mr r22,r4
	ctx.r22.u64 = ctx.r4.u64;
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// mr r27,r6
	ctx.r27.u64 = ctx.r6.u64;
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r26,r7
	ctx.r26.u64 = ctx.r7.u64;
	// lwz r11,15472(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 15472);
	// mr r25,r8
	ctx.r25.u64 = ctx.r8.u64;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825dae58
	if (!ctx.cr6.eq) goto loc_825DAE58;
	// li r30,8
	ctx.r30.s64 = 8;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825da790
	if (!ctx.cr6.lt) goto loc_825DA790;
loc_825DA738:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825da790
	if (ctx.cr6.eq) goto loc_825DA790;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825da780
	if (!ctx.cr0.lt) goto loc_825DA780;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA780;
	sub_825D5398(ctx, base);
loc_825DA780:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825da738
	if (ctx.cr6.gt) goto loc_825DA738;
loc_825DA790:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825da7cc
	if (!ctx.cr0.lt) goto loc_825DA7CC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA7CC;
	sub_825D5398(ctx, base);
loc_825DA7CC:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825dc288
	if (!ctx.cr6.eq) goto loc_825DC288;
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825da848
	if (!ctx.cr6.lt) goto loc_825DA848;
loc_825DA7F0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825da848
	if (ctx.cr6.eq) goto loc_825DA848;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825da838
	if (!ctx.cr0.lt) goto loc_825DA838;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA838;
	sub_825D5398(ctx, base);
loc_825DA838:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825da7f0
	if (ctx.cr6.gt) goto loc_825DA7F0;
loc_825DA848:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825da884
	if (!ctx.cr0.lt) goto loc_825DA884;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA884;
	sub_825D5398(ctx, base);
loc_825DA884:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825dc288
	if (!ctx.cr6.eq) goto loc_825DC288;
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825da900
	if (!ctx.cr6.lt) goto loc_825DA900;
loc_825DA8A8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825da900
	if (ctx.cr6.eq) goto loc_825DA900;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825da8f0
	if (!ctx.cr0.lt) goto loc_825DA8F0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA8F0;
	sub_825D5398(ctx, base);
loc_825DA8F0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825da8a8
	if (ctx.cr6.gt) goto loc_825DA8A8;
loc_825DA900:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825da93c
	if (!ctx.cr0.lt) goto loc_825DA93C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA93C;
	sub_825D5398(ctx, base);
loc_825DA93C:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bne cr6,0x825dc288
	if (!ctx.cr6.eq) goto loc_825DC288;
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825da9b8
	if (!ctx.cr6.lt) goto loc_825DA9B8;
loc_825DA960:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825da9b8
	if (ctx.cr6.eq) goto loc_825DA9B8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825da9a8
	if (!ctx.cr0.lt) goto loc_825DA9A8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA9A8;
	sub_825D5398(ctx, base);
loc_825DA9A8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825da960
	if (ctx.cr6.gt) goto loc_825DA960;
loc_825DA9B8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825da9f4
	if (!ctx.cr0.lt) goto loc_825DA9F4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DA9F4;
	sub_825D5398(ctx, base);
loc_825DA9F4:
	// cmpwi cr6,r30,15
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 15, ctx.xer);
	// bne cr6,0x825dc288
	if (!ctx.cr6.eq) goto loc_825DC288;
	// mr r8,r25
	ctx.r8.u64 = ctx.r25.u64;
	// mr r7,r26
	ctx.r7.u64 = ctx.r26.u64;
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x825d8910
	ctx.lr = 0x825DAA18;
	sub_825D8910(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825dc28c
	if (!ctx.cr6.eq) goto loc_825DC28C;
	// lwz r10,21356(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 21356);
	// li r27,1
	ctx.r27.s64 = 1;
	// lwz r11,21352(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 21352);
	// lwz r9,21176(r24)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r24.u32 + 21176);
	// mullw r8,r11,r10
	ctx.r8.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// cmpw cr6,r8,r9
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, ctx.xer);
	// bgt cr6,0x825daa54
	if (ctx.cr6.gt) goto loc_825DAA54;
	// lwz r9,21192(r24)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r24.u32 + 21192);
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// bgt cr6,0x825daa54
	if (ctx.cr6.gt) goto loc_825DAA54;
	// lwz r11,21196(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 21196);
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// ble cr6,0x825daacc
	if (!ctx.cr6.gt) goto loc_825DAACC;
loc_825DAA54:
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x825cc908
	ctx.lr = 0x825DAA5C;
	sub_825CC908(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825daa70
	if (!ctx.cr6.eq) goto loc_825DAA70;
	// li r3,14
	ctx.r3.s64 = 14;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825DAA70:
	// lwz r11,21356(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 21356);
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// lwz r10,21352(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 21352);
	// stw r27,3676(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3676, ctx.r27.u32);
	// mullw r9,r10,r11
	ctx.r9.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// stw r11,21196(r24)
	PPC_STORE_U32(ctx.r24.u32 + 21196, ctx.r11.u32);
	// stw r10,21192(r24)
	PPC_STORE_U32(ctx.r24.u32 + 21192, ctx.r10.u32);
	// stw r9,21176(r24)
	PPC_STORE_U32(ctx.r24.u32 + 21176, ctx.r9.u32);
	// bl 0x825eefd0
	ctx.lr = 0x825DAA94;
	sub_825EEFD0(ctx, base);
	// lwz r5,21356(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 21356);
	// lwz r4,21352(r24)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r24.u32 + 21352);
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x825efc00
	ctx.lr = 0x825DAAA4;
	sub_825EFC00(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825dc28c
	if (!ctx.cr6.eq) goto loc_825DC28C;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// lwz r5,21356(r24)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r24.u32 + 21356);
	// lwz r4,21352(r24)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r24.u32 + 21352);
	// bl 0x825f0508
	ctx.lr = 0x825DAABC;
	sub_825F0508(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825dc28c
	if (!ctx.cr6.eq) goto loc_825DC28C;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x825ebc08
	ctx.lr = 0x825DAACC;
	sub_825EBC08(ctx, base);
loc_825DAACC:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// lwz r11,28(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dab58
	if (ctx.cr6.eq) goto loc_825DAB58;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dab30
	if (!ctx.cr6.lt) goto loc_825DAB30;
loc_825DAAF0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dab30
	if (ctx.cr6.eq) goto loc_825DAB30;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825dab20
	if (!ctx.cr0.lt) goto loc_825DAB20;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DAB20;
	sub_825D5398(ctx, base);
loc_825DAB20:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825daaf0
	if (ctx.cr6.gt) goto loc_825DAAF0;
loc_825DAB30:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825dab58
	if (!ctx.cr0.lt) goto loc_825DAB58;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DAB58;
	sub_825D5398(ctx, base);
loc_825DAB58:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = ctx.r11.u32 & 0x7;
	// bl 0x825d5468
	ctx.lr = 0x825DAB68;
	sub_825D5468(ctx, base);
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825dabdc
	if (!ctx.cr6.lt) goto loc_825DABDC;
loc_825DAB84:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dabdc
	if (ctx.cr6.eq) goto loc_825DABDC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dabcc
	if (!ctx.cr0.lt) goto loc_825DABCC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DABCC;
	sub_825D5398(ctx, base);
loc_825DABCC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dab84
	if (ctx.cr6.gt) goto loc_825DAB84;
loc_825DABDC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dac18
	if (!ctx.cr0.lt) goto loc_825DAC18;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DAC18;
	sub_825D5398(ctx, base);
loc_825DAC18:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825dc288
	if (!ctx.cr6.eq) goto loc_825DC288;
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825dac94
	if (!ctx.cr6.lt) goto loc_825DAC94;
loc_825DAC3C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dac94
	if (ctx.cr6.eq) goto loc_825DAC94;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dac84
	if (!ctx.cr0.lt) goto loc_825DAC84;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DAC84;
	sub_825D5398(ctx, base);
loc_825DAC84:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dac3c
	if (ctx.cr6.gt) goto loc_825DAC3C;
loc_825DAC94:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dacd0
	if (!ctx.cr0.lt) goto loc_825DACD0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DACD0;
	sub_825D5398(ctx, base);
loc_825DACD0:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825dc288
	if (!ctx.cr6.eq) goto loc_825DC288;
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825dad4c
	if (!ctx.cr6.lt) goto loc_825DAD4C;
loc_825DACF4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dad4c
	if (ctx.cr6.eq) goto loc_825DAD4C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dad3c
	if (!ctx.cr0.lt) goto loc_825DAD3C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DAD3C;
	sub_825D5398(ctx, base);
loc_825DAD3C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dacf4
	if (ctx.cr6.gt) goto loc_825DACF4;
loc_825DAD4C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dad88
	if (!ctx.cr0.lt) goto loc_825DAD88;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DAD88;
	sub_825D5398(ctx, base);
loc_825DAD88:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bne cr6,0x825dc288
	if (!ctx.cr6.eq) goto loc_825DC288;
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825dae04
	if (!ctx.cr6.lt) goto loc_825DAE04;
loc_825DADAC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dae04
	if (ctx.cr6.eq) goto loc_825DAE04;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dadf4
	if (!ctx.cr0.lt) goto loc_825DADF4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DADF4;
	sub_825D5398(ctx, base);
loc_825DADF4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dadac
	if (ctx.cr6.gt) goto loc_825DADAC;
loc_825DAE04:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dae40
	if (!ctx.cr0.lt) goto loc_825DAE40;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DAE40;
	sub_825D5398(ctx, base);
loc_825DAE40:
	// cmpwi cr6,r30,14
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 14, ctx.xer);
	// bne cr6,0x825dc288
	if (!ctx.cr6.eq) goto loc_825DC288;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x825d3160
	ctx.lr = 0x825DAE50;
	sub_825D3160(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825DAE58:
	// li r30,2
	ctx.r30.s64 = 2;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825daebc
	if (!ctx.cr6.lt) goto loc_825DAEBC;
loc_825DAE64:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825daebc
	if (ctx.cr6.eq) goto loc_825DAEBC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825daeac
	if (!ctx.cr0.lt) goto loc_825DAEAC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DAEAC;
	sub_825D5398(ctx, base);
loc_825DAEAC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dae64
	if (ctx.cr6.gt) goto loc_825DAE64;
loc_825DAEBC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825daef8
	if (!ctx.cr0.lt) goto loc_825DAEF8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DAEF8;
	sub_825D5398(ctx, base);
loc_825DAEF8:
	// li r27,1
	ctx.r27.s64 = 1;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825daf0c
	if (!ctx.cr6.eq) goto loc_825DAF0C;
	// stw r23,3908(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3908, ctx.r23.u32);
	// b 0x825daf2c
	goto loc_825DAF2C;
loc_825DAF0C:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bne cr6,0x825daf1c
	if (!ctx.cr6.eq) goto loc_825DAF1C;
	// stw r27,3908(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3908, ctx.r27.u32);
	// b 0x825daf2c
	goto loc_825DAF2C;
loc_825DAF1C:
	// cmpwi cr6,r30,2
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 2, ctx.xer);
	// bne cr6,0x825daf2c
	if (!ctx.cr6.eq) goto loc_825DAF2C;
	// li r11,2
	ctx.r11.s64 = 2;
	// stw r11,3908(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3908, ctx.r11.u32);
loc_825DAF2C:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dafa0
	if (!ctx.cr6.lt) goto loc_825DAFA0;
loc_825DAF48:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dafa0
	if (ctx.cr6.eq) goto loc_825DAFA0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825daf90
	if (!ctx.cr0.lt) goto loc_825DAF90;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DAF90;
	sub_825D5398(ctx, base);
loc_825DAF90:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825daf48
	if (ctx.cr6.gt) goto loc_825DAF48;
loc_825DAFA0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dafdc
	if (!ctx.cr0.lt) goto loc_825DAFDC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DAFDC;
	sub_825D5398(ctx, base);
loc_825DAFDC:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,3924(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3924, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825db054
	if (!ctx.cr6.lt) goto loc_825DB054;
loc_825DAFFC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db054
	if (ctx.cr6.eq) goto loc_825DB054;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db044
	if (!ctx.cr0.lt) goto loc_825DB044;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB044;
	sub_825D5398(ctx, base);
loc_825DB044:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825daffc
	if (ctx.cr6.gt) goto loc_825DAFFC;
loc_825DB054:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db090
	if (!ctx.cr0.lt) goto loc_825DB090;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB090;
	sub_825D5398(ctx, base);
loc_825DB090:
	// lwz r11,3924(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3924);
	// lwz r10,3936(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3936);
	// stw r30,15300(r24)
	PPC_STORE_U32(ctx.r24.u32 + 15300, ctx.r30.u32);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bne cr6,0x825dc288
	if (!ctx.cr6.eq) goto loc_825DC288;
	// lwz r11,3940(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3940);
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// bne cr6,0x825dc288
	if (!ctx.cr6.eq) goto loc_825DC288;
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825db124
	if (!ctx.cr6.lt) goto loc_825DB124;
loc_825DB0CC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db124
	if (ctx.cr6.eq) goto loc_825DB124;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db114
	if (!ctx.cr0.lt) goto loc_825DB114;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB114;
	sub_825D5398(ctx, base);
loc_825DB114:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db0cc
	if (ctx.cr6.gt) goto loc_825DB0CC;
loc_825DB124:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db160
	if (!ctx.cr0.lt) goto loc_825DB160;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB160;
	sub_825D5398(ctx, base);
loc_825DB160:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// li r30,5
	ctx.r30.s64 = 5;
	// stw r28,3656(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3656, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825db1d8
	if (!ctx.cr6.lt) goto loc_825DB1D8;
loc_825DB180:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db1d8
	if (ctx.cr6.eq) goto loc_825DB1D8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db1c8
	if (!ctx.cr0.lt) goto loc_825DB1C8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB1C8;
	sub_825D5398(ctx, base);
loc_825DB1C8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db180
	if (ctx.cr6.gt) goto loc_825DB180;
loc_825DB1D8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db214
	if (!ctx.cr0.lt) goto loc_825DB214;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB214;
	sub_825D5398(ctx, base);
loc_825DB214:
	// rlwinm r11,r30,6,0,25
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 6) & 0xFFFFFFC0;
	// lwz r10,3656(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3656);
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// addi r9,r11,32
	ctx.r9.s64 = ctx.r11.s64 + 32;
	// stw r27,3900(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3900, ctx.r27.u32);
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// stw r9,3660(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3660, ctx.r9.u32);
	// stw r11,3656(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3656, ctx.r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825db2a8
	if (!ctx.cr6.lt) goto loc_825DB2A8;
loc_825DB250:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db2a8
	if (ctx.cr6.eq) goto loc_825DB2A8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db298
	if (!ctx.cr0.lt) goto loc_825DB298;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB298;
	sub_825D5398(ctx, base);
loc_825DB298:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db250
	if (ctx.cr6.gt) goto loc_825DB250;
loc_825DB2A8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db2e4
	if (!ctx.cr0.lt) goto loc_825DB2E4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB2E4;
	sub_825D5398(ctx, base);
loc_825DB2E4:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,3892(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3892, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825db35c
	if (!ctx.cr6.lt) goto loc_825DB35C;
loc_825DB304:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db35c
	if (ctx.cr6.eq) goto loc_825DB35C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db34c
	if (!ctx.cr0.lt) goto loc_825DB34C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB34C;
	sub_825D5398(ctx, base);
loc_825DB34C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db304
	if (ctx.cr6.gt) goto loc_825DB304;
loc_825DB35C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db398
	if (!ctx.cr0.lt) goto loc_825DB398;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB398;
	sub_825D5398(ctx, base);
loc_825DB398:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,3884(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3884, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825db410
	if (!ctx.cr6.lt) goto loc_825DB410;
loc_825DB3B8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db410
	if (ctx.cr6.eq) goto loc_825DB410;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db400
	if (!ctx.cr0.lt) goto loc_825DB400;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB400;
	sub_825D5398(ctx, base);
loc_825DB400:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db3b8
	if (ctx.cr6.gt) goto loc_825DB3B8;
loc_825DB410:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db44c
	if (!ctx.cr0.lt) goto loc_825DB44C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB44C;
	sub_825D5398(ctx, base);
loc_825DB44C:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,14820(r24)
	PPC_STORE_U32(ctx.r24.u32 + 14820, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825db4c4
	if (!ctx.cr6.lt) goto loc_825DB4C4;
loc_825DB46C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db4c4
	if (ctx.cr6.eq) goto loc_825DB4C4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db4b4
	if (!ctx.cr0.lt) goto loc_825DB4B4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB4B4;
	sub_825D5398(ctx, base);
loc_825DB4B4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db46c
	if (ctx.cr6.gt) goto loc_825DB46C;
loc_825DB4C4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db500
	if (!ctx.cr0.lt) goto loc_825DB500;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB500;
	sub_825D5398(ctx, base);
loc_825DB500:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,1788(r24)
	PPC_STORE_U32(ctx.r24.u32 + 1788, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825db578
	if (!ctx.cr6.lt) goto loc_825DB578;
loc_825DB520:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db578
	if (ctx.cr6.eq) goto loc_825DB578;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db568
	if (!ctx.cr0.lt) goto loc_825DB568;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB568;
	sub_825D5398(ctx, base);
loc_825DB568:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db520
	if (ctx.cr6.gt) goto loc_825DB520;
loc_825DB578:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db5b4
	if (!ctx.cr0.lt) goto loc_825DB5B4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB5B4;
	sub_825D5398(ctx, base);
loc_825DB5B4:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,1792(r24)
	PPC_STORE_U32(ctx.r24.u32 + 1792, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825db62c
	if (!ctx.cr6.lt) goto loc_825DB62C;
loc_825DB5D4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db62c
	if (ctx.cr6.eq) goto loc_825DB62C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db61c
	if (!ctx.cr0.lt) goto loc_825DB61C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB61C;
	sub_825D5398(ctx, base);
loc_825DB61C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db5d4
	if (ctx.cr6.gt) goto loc_825DB5D4;
loc_825DB62C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db668
	if (!ctx.cr0.lt) goto loc_825DB668;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB668;
	sub_825D5398(ctx, base);
loc_825DB668:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// stw r28,20864(r24)
	PPC_STORE_U32(ctx.r24.u32 + 20864, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825db6e0
	if (!ctx.cr6.lt) goto loc_825DB6E0;
loc_825DB688:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db6e0
	if (ctx.cr6.eq) goto loc_825DB6E0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db6d0
	if (!ctx.cr0.lt) goto loc_825DB6D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB6D0;
	sub_825D5398(ctx, base);
loc_825DB6D0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db688
	if (ctx.cr6.gt) goto loc_825DB688;
loc_825DB6E0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db71c
	if (!ctx.cr0.lt) goto loc_825DB71C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB71C;
	sub_825D5398(ctx, base);
loc_825DB71C:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,3980(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3980, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825db794
	if (!ctx.cr6.lt) goto loc_825DB794;
loc_825DB73C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db794
	if (ctx.cr6.eq) goto loc_825DB794;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db784
	if (!ctx.cr0.lt) goto loc_825DB784;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB784;
	sub_825D5398(ctx, base);
loc_825DB784:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db73c
	if (ctx.cr6.gt) goto loc_825DB73C;
loc_825DB794:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db7d0
	if (!ctx.cr0.lt) goto loc_825DB7D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB7D0;
	sub_825D5398(ctx, base);
loc_825DB7D0:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,436(r24)
	PPC_STORE_U32(ctx.r24.u32 + 436, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825db848
	if (!ctx.cr6.lt) goto loc_825DB848;
loc_825DB7F0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db848
	if (ctx.cr6.eq) goto loc_825DB848;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db838
	if (!ctx.cr0.lt) goto loc_825DB838;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB838;
	sub_825D5398(ctx, base);
loc_825DB838:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db7f0
	if (ctx.cr6.gt) goto loc_825DB7F0;
loc_825DB848:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db884
	if (!ctx.cr0.lt) goto loc_825DB884;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB884;
	sub_825D5398(ctx, base);
loc_825DB884:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,396(r24)
	PPC_STORE_U32(ctx.r24.u32 + 396, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825db8fc
	if (!ctx.cr6.lt) goto loc_825DB8FC;
loc_825DB8A4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db8fc
	if (ctx.cr6.eq) goto loc_825DB8FC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db8ec
	if (!ctx.cr0.lt) goto loc_825DB8EC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB8EC;
	sub_825D5398(ctx, base);
loc_825DB8EC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db8a4
	if (ctx.cr6.gt) goto loc_825DB8A4;
loc_825DB8FC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db938
	if (!ctx.cr0.lt) goto loc_825DB938;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB938;
	sub_825D5398(ctx, base);
loc_825DB938:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,2972(r24)
	PPC_STORE_U32(ctx.r24.u32 + 2972, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825db9b0
	if (!ctx.cr6.lt) goto loc_825DB9B0;
loc_825DB958:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825db9b0
	if (ctx.cr6.eq) goto loc_825DB9B0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825db9a0
	if (!ctx.cr0.lt) goto loc_825DB9A0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB9A0;
	sub_825D5398(ctx, base);
loc_825DB9A0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825db958
	if (ctx.cr6.gt) goto loc_825DB958;
loc_825DB9B0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825db9ec
	if (!ctx.cr0.lt) goto loc_825DB9EC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DB9EC;
	sub_825D5398(ctx, base);
loc_825DB9EC:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,3932(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3932, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dba64
	if (!ctx.cr6.lt) goto loc_825DBA64;
loc_825DBA0C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dba64
	if (ctx.cr6.eq) goto loc_825DBA64;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dba54
	if (!ctx.cr0.lt) goto loc_825DBA54;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBA54;
	sub_825D5398(ctx, base);
loc_825DBA54:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dba0c
	if (ctx.cr6.gt) goto loc_825DBA0C;
loc_825DBA64:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dbaa0
	if (!ctx.cr0.lt) goto loc_825DBAA0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBAA0;
	sub_825D5398(ctx, base);
loc_825DBAA0:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// stw r28,14792(r24)
	PPC_STORE_U32(ctx.r24.u32 + 14792, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825dbb18
	if (!ctx.cr6.lt) goto loc_825DBB18;
loc_825DBAC0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dbb18
	if (ctx.cr6.eq) goto loc_825DBB18;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dbb08
	if (!ctx.cr0.lt) goto loc_825DBB08;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBB08;
	sub_825D5398(ctx, base);
loc_825DBB08:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dbac0
	if (ctx.cr6.gt) goto loc_825DBAC0;
loc_825DBB18:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dbb54
	if (!ctx.cr0.lt) goto loc_825DBB54;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBB54;
	sub_825D5398(ctx, base);
loc_825DBB54:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r28,14772(r24)
	PPC_STORE_U32(ctx.r24.u32 + 14772, ctx.r28.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dbbcc
	if (!ctx.cr6.lt) goto loc_825DBBCC;
loc_825DBB74:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dbbcc
	if (ctx.cr6.eq) goto loc_825DBBCC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dbbbc
	if (!ctx.cr0.lt) goto loc_825DBBBC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBBBC;
	sub_825D5398(ctx, base);
loc_825DBBBC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dbb74
	if (ctx.cr6.gt) goto loc_825DBB74;
loc_825DBBCC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dbc08
	if (!ctx.cr0.lt) goto loc_825DBC08;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBC08;
	sub_825D5398(ctx, base);
loc_825DBC08:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// stw r30,3436(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3436, ctx.r30.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825dbcd4
	if (ctx.cr6.eq) goto loc_825DBCD4;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dbc88
	if (!ctx.cr6.lt) goto loc_825DBC88;
loc_825DBC30:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dbc88
	if (ctx.cr6.eq) goto loc_825DBC88;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dbc78
	if (!ctx.cr0.lt) goto loc_825DBC78;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBC78;
	sub_825D5398(ctx, base);
loc_825DBC78:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dbc30
	if (ctx.cr6.gt) goto loc_825DBC30;
loc_825DBC88:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dbcc4
	if (!ctx.cr0.lt) goto loc_825DBCC4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBCC4;
	sub_825D5398(ctx, base);
loc_825DBCC4:
	// addi r11,r30,2
	ctx.r11.s64 = ctx.r30.s64 + 2;
	// stw r30,3428(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3428, ctx.r30.u32);
	// stw r11,21524(r24)
	PPC_STORE_U32(ctx.r24.u32 + 21524, ctx.r11.u32);
	// b 0x825dbd78
	goto loc_825DBD78;
loc_825DBCD4:
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dbd34
	if (!ctx.cr6.lt) goto loc_825DBD34;
loc_825DBCDC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dbd34
	if (ctx.cr6.eq) goto loc_825DBD34;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dbd24
	if (!ctx.cr0.lt) goto loc_825DBD24;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBD24;
	sub_825D5398(ctx, base);
loc_825DBD24:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dbcdc
	if (ctx.cr6.gt) goto loc_825DBCDC;
loc_825DBD34:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dbd70
	if (!ctx.cr0.lt) goto loc_825DBD70;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBD70;
	sub_825D5398(ctx, base);
loc_825DBD70:
	// stw r30,3440(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3440, ctx.r30.u32);
	// stw r30,21524(r24)
	PPC_STORE_U32(ctx.r24.u32 + 21524, ctx.r30.u32);
loc_825DBD78:
	// lwz r11,3436(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3436);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbd94
	if (!ctx.cr6.eq) goto loc_825DBD94;
	// lwz r11,3440(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3440);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
	// beq cr6,0x825dbd98
	if (ctx.cr6.eq) goto loc_825DBD98;
loc_825DBD94:
	// mr r11,r27
	ctx.r11.u64 = ctx.r27.u64;
loc_825DBD98:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r11,3432(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3432, ctx.r11.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dbe10
	if (!ctx.cr6.lt) goto loc_825DBE10;
loc_825DBDB8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dbe10
	if (ctx.cr6.eq) goto loc_825DBE10;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dbe00
	if (!ctx.cr0.lt) goto loc_825DBE00;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBE00;
	sub_825D5398(ctx, base);
loc_825DBE00:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dbdb8
	if (ctx.cr6.gt) goto loc_825DBDB8;
loc_825DBE10:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dbe4c
	if (!ctx.cr0.lt) goto loc_825DBE4C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DBE4C;
	sub_825D5398(ctx, base);
loc_825DBE4C:
	// lwz r11,15300(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 15300);
	// stw r30,3444(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3444, ctx.r30.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbf18
	if (!ctx.cr6.eq) goto loc_825DBF18;
	// lwz r11,3908(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3908);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbeec
	if (!ctx.cr6.eq) goto loc_825DBEEC;
	// lwz r11,3884(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3884);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbf0c
	if (!ctx.cr6.eq) goto loc_825DBF0C;
	// lwz r11,1788(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 1788);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825dbf0c
	if (!ctx.cr6.eq) goto loc_825DBF0C;
	// lwz r11,1792(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 1792);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825dbf0c
	if (!ctx.cr6.eq) goto loc_825DBF0C;
	// lwz r11,3932(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3932);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbf0c
	if (!ctx.cr6.eq) goto loc_825DBF0C;
	// lwz r11,20864(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 20864);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbf0c
	if (!ctx.cr6.eq) goto loc_825DBF0C;
	// lwz r11,3892(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3892);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbf0c
	if (!ctx.cr6.eq) goto loc_825DBF0C;
	// lwz r11,3924(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbf0c
	if (!ctx.cr6.eq) goto loc_825DBF0C;
	// lwz r11,14820(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 14820);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbf0c
	if (!ctx.cr6.eq) goto loc_825DBF0C;
	// lwz r11,3980(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3980);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbf0c
	if (!ctx.cr6.eq) goto loc_825DBF0C;
	// lwz r11,14772(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbf0c
	if (!ctx.cr6.eq) goto loc_825DBF0C;
	// lwz r11,14792(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 14792);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// b 0x825dbf08
	goto loc_825DBF08;
loc_825DBEEC:
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825dbf18
	if (!ctx.cr6.eq) goto loc_825DBF18;
	// lwz r11,3884(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3884);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dbf0c
	if (!ctx.cr6.eq) goto loc_825DBF0C;
	// lwz r11,1788(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 1788);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
loc_825DBF08:
	// beq cr6,0x825dbf18
	if (ctx.cr6.eq) goto loc_825DBF18;
loc_825DBF0C:
	// li r3,6
	ctx.r3.s64 = 6;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825DBF18:
	// lwz r10,14772(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 14772);
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r9,3924(r24)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3924);
	// addi r11,r11,-22904
	ctx.r11.s64 = ctx.r11.s64 + -22904;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// lwzx r11,r10,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// stw r11,14776(r24)
	PPC_STORE_U32(ctx.r24.u32 + 14776, ctx.r11.u32);
	// beq cr6,0x825dbf44
	if (ctx.cr6.eq) goto loc_825DBF44;
	// stw r23,3948(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3948, ctx.r23.u32);
	// stw r23,3884(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3884, ctx.r23.u32);
loc_825DBF44:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x825ed4d0
	ctx.lr = 0x825DBF50;
	sub_825ED4D0(ctx, base);
	// lwz r11,1788(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 1788);
	// stw r23,444(r24)
	PPC_STORE_U32(ctx.r24.u32 + 444, ctx.r23.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r23,3888(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3888, ctx.r23.u32);
	// beq cr6,0x825dbf98
	if (ctx.cr6.eq) goto loc_825DBF98;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x825ed560
	ctx.lr = 0x825DBF70;
	sub_825ED560(ctx, base);
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x825ed678
	ctx.lr = 0x825DBF78;
	sub_825ED678(ctx, base);
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x825ed7f8
	ctx.lr = 0x825DBF80;
	sub_825ED7F8(ctx, base);
	// lis r11,-32157
	ctx.r11.s64 = -2107441152;
	// lwz r10,1964(r24)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r24.u32 + 1964);
	// lwz r9,3180(r24)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3180);
	// addi r11,r11,28040
	ctx.r11.s64 = ctx.r11.s64 + 28040;
	// stw r9,52(r10)
	PPC_STORE_U32(ctx.r10.u32 + 52, ctx.r9.u32);
	// stw r11,3156(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3156, ctx.r11.u32);
loc_825DBF98:
	// lwz r11,15300(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 15300);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dc068
	if (ctx.cr6.eq) goto loc_825DC068;
	// lwz r11,3884(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3884);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,396(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 396);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,3924(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,3892(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3892);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,20864(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 20864);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,14820(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 14820);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,1792(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 1792);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,3980(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3980);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,436(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 436);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,3932(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3932);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,14792(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 14792);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,3436(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3436);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,3428(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3428);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// lwz r11,3440(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 3440);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc05c
	if (!ctx.cr6.eq) goto loc_825DC05C;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x825d84b8
	ctx.lr = 0x825DC054;
	sub_825D84B8(ctx, base);
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825DC05C:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825DC068:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r27,3364(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3364, ctx.r27.u32);
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dc0e0
	if (!ctx.cr6.lt) goto loc_825DC0E0;
loc_825DC088:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc0e0
	if (ctx.cr6.eq) goto loc_825DC0E0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dc0d0
	if (!ctx.cr0.lt) goto loc_825DC0D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC0D0;
	sub_825D5398(ctx, base);
loc_825DC0D0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc088
	if (ctx.cr6.gt) goto loc_825DC088;
loc_825DC0E0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dc11c
	if (!ctx.cr0.lt) goto loc_825DC11C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC11C;
	sub_825D5398(ctx, base);
loc_825DC11C:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// bne cr6,0x825dc140
	if (!ctx.cr6.eq) goto loc_825DC140;
	// lwz r11,84(r24)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc140
	if (!ctx.cr6.eq) goto loc_825DC140;
	// stw r27,3372(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3372, ctx.r27.u32);
	// stw r23,3368(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3368, ctx.r23.u32);
	// stw r23,3364(r24)
	PPC_STORE_U32(ctx.r24.u32 + 3364, ctx.r23.u32);
loc_825DC140:
	// cmplwi cr6,r22,5
	ctx.cr6.compare<uint32_t>(ctx.r22.u32, 5, ctx.xer);
	// bne cr6,0x825dc27c
	if (!ctx.cr6.eq) goto loc_825DC27C;
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// mr r29,r23
	ctx.r29.u64 = ctx.r23.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dc1bc
	if (!ctx.cr6.lt) goto loc_825DC1BC;
loc_825DC164:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc1bc
	if (ctx.cr6.eq) goto loc_825DC1BC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dc1ac
	if (!ctx.cr0.lt) goto loc_825DC1AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC1AC;
	sub_825D5398(ctx, base);
loc_825DC1AC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc164
	if (ctx.cr6.gt) goto loc_825DC164;
loc_825DC1BC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dc1f8
	if (!ctx.cr0.lt) goto loc_825DC1F8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC1F8;
	sub_825D5398(ctx, base);
loc_825DC1F8:
	// lwz r31,84(r24)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r24.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// stw r29,20868(r24)
	PPC_STORE_U32(ctx.r24.u32 + 20868, ctx.r29.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dc254
	if (!ctx.cr6.lt) goto loc_825DC254;
loc_825DC214:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc254
	if (ctx.cr6.eq) goto loc_825DC254;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825dc244
	if (!ctx.cr0.lt) goto loc_825DC244;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC244;
	sub_825D5398(ctx, base);
loc_825DC244:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc214
	if (ctx.cr6.gt) goto loc_825DC214;
loc_825DC254:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825dc27c
	if (!ctx.cr0.lt) goto loc_825DC27C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC27C;
	sub_825D5398(ctx, base);
loc_825DC27C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825DC288:
	// li r3,4
	ctx.r3.s64 = 4;
loc_825DC28C:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
}

__attribute__((alias("__imp__sub_825DC294"))) PPC_WEAK_FUNC(sub_825DC294);
PPC_FUNC_IMPL(__imp__sub_825DC294) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DC298"))) PPC_WEAK_FUNC(sub_825DC298);
PPC_FUNC_IMPL(__imp__sub_825DC298) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba04
	ctx.lr = 0x825DC2A0;
	sub_8239BA04(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// mr r23,r5
	ctx.r23.u64 = ctx.r5.u64;
	// li r24,0
	ctx.r24.s64 = 0;
	// li r30,1
	ctx.r30.s64 = 1;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dc328
	if (!ctx.cr6.lt) goto loc_825DC328;
loc_825DC2D0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc328
	if (ctx.cr6.eq) goto loc_825DC328;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dc318
	if (!ctx.cr0.lt) goto loc_825DC318;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC318;
	sub_825D5398(ctx, base);
loc_825DC318:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc2d0
	if (ctx.cr6.gt) goto loc_825DC2D0;
loc_825DC328:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dc364
	if (!ctx.cr0.lt) goto loc_825DC364;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC364;
	sub_825D5398(ctx, base);
loc_825DC364:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825dc838
	if (ctx.cr6.eq) goto loc_825DC838;
	// lwz r11,21160(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21160);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dc3a4
	if (ctx.cr6.eq) goto loc_825DC3A4;
	// lwz r11,21548(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21548);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dc3a4
	if (!ctx.cr6.eq) goto loc_825DC3A4;
	// lwz r11,20832(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 20832);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dc39c
	if (ctx.cr6.eq) goto loc_825DC39C;
	// lwz r11,20840(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 20840);
	// addi r24,r11,2
	ctx.r24.s64 = ctx.r11.s64 + 2;
	// b 0x825dc3c0
	goto loc_825DC3C0;
loc_825DC39C:
	// li r24,2
	ctx.r24.s64 = 2;
	// b 0x825dc3c0
	goto loc_825DC3C0;
loc_825DC3A4:
	// lwz r11,20832(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 20832);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dc3bc
	if (ctx.cr6.eq) goto loc_825DC3BC;
	// lwz r11,21164(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21164);
	// addi r24,r11,1
	ctx.r24.s64 = ctx.r11.s64 + 1;
	// b 0x825dc3c0
	goto loc_825DC3C0;
loc_825DC3BC:
	// li r24,1
	ctx.r24.s64 = 1;
loc_825DC3C0:
	// cmpwi cr6,r24,0
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// ble cr6,0x825dc838
	if (!ctx.cr6.gt) goto loc_825DC838;
	// addi r26,r28,12
	ctx.r26.s64 = ctx.r28.s64 + 12;
	// mr r25,r24
	ctx.r25.u64 = ctx.r24.u64;
loc_825DC3D0:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,16
	ctx.r30.s64 = 16;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,16
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 16, ctx.xer);
	// bge cr6,0x825dc448
	if (!ctx.cr6.lt) goto loc_825DC448;
loc_825DC3EC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc448
	if (ctx.cr6.eq) goto loc_825DC448;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dc438
	if (!ctx.cr0.lt) goto loc_825DC438;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC438;
	sub_825D5398(ctx, base);
loc_825DC438:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc3ec
	if (ctx.cr6.gt) goto loc_825DC3EC;
loc_825DC448:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dc484
	if (!ctx.cr0.lt) goto loc_825DC484;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC484;
	sub_825D5398(ctx, base);
loc_825DC484:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825dc4fc
	if (!ctx.cr6.lt) goto loc_825DC4FC;
loc_825DC4A0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc4fc
	if (ctx.cr6.eq) goto loc_825DC4FC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dc4ec
	if (!ctx.cr0.lt) goto loc_825DC4EC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC4EC;
	sub_825D5398(ctx, base);
loc_825DC4EC:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc4a0
	if (ctx.cr6.gt) goto loc_825DC4A0;
loc_825DC4FC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dc538
	if (!ctx.cr0.lt) goto loc_825DC538;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC538;
	sub_825D5398(ctx, base);
loc_825DC538:
	// rlwinm r11,r28,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// li r30,16
	ctx.r30.s64 = 16;
	// or r11,r11,r29
	ctx.r11.u64 = ctx.r11.u64 | ctx.r29.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// stw r11,-4(r26)
	PPC_STORE_U32(ctx.r26.u32 + -4, ctx.r11.u32);
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,16
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 16, ctx.xer);
	// bge cr6,0x825dc5bc
	if (!ctx.cr6.lt) goto loc_825DC5BC;
loc_825DC560:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc5bc
	if (ctx.cr6.eq) goto loc_825DC5BC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dc5ac
	if (!ctx.cr0.lt) goto loc_825DC5AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC5AC;
	sub_825D5398(ctx, base);
loc_825DC5AC:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc560
	if (ctx.cr6.gt) goto loc_825DC560;
loc_825DC5BC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dc5f8
	if (!ctx.cr0.lt) goto loc_825DC5F8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC5F8;
	sub_825D5398(ctx, base);
loc_825DC5F8:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825dc670
	if (!ctx.cr6.lt) goto loc_825DC670;
loc_825DC614:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc670
	if (ctx.cr6.eq) goto loc_825DC670;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dc660
	if (!ctx.cr0.lt) goto loc_825DC660;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC660;
	sub_825D5398(ctx, base);
loc_825DC660:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc614
	if (ctx.cr6.gt) goto loc_825DC614;
loc_825DC670:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dc6ac
	if (!ctx.cr0.lt) goto loc_825DC6AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC6AC;
	sub_825D5398(ctx, base);
loc_825DC6AC:
	// rlwinm r11,r28,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// li r30,14
	ctx.r30.s64 = 14;
	// or r11,r11,r29
	ctx.r11.u64 = ctx.r11.u64 | ctx.r29.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// stw r11,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r11.u32);
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,14
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 14, ctx.xer);
	// bge cr6,0x825dc730
	if (!ctx.cr6.lt) goto loc_825DC730;
loc_825DC6D4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc730
	if (ctx.cr6.eq) goto loc_825DC730;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dc720
	if (!ctx.cr0.lt) goto loc_825DC720;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC720;
	sub_825D5398(ctx, base);
loc_825DC720:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc6d4
	if (ctx.cr6.gt) goto loc_825DC6D4;
loc_825DC730:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dc76c
	if (!ctx.cr0.lt) goto loc_825DC76C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC76C;
	sub_825D5398(ctx, base);
loc_825DC76C:
	// stw r28,-12(r26)
	PPC_STORE_U32(ctx.r26.u32 + -12, ctx.r28.u32);
	// li r30,14
	ctx.r30.s64 = 14;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplwi cr6,r11,14
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 14, ctx.xer);
	// bge cr6,0x825dc7e8
	if (!ctx.cr6.lt) goto loc_825DC7E8;
loc_825DC78C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc7e8
	if (ctx.cr6.eq) goto loc_825DC7E8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dc7d8
	if (!ctx.cr0.lt) goto loc_825DC7D8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC7D8;
	sub_825D5398(ctx, base);
loc_825DC7D8:
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc78c
	if (ctx.cr6.gt) goto loc_825DC78C;
loc_825DC7E8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dc824
	if (!ctx.cr0.lt) goto loc_825DC824;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC824;
	sub_825D5398(ctx, base);
loc_825DC824:
	// addi r25,r25,-1
	ctx.r25.s64 = ctx.r25.s64 + -1;
	// stw r30,-8(r26)
	PPC_STORE_U32(ctx.r26.u32 + -8, ctx.r30.u32);
	// addi r26,r26,16
	ctx.r26.s64 = ctx.r26.s64 + 16;
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// bne cr6,0x825dc3d0
	if (!ctx.cr6.eq) goto loc_825DC3D0;
loc_825DC838:
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r24,0(r23)
	PPC_STORE_U32(ctx.r23.u32 + 0, ctx.r24.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba54
	// ERROR 8239BA54
	return;
}

__attribute__((alias("__imp__sub_825DC848"))) PPC_WEAK_FUNC(sub_825DC848);
PPC_FUNC_IMPL(__imp__sub_825DC848) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba14
	ctx.lr = 0x825DC850;
	sub_8239BA14(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// li r30,1
	ctx.r30.s64 = 1;
	// li r28,0
	ctx.r28.s64 = 0;
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dc8d0
	if (!ctx.cr6.lt) goto loc_825DC8D0;
loc_825DC878:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc8d0
	if (ctx.cr6.eq) goto loc_825DC8D0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825dc8c0
	if (!ctx.cr0.lt) goto loc_825DC8C0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC8C0;
	sub_825D5398(ctx, base);
loc_825DC8C0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc878
	if (ctx.cr6.gt) goto loc_825DC878;
loc_825DC8D0:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dc90c
	if (!ctx.cr0.lt) goto loc_825DC90C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC90C;
	sub_825D5398(ctx, base);
loc_825DC90C:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825dc920
	if (!ctx.cr6.eq) goto loc_825DC920;
	// stw r30,284(r27)
	PPC_STORE_U32(ctx.r27.u32 + 284, ctx.r30.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
loc_825DC920:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// li r28,0
	ctx.r28.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dc994
	if (!ctx.cr6.lt) goto loc_825DC994;
loc_825DC93C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dc994
	if (ctx.cr6.eq) goto loc_825DC994;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825dc984
	if (!ctx.cr0.lt) goto loc_825DC984;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC984;
	sub_825D5398(ctx, base);
loc_825DC984:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dc93c
	if (ctx.cr6.gt) goto loc_825DC93C;
loc_825DC994:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dc9d0
	if (!ctx.cr0.lt) goto loc_825DC9D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DC9D0;
	sub_825D5398(ctx, base);
loc_825DC9D0:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825dc9e8
	if (!ctx.cr6.eq) goto loc_825DC9E8;
	// li r11,2
	ctx.r11.s64 = 2;
	// stw r11,284(r27)
	PPC_STORE_U32(ctx.r27.u32 + 284, ctx.r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
loc_825DC9E8:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// li r28,0
	ctx.r28.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dca5c
	if (!ctx.cr6.lt) goto loc_825DCA5C;
loc_825DCA04:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dca5c
	if (ctx.cr6.eq) goto loc_825DCA5C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825dca4c
	if (!ctx.cr0.lt) goto loc_825DCA4C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCA4C;
	sub_825D5398(ctx, base);
loc_825DCA4C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dca04
	if (ctx.cr6.gt) goto loc_825DCA04;
loc_825DCA5C:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dca98
	if (!ctx.cr0.lt) goto loc_825DCA98;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCA98;
	sub_825D5398(ctx, base);
loc_825DCA98:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825dcab4
	if (!ctx.cr6.eq) goto loc_825DCAB4;
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r30,21584(r27)
	PPC_STORE_U32(ctx.r27.u32 + 21584, ctx.r30.u32);
	// stw r11,284(r27)
	PPC_STORE_U32(ctx.r27.u32 + 284, ctx.r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
loc_825DCAB4:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dcb24
	if (!ctx.cr6.lt) goto loc_825DCB24;
loc_825DCACC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dcb24
	if (ctx.cr6.eq) goto loc_825DCB24;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dcb14
	if (!ctx.cr0.lt) goto loc_825DCB14;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCB14;
	sub_825D5398(ctx, base);
loc_825DCB14:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dcacc
	if (ctx.cr6.gt) goto loc_825DCACC;
loc_825DCB24:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dcb60
	if (!ctx.cr0.lt) goto loc_825DCB60;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCB60;
	sub_825D5398(ctx, base);
loc_825DCB60:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// li r11,4
	ctx.r11.s64 = 4;
	// beq cr6,0x825dcb70
	if (ctx.cr6.eq) goto loc_825DCB70;
	// li r11,5
	ctx.r11.s64 = 5;
loc_825DCB70:
	// stw r11,284(r27)
	PPC_STORE_U32(ctx.r27.u32 + 284, ctx.r11.u32);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
}

__attribute__((alias("__imp__sub_825DCB7C"))) PPC_WEAK_FUNC(sub_825DCB7C);
PPC_FUNC_IMPL(__imp__sub_825DCB7C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DCB80"))) PPC_WEAK_FUNC(sub_825DCB80);
PPC_FUNC_IMPL(__imp__sub_825DCB80) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,1828(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1828);
	// lwz r10,1836(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1836);
	// lwz r9,1840(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1840);
	// lwz r8,1864(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1864);
	// lwz r7,1788(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1788);
	// stw r11,1832(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1832, ctx.r11.u32);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// stw r10,1852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1852, ctx.r10.u32);
	// stw r9,1856(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1856, ctx.r9.u32);
	// stw r8,1860(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1860, ctx.r8.u32);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// lwz r11,1824(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1824);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r10,1844(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1844);
	// lwz r8,1848(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1848);
	// lwz r7,1868(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1868);
	// stw r11,1832(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1832, ctx.r11.u32);
	// stw r9,1796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1796, ctx.r9.u32);
	// stw r10,1852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1852, ctx.r10.u32);
	// stw r8,1856(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1856, ctx.r8.u32);
	// stw r7,1860(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1860, ctx.r7.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DCBD8"))) PPC_WEAK_FUNC(sub_825DCBD8);
PPC_FUNC_IMPL(__imp__sub_825DCBD8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825DCBE0;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// li r26,0
	ctx.r26.s64 = 0;
	// li r25,2
	ctx.r25.s64 = 2;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// lwz r11,432(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 432);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825dccb8
	if (!ctx.cr6.eq) goto loc_825DCCB8;
	// li r27,1
	ctx.r27.s64 = 1;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// bge cr6,0x825dcc74
	if (!ctx.cr6.lt) goto loc_825DCC74;
loc_825DCC1C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dcc74
	if (ctx.cr6.eq) goto loc_825DCC74;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dcc64
	if (!ctx.cr0.lt) goto loc_825DCC64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCC64;
	sub_825D5398(ctx, base);
loc_825DCC64:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dcc1c
	if (ctx.cr6.gt) goto loc_825DCC1C;
loc_825DCC74:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dccb0
	if (!ctx.cr0.lt) goto loc_825DCCB0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCCB0;
	sub_825D5398(ctx, base);
loc_825DCCB0:
	// stw r30,448(r28)
	PPC_STORE_U32(ctx.r28.u32 + 448, ctx.r30.u32);
	// b 0x825dcd9c
	goto loc_825DCD9C;
loc_825DCCB8:
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825dcd1c
	if (!ctx.cr6.lt) goto loc_825DCD1C;
loc_825DCCC4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dcd1c
	if (ctx.cr6.eq) goto loc_825DCD1C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dcd0c
	if (!ctx.cr0.lt) goto loc_825DCD0C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCD0C;
	sub_825D5398(ctx, base);
loc_825DCD0C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dccc4
	if (ctx.cr6.gt) goto loc_825DCCC4;
loc_825DCD1C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dcd58
	if (!ctx.cr0.lt) goto loc_825DCD58;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCD58;
	sub_825D5398(ctx, base);
loc_825DCD58:
	// li r27,1
	ctx.r27.s64 = 1;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// stw r27,448(r28)
	PPC_STORE_U32(ctx.r28.u32 + 448, ctx.r27.u32);
	// bne cr6,0x825dcd70
	if (!ctx.cr6.eq) goto loc_825DCD70;
	// stw r26,448(r28)
	PPC_STORE_U32(ctx.r28.u32 + 448, ctx.r26.u32);
	// b 0x825dcd9c
	goto loc_825DCD9C;
loc_825DCD70:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bne cr6,0x825dcd80
	if (!ctx.cr6.eq) goto loc_825DCD80;
	// stw r26,3944(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3944, ctx.r26.u32);
	// b 0x825dcd94
	goto loc_825DCD94;
loc_825DCD80:
	// cmpwi cr6,r30,2
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 2, ctx.xer);
	// bne cr6,0x825dcd90
	if (!ctx.cr6.eq) goto loc_825DCD90;
	// stw r27,3944(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3944, ctx.r27.u32);
	// b 0x825dcd94
	goto loc_825DCD94;
loc_825DCD90:
	// stw r25,3944(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3944, ctx.r25.u32);
loc_825DCD94:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x825d7bc0
	ctx.lr = 0x825DCD9C;
	sub_825D7BC0(ctx, base);
loc_825DCD9C:
	// lwz r11,440(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 440);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dd214
	if (ctx.cr6.eq) goto loc_825DD214;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// lwz r11,248(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 248);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// cmpwi cr6,r11,10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 10, ctx.xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// bgt cr6,0x825dcf2c
	if (ctx.cr6.gt) goto loc_825DCF2C;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dce28
	if (!ctx.cr6.lt) goto loc_825DCE28;
loc_825DCDD0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dce28
	if (ctx.cr6.eq) goto loc_825DCE28;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dce18
	if (!ctx.cr0.lt) goto loc_825DCE18;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCE18;
	sub_825D5398(ctx, base);
loc_825DCE18:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dcdd0
	if (ctx.cr6.gt) goto loc_825DCDD0;
loc_825DCE28:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dce64
	if (!ctx.cr0.lt) goto loc_825DCE64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCE64;
	sub_825D5398(ctx, base);
loc_825DCE64:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825dd20c
	if (ctx.cr6.eq) goto loc_825DD20C;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dcee0
	if (!ctx.cr6.lt) goto loc_825DCEE0;
loc_825DCE88:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dcee0
	if (ctx.cr6.eq) goto loc_825DCEE0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dced0
	if (!ctx.cr0.lt) goto loc_825DCED0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCED0;
	sub_825D5398(ctx, base);
loc_825DCED0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dce88
	if (ctx.cr6.gt) goto loc_825DCE88;
loc_825DCEE0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dcf1c
	if (!ctx.cr0.lt) goto loc_825DCF1C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCF1C;
	sub_825D5398(ctx, base);
loc_825DCF1C:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825dcfdc
	if (!ctx.cr6.eq) goto loc_825DCFDC;
	// addi r11,r28,2168
	ctx.r11.s64 = ctx.r28.s64 + 2168;
	// b 0x825dd210
	goto loc_825DD210;
loc_825DCF2C:
	// cmpwi cr6,r11,20
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 20, ctx.xer);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// bgt cr6,0x825dd0a4
	if (ctx.cr6.gt) goto loc_825DD0A4;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dcf98
	if (!ctx.cr6.lt) goto loc_825DCF98;
loc_825DCF40:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dcf98
	if (ctx.cr6.eq) goto loc_825DCF98;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dcf88
	if (!ctx.cr0.lt) goto loc_825DCF88;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCF88;
	sub_825D5398(ctx, base);
loc_825DCF88:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dcf40
	if (ctx.cr6.gt) goto loc_825DCF40;
loc_825DCF98:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dcfd4
	if (!ctx.cr0.lt) goto loc_825DCFD4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DCFD4;
	sub_825D5398(ctx, base);
loc_825DCFD4:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825dcfe4
	if (!ctx.cr6.eq) goto loc_825DCFE4;
loc_825DCFDC:
	// addi r11,r28,2156
	ctx.r11.s64 = ctx.r28.s64 + 2156;
	// b 0x825dd210
	goto loc_825DD210;
loc_825DCFE4:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dd058
	if (!ctx.cr6.lt) goto loc_825DD058;
loc_825DD000:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dd058
	if (ctx.cr6.eq) goto loc_825DD058;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dd048
	if (!ctx.cr0.lt) goto loc_825DD048;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DD048;
	sub_825D5398(ctx, base);
loc_825DD048:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dd000
	if (ctx.cr6.gt) goto loc_825DD000;
loc_825DD058:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dd094
	if (!ctx.cr0.lt) goto loc_825DD094;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DD094;
	sub_825D5398(ctx, base);
loc_825DD094:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825dd20c
	if (ctx.cr6.eq) goto loc_825DD20C;
	// addi r11,r28,2168
	ctx.r11.s64 = ctx.r28.s64 + 2168;
	// b 0x825dd210
	goto loc_825DD210;
loc_825DD0A4:
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dd104
	if (!ctx.cr6.lt) goto loc_825DD104;
loc_825DD0AC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dd104
	if (ctx.cr6.eq) goto loc_825DD104;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dd0f4
	if (!ctx.cr0.lt) goto loc_825DD0F4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DD0F4;
	sub_825D5398(ctx, base);
loc_825DD0F4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dd0ac
	if (ctx.cr6.gt) goto loc_825DD0AC;
loc_825DD104:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dd140
	if (!ctx.cr0.lt) goto loc_825DD140;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DD140;
	sub_825D5398(ctx, base);
loc_825DD140:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825dd150
	if (!ctx.cr6.eq) goto loc_825DD150;
	// addi r11,r28,2168
	ctx.r11.s64 = ctx.r28.s64 + 2168;
	// b 0x825dd210
	goto loc_825DD210;
loc_825DD150:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dd1c4
	if (!ctx.cr6.lt) goto loc_825DD1C4;
loc_825DD16C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dd1c4
	if (ctx.cr6.eq) goto loc_825DD1C4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dd1b4
	if (!ctx.cr0.lt) goto loc_825DD1B4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DD1B4;
	sub_825D5398(ctx, base);
loc_825DD1B4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dd16c
	if (ctx.cr6.gt) goto loc_825DD16C;
loc_825DD1C4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dd200
	if (!ctx.cr0.lt) goto loc_825DD200;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DD200;
	sub_825D5398(ctx, base);
loc_825DD200:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// addi r11,r28,2156
	ctx.r11.s64 = ctx.r28.s64 + 2156;
	// beq cr6,0x825dd210
	if (ctx.cr6.eq) goto loc_825DD210;
loc_825DD20C:
	// addi r11,r28,2144
	ctx.r11.s64 = ctx.r28.s64 + 2144;
loc_825DD210:
	// stw r11,2140(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2140, ctx.r11.u32);
loc_825DD214:
	// lwz r11,3888(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3888);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dd2d4
	if (ctx.cr6.eq) goto loc_825DD2D4;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dd294
	if (!ctx.cr6.lt) goto loc_825DD294;
loc_825DD23C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dd294
	if (ctx.cr6.eq) goto loc_825DD294;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dd284
	if (!ctx.cr0.lt) goto loc_825DD284;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DD284;
	sub_825D5398(ctx, base);
loc_825DD284:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dd23c
	if (ctx.cr6.gt) goto loc_825DD23C;
loc_825DD294:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dd2d0
	if (!ctx.cr0.lt) goto loc_825DD2D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DD2D0;
	sub_825D5398(ctx, base);
loc_825DD2D0:
	// stw r30,452(r28)
	PPC_STORE_U32(ctx.r28.u32 + 452, ctx.r30.u32);
loc_825DD2D4:
	// lwz r11,436(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 436);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dd3a8
	if (ctx.cr6.eq) goto loc_825DD3A8;
	// lwz r3,84(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825dd30c
	if (!ctx.cr0.lt) goto loc_825DD30C;
	// bl 0x825d5398
	ctx.lr = 0x825DD30C;
	sub_825D5398(ctx, base);
loc_825DD30C:
	// cmplwi cr6,r31,1
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 1, ctx.xer);
	// bne cr6,0x825dd3a4
	if (!ctx.cr6.eq) goto loc_825DD3A4;
	// lwz r3,84(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// stw r26,328(r28)
	PPC_STORE_U32(ctx.r28.u32 + 328, ctx.r26.u32);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825dd344
	if (!ctx.cr0.lt) goto loc_825DD344;
	// bl 0x825d5398
	ctx.lr = 0x825DD344;
	sub_825D5398(ctx, base);
loc_825DD344:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825dd358
	if (!ctx.cr6.eq) goto loc_825DD358;
	// stw r26,336(r28)
	PPC_STORE_U32(ctx.r28.u32 + 336, ctx.r26.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825DD358:
	// lwz r3,84(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825dd384
	if (!ctx.cr0.lt) goto loc_825DD384;
	// bl 0x825d5398
	ctx.lr = 0x825DD384;
	sub_825D5398(ctx, base);
loc_825DD384:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825dd398
	if (!ctx.cr6.eq) goto loc_825DD398;
	// stw r27,336(r28)
	PPC_STORE_U32(ctx.r28.u32 + 336, ctx.r27.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825DD398:
	// stw r25,336(r28)
	PPC_STORE_U32(ctx.r28.u32 + 336, ctx.r25.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825DD3A4:
	// stw r27,328(r28)
	PPC_STORE_U32(ctx.r28.u32 + 328, ctx.r27.u32);
loc_825DD3A8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825DD3B0"))) PPC_WEAK_FUNC(sub_825DD3B0);
PPC_FUNC_IMPL(__imp__sub_825DD3B0) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,188(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// lwz r10,180(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 180);
	// lwz r9,3660(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3660);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// cmpwi cr6,r9,300
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 300, ctx.xer);
	// bgt cr6,0x825dd400
	if (ctx.cr6.gt) goto loc_825DD400;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// ori r10,r10,11264
	ctx.r10.u64 = ctx.r10.u64 | 11264;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bgt cr6,0x825dd400
	if (ctx.cr6.gt) goto loc_825DD400;
	// lis r10,0
	ctx.r10.s64 = 0;
	// ori r10,r10,42240
	ctx.r10.u64 = ctx.r10.u64 | 42240;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// ble cr6,0x825dd3f4
	if (!ctx.cr6.gt) goto loc_825DD3F4;
	// li r11,4
	ctx.r11.s64 = 4;
	// stw r11,15532(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15532, ctx.r11.u32);
	// blr 
	return;
loc_825DD3F4:
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,15532(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15532, ctx.r11.u32);
	// blr 
	return;
loc_825DD400:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,15520(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15520, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DD40C"))) PPC_WEAK_FUNC(sub_825DD40C);
PPC_FUNC_IMPL(__imp__sub_825DD40C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DD410"))) PPC_WEAK_FUNC(sub_825DD410);
PPC_FUNC_IMPL(__imp__sub_825DD410) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,3704(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3704);
	// lwz r10,3700(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3700);
	// stw r11,3700(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3700, ctx.r11.u32);
	// stw r10,3704(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3704, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r9,3744(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3744, ctx.r9.u32);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r9,3748(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3748, ctx.r9.u32);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r11,3752(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3752, ctx.r11.u32);
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r11,3776(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3776, ctx.r11.u32);
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r11,3780(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3780, ctx.r11.u32);
	// lwz r11,8(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r11,3784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3784, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DD458"))) PPC_WEAK_FUNC(sub_825DD458);
PPC_FUNC_IMPL(__imp__sub_825DD458) {
	PPC_FUNC_PROLOGUE();
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// beq cr6,0x825dd47c
	if (ctx.cr6.eq) goto loc_825DD47C;
	// addi r11,r4,-112
	ctx.r11.s64 = ctx.r4.s64 + -112;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,3568
	ctx.r10.s64 = ctx.r10.s64 + 3568;
	// addi r9,r9,3624
	ctx.r9.s64 = ctx.r9.s64 + 3624;
	// b 0x825dd490
	goto loc_825DD490;
loc_825DD47C:
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,3512
	ctx.r10.s64 = ctx.r10.s64 + 3512;
	// addi r9,r9,3540
	ctx.r9.s64 = ctx.r9.s64 + 3540;
loc_825DD490:
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	// stw r10,3392(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3392, ctx.r10.u32);
	// lwzx r11,r11,r9
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	// rotlwi r10,r11,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,3388(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3388, ctx.r11.u32);
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// addi r11,r11,-22904
	ctx.r11.s64 = ctx.r11.s64 + -22904;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lwz r11,-4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// stw r11,14776(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14776, ctx.r11.u32);
	// lwz r11,14772(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bnelr cr6
	if (!ctx.cr6.eq) return;
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,14772(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14772, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DD4D4"))) PPC_WEAK_FUNC(sub_825DD4D4);
PPC_FUNC_IMPL(__imp__sub_825DD4D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DD4D8"))) PPC_WEAK_FUNC(sub_825DD4D8);
PPC_FUNC_IMPL(__imp__sub_825DD4D8) {
	PPC_FUNC_PROLOGUE();
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lwz r10,3668(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 3668);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825dd4f0
	if (!ctx.cr6.eq) goto loc_825DD4F0;
	// li r3,3
	ctx.r3.s64 = 3;
	// blr 
	return;
loc_825DD4F0:
	// li r10,1
	ctx.r10.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r10,284(r11)
	PPC_STORE_U32(ctx.r11.u32 + 284, ctx.r10.u32);
	// stw r10,3384(r11)
	PPC_STORE_U32(ctx.r11.u32 + 3384, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DD504"))) PPC_WEAK_FUNC(sub_825DD504);
PPC_FUNC_IMPL(__imp__sub_825DD504) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DD508"))) PPC_WEAK_FUNC(sub_825DD508);
PPC_FUNC_IMPL(__imp__sub_825DD508) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825DD510;
	sub_8239BA1C(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r10,3412(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3412);
	// lwz r11,14892(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14892);
	// lwz r29,14896(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14896);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lwz r6,14884(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14884);
	// lwz r30,14888(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14888);
	// bne cr6,0x825dd680
	if (!ctx.cr6.eq) goto loc_825DD680;
	// lwz r10,15564(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825dd680
	if (!ctx.cr6.eq) goto loc_825DD680;
	// li r10,1
	ctx.r10.s64 = 1;
	// lwz r9,3708(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3708);
	// stw r10,3412(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3412, ctx.r10.u32);
	// lwz r10,3696(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3696);
	// lwz r10,608(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 608);
	// stw r10,608(r9)
	PPC_STORE_U32(ctx.r9.u32 + 608, ctx.r10.u32);
	// lwz r10,3696(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3696);
	// lwz r9,3708(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3708);
	// lwz r10,596(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 596);
	// stw r10,596(r9)
	PPC_STORE_U32(ctx.r9.u32 + 596, ctx.r10.u32);
	// lwz r10,3696(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3696);
	// lwz r9,3708(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3708);
	// lwz r10,600(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 600);
	// stw r10,600(r9)
	PPC_STORE_U32(ctx.r9.u32 + 600, ctx.r10.u32);
	// lwz r10,3696(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3696);
	// lwz r9,3708(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3708);
	// lwz r10,604(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 604);
	// stw r10,604(r9)
	PPC_STORE_U32(ctx.r9.u32 + 604, ctx.r10.u32);
	// lwz r10,15900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15900);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825dd5f8
	if (ctx.cr6.eq) goto loc_825DD5F8;
	// lwz r10,19712(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19712);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825dd5f8
	if (!ctx.cr6.eq) goto loc_825DD5F8;
	// lwz r10,3708(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3708);
	// li r8,2
	ctx.r8.s64 = 2;
	// lwz r9,584(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 584);
	// mulli r11,r9,68
	ctx.r11.s64 = ctx.r9.s64 * 68;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r9,584(r10)
	PPC_STORE_U32(ctx.r10.u32 + 584, ctx.r9.u32);
	// stw r8,40(r11)
	PPC_STORE_U32(ctx.r11.u32 + 40, ctx.r8.u32);
	// lwz r10,3696(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3696);
	// stw r10,44(r11)
	PPC_STORE_U32(ctx.r11.u32 + 44, ctx.r10.u32);
	// lwz r10,14884(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14884);
	// stw r10,52(r11)
	PPC_STORE_U32(ctx.r11.u32 + 52, ctx.r10.u32);
	// lwz r10,14888(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14888);
	// stw r10,56(r11)
	PPC_STORE_U32(ctx.r11.u32 + 56, ctx.r10.u32);
	// lwz r10,14872(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14872);
	// stw r10,48(r11)
	PPC_STORE_U32(ctx.r11.u32 + 48, ctx.r10.u32);
	// lwz r10,14900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14900);
	// stw r10,60(r11)
	PPC_STORE_U32(ctx.r11.u32 + 60, ctx.r10.u32);
	// lwz r10,14904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14904);
	// stw r10,64(r11)
	PPC_STORE_U32(ctx.r11.u32 + 64, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
loc_825DD5F8:
	// lwz r10,21184(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21184);
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x825dd64c
	if (!ctx.cr6.eq) goto loc_825DD64C;
	// lwz r10,14772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825dd64c
	if (!ctx.cr6.gt) goto loc_825DD64C;
	// ld r10,3576(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 3576);
	// cmpdi cr6,r10,1
	ctx.cr6.compare<int64_t>(ctx.r10.s64, 1, ctx.xer);
	// ble cr6,0x825dd64c
	if (!ctx.cr6.gt) goto loc_825DD64C;
	// lwz r11,19696(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19696);
	// lwz r10,19700(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19700);
	// lwz r6,21484(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21484);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r7,21488(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21488);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r8,21492(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21492);
	// add r6,r6,r11
	ctx.r6.u64 = ctx.r6.u64 + ctx.r11.u64;
	// lwz r9,21496(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21496);
	// add r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 + ctx.r11.u64;
	// add r30,r8,r10
	ctx.r30.u64 = ctx.r8.u64 + ctx.r10.u64;
	// add r29,r9,r10
	ctx.r29.u64 = ctx.r9.u64 + ctx.r10.u64;
loc_825DD64C:
	// mullw r5,r11,r6
	ctx.r5.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r6.s32);
	// lwz r4,3732(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// lwz r3,3788(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3788);
	// bl 0x8239cb70
	ctx.lr = 0x825DD65C;
	sub_8239CB70(ctx, base);
	// mullw r30,r29,r30
	ctx.r30.s64 = int64_t(ctx.r29.s32) * int64_t(ctx.r30.s32);
	// lwz r4,3736(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// lwz r3,3792(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3792);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825DD670;
	sub_8239CB70(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// lwz r3,3796(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3796);
	// bl 0x8239cb70
	ctx.lr = 0x825DD680;
	sub_8239CB70(ctx, base);
loc_825DD680:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825DD688"))) PPC_WEAK_FUNC(sub_825DD688);
PPC_FUNC_IMPL(__imp__sub_825DD688) {
	PPC_FUNC_PROLOGUE();
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r10,15300(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15300);
	// li r9,-3
	ctx.r9.s64 = -3;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r11,284(r3)
	PPC_STORE_U32(ctx.r3.u32 + 284, ctx.r11.u32);
	// stw r9,3376(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3376, ctx.r9.u32);
	// stw r11,3380(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3380, ctx.r11.u32);
	// stw r11,3396(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3396, ctx.r11.u32);
	// beq cr6,0x825dd6c0
	if (ctx.cr6.eq) goto loc_825DD6C0;
	// stw r11,14788(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14788, ctx.r11.u32);
	// stw r11,3384(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3384, ctx.r11.u32);
	// stw r11,3400(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3400, ctx.r11.u32);
	// stw r11,21432(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21432, ctx.r11.u32);
	// blr 
	return;
loc_825DD6C0:
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r11,3384(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3384, ctx.r11.u32);
	// stw r11,3400(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3400, ctx.r11.u32);
	// stw r11,21432(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21432, ctx.r11.u32);
	// stw r10,14788(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14788, ctx.r10.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DD6D8"))) PPC_WEAK_FUNC(sub_825DD6D8);
PPC_FUNC_IMPL(__imp__sub_825DD6D8) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,15472(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15472);
	// li r10,2
	ctx.r10.s64 = 2;
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// bne cr6,0x825dd6f0
	if (!ctx.cr6.eq) goto loc_825DD6F0;
	// lwz r11,3924(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3924);
	// b 0x825dd708
	goto loc_825DD708;
loc_825DD6F0:
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825dd714
	if (!ctx.cr6.eq) goto loc_825DD714;
	// lwz r11,19976(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19976);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dd710
	if (!ctx.cr6.eq) goto loc_825DD710;
	// lwz r11,19980(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19980);
loc_825DD708:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dd714
	if (ctx.cr6.eq) goto loc_825DD714;
loc_825DD710:
	// li r10,3
	ctx.r10.s64 = 3;
loc_825DD714:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r10,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r10.u32);
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DD724"))) PPC_WEAK_FUNC(sub_825DD724);
PPC_FUNC_IMPL(__imp__sub_825DD724) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DD728"))) PPC_WEAK_FUNC(sub_825DD728);
PPC_FUNC_IMPL(__imp__sub_825DD728) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9e0
	ctx.lr = 0x825DD730;
	sub_8239B9E0(ctx, base);
	// stwu r1,-448(r1)
	ea = -448 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r11,r1,128
	ctx.r11.s64 = ctx.r1.s64 + 128;
	// li r29,0
	ctx.r29.s64 = 0;
	// addi r3,r1,160
	ctx.r3.s64 = ctx.r1.s64 + 160;
	// addi r30,r1,180
	ctx.r30.s64 = ctx.r1.s64 + 180;
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// addi r28,r1,200
	ctx.r28.s64 = ctx.r1.s64 + 200;
	// stw r11,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r11.u32);
	// li r16,1
	ctx.r16.s64 = 1;
	// lwz r11,19984(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19984);
	// srawi r4,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r10.s32 >> 1;
	// lwz r9,208(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// mr r17,r29
	ctx.r17.u64 = ctx.r29.u64;
	// mullw r4,r4,r11
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r11.s32);
	// lwz r23,3720(r31)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// lwz r25,220(r31)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// lwz r22,144(r31)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// lwz r21,228(r31)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r31.u32 + 228);
	// lwz r26,3728(r31)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// lwz r24,3724(r31)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// stw r29,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r29.u32);
	// lwz r27,268(r31)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// srawi r5,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r9.s32 >> 1;
	// stw r21,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r21.u32);
	// add r4,r4,r23
	ctx.r4.u64 = ctx.r4.u64 + ctx.r23.u64;
	// lwz r9,232(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 232);
	// mullw r5,r5,r11
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// lwz r8,1892(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1892);
	// lwz r7,1896(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1896);
	// lwz r6,136(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// stw r8,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r8.u32);
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// add r4,r4,r25
	ctx.r4.u64 = ctx.r4.u64 + ctx.r25.u64;
	// add r25,r10,r5
	ctx.r25.u64 = ctx.r10.u64 + ctx.r5.u64;
	// add r5,r10,r5
	ctx.r5.u64 = ctx.r10.u64 + ctx.r5.u64;
	// mullw r11,r11,r22
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r22.s32);
	// stw r4,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r4.u32);
	// stw r29,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r29.u32);
	// stw r4,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r4.u32);
	// stw r9,176(r1)
	PPC_STORE_U32(ctx.r1.u32 + 176, ctx.r9.u32);
	// stw r29,172(r1)
	PPC_STORE_U32(ctx.r1.u32 + 172, ctx.r29.u32);
	// add r5,r5,r26
	ctx.r5.u64 = ctx.r5.u64 + ctx.r26.u64;
	// rlwinm r26,r11,2,0,29
	ctx.r26.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r25,r24
	ctx.r10.u64 = ctx.r25.u64 + ctx.r24.u64;
	// add r11,r11,r26
	ctx.r11.u64 = ctx.r11.u64 + ctx.r26.u64;
	// addi r3,r1,136
	ctx.r3.s64 = ctx.r1.s64 + 136;
	// addi r4,r1,132
	ctx.r4.s64 = ctx.r1.s64 + 132;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r5,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r5.u32);
	// mr r18,r16
	ctx.r18.u64 = ctx.r16.u64;
	// stw r10,168(r1)
	PPC_STORE_U32(ctx.r1.u32 + 168, ctx.r10.u32);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// stw r10,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r10.u32);
	// addi r10,r1,124
	ctx.r10.s64 = ctx.r1.s64 + 124;
	// stw r3,164(r1)
	PPC_STORE_U32(ctx.r1.u32 + 164, ctx.r3.u32);
	// mr r14,r16
	ctx.r14.u64 = ctx.r16.u64;
	// stw r29,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r29.u32);
	// stw r9,196(r1)
	PPC_STORE_U32(ctx.r1.u32 + 196, ctx.r9.u32);
	// addi r9,r1,220
	ctx.r9.s64 = ctx.r1.s64 + 220;
	// stw r4,184(r1)
	PPC_STORE_U32(ctx.r1.u32 + 184, ctx.r4.u32);
	// addi r19,r6,1
	ctx.r19.s64 = ctx.r6.s64 + 1;
	// stw r5,188(r1)
	PPC_STORE_U32(ctx.r1.u32 + 188, ctx.r5.u32);
	// stw r29,192(r1)
	PPC_STORE_U32(ctx.r1.u32 + 192, ctx.r29.u32);
	// stw r29,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r29.u32);
	// stw r11,208(r1)
	PPC_STORE_U32(ctx.r1.u32 + 208, ctx.r11.u32);
	// stw r11,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r11.u32);
	// li r11,20
	ctx.r11.s64 = 20;
	// stw r10,204(r1)
	PPC_STORE_U32(ctx.r1.u32 + 204, ctx.r10.u32);
	// addi r10,r1,276
	ctx.r10.s64 = ctx.r1.s64 + 276;
	// stw r29,216(r1)
	PPC_STORE_U32(ctx.r1.u32 + 216, ctx.r29.u32);
	// stw r11,212(r1)
	PPC_STORE_U32(ctx.r1.u32 + 212, ctx.r11.u32);
	// rlwinm r11,r6,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r29,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r29.u32);
	// addi r9,r1,120
	ctx.r9.s64 = ctx.r1.s64 + 120;
	// stw r9,224(r1)
	PPC_STORE_U32(ctx.r1.u32 + 224, ctx.r9.u32);
	// lwz r9,19976(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// stw r8,228(r1)
	PPC_STORE_U32(ctx.r1.u32 + 228, ctx.r8.u32);
	// li r8,192
	ctx.r8.s64 = 192;
	// stw r29,236(r1)
	PPC_STORE_U32(ctx.r1.u32 + 236, ctx.r29.u32);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stw r11,240(r1)
	PPC_STORE_U32(ctx.r1.u32 + 240, ctx.r11.u32);
	// stw r7,248(r1)
	PPC_STORE_U32(ctx.r1.u32 + 248, ctx.r7.u32);
	// stw r29,256(r1)
	PPC_STORE_U32(ctx.r1.u32 + 256, ctx.r29.u32);
	// stw r8,232(r1)
	PPC_STORE_U32(ctx.r1.u32 + 232, ctx.r8.u32);
	// addi r8,r1,116
	ctx.r8.s64 = ctx.r1.s64 + 116;
	// stw r11,260(r1)
	PPC_STORE_U32(ctx.r1.u32 + 260, ctx.r11.u32);
	// stw r29,264(r1)
	PPC_STORE_U32(ctx.r1.u32 + 264, ctx.r29.u32);
	// stw r29,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r29.u32);
	// stw r29,272(r1)
	PPC_STORE_U32(ctx.r1.u32 + 272, ctx.r29.u32);
	// stw r8,244(r1)
	PPC_STORE_U32(ctx.r1.u32 + 244, ctx.r8.u32);
	// li r8,144
	ctx.r8.s64 = 144;
	// stw r8,252(r1)
	PPC_STORE_U32(ctx.r1.u32 + 252, ctx.r8.u32);
	// stw r29,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r29.u32);
	// stw r29,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r29.u32);
	// beq cr6,0x825dd8e0
	if (ctx.cr6.eq) goto loc_825DD8E0;
	// lwz r11,19980(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19980);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dd8e0
	if (ctx.cr6.eq) goto loc_825DD8E0;
	// lwz r11,21000(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21000);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825dd8e0
	if (!ctx.cr6.eq) goto loc_825DD8E0;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// lwz r10,21268(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21268);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// b 0x825dd8e4
	goto loc_825DD8E4;
loc_825DD8E0:
	// lwz r11,21268(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21268);
loc_825DD8E4:
	// stw r11,21264(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21264, ctx.r11.u32);
	// lwz r11,2968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2968);
	// clrlwi r11,r11,31
	ctx.r11.u64 = ctx.r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dd960
	if (ctx.cr6.eq) goto loc_825DD960;
	// rlwinm r10,r22,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
	// li r9,16384
	ctx.r9.s64 = 16384;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825dd930
	if (!ctx.cr6.gt) goto loc_825DD930;
	// mr r10,r29
	ctx.r10.u64 = ctx.r29.u64;
loc_825DD910:
	// lwz r8,1772(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// sthx r9,r10,r8
	PPC_STORE_U16(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u16);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lwz r8,144(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x825dd910
	if (ctx.cr6.lt) goto loc_825DD910;
loc_825DD930:
	// lwz r11,144(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// mr r10,r29
	ctx.r10.u64 = ctx.r29.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825dd960
	if (!ctx.cr6.gt) goto loc_825DD960;
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_825DD944:
	// lwz r8,1780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1780);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// sthx r9,r11,r8
	PPC_STORE_U16(ctx.r11.u32 + ctx.r8.u32, ctx.r9.u16);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// lwz r8,144(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// cmpw cr6,r10,r8
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x825dd944
	if (ctx.cr6.lt) goto loc_825DD944;
loc_825DD960:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// beq cr6,0x825dd988
	if (ctx.cr6.eq) goto loc_825DD988;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bge cr6,0x825dd988
	if (!ctx.cr6.lt) goto loc_825DD988;
	// lwz r11,2912(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2912);
	// lwz r10,2924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2924);
	// stw r11,2880(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2880, ctx.r11.u32);
	// stw r10,2892(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2892, ctx.r10.u32);
	// b 0x825dda04
	goto loc_825DDA04;
loc_825DD988:
	// lwz r11,2928(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2928);
	// lwz r8,2940(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2940);
	// addi r9,r11,726
	ctx.r9.s64 = ctx.r11.s64 + 726;
	// lwz r11,2944(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2944);
	// addi r8,r8,729
	ctx.r8.s64 = ctx.r8.s64 + 729;
	// lwz r10,2088(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2088);
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,2948(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2948);
	// addi r11,r11,729
	ctx.r11.s64 = ctx.r11.s64 + 729;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r6,r11,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r9,729
	ctx.r9.s64 = ctx.r9.s64 + 729;
	// lwzx r11,r7,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r31.u32);
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r10,r10,263
	ctx.r10.s64 = ctx.r10.s64 + 263;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + ctx.r31.u64;
	// stw r11,2888(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2888, ctx.r11.u32);
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r11,2884(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2884, ctx.r11.u32);
	// stw r11,2880(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2880, ctx.r11.u32);
	// lwzx r11,r8,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + ctx.r31.u32);
	// stw r11,2892(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2892, ctx.r11.u32);
	// lwzx r11,r6,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r31.u32);
	// stw r11,2896(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2896, ctx.r11.u32);
	// lwzx r11,r7,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r31.u32);
	// stw r11,2900(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2900, ctx.r11.u32);
	// lwz r11,2100(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2100);
	// stw r11,2092(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2092, ctx.r11.u32);
	// lwzx r11,r10,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r31.u32);
	// stw r11,2096(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2096, ctx.r11.u32);
loc_825DDA04:
	// addi r15,r31,248
	ctx.r15.s64 = ctx.r31.s64 + 248;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,0(r15)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	// bl 0x825d56a0
	ctx.lr = 0x825DDA14;
	sub_825D56A0(ctx, base);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// ble cr6,0x825de0c8
	if (!ctx.cr6.gt) goto loc_825DE0C8;
loc_825DDA28:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// clrlwi r10,r3,31
	ctx.r10.u64 = ctx.r3.u32 & 0x1;
	// lwz r24,128(r1)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// lwz r23,136(r1)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r22,132(r1)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// subf r11,r3,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r3.s64;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r20,r11,27,31,31
	ctx.r20.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// bne cr6,0x825dda64
	if (!ctx.cr6.eq) goto loc_825DDA64;
	// lwz r11,1892(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1892);
	// stw r11,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r11.u32);
	// lwz r11,1896(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1896);
	// b 0x825ddaa4
	goto loc_825DDAA4;
loc_825DDA64:
	// lwz r11,14820(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14820);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ddaa8
	if (ctx.cr6.eq) goto loc_825DDAA8;
	// lwz r11,14832(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14832);
	// lwz r8,1892(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1892);
	// srawi r11,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 4;
	// lwz r10,1896(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1896);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r7,r11,r7
	ctx.r7.u64 = ctx.r11.u64 + ctx.r7.u64;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r9,r7,6,0,25
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 6) & 0xFFFFFFC0;
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r9,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r9.u32);
loc_825DDAA4:
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r11.u32);
loc_825DDAA8:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bge cr6,0x825ddac4
	if (!ctx.cr6.lt) goto loc_825DDAC4;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ddac4
	if (ctx.cr6.eq) goto loc_825DDAC4;
	// lwz r4,15468(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15468);
	// bl 0x82624478
	ctx.lr = 0x825DDAC4;
	sub_82624478(ctx, base);
loc_825DDAC4:
	// cntlzw r11,r3
	ctx.r11.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// xori r21,r11,1
	ctx.r21.u64 = ctx.r11.u64 ^ 1;
	// lwz r11,21236(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21236);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ddcb0
	if (ctx.cr6.eq) goto loc_825DDCB0;
	// lwz r11,21000(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21000);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825ddb00
	if (!ctx.cr6.eq) goto loc_825DDB00;
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825ddb00
	if (!ctx.cr6.eq) goto loc_825DDB00;
	// lwz r11,21272(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21272);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,21272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21272, ctx.r11.u32);
loc_825DDB00:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,21264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21264);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ddc94
	if (ctx.cr6.eq) goto loc_825DDC94;
	// lwz r11,21272(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21272);
	// lwz r30,84(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,21272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21272, ctx.r11.u32);
	// lwz r11,28(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 28);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ddbb0
	if (ctx.cr6.eq) goto loc_825DDBB0;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// mr r28,r16
	ctx.r28.u64 = ctx.r16.u64;
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825ddb88
	if (!ctx.cr6.lt) goto loc_825DDB88;
loc_825DDB48:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ddb88
	if (ctx.cr6.eq) goto loc_825DDB88;
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r28,r11,r28
	ctx.r28.s64 = ctx.r28.s64 - ctx.r11.s64;
	// stw r10,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r11.u64);
	// bge 0x825ddb78
	if (!ctx.cr0.lt) goto loc_825DDB78;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DDB78;
	sub_825D5398(ctx, base);
loc_825DDB78:
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825ddb48
	if (ctx.cr6.gt) goto loc_825DDB48;
loc_825DDB88:
	// ld r9,0(r30)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// clrldi r10,r28,32
	ctx.r10.u64 = ctx.r28.u64 & 0xFFFFFFFF;
	// lwz r11,8(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// subf. r11,r28,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r28.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r10.u64);
	// bge 0x825ddbb0
	if (!ctx.cr0.lt) goto loc_825DDBB0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DDBB0;
	sub_825D5398(ctx, base);
loc_825DDBB0:
	// lwz r11,8(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// clrlwi r4,r11,29
	ctx.r4.u64 = ctx.r11.u32 & 0x7;
	// bl 0x825d5468
	ctx.lr = 0x825DDBC0;
	sub_825D5468(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r28,19976(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r27,19984(r31)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19984);
	// lwz r25,284(r31)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// lwz r26,19980(r31)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19980);
	// bl 0x826235a8
	ctx.lr = 0x825DDBDC;
	sub_826235A8(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// stw r16,1944(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1944, ctx.r16.u32);
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825ddc58
	if (ctx.cr6.eq) goto loc_825DDC58;
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// stw r25,284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 284, ctx.r25.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r28,19976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19976, ctx.r28.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r26,19980(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19980, ctx.r26.u32);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stw r27,19984(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19984, ctx.r27.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r18
	ctx.r5.u64 = ctx.r18.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82612480
	ctx.lr = 0x825DDC20;
	sub_82612480(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825de164
	if (!ctx.cr6.eq) goto loc_825DE164;
	// cmpwi cr6,r17,0
	ctx.cr6.compare<int32_t>(ctx.r17.s32, 0, ctx.xer);
	// beq cr6,0x825ddc38
	if (ctx.cr6.eq) goto loc_825DDC38;
	// cmpwi cr6,r17,4
	ctx.cr6.compare<int32_t>(ctx.r17.s32, 4, ctx.xer);
	// bne cr6,0x825ddc3c
	if (!ctx.cr6.eq) goto loc_825DDC3C;
loc_825DDC38:
	// mr r17,r30
	ctx.r17.u64 = ctx.r30.u64;
loc_825DDC3C:
	// cmpwi cr6,r18,0
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 0, ctx.xer);
	// beq cr6,0x825de0c8
	if (ctx.cr6.eq) goto loc_825DE0C8;
	// lwz r11,21272(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21272);
	// mr r18,r29
	ctx.r18.u64 = ctx.r29.u64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r11,21272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21272, ctx.r11.u32);
	// b 0x825de0b0
	goto loc_825DE0B0;
loc_825DDC58:
	// lwz r11,19976(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// cmpw cr6,r11,r28
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r28.s32, ctx.xer);
	// bne cr6,0x825ddeac
	if (!ctx.cr6.eq) goto loc_825DDEAC;
	// lwz r11,19980(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19980);
	// cmpw cr6,r11,r26
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r26.s32, ctx.xer);
	// bne cr6,0x825ddeac
	if (!ctx.cr6.eq) goto loc_825DDEAC;
	// lwz r11,19984(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19984);
	// cmpw cr6,r11,r27
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r27.s32, ctx.xer);
	// bne cr6,0x825ddeac
	if (!ctx.cr6.eq) goto loc_825DDEAC;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ddc90
	if (ctx.cr6.eq) goto loc_825DDC90;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bne cr6,0x825ddeac
	if (!ctx.cr6.eq) goto loc_825DDEAC;
loc_825DDC90:
	// mr r18,r16
	ctx.r18.u64 = ctx.r16.u64;
loc_825DDC94:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,21264(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21264);
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r10,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ddcb0
	if (ctx.cr6.eq) goto loc_825DDCB0;
	// mr r14,r16
	ctx.r14.u64 = ctx.r16.u64;
loc_825DDCB0:
	// lwz r11,3932(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3932);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ddcdc
	if (ctx.cr6.eq) goto loc_825DDCDC;
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825ddcdc
	if (ctx.cr6.eq) goto loc_825DDCDC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// bl 0x82623988
	ctx.lr = 0x825DDCD4;
	sub_82623988(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825de1dc
	if (!ctx.cr6.eq) goto loc_825DE1DC;
loc_825DDCDC:
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mr r30,r29
	ctx.r30.u64 = ctx.r29.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// li r11,128
	ctx.r11.s64 = 128;
	// stw r11,2964(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2964, ctx.r11.u32);
	// stw r11,2960(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2960, ctx.r11.u32);
	// stw r11,2956(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2956, ctx.r11.u32);
	// ble cr6,0x825ddff8
	if (!ctx.cr6.gt) goto loc_825DDFF8;
loc_825DDCFC:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825ddd30
	if (!ctx.cr6.eq) goto loc_825DDD30;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e41d8
	ctx.lr = 0x825DDD10;
	sub_825E41D8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825ddd30
	if (ctx.cr6.eq) goto loc_825DDD30;
	// mr r4,r15
	ctx.r4.u64 = ctx.r15.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e4268
	ctx.lr = 0x825DDD24;
	sub_825E4268(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825de1dc
	if (!ctx.cr6.eq) goto loc_825DE1DC;
	// mr r19,r29
	ctx.r19.u64 = ctx.r29.u64;
loc_825DDD30:
	// lwz r11,3072(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3072);
	// addi r19,r19,1
	ctx.r19.s64 = ctx.r19.s64 + 1;
	// lwz r6,112(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r4,124(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x825DDD50;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// bne cr6,0x825ddfb0
	if (!ctx.cr6.eq) goto loc_825DDFB0;
	// lwz r11,2968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2968);
	// rlwinm r10,r11,0,29,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825dde34
	if (ctx.cr6.eq) goto loc_825DDE34;
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r10,r10,0,20,20
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x800;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825dde34
	if (!ctx.cr6.eq) goto loc_825DDE34;
	// rlwinm r9,r11,0,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r8,1780(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1780);
	// mullw r11,r10,r11
	ctx.r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// stw r9,2968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2968, ctx.r9.u32);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// sthx r29,r11,r8
	PPC_STORE_U16(ctx.r11.u32 + ctx.r8.u32, ctx.r29.u16);
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// sth r29,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r29.u16);
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r9,1772(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// sthx r29,r11,r9
	PPC_STORE_U16(ctx.r11.u32 + ctx.r9.u32, ctx.r29.u16);
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// mullw r11,r11,r9
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r9.s32);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// sth r29,2(r11)
	PPC_STORE_U16(ctx.r11.u32 + 2, ctx.r29.u16);
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r9,112(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r10,1772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// mullw r11,r11,r9
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r9.s32);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// sthx r29,r11,r10
	PPC_STORE_U16(ctx.r11.u32 + ctx.r10.u32, ctx.r29.u16);
loc_825DDE34:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825dde48
	if (ctx.cr6.eq) goto loc_825DDE48;
	// cmplwi cr6,r19,1
	ctx.cr6.compare<uint32_t>(ctx.r19.u32, 1, ctx.xer);
	// mr r10,r16
	ctx.r10.u64 = ctx.r16.u64;
	// bgt cr6,0x825dde4c
	if (ctx.cr6.gt) goto loc_825DDE4C;
loc_825DDE48:
	// mr r10,r29
	ctx.r10.u64 = ctx.r29.u64;
loc_825DDE4C:
	// cmpwi cr6,r21,0
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 0, ctx.xer);
	// beq cr6,0x825dde64
	if (ctx.cr6.eq) goto loc_825DDE64;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mr r7,r16
	ctx.r7.u64 = ctx.r16.u64;
	// cmplw cr6,r19,r11
	ctx.cr6.compare<uint32_t>(ctx.r19.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dde68
	if (ctx.cr6.gt) goto loc_825DDE68;
loc_825DDE64:
	// mr r7,r29
	ctx.r7.u64 = ctx.r29.u64;
loc_825DDE68:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825dde8c
	if (ctx.cr6.eq) goto loc_825DDE8C;
	// cmpwi cr6,r21,0
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 0, ctx.xer);
	// beq cr6,0x825dde8c
	if (ctx.cr6.eq) goto loc_825DDE8C;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mr r8,r16
	ctx.r8.u64 = ctx.r16.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmplw cr6,r19,r11
	ctx.cr6.compare<uint32_t>(ctx.r19.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dde90
	if (ctx.cr6.gt) goto loc_825DDE90;
loc_825DDE8C:
	// mr r8,r29
	ctx.r8.u64 = ctx.r29.u64;
loc_825DDE90:
	// lwz r11,20056(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20056);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ddf10
	if (ctx.cr6.eq) goto loc_825DDF10;
	// lwz r11,112(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// rlwinm r11,r11,16,0,15
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 16) & 0xFFFF0000;
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// b 0x825ddf14
	goto loc_825DDF14;
loc_825DDEAC:
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// stw r25,284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 284, ctx.r25.u32);
	// li r9,0
	ctx.r9.s64 = 0;
	// stw r28,19976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19976, ctx.r28.u32);
	// li r8,0
	ctx.r8.s64 = 0;
	// stw r26,19980(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19980, ctx.r26.u32);
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// stw r27,19984(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19984, ctx.r27.u32);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r18
	ctx.r5.u64 = ctx.r18.u64;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82612480
	ctx.lr = 0x825DDEE0;
	sub_82612480(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825de170
	if (!ctx.cr6.eq) goto loc_825DE170;
	// cmpwi cr6,r17,0
	ctx.cr6.compare<int32_t>(ctx.r17.s32, 0, ctx.xer);
	// bne cr6,0x825ddef4
	if (!ctx.cr6.eq) goto loc_825DDEF4;
	// li r17,4
	ctx.r17.s64 = 4;
loc_825DDEF4:
	// cmpwi cr6,r18,0
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 0, ctx.xer);
	// beq cr6,0x825de0c8
	if (ctx.cr6.eq) goto loc_825DE0C8;
	// lwz r11,21272(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21272);
	// mr r18,r29
	ctx.r18.u64 = ctx.r29.u64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stw r11,21272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21272, ctx.r11.u32);
	// b 0x825de0b0
	goto loc_825DE0B0;
loc_825DDF10:
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_825DDF14:
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// mr r6,r23
	ctx.r6.u64 = ctx.r23.u64;
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// lwz r28,3068(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3068);
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
	// lwz r9,116(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r8,120(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// lwz r4,124(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r11.u32);
	// mtctr r28
	ctx.ctr.u64 = ctx.r28.u64;
	// bctrl 
	ctx.lr = 0x825DDF48;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,2968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2968);
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// rlwinm r11,r11,0,29,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x4;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ddf64
	if (ctx.cr6.eq) goto loc_825DDF64;
	// li r11,7
	ctx.r11.s64 = 7;
	// stw r11,2968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2968, ctx.r11.u32);
loc_825DDF64:
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// bne cr6,0x825ddfb8
	if (!ctx.cr6.eq) goto loc_825DDFB8;
	// lwz r10,124(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// addi r24,r24,16
	ctx.r24.s64 = ctx.r24.s64 + 16;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// addi r23,r23,8
	ctx.r23.s64 = ctx.r23.s64 + 8;
	// addi r22,r22,8
	ctx.r22.s64 = ctx.r22.s64 + 8;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// lwz r10,120(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// addi r10,r10,192
	ctx.r10.s64 = ctx.r10.s64 + 192;
	// stw r10,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r10.u32);
	// lwz r10,116(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r10,r10,144
	ctx.r10.s64 = ctx.r10.s64 + 144;
	// stw r10,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r10.u32);
	// blt cr6,0x825ddcfc
	if (ctx.cr6.lt) goto loc_825DDCFC;
	// b 0x825ddff8
	goto loc_825DDFF8;
loc_825DDFB0:
	// li r5,-1
	ctx.r5.s64 = -1;
	// b 0x825ddfbc
	goto loc_825DDFBC;
loc_825DDFB8:
	// li r5,-2
	ctx.r5.s64 = -2;
loc_825DDFBC:
	// addi r10,r1,144
	ctx.r10.s64 = ctx.r1.s64 + 144;
	// li r9,0
	ctx.r9.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r7,r1,112
	ctx.r7.s64 = ctx.r1.s64 + 112;
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82612480
	ctx.lr = 0x825DDFDC;
	sub_82612480(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825de17c
	if (!ctx.cr6.eq) goto loc_825DE17C;
	// cmpwi cr6,r17,0
	ctx.cr6.compare<int32_t>(ctx.r17.s32, 0, ctx.xer);
	// beq cr6,0x825ddff4
	if (ctx.cr6.eq) goto loc_825DDFF4;
	// cmpwi cr6,r17,4
	ctx.cr6.compare<int32_t>(ctx.r17.s32, 4, ctx.xer);
	// bne cr6,0x825ddff8
	if (!ctx.cr6.eq) goto loc_825DDFF8;
loc_825DDFF4:
	// mr r17,r28
	ctx.r17.u64 = ctx.r28.u64;
loc_825DDFF8:
	// lwz r11,2968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2968);
	// clrlwi r11,r11,31
	ctx.r11.u64 = ctx.r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825de028
	if (ctx.cr6.eq) goto loc_825DE028;
	// li r9,0
	ctx.r9.s64 = 0;
	// lwz r7,132(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// mr r8,r14
	ctx.r8.u64 = ctx.r14.u64;
	// lwz r6,136(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,128(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// bl 0x8263cde0
	ctx.lr = 0x825DE028;
	sub_8263CDE0(ctx, base);
loc_825DE028:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// mr r14,r29
	ctx.r14.u64 = ctx.r29.u64;
	// lwz r4,112(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmplw cr6,r4,r11
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x825de05c
	if (!ctx.cr6.lt) goto loc_825DE05C;
	// addi r11,r4,1
	ctx.r11.s64 = ctx.r4.s64 + 1;
	// lwz r10,21264(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21264);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r10
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825de05c
	if (ctx.cr6.eq) goto loc_825DE05C;
	// mr r20,r16
	ctx.r20.u64 = ctx.r16.u64;
loc_825DE05C:
	// lwz r10,228(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 228);
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// lwz r9,128(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// lwz r11,232(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 232);
	// add r5,r10,r9
	ctx.r5.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r10,136(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// add r6,r11,r10
	ctx.r6.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// add r7,r11,r10
	ctx.r7.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r5,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r5.u32);
	// stw r6,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r6.u32);
	// stw r7,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r7.u32);
	// beq cr6,0x825de0b0
	if (ctx.cr6.eq) goto loc_825DE0B0;
	// lwz r11,2968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2968);
	// clrlwi r11,r11,31
	ctx.r11.u64 = ctx.r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825de0b0
	if (ctx.cr6.eq) goto loc_825DE0B0;
	// li r9,1
	ctx.r9.s64 = 1;
	// li r8,0
	ctx.r8.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263cde0
	ctx.lr = 0x825DE0B0;
	sub_8263CDE0(ctx, base);
loc_825DE0B0:
	// lwz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 112);
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// addi r3,r10,1
	ctx.r3.s64 = ctx.r10.s64 + 1;
	// cmplw cr6,r3,r11
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r11.u32, ctx.xer);
	// stw r3,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r3.u32);
	// blt cr6,0x825dda28
	if (ctx.cr6.lt) goto loc_825DDA28;
loc_825DE0C8:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// lwz r11,3892(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3892);
	// blt cr6,0x825de188
	if (ctx.cr6.lt) goto loc_825DE188;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825de1d4
	if (ctx.cr6.eq) goto loc_825DE1D4;
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r11,19984(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19984);
	// li r8,0
	ctx.r8.s64 = 0;
	// srawi r5,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r10.s32 >> 1;
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r28,3720(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// li r7,0
	ctx.r7.s64 = 0;
	// srawi r26,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r26.s64 = ctx.r10.s32 >> 1;
	// lwz r4,220(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// mullw r5,r5,r11
	ctx.r5.s64 = int64_t(ctx.r5.s32) * int64_t(ctx.r11.s32);
	// lwz r6,224(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// lwz r25,0(r15)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	// lwz r24,140(r31)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// lwz r27,3728(r31)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// lwz r30,3724(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// stw r25,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r25.u32);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r24.u32);
	// stw r16,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r16.u32);
	// mullw r11,r26,r11
	ctx.r11.s64 = int64_t(ctx.r26.s32) * int64_t(ctx.r11.s32);
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + ctx.r28.u64;
	// add r28,r6,r5
	ctx.r28.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r4,r11,r4
	ctx.r4.u64 = ctx.r11.u64 + ctx.r4.u64;
	// add r11,r6,r5
	ctx.r11.u64 = ctx.r6.u64 + ctx.r5.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// add r6,r28,r27
	ctx.r6.u64 = ctx.r28.u64 + ctx.r27.u64;
	// add r5,r11,r30
	ctx.r5.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x82605120
	ctx.lr = 0x825DE154;
	sub_82605120(ctx, base);
	// mr r3,r17
	ctx.r3.u64 = ctx.r17.u64;
	// stw r29,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r29.u32);
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825DE164:
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825DE170:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825DE17C:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825DE188:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825de1d4
	if (ctx.cr6.eq) goto loc_825DE1D4;
	// lwz r28,140(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,3720(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// add r6,r11,r6
	ctx.r6.u64 = ctx.r11.u64 + ctx.r6.u64;
	// lwz r30,220(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// add r5,r11,r5
	ctx.r5.u64 = ctx.r11.u64 + ctx.r5.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + ctx.r30.u64;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r28.u32);
	// stw r16,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r16.u32);
	// bl 0x82600d48
	ctx.lr = 0x825DE1D4;
	sub_82600D48(ctx, base);
loc_825DE1D4:
	// mr r3,r17
	ctx.r3.u64 = ctx.r17.u64;
	// stw r29,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r29.u32);
loc_825DE1DC:
	// addi r1,r1,448
	ctx.r1.s64 = ctx.r1.s64 + 448;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825DE1E4"))) PPC_WEAK_FUNC(sub_825DE1E4);
PPC_FUNC_IMPL(__imp__sub_825DE1E4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DE1E8"))) PPC_WEAK_FUNC(sub_825DE1E8);
PPC_FUNC_IMPL(__imp__sub_825DE1E8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9e0
	ctx.lr = 0x825DE1F0;
	sub_8239B9E0(ctx, base);
	// stwu r1,-320(r1)
	ea = -320 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// lwz r30,3720(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// lwz r5,3732(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// add r26,r9,r30
	ctx.r26.u64 = ctx.r9.u64 + ctx.r30.u64;
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// add r9,r5,r9
	ctx.r9.u64 = ctx.r5.u64 + ctx.r9.u64;
	// lwz r6,3736(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// lwz r4,3728(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// lwz r7,3740(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// add r3,r3,r11
	ctx.r3.u64 = ctx.r3.u64 + ctx.r11.u64;
	// add r4,r11,r4
	ctx.r4.u64 = ctx.r11.u64 + ctx.r4.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// stw r9,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r9.u32);
	// add r9,r6,r11
	ctx.r9.u64 = ctx.r6.u64 + ctx.r11.u64;
	// add r11,r7,r11
	ctx.r11.u64 = ctx.r7.u64 + ctx.r11.u64;
	// lwz r8,15472(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// lwz r28,268(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// addi r15,r10,1
	ctx.r15.s64 = ctx.r10.s64 + 1;
	// lwz r27,264(r31)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// cmpwi cr6,r8,3
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 3, ctx.xer);
	// lwz r21,1892(r31)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1892);
	// lwz r20,1896(r31)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1896);
	// stw r26,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r26.u32);
	// stw r3,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r3.u32);
	// stw r4,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r4.u32);
	// stw r9,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r9.u32);
	// stw r10,340(r31)
	PPC_STORE_U32(ctx.r31.u32 + 340, ctx.r10.u32);
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r11.u32);
	// beq cr6,0x825de28c
	if (ctx.cr6.eq) goto loc_825DE28C;
	// cmpwi cr6,r8,4
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 4, ctx.xer);
	// bge cr6,0x825de28c
	if (!ctx.cr6.lt) goto loc_825DE28C;
	// lwz r11,2912(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2912);
	// lwz r10,2924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2924);
	// stw r11,2880(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2880, ctx.r11.u32);
	// stw r10,2892(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2892, ctx.r10.u32);
	// b 0x825de320
	goto loc_825DE320;
loc_825DE28C:
	// lwz r10,2928(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2928);
	// lwz r9,2088(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2088);
	// addi r8,r10,726
	ctx.r8.s64 = ctx.r10.s64 + 726;
	// lwz r11,2036(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2036);
	// addi r7,r10,729
	ctx.r7.s64 = ctx.r10.s64 + 729;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r9,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r9,r9,263
	ctx.r9.s64 = ctx.r9.s64 + 263;
	// addi r5,r11,503
	ctx.r5.s64 = ctx.r11.s64 + 503;
	// rlwinm r6,r9,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// addi r11,r11,253
	ctx.r11.s64 = ctx.r11.s64 + 253;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r4,r11,3,0,28
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwzx r11,r10,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r31.u32);
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// rlwinm r5,r5,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + ctx.r31.u64;
	// stw r11,2880(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2880, ctx.r11.u32);
	// lwzx r11,r10,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r31.u32);
	// stw r11,2884(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2884, ctx.r11.u32);
	// lwzx r11,r10,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r31.u32);
	// stw r11,2888(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2888, ctx.r11.u32);
	// lwzx r11,r7,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r31.u32);
	// stw r11,2900(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2900, ctx.r11.u32);
	// stw r11,2896(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2896, ctx.r11.u32);
	// stw r11,2892(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2892, ctx.r11.u32);
	// lwz r11,2100(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 2100);
	// stw r11,2092(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2092, ctx.r11.u32);
	// lwzx r11,r6,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r6.u32 + ctx.r31.u32);
	// stw r11,2096(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2096, ctx.r11.u32);
	// lwzx r11,r5,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + ctx.r31.u32);
	// stw r11,1984(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1984, ctx.r11.u32);
	// lwz r11,2020(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 2020);
	// stw r11,1976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1976, ctx.r11.u32);
	// lwzx r11,r4,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + ctx.r31.u32);
	// stw r11,1980(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1980, ctx.r11.u32);
loc_825DE320:
	// addi r11,r31,248
	ctx.r11.s64 = ctx.r31.s64 + 248;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// bl 0x825d56a0
	ctx.lr = 0x825DE330;
	sub_825D56A0(ctx, base);
	// lwz r10,312(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 312);
	// li r22,0
	ctx.r22.s64 = 0;
	// lwz r11,316(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 316);
	// mr r30,r22
	ctx.r30.u64 = ctx.r22.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// mr r18,r22
	ctx.r18.u64 = ctx.r22.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// neg r10,r11
	ctx.r10.s64 = -ctx.r11.s64;
	// stw r30,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r30.u32);
	// stw r11,304(r31)
	PPC_STORE_U32(ctx.r31.u32 + 304, ctx.r11.u32);
	// stw r10,308(r31)
	PPC_STORE_U32(ctx.r31.u32 + 308, ctx.r10.u32);
	// ble cr6,0x825dea38
	if (!ctx.cr6.gt) goto loc_825DEA38;
	// mr r17,r22
	ctx.r17.u64 = ctx.r22.u64;
loc_825DE368:
	// cntlzw r11,r30
	ctx.r11.u64 = ctx.r30.u32 == 0 ? 32 : __builtin_clz(ctx.r30.u32);
	// lwz r4,15468(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15468);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// xori r14,r11,1
	ctx.r14.u64 = ctx.r11.u64 ^ 1;
	// stw r14,140(r1)
	PPC_STORE_U32(ctx.r1.u32 + 140, ctx.r14.u32);
	// bl 0x82624478
	ctx.lr = 0x825DE384;
	sub_82624478(ctx, base);
	// cntlzw r9,r3
	ctx.r9.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// lwz r11,340(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 340);
	// clrlwi r10,r30,31
	ctx.r10.u64 = ctx.r30.u32 & 0x1;
	// rlwinm r9,r9,27,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
	// xori r16,r9,1
	ctx.r16.u64 = ctx.r9.u64 ^ 1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stw r11,340(r31)
	PPC_STORE_U32(ctx.r31.u32 + 340, ctx.r11.u32);
	// stw r16,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r16.u32);
	// bne cr6,0x825de3b8
	if (!ctx.cr6.eq) goto loc_825DE3B8;
	// lwz r21,1892(r31)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1892);
	// lwz r20,1896(r31)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1896);
	// lwz r27,264(r31)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
loc_825DE3B8:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825de3d4
	if (!ctx.cr6.eq) goto loc_825DE3D4;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825de3d4
	if (ctx.cr6.eq) goto loc_825DE3D4;
	// li r16,1
	ctx.r16.s64 = 1;
	// stw r16,136(r1)
	PPC_STORE_U32(ctx.r1.u32 + 136, ctx.r16.u32);
loc_825DE3D4:
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mr r23,r22
	ctx.r23.u64 = ctx.r22.u64;
	// mr r24,r22
	ctx.r24.u64 = ctx.r22.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x825de9d0
	if (!ctx.cr6.gt) goto loc_825DE9D0;
	// lwz r11,128(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// mr r19,r22
	ctx.r19.u64 = ctx.r22.u64;
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// addi r30,r28,13
	ctx.r30.s64 = ctx.r28.s64 + 13;
	// lwz r29,116(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	// subf r11,r29,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r29.s64;
	// stw r10,156(r1)
	PPC_STORE_U32(ctx.r1.u32 + 156, ctx.r10.u32);
	// stw r11,152(r1)
	PPC_STORE_U32(ctx.r1.u32 + 152, ctx.r11.u32);
	// lwz r11,120(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// subf r25,r29,r11
	ctx.r25.s64 = ctx.r11.s64 - ctx.r29.s64;
loc_825DE414:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825de448
	if (!ctx.cr6.eq) goto loc_825DE448;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e41d8
	ctx.lr = 0x825DE428;
	sub_825E41D8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825de448
	if (ctx.cr6.eq) goto loc_825DE448;
	// addi r4,r31,248
	ctx.r4.s64 = ctx.r31.s64 + 248;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e4268
	ctx.lr = 0x825DE43C;
	sub_825E4268(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825dea90
	if (!ctx.cr6.eq) goto loc_825DEA90;
	// mr r15,r22
	ctx.r15.u64 = ctx.r22.u64;
loc_825DE448:
	// addi r15,r15,1
	ctx.r15.s64 = ctx.r15.s64 + 1;
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r16
	ctx.r5.u64 = ctx.r16.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r15,160(r1)
	PPC_STORE_U32(ctx.r1.u32 + 160, ctx.r15.u32);
	// bl 0x826243d0
	ctx.lr = 0x825DE464;
	sub_826243D0(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lwz r10,3076(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3076);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,324(r31)
	PPC_STORE_U32(ctx.r31.u32 + 324, ctx.r11.u32);
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
	// bctrl 
	ctx.lr = 0x825DE480;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825dea90
	if (!ctx.cr6.eq) goto loc_825DEA90;
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// rlwinm r10,r11,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825de5f8
	if (!ctx.cr6.eq) goto loc_825DE5F8;
	// rlwinm r11,r11,0,14,14
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000;
	// lis r10,2
	ctx.r10.s64 = 131072;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bne cr6,0x825de6b0
	if (!ctx.cr6.eq) goto loc_825DE6B0;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bge cr6,0x825de564
	if (!ctx.cr6.lt) goto loc_825DE564;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// beq cr6,0x825de564
	if (ctx.cr6.eq) goto loc_825DE564;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825de530
	if (ctx.cr6.eq) goto loc_825DE530;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x825de530
	if (ctx.cr6.eq) goto loc_825DE530;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825de5ac
	if (!ctx.cr6.eq) goto loc_825DE5AC;
	// cmpwi cr6,r16,0
	ctx.cr6.compare<int32_t>(ctx.r16.s32, 0, ctx.xer);
	// beq cr6,0x825de4f0
	if (ctx.cr6.eq) goto loc_825DE4F0;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mr r8,r22
	ctx.r8.u64 = ctx.r22.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmplw cr6,r15,r11
	ctx.cr6.compare<uint32_t>(ctx.r15.u32, ctx.r11.u32, ctx.xer);
	// bge cr6,0x825de4f4
	if (!ctx.cr6.lt) goto loc_825DE4F4;
loc_825DE4F0:
	// li r8,1
	ctx.r8.s64 = 1;
loc_825DE4F4:
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x825de508
	if (ctx.cr6.eq) goto loc_825DE508;
	// cmplwi cr6,r15,1
	ctx.cr6.compare<uint32_t>(ctx.r15.u32, 1, ctx.xer);
	// mr r6,r22
	ctx.r6.u64 = ctx.r22.u64;
	// bne cr6,0x825de50c
	if (!ctx.cr6.eq) goto loc_825DE50C;
loc_825DE508:
	// li r6,1
	ctx.r6.s64 = 1;
loc_825DE50C:
	// lwz r11,148(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// subf r11,r24,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r24.s64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// bl 0x825e7bc8
	ctx.lr = 0x825DE52C;
	sub_825E7BC8(ctx, base);
	// b 0x825de5a4
	goto loc_825DE5A4;
loc_825DE530:
	// lwz r11,148(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// cntlzw r10,r16
	ctx.r10.u64 = ctx.r16.u32 == 0 ? 32 : __builtin_clz(ctx.r16.u32);
	// cntlzw r9,r24
	ctx.r9.u64 = ctx.r24.u32 == 0 ? 32 : __builtin_clz(ctx.r24.u32);
	// subf r11,r24,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r24.s64;
	// rlwinm r8,r10,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r6,r9,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e7bc8
	ctx.lr = 0x825DE560;
	sub_825E7BC8(ctx, base);
	// b 0x825de5a4
	goto loc_825DE5A4;
loc_825DE564:
	// lwz r11,148(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 148);
	// cntlzw r7,r24
	ctx.r7.u64 = ctx.r24.u32 == 0 ? 32 : __builtin_clz(ctx.r24.u32);
	// lwz r5,1980(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1980);
	// cntlzw r9,r16
	ctx.r9.u64 = ctx.r16.u32 == 0 ? 32 : __builtin_clz(ctx.r16.u32);
	// subf r11,r24,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r24.s64;
	// lwz r10,1976(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1976);
	// rlwinm r6,r7,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r8,r9,27,31,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 27) & 0x1;
	// lwz r9,1984(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1984);
	// stw r5,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r5.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82623f68
	ctx.lr = 0x825DE5A4;
	sub_82623F68(ctx, base);
loc_825DE5A4:
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825dea90
	if (!ctx.cr6.eq) goto loc_825DEA90;
loc_825DE5AC:
	// lbz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r27.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825de5c8
	if (!ctx.cr6.eq) goto loc_825DE5C8;
	// lbz r11,1(r27)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r27.u32 + 1);
	// li r10,1
	ctx.r10.s64 = 1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825de5cc
	if (ctx.cr6.eq) goto loc_825DE5CC;
loc_825DE5C8:
	// mr r10,r22
	ctx.r10.u64 = ctx.r22.u64;
loc_825DE5CC:
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// rlwimi r11,r10,29,2,2
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 29) & 0x20000000) | (ctx.r11.u64 & 0xFFFFFFFFDFFFFFFF);
	// rlwinm r10,r11,0,1,1
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x40000000;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stw r11,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r11.u32);
	// beq cr6,0x825de63c
	if (ctx.cr6.eq) goto loc_825DE63C;
	// rlwinm r10,r11,0,2,2
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000000;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825de63c
	if (ctx.cr6.eq) goto loc_825DE63C;
	// oris r11,r11,32768
	ctx.r11.u64 = ctx.r11.u64 | 2147483648;
	// stw r11,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r11.u32);
loc_825DE5F8:
	// lwz r11,152(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 152);
	// add r5,r25,r29
	ctx.r5.u64 = ctx.r25.u64 + ctx.r29.u64;
	// lwz r8,156(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 156);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// add r7,r11,r29
	ctx.r7.u64 = ctx.r11.u64 + ctx.r29.u64;
	// lwz r11,124(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// stb r22,1(r27)
	PPC_STORE_U8(ctx.r27.u32 + 1, ctx.r22.u8);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// stb r22,0(r27)
	PPC_STORE_U8(ctx.r27.u32 + 0, ctx.r22.u8);
	// add r6,r23,r11
	ctx.r6.u64 = ctx.r23.u64 + ctx.r11.u64;
	// lwz r10,208(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// lwz r9,204(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r11,3120(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3120);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x825DE638;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// b 0x825de974
	goto loc_825DE974;
loc_825DE63C:
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// lbz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r27.u32 + 0);
	// lbz r10,1(r27)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r27.u32 + 1);
	// add r7,r25,r29
	ctx.r7.u64 = ctx.r25.u64 + ctx.r29.u64;
	// extsb r11,r11
	ctx.r11.s64 = ctx.r11.s8;
	// lwz r14,3060(r31)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3060);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// lwz r9,4432(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4432);
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lbzx r8,r11,r9
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r9.u32);
	// stb r8,112(r1)
	PPC_STORE_U8(ctx.r1.u32 + 112, ctx.r8.u8);
	// add r8,r11,r19
	ctx.r8.u64 = ctx.r11.u64 + ctx.r19.u64;
	// lbzx r11,r10,r9
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r9.u32);
	// add r9,r10,r17
	ctx.r9.u64 = ctx.r10.u64 + ctx.r17.u64;
	// extsb r11,r11
	ctx.r11.s64 = ctx.r11.s8;
	// add r11,r11,r18
	ctx.r11.u64 = ctx.r11.u64 + ctx.r18.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// lbz r10,112(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 112);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r10,r10,r23
	ctx.r10.u64 = ctx.r10.u64 + ctx.r23.u64;
	// mtctr r14
	ctx.ctr.u64 = ctx.r14.u64;
	// bctrl 
	ctx.lr = 0x825DE6A0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825dea90
	if (!ctx.cr6.eq) goto loc_825DEA90;
	// lwz r14,140(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// b 0x825de974
	goto loc_825DE974;
loc_825DE6B0:
	// stb r22,1(r27)
	PPC_STORE_U8(ctx.r27.u32 + 1, ctx.r22.u8);
	// stb r22,0(r27)
	PPC_STORE_U8(ctx.r27.u32 + 0, ctx.r22.u8);
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// blt cr6,0x825de858
	if (ctx.cr6.lt) goto loc_825DE858;
	// cntlzw r11,r24
	ctx.r11.u64 = ctx.r24.u32 == 0 ? 32 : __builtin_clz(ctx.r24.u32);
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// xori r10,r11,1
	ctx.r10.u64 = ctx.r11.u64 ^ 1;
	// beq cr6,0x825de6e4
	if (ctx.cr6.eq) goto loc_825DE6E4;
	// cmpwi cr6,r14,0
	ctx.cr6.compare<int32_t>(ctx.r14.s32, 0, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x825de6e8
	if (!ctx.cr6.eq) goto loc_825DE6E8;
loc_825DE6E4:
	// mr r11,r22
	ctx.r11.u64 = ctx.r22.u64;
loc_825DE6E8:
	// lwz r9,400(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 400);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x825de764
	if (ctx.cr6.eq) goto loc_825DE764;
	// lbz r16,4(r30)
	ctx.r16.u64 = PPC_LOAD_U8(ctx.r30.u32 + 4);
	// mr r9,r20
	ctx.r9.u64 = ctx.r20.u64;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// mr r8,r21
	ctx.r8.u64 = ctx.r21.u64;
	// lbz r11,-1(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + -1);
	// add r7,r25,r29
	ctx.r7.u64 = ctx.r25.u64 + ctx.r29.u64;
	// stw r14,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r14.u32);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// lbz r15,3(r30)
	ctx.r15.u64 = PPC_LOAD_U8(ctx.r30.u32 + 3);
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// stb r16,-1(r30)
	PPC_STORE_U8(ctx.r30.u32 + -1, ctx.r16.u8);
	// mr r16,r11
	ctx.r16.u64 = ctx.r11.u64;
	// lbz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 0);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lbz r14,2(r30)
	ctx.r14.u64 = PPC_LOAD_U8(ctx.r30.u32 + 2);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stb r15,0(r30)
	PPC_STORE_U8(ctx.r30.u32 + 0, ctx.r15.u8);
	// stb r16,4(r30)
	PPC_STORE_U8(ctx.r30.u32 + 4, ctx.r16.u8);
	// mr r16,r11
	ctx.r16.u64 = ctx.r11.u64;
	// lbz r11,1(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 1);
	// stb r14,1(r30)
	PPC_STORE_U8(ctx.r30.u32 + 1, ctx.r14.u8);
	// stb r16,3(r30)
	PPC_STORE_U8(ctx.r30.u32 + 3, ctx.r16.u8);
	// stb r11,2(r30)
	PPC_STORE_U8(ctx.r30.u32 + 2, ctx.r11.u8);
	// bl 0x82636820
	ctx.lr = 0x825DE754;
	sub_82636820(ctx, base);
	// lwz r16,136(r1)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r1.u32 + 136);
	// lwz r14,140(r1)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r1.u32 + 140);
	// lwz r15,160(r1)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r1.u32 + 160);
	// b 0x825de96c
	goto loc_825DE96C;
loc_825DE764:
	// lbz r9,4(r30)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r30.u32 + 4);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lbz r11,-1(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + -1);
	// lbz r8,3(r30)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r30.u32 + 3);
	// lbz r7,2(r30)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r30.u32 + 2);
	// stb r9,-1(r30)
	PPC_STORE_U8(ctx.r30.u32 + -1, ctx.r9.u8);
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// lbz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 0);
	// stb r8,0(r30)
	PPC_STORE_U8(ctx.r30.u32 + 0, ctx.r8.u8);
	// stb r9,4(r30)
	PPC_STORE_U8(ctx.r30.u32 + 4, ctx.r9.u8);
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// lbz r11,1(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 1);
	// stb r7,1(r30)
	PPC_STORE_U8(ctx.r30.u32 + 1, ctx.r7.u8);
	// stb r9,3(r30)
	PPC_STORE_U8(ctx.r30.u32 + 3, ctx.r9.u8);
	// stb r11,2(r30)
	PPC_STORE_U8(ctx.r30.u32 + 2, ctx.r11.u8);
	// beq cr6,0x825de7d0
	if (ctx.cr6.eq) goto loc_825DE7D0;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// li r8,1
	ctx.r8.s64 = 1;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r28
	ctx.r11.s64 = ctx.r28.s64 - ctx.r11.s64;
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r11,r11,0,14,14
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825de7d4
	if (ctx.cr6.eq) goto loc_825DE7D4;
loc_825DE7D0:
	// mr r8,r22
	ctx.r8.u64 = ctx.r22.u64;
loc_825DE7D4:
	// cmpwi cr6,r14,0
	ctx.cr6.compare<int32_t>(ctx.r14.s32, 0, ctx.xer);
	// beq cr6,0x825de804
	if (ctx.cr6.eq) goto loc_825DE804;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r28
	ctx.r11.s64 = ctx.r28.s64 - ctx.r11.s64;
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r11,r11,0,14,14
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// beq cr6,0x825de808
	if (ctx.cr6.eq) goto loc_825DE808;
loc_825DE804:
	// mr r11,r22
	ctx.r11.u64 = ctx.r22.u64;
loc_825DE808:
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825de824
	if (ctx.cr6.eq) goto loc_825DE824;
	// lwz r10,-33(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + -33);
	// rlwinm r10,r10,0,14,14
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// beq cr6,0x825de828
	if (ctx.cr6.eq) goto loc_825DE828;
loc_825DE824:
	// mr r10,r22
	ctx.r10.u64 = ctx.r22.u64;
loc_825DE828:
	// stw r8,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r8.u32);
	// mr r9,r20
	ctx.r9.u64 = ctx.r20.u64;
	// mr r8,r21
	ctx.r8.u64 = ctx.r21.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// add r7,r25,r29
	ctx.r7.u64 = ctx.r25.u64 + ctx.r29.u64;
	// stw r22,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r22.u32);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82635550
	ctx.lr = 0x825DE854;
	sub_82635550(ctx, base);
	// b 0x825de96c
	goto loc_825DE96C;
loc_825DE858:
	// lbz r10,4(r30)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r30.u32 + 4);
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// lbz r11,-1(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + -1);
	// lbz r9,3(r30)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r30.u32 + 3);
	// lbz r8,2(r30)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r30.u32 + 2);
	// stb r10,-1(r30)
	PPC_STORE_U8(ctx.r30.u32 + -1, ctx.r10.u8);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// lbz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 0);
	// stb r9,0(r30)
	PPC_STORE_U8(ctx.r30.u32 + 0, ctx.r9.u8);
	// stb r10,4(r30)
	PPC_STORE_U8(ctx.r30.u32 + 4, ctx.r10.u8);
	// mr r10,r11
	ctx.r10.u64 = ctx.r11.u64;
	// lbz r11,1(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 1);
	// stb r8,1(r30)
	PPC_STORE_U8(ctx.r30.u32 + 1, ctx.r8.u8);
	// stb r10,3(r30)
	PPC_STORE_U8(ctx.r30.u32 + 3, ctx.r10.u8);
	// stb r11,2(r30)
	PPC_STORE_U8(ctx.r30.u32 + 2, ctx.r11.u8);
	// beq cr6,0x825de8b4
	if (ctx.cr6.eq) goto loc_825DE8B4;
	// lwz r11,-33(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + -33);
	// rlwinm r11,r11,0,14,14
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825de8b4
	if (!ctx.cr6.eq) goto loc_825DE8B4;
	// cmplwi cr6,r15,1
	ctx.cr6.compare<uint32_t>(ctx.r15.u32, 1, ctx.xer);
	// li r10,1
	ctx.r10.s64 = 1;
	// bgt cr6,0x825de8b8
	if (ctx.cr6.gt) goto loc_825DE8B8;
loc_825DE8B4:
	// mr r10,r22
	ctx.r10.u64 = ctx.r22.u64;
loc_825DE8B8:
	// cmpwi cr6,r16,0
	ctx.cr6.compare<int32_t>(ctx.r16.s32, 0, ctx.xer);
	// beq cr6,0x825de8f0
	if (ctx.cr6.eq) goto loc_825DE8F0;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r9,r11,r9
	ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r9,r28
	ctx.r9.s64 = ctx.r28.s64 - ctx.r9.s64;
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r9,r9,0,14,14
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825de8f0
	if (!ctx.cr6.eq) goto loc_825DE8F0;
	// cmplw cr6,r15,r11
	ctx.cr6.compare<uint32_t>(ctx.r15.u32, ctx.r11.u32, ctx.xer);
	// li r7,1
	ctx.r7.s64 = 1;
	// bgt cr6,0x825de8f4
	if (ctx.cr6.gt) goto loc_825DE8F4;
loc_825DE8F0:
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
loc_825DE8F4:
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x825de93c
	if (ctx.cr6.eq) goto loc_825DE93C;
	// cmpwi cr6,r16,0
	ctx.cr6.compare<int32_t>(ctx.r16.s32, 0, ctx.xer);
	// beq cr6,0x825de93c
	if (ctx.cr6.eq) goto loc_825DE93C;
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// addi r11,r9,1
	ctx.r11.s64 = ctx.r9.s64 + 1;
	// rlwinm r8,r11,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r11,r11,r28
	ctx.r11.s64 = ctx.r28.s64 - ctx.r11.s64;
	// lwz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r11,r11,0,14,14
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825de93c
	if (!ctx.cr6.eq) goto loc_825DE93C;
	// addi r11,r9,1
	ctx.r11.s64 = ctx.r9.s64 + 1;
	// cmplw cr6,r15,r11
	ctx.cr6.compare<uint32_t>(ctx.r15.u32, ctx.r11.u32, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x825de940
	if (ctx.cr6.gt) goto loc_825DE940;
loc_825DE93C:
	// mr r11,r22
	ctx.r11.u64 = ctx.r22.u64;
loc_825DE940:
	// stw r7,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r7.u32);
	// mr r9,r20
	ctx.r9.u64 = ctx.r20.u64;
	// mr r8,r21
	ctx.r8.u64 = ctx.r21.u64;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// add r7,r25,r29
	ctx.r7.u64 = ctx.r25.u64 + ctx.r29.u64;
	// stw r22,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r22.u32);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e76d8
	ctx.lr = 0x825DE96C;
	sub_825E76D8(ctx, base);
loc_825DE96C:
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825dea90
	if (!ctx.cr6.eq) goto loc_825DEA90;
loc_825DE974:
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// lis r10,2
	ctx.r10.s64 = 131072;
	// rlwinm r11,r11,0,14,14
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x20000;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bne cr6,0x825de994
	if (!ctx.cr6.eq) goto loc_825DE994;
	// sth r22,160(r21)
	PPC_STORE_U16(ctx.r21.u32 + 160, ctx.r22.u16);
	// sth r22,128(r21)
	PPC_STORE_U16(ctx.r21.u32 + 128, ctx.r22.u16);
	// sth r22,0(r21)
	PPC_STORE_U16(ctx.r21.u32 + 0, ctx.r22.u16);
loc_825DE994:
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// addi r24,r24,1
	ctx.r24.s64 = ctx.r24.s64 + 1;
	// addi r28,r28,20
	ctx.r28.s64 = ctx.r28.s64 + 20;
	// addi r30,r30,20
	ctx.r30.s64 = ctx.r30.s64 + 20;
	// addi r27,r27,2
	ctx.r27.s64 = ctx.r27.s64 + 2;
	// addi r26,r26,16
	ctx.r26.s64 = ctx.r26.s64 + 16;
	// addi r29,r29,8
	ctx.r29.s64 = ctx.r29.s64 + 8;
	// addi r21,r21,192
	ctx.r21.s64 = ctx.r21.s64 + 192;
	// addi r20,r20,144
	ctx.r20.s64 = ctx.r20.s64 + 144;
	// addi r23,r23,16
	ctx.r23.s64 = ctx.r23.s64 + 16;
	// addi r19,r19,32
	ctx.r19.s64 = ctx.r19.s64 + 32;
	// cmplw cr6,r24,r11
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x825de414
	if (ctx.cr6.lt) goto loc_825DE414;
	// lwz r30,144(r1)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 144);
	// lwz r26,148(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 148);
loc_825DE9D0:
	// lwz r11,232(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 232);
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// lwz r8,116(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 116);
	// addi r18,r18,16
	ctx.r18.s64 = ctx.r18.s64 + 16;
	// lwz r10,228(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 228);
	// addi r17,r17,32
	ctx.r17.s64 = ctx.r17.s64 + 32;
	// add r8,r11,r8
	ctx.r8.u64 = ctx.r11.u64 + ctx.r8.u64;
	// lwz r9,140(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// add r26,r10,r26
	ctx.r26.u64 = ctx.r10.u64 + ctx.r26.u64;
	// stw r30,144(r1)
	PPC_STORE_U32(ctx.r1.u32 + 144, ctx.r30.u32);
	// cmplw cr6,r30,r9
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r9.u32, ctx.xer);
	// stw r8,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r8.u32);
	// lwz r8,120(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 120);
	// stw r26,148(r1)
	PPC_STORE_U32(ctx.r1.u32 + 148, ctx.r26.u32);
	// add r8,r11,r8
	ctx.r8.u64 = ctx.r11.u64 + ctx.r8.u64;
	// stw r8,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r8.u32);
	// lwz r8,124(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 124);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r10,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r10.u32);
	// lwz r10,128(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// lwz r10,132(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 132);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r11,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r11.u32);
	// blt cr6,0x825de368
	if (ctx.cr6.lt) goto loc_825DE368;
loc_825DEA38:
	// lwz r11,3892(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3892);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dea8c
	if (ctx.cr6.eq) goto loc_825DEA8C;
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r29,140(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r6,3728(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// li r8,0
	ctx.r8.s64 = 0;
	// lwz r5,3724(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// li r7,0
	ctx.r7.s64 = 0;
	// lwz r4,220(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// add r6,r11,r6
	ctx.r6.u64 = ctx.r11.u64 + ctx.r6.u64;
	// lwz r30,3720(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// add r5,r5,r11
	ctx.r5.u64 = ctx.r5.u64 + ctx.r11.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// li r31,1
	ctx.r31.s64 = 1;
	// add r4,r4,r30
	ctx.r4.u64 = ctx.r4.u64 + ctx.r30.u64;
	// stw r29,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r29.u32);
	// stw r31,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r31.u32);
	// bl 0x82600d48
	ctx.lr = 0x825DEA8C;
	sub_82600D48(ctx, base);
loc_825DEA8C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825DEA90:
	// addi r1,r1,320
	ctx.r1.s64 = ctx.r1.s64 + 320;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825DEA98"))) PPC_WEAK_FUNC(sub_825DEA98);
PPC_FUNC_IMPL(__imp__sub_825DEA98) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,3664(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3664);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825deb5c
	if (!ctx.cr6.eq) goto loc_825DEB5C;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// blt cr6,0x825deaf4
	if (ctx.cr6.lt) goto loc_825DEAF4;
	// bl 0x825dd3b0
	ctx.lr = 0x825DEAC8;
	sub_825DD3B0(ctx, base);
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// bne cr6,0x825deb30
	if (!ctx.cr6.eq) goto loc_825DEB30;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// bne cr6,0x825deb18
	if (!ctx.cr6.eq) goto loc_825DEB18;
loc_825DEADC:
	// li r3,7
	ctx.r3.s64 = 7;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825DEAF4:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825deb28
	if (ctx.cr6.eq) goto loc_825DEB28;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bne cr6,0x825deb0c
	if (!ctx.cr6.eq) goto loc_825DEB0C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825dd3b0
	ctx.lr = 0x825DEB0C;
	sub_825DD3B0(ctx, base);
loc_825DEB0C:
	// cmpwi cr6,r4,22
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 22, ctx.xer);
	// ble cr6,0x825deadc
	if (!ctx.cr6.gt) goto loc_825DEADC;
	// addi r4,r4,-22
	ctx.r4.s64 = ctx.r4.s64 + -22;
loc_825DEB18:
	// lwz r3,140(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// bl 0x82624490
	ctx.lr = 0x825DEB20;
	sub_82624490(ctx, base);
	// stw r3,15468(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15468, ctx.r3.u32);
	// b 0x825deb30
	goto loc_825DEB30;
loc_825DEB28:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// stw r11,15468(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15468, ctx.r11.u32);
loc_825DEB30:
	// lwz r11,15508(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15508);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825deb4c
	if (!ctx.cr6.eq) goto loc_825DEB4C;
	// lwz r11,152(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// bne cr6,0x825deb50
	if (!ctx.cr6.eq) goto loc_825DEB50;
loc_825DEB4C:
	// li r11,0
	ctx.r11.s64 = 0;
loc_825DEB50:
	// stw r11,15476(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15476, ctx.r11.u32);
	// li r11,1
	ctx.r11.s64 = 1;
	// stw r11,3664(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3664, ctx.r11.u32);
loc_825DEB5C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DEB74"))) PPC_WEAK_FUNC(sub_825DEB74);
PPC_FUNC_IMPL(__imp__sub_825DEB74) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DEB78"))) PPC_WEAK_FUNC(sub_825DEB78);
PPC_FUNC_IMPL(__imp__sub_825DEB78) {
	PPC_FUNC_PROLOGUE();
	// lwz r10,3696(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3696);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// lwz r11,3688(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3688);
	// lwz r8,224(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// lwz r9,220(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// stw r10,3688(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3688, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r11,3696(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3696, ctx.r11.u32);
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r7,3720(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3720, ctx.r7.u32);
	// rotlwi r7,r7,0
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r7.u32, 0);
	// lwz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// stw r6,3724(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3724, ctx.r6.u32);
	// lwz r6,8(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// lwz r10,3724(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3724);
	// add r5,r10,r8
	ctx.r5.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r6,3728(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3728, ctx.r6.u32);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r10,3728(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3728);
	// add r8,r8,r10
	ctx.r8.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rotlwi r10,r6,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r6.u32, 0);
	// stw r6,3732(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3732, ctx.r6.u32);
	// lwz r6,4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r6,3736(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3736, ctx.r6.u32);
	// rotlwi r6,r6,0
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r6.u32, 0);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r7,3800(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3800, ctx.r7.u32);
	// stw r5,3804(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3804, ctx.r5.u32);
	// stw r8,3808(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3808, ctx.r8.u32);
	// stw r6,14764(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14764, ctx.r6.u32);
	// stw r11,3740(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3740, ctx.r11.u32);
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// stw r10,14760(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14760, ctx.r10.u32);
	// stw r11,14768(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14768, ctx.r11.u32);
	// add r11,r9,r10
	ctx.r11.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r11,3756(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3756, ctx.r11.u32);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// lwz r10,3708(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3708);
	// lwz r11,3704(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3704);
	// stw r10,3704(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3704, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r11,3708(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3708, ctx.r11.u32);
	// lwz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r9,3776(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3776, ctx.r9.u32);
	// lwz r9,4(r10)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r9,3780(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3780, ctx.r9.u32);
	// lwz r10,8(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r10,3784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3784, ctx.r10.u32);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r10,3788(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3788, ctx.r10.u32);
	// lwz r10,4(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r10,3792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3792, ctx.r10.u32);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r11,3796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3796, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825DEC58"))) PPC_WEAK_FUNC(sub_825DEC58);
PPC_FUNC_IMPL(__imp__sub_825DEC58) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825DEC60;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// li r26,0
	ctx.r26.s64 = 0;
	// li r25,1
	ctx.r25.s64 = 1;
	// mr r30,r26
	ctx.r30.u64 = ctx.r26.u64;
	// li r24,2
	ctx.r24.s64 = 2;
	// lwz r11,284(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 284);
	// stw r26,3964(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3964, ctx.r26.u32);
	// stw r26,20028(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20028, ctx.r26.u32);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// stw r26,20024(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20024, ctx.r26.u32);
	// bne cr6,0x825df018
	if (!ctx.cr6.eq) goto loc_825DF018;
	// lwz r11,19980(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 19980);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825df018
	if (!ctx.cr6.eq) goto loc_825DF018;
	// lwz r11,3924(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825defd4
	if (ctx.cr6.eq) goto loc_825DEFD4;
	// stw r25,3960(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3960, ctx.r25.u32);
loc_825DECAC:
	// lwz r11,3964(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3964);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825df6e8
	if (ctx.cr6.eq) goto loc_825DF6E8;
	// lwz r11,19980(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 19980);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825df580
	if (ctx.cr6.eq) goto loc_825DF580;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825decf0
	if (!ctx.cr0.lt) goto loc_825DECF0;
	// bl 0x825d5398
	ctx.lr = 0x825DECF0;
	sub_825D5398(ctx, base);
loc_825DECF0:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825df278
	if (ctx.cr6.eq) goto loc_825DF278;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,6
	ctx.r30.s64 = 6;
	// stw r25,20028(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20028, ctx.r25.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// stw r25,20024(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20024, ctx.r25.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825ded74
	if (!ctx.cr6.lt) goto loc_825DED74;
loc_825DED1C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ded74
	if (ctx.cr6.eq) goto loc_825DED74;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825ded64
	if (!ctx.cr0.lt) goto loc_825DED64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DED64;
	sub_825D5398(ctx, base);
loc_825DED64:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825ded1c
	if (ctx.cr6.gt) goto loc_825DED1C;
loc_825DED74:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dedb0
	if (!ctx.cr0.lt) goto loc_825DEDB0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DEDB0;
	sub_825D5398(ctx, base);
loc_825DEDB0:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,6
	ctx.r30.s64 = 6;
	// stw r28,20032(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20032, ctx.r28.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825dee28
	if (!ctx.cr6.lt) goto loc_825DEE28;
loc_825DEDD0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dee28
	if (ctx.cr6.eq) goto loc_825DEE28;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dee18
	if (!ctx.cr0.lt) goto loc_825DEE18;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DEE18;
	sub_825D5398(ctx, base);
loc_825DEE18:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dedd0
	if (ctx.cr6.gt) goto loc_825DEDD0;
loc_825DEE28:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dee64
	if (!ctx.cr0.lt) goto loc_825DEE64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DEE64;
	sub_825D5398(ctx, base);
loc_825DEE64:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,6
	ctx.r30.s64 = 6;
	// stw r28,20036(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20036, ctx.r28.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825deedc
	if (!ctx.cr6.lt) goto loc_825DEEDC;
loc_825DEE84:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825deedc
	if (ctx.cr6.eq) goto loc_825DEEDC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825deecc
	if (!ctx.cr0.lt) goto loc_825DEECC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DEECC;
	sub_825D5398(ctx, base);
loc_825DEECC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dee84
	if (ctx.cr6.gt) goto loc_825DEE84;
loc_825DEEDC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825def18
	if (!ctx.cr0.lt) goto loc_825DEF18;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DEF18;
	sub_825D5398(ctx, base);
loc_825DEF18:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,6
	ctx.r30.s64 = 6;
	// stw r28,20040(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20040, ctx.r28.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825def90
	if (!ctx.cr6.lt) goto loc_825DEF90;
loc_825DEF38:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825def90
	if (ctx.cr6.eq) goto loc_825DEF90;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825def80
	if (!ctx.cr0.lt) goto loc_825DEF80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DEF80;
	sub_825D5398(ctx, base);
loc_825DEF80:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825def38
	if (ctx.cr6.gt) goto loc_825DEF38;
loc_825DEF90:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825df414
	if (!ctx.cr0.lt) goto loc_825DF414;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DEFCC;
	sub_825D5398(ctx, base);
	// stw r30,20044(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20044, ctx.r30.u32);
	// b 0x825df6e8
	goto loc_825DF6E8;
loc_825DEFD4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825df000
	if (!ctx.cr0.lt) goto loc_825DF000;
	// bl 0x825d5398
	ctx.lr = 0x825DF000;
	sub_825D5398(ctx, base);
loc_825DF000:
	// subfic r11,r31,0
	ctx.xer.ca = ctx.r31.u32 <= 0;
	ctx.r11.s64 = 0 - ctx.r31.s64;
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// rlwinm r11,r11,0,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFE;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// stw r11,3960(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3960, ctx.r11.u32);
	// b 0x825decac
	goto loc_825DECAC;
loc_825DF018:
	// lwz r11,3924(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825df06c
	if (ctx.cr6.eq) goto loc_825DF06C;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// stw r25,3960(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3960, ctx.r25.u32);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// addic. r10,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r10.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r31,r11,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r10,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r10.u32);
	// bge 0x825df054
	if (!ctx.cr0.lt) goto loc_825DF054;
	// bl 0x825d5398
	ctx.lr = 0x825DF054;
	sub_825D5398(ctx, base);
loc_825DF054:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825df244
	if (ctx.cr6.eq) goto loc_825DF244;
	// stw r25,3964(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3964, ctx.r25.u32);
	// b 0x825df244
	goto loc_825DF244;
loc_825DF064:
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// stw r25,3964(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3964, ctx.r25.u32);
loc_825DF06C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,248(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 248);
	// cmpwi cr6,r11,12
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 12, ctx.xer);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// ble cr6,0x825df170
	if (!ctx.cr6.gt) goto loc_825DF170;
	// bge 0x825df0a4
	if (!ctx.cr0.lt) goto loc_825DF0A4;
	// bl 0x825d5398
	ctx.lr = 0x825DF0A4;
	sub_825D5398(ctx, base);
loc_825DF0A4:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825df23c
	if (!ctx.cr6.eq) goto loc_825DF23C;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825df0d8
	if (!ctx.cr0.lt) goto loc_825DF0D8;
	// bl 0x825d5398
	ctx.lr = 0x825DF0D8;
	sub_825D5398(ctx, base);
loc_825DF0D8:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825df268
	if (!ctx.cr6.eq) goto loc_825DF268;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825df10c
	if (!ctx.cr0.lt) goto loc_825DF10C;
	// bl 0x825d5398
	ctx.lr = 0x825DF10C;
	sub_825D5398(ctx, base);
loc_825DF10C:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825df270
	if (!ctx.cr6.eq) goto loc_825DF270;
	// lwz r11,284(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825df12c
	if (!ctx.cr6.eq) goto loc_825DF12C;
	// lwz r11,19980(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 19980);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825df168
	if (ctx.cr6.eq) goto loc_825DF168;
loc_825DF12C:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825df168
	if (!ctx.cr6.eq) goto loc_825DF168;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825df160
	if (!ctx.cr0.lt) goto loc_825DF160;
	// bl 0x825d5398
	ctx.lr = 0x825DF160;
	sub_825D5398(ctx, base);
loc_825DF160:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825df064
	if (!ctx.cr6.eq) goto loc_825DF064;
loc_825DF168:
	// stw r26,3960(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3960, ctx.r26.u32);
	// b 0x825df244
	goto loc_825DF244;
loc_825DF170:
	// bge 0x825df178
	if (!ctx.cr0.lt) goto loc_825DF178;
	// bl 0x825d5398
	ctx.lr = 0x825DF178;
	sub_825D5398(ctx, base);
loc_825DF178:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825df268
	if (!ctx.cr6.eq) goto loc_825DF268;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825df1ac
	if (!ctx.cr0.lt) goto loc_825DF1AC;
	// bl 0x825d5398
	ctx.lr = 0x825DF1AC;
	sub_825D5398(ctx, base);
loc_825DF1AC:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825df168
	if (!ctx.cr6.eq) goto loc_825DF168;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825df1e0
	if (!ctx.cr0.lt) goto loc_825DF1E0;
	// bl 0x825d5398
	ctx.lr = 0x825DF1E0;
	sub_825D5398(ctx, base);
loc_825DF1E0:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825df270
	if (!ctx.cr6.eq) goto loc_825DF270;
	// lwz r11,284(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825df200
	if (!ctx.cr6.eq) goto loc_825DF200;
	// lwz r11,19980(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 19980);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825df23c
	if (ctx.cr6.eq) goto loc_825DF23C;
loc_825DF200:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825df23c
	if (!ctx.cr6.eq) goto loc_825DF23C;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825df234
	if (!ctx.cr0.lt) goto loc_825DF234;
	// bl 0x825d5398
	ctx.lr = 0x825DF234;
	sub_825D5398(ctx, base);
loc_825DF234:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825df064
	if (!ctx.cr6.eq) goto loc_825DF064;
loc_825DF23C:
	// li r11,3
	ctx.r11.s64 = 3;
	// stw r11,3960(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3960, ctx.r11.u32);
loc_825DF244:
	// lwz r11,3908(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3908);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825decac
	if (!ctx.cr6.eq) goto loc_825DECAC;
	// lwz r11,3964(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3964);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825decac
	if (ctx.cr6.eq) goto loc_825DECAC;
loc_825DF25C:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825DF268:
	// stw r25,3960(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3960, ctx.r25.u32);
	// b 0x825df244
	goto loc_825DF244;
loc_825DF270:
	// stw r24,3960(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3960, ctx.r24.u32);
	// b 0x825df244
	goto loc_825DF244;
loc_825DF278:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825df2a4
	if (!ctx.cr0.lt) goto loc_825DF2A4;
	// bl 0x825d5398
	ctx.lr = 0x825DF2A4;
	sub_825D5398(ctx, base);
loc_825DF2A4:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,6
	ctx.r30.s64 = 6;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// beq cr6,0x825df41c
	if (ctx.cr6.eq) goto loc_825DF41C;
	// stw r25,20028(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20028, ctx.r25.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825df324
	if (!ctx.cr6.lt) goto loc_825DF324;
loc_825DF2CC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825df324
	if (ctx.cr6.eq) goto loc_825DF324;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825df314
	if (!ctx.cr0.lt) goto loc_825DF314;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF314;
	sub_825D5398(ctx, base);
loc_825DF314:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825df2cc
	if (ctx.cr6.gt) goto loc_825DF2CC;
loc_825DF324:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825df360
	if (!ctx.cr0.lt) goto loc_825DF360;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF360;
	sub_825D5398(ctx, base);
loc_825DF360:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,6
	ctx.r30.s64 = 6;
	// stw r28,20040(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20040, ctx.r28.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825df3d8
	if (!ctx.cr6.lt) goto loc_825DF3D8;
loc_825DF380:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825df3d8
	if (ctx.cr6.eq) goto loc_825DF3D8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825df3c8
	if (!ctx.cr0.lt) goto loc_825DF3C8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF3C8;
	sub_825D5398(ctx, base);
loc_825DF3C8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825df380
	if (ctx.cr6.gt) goto loc_825DF380;
loc_825DF3D8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825df414
	if (!ctx.cr0.lt) goto loc_825DF414;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF414;
	sub_825D5398(ctx, base);
loc_825DF414:
	// stw r30,20044(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20044, ctx.r30.u32);
	// b 0x825df6e8
	goto loc_825DF6E8;
loc_825DF41C:
	// stw r25,20024(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20024, ctx.r25.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825df488
	if (!ctx.cr6.lt) goto loc_825DF488;
loc_825DF430:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825df488
	if (ctx.cr6.eq) goto loc_825DF488;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825df478
	if (!ctx.cr0.lt) goto loc_825DF478;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF478;
	sub_825D5398(ctx, base);
loc_825DF478:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825df430
	if (ctx.cr6.gt) goto loc_825DF430;
loc_825DF488:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825df4c4
	if (!ctx.cr0.lt) goto loc_825DF4C4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF4C4;
	sub_825D5398(ctx, base);
loc_825DF4C4:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,6
	ctx.r30.s64 = 6;
	// stw r28,20032(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20032, ctx.r28.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825df53c
	if (!ctx.cr6.lt) goto loc_825DF53C;
loc_825DF4E4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825df53c
	if (ctx.cr6.eq) goto loc_825DF53C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825df52c
	if (!ctx.cr0.lt) goto loc_825DF52C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF52C;
	sub_825D5398(ctx, base);
loc_825DF52C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825df4e4
	if (ctx.cr6.gt) goto loc_825DF4E4;
loc_825DF53C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825df578
	if (!ctx.cr0.lt) goto loc_825DF578;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF578;
	sub_825D5398(ctx, base);
loc_825DF578:
	// stw r30,20036(r27)
	PPC_STORE_U32(ctx.r27.u32 + 20036, ctx.r30.u32);
	// b 0x825df6e8
	goto loc_825DF6E8;
loc_825DF580:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,6
	ctx.r30.s64 = 6;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825df5f4
	if (!ctx.cr6.lt) goto loc_825DF5F4;
loc_825DF59C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825df5f4
	if (ctx.cr6.eq) goto loc_825DF5F4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825df5e4
	if (!ctx.cr0.lt) goto loc_825DF5E4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF5E4;
	sub_825D5398(ctx, base);
loc_825DF5E4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825df59c
	if (ctx.cr6.gt) goto loc_825DF59C;
loc_825DF5F4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825df630
	if (!ctx.cr0.lt) goto loc_825DF630;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF630;
	sub_825D5398(ctx, base);
loc_825DF630:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,6
	ctx.r30.s64 = 6;
	// stw r28,3968(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3968, ctx.r28.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825df6a8
	if (!ctx.cr6.lt) goto loc_825DF6A8;
loc_825DF650:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825df6a8
	if (ctx.cr6.eq) goto loc_825DF6A8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825df698
	if (!ctx.cr0.lt) goto loc_825DF698;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF698;
	sub_825D5398(ctx, base);
loc_825DF698:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825df650
	if (ctx.cr6.gt) goto loc_825DF650;
loc_825DF6A8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825df6e4
	if (!ctx.cr0.lt) goto loc_825DF6E4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF6E4;
	sub_825D5398(ctx, base);
loc_825DF6E4:
	// stw r30,3972(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3972, ctx.r30.u32);
loc_825DF6E8:
	// lwz r11,3960(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3960);
	// lwz r31,268(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 268);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825df778
	if (!ctx.cr6.eq) goto loc_825DF778;
	// lwz r11,21580(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21580);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825df720
	if (ctx.cr6.eq) goto loc_825DF720;
	// bl 0x825eb218
	ctx.lr = 0x825DF710;
	sub_825EB218(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825df72c
	if (ctx.cr6.eq) goto loc_825DF72C;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825DF720:
	// bl 0x825d75b8
	ctx.lr = 0x825DF724;
	sub_825D75B8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825dfb3c
	if (!ctx.cr6.eq) goto loc_825DFB3C;
loc_825DF72C:
	// lwz r11,348(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 348);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825df7b8
	if (ctx.cr6.eq) goto loc_825DF7B8;
	// lwz r11,144(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 144);
	// mr r10,r26
	ctx.r10.u64 = ctx.r26.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825df7b8
	if (!ctx.cr6.gt) goto loc_825DF7B8;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
loc_825DF74C:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// mr r8,r9
	ctx.r8.u64 = ctx.r9.u64;
	// rlwimi r8,r9,9,23,23
	ctx.r8.u64 = (__builtin_rotateleft32(ctx.r9.u32, 9) & 0x100) | (ctx.r8.u64 & 0xFFFFFFFFFFFFFEFF);
	// rlwinm r9,r8,0,23,20
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFFF9FF;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,20
	ctx.r11.s64 = ctx.r11.s64 + 20;
	// lwz r9,144(r27)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r27.u32 + 144);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x825df74c
	if (ctx.cr6.lt) goto loc_825DF74C;
	// b 0x825df7b8
	goto loc_825DF7B8;
loc_825DF778:
	// lwz r11,3924(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825df7b8
	if (!ctx.cr6.eq) goto loc_825DF7B8;
	// lwz r11,144(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 144);
	// mr r10,r26
	ctx.r10.u64 = ctx.r26.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825df7b8
	if (!ctx.cr6.gt) goto loc_825DF7B8;
	// mr r11,r31
	ctx.r11.u64 = ctx.r31.u64;
loc_825DF798:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r9,r9,0,24,20
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFFFFFFFF8FF;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,20
	ctx.r11.s64 = ctx.r11.s64 + 20;
	// lwz r9,144(r27)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r27.u32 + 144);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x825df798
	if (ctx.cr6.lt) goto loc_825DF798;
loc_825DF7B8:
	// lwz r11,14788(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 14788);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825df838
	if (ctx.cr6.eq) goto loc_825DF838;
	// lwz r11,284(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825df838
	if (!ctx.cr6.eq) goto loc_825DF838;
	// li r4,3
	ctx.r4.s64 = 3;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x825d75b8
	ctx.lr = 0x825DF7DC;
	sub_825D75B8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825dfb3c
	if (!ctx.cr6.eq) goto loc_825DFB3C;
	// lwz r11,14804(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 14804);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825df838
	if (ctx.cr6.eq) goto loc_825DF838;
	// lwz r10,144(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 144);
	// mr r9,r26
	ctx.r9.u64 = ctx.r26.u64;
	// lwz r11,268(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 268);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825df838
	if (!ctx.cr6.gt) goto loc_825DF838;
loc_825DF804:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r8,r10,0,0,0
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// beq cr6,0x825df81c
	if (ctx.cr6.eq) goto loc_825DF81C;
	// rlwimi r10,r25,5,24,26
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r25.u32, 5) & 0xE0) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFF1F);
	// b 0x825df820
	goto loc_825DF820;
loc_825DF81C:
	// rlwinm r10,r10,0,27,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFFFF1F;
loc_825DF820:
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// lwz r10,144(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 144);
	// addi r11,r11,20
	ctx.r11.s64 = ctx.r11.s64 + 20;
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825df804
	if (ctx.cr6.lt) goto loc_825DF804;
loc_825DF838:
	// lwz r11,21580(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 21580);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825df860
	if (ctx.cr6.eq) goto loc_825DF860;
	// bl 0x825eb218
	ctx.lr = 0x825DF850;
	sub_825EB218(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825df86c
	if (ctx.cr6.eq) goto loc_825DF86C;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825DF860:
	// bl 0x825d75b8
	ctx.lr = 0x825DF864;
	sub_825D75B8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825dfb3c
	if (!ctx.cr6.eq) goto loc_825DFB3C;
loc_825DF86C:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// stw r25,448(r27)
	PPC_STORE_U32(ctx.r27.u32 + 448, ctx.r25.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825df8e4
	if (!ctx.cr6.lt) goto loc_825DF8E4;
loc_825DF88C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825df8e4
	if (ctx.cr6.eq) goto loc_825DF8E4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825df8d4
	if (!ctx.cr0.lt) goto loc_825DF8D4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF8D4;
	sub_825D5398(ctx, base);
loc_825DF8D4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825df88c
	if (ctx.cr6.gt) goto loc_825DF88C;
loc_825DF8E4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825df920
	if (!ctx.cr0.lt) goto loc_825DF920;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF920;
	sub_825D5398(ctx, base);
loc_825DF920:
	// addi r11,r30,599
	ctx.r11.s64 = ctx.r30.s64 + 599;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwzx r11,r11,r27
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r27.u32);
	// stw r11,2376(r27)
	PPC_STORE_U32(ctx.r27.u32 + 2376, ctx.r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825df9a4
	if (!ctx.cr6.lt) goto loc_825DF9A4;
loc_825DF94C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825df9a4
	if (ctx.cr6.eq) goto loc_825DF9A4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825df994
	if (!ctx.cr0.lt) goto loc_825DF994;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF994;
	sub_825D5398(ctx, base);
loc_825DF994:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825df94c
	if (ctx.cr6.gt) goto loc_825DF94C;
loc_825DF9A4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825df9e0
	if (!ctx.cr0.lt) goto loc_825DF9E0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DF9E0;
	sub_825D5398(ctx, base);
loc_825DF9E0:
	// addi r11,r30,595
	ctx.r11.s64 = ctx.r30.s64 + 595;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r11,r11,r27
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r27.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r11,2140(r27)
	PPC_STORE_U32(ctx.r27.u32 + 2140, ctx.r11.u32);
	// beq cr6,0x825df25c
	if (ctx.cr6.eq) goto loc_825DF25C;
	// lwz r11,3980(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3980);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dfa10
	if (ctx.cr6.eq) goto loc_825DFA10;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x825d8180
	ctx.lr = 0x825DFA10;
	sub_825D8180(ctx, base);
loc_825DFA10:
	// lwz r11,436(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 436);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dfb34
	if (ctx.cr6.eq) goto loc_825DFB34;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825dfa48
	if (!ctx.cr0.lt) goto loc_825DFA48;
	// bl 0x825d5398
	ctx.lr = 0x825DFA48;
	sub_825D5398(ctx, base);
loc_825DFA48:
	// cmplwi cr6,r31,1
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 1, ctx.xer);
	// bne cr6,0x825dfb24
	if (!ctx.cr6.eq) goto loc_825DFB24;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// stw r26,328(r27)
	PPC_STORE_U32(ctx.r27.u32 + 328, ctx.r26.u32);
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825dfac8
	if (!ctx.cr6.lt) goto loc_825DFAC8;
loc_825DFA70:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dfac8
	if (ctx.cr6.eq) goto loc_825DFAC8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dfab8
	if (!ctx.cr0.lt) goto loc_825DFAB8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFAB8;
	sub_825D5398(ctx, base);
loc_825DFAB8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dfa70
	if (ctx.cr6.gt) goto loc_825DFA70;
loc_825DFAC8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dfb04
	if (!ctx.cr0.lt) goto loc_825DFB04;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFB04;
	sub_825D5398(ctx, base);
loc_825DFB04:
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// rlwinm r10,r30,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r11,3992
	ctx.r11.s64 = ctx.r11.s64 + 3992;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwzx r11,r10,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r11.u32);
	// stw r11,336(r27)
	PPC_STORE_U32(ctx.r27.u32 + 336, ctx.r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825DFB24:
	// stw r25,328(r27)
	PPC_STORE_U32(ctx.r27.u32 + 328, ctx.r25.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825DFB34:
	// stw r26,328(r27)
	PPC_STORE_U32(ctx.r27.u32 + 328, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
loc_825DFB3C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825DFB44"))) PPC_WEAK_FUNC(sub_825DFB44);
PPC_FUNC_IMPL(__imp__sub_825DFB44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825DFB48"))) PPC_WEAK_FUNC(sub_825DFB48);
PPC_FUNC_IMPL(__imp__sub_825DFB48) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825DFB50;
	sub_8239BA08(ctx, base);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r24,0
	ctx.r24.s64 = 0;
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// mr r25,r24
	ctx.r25.u64 = ctx.r24.u64;
	// stw r24,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r24.u32);
	// bl 0x825dc848
	ctx.lr = 0x825DFB68;
	sub_825DC848(ctx, base);
	// lwz r11,284(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dfb9c
	if (!ctx.cr6.eq) goto loc_825DFB9C;
	// lwz r11,21580(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21580);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dfb9c
	if (ctx.cr6.eq) goto loc_825DFB9C;
	// lwz r11,21704(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21704);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dfb9c
	if (!ctx.cr6.eq) goto loc_825DFB9C;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x8263d520
	ctx.lr = 0x825DFB94;
	sub_8263D520(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e1548
	if (!ctx.cr6.eq) goto loc_825E1548;
loc_825DFB9C:
	// lwz r11,284(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 284);
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// bne cr6,0x825dfe38
	if (!ctx.cr6.eq) goto loc_825DFE38;
	// lwz r11,20832(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 20832);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dfdec
	if (ctx.cr6.eq) goto loc_825DFDEC;
	// lwz r11,21160(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21160);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dfd38
	if (ctx.cr6.eq) goto loc_825DFD38;
	// lwz r11,21548(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21548);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825dfd38
	if (!ctx.cr6.eq) goto loc_825DFD38;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dfc44
	if (!ctx.cr6.lt) goto loc_825DFC44;
loc_825DFBEC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dfc44
	if (ctx.cr6.eq) goto loc_825DFC44;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825dfc34
	if (!ctx.cr0.lt) goto loc_825DFC34;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFC34;
	sub_825D5398(ctx, base);
loc_825DFC34:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dfbec
	if (ctx.cr6.gt) goto loc_825DFBEC;
loc_825DFC44:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dfc80
	if (!ctx.cr0.lt) goto loc_825DFC80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFC80;
	sub_825D5398(ctx, base);
loc_825DFC80:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// stw r28,20836(r26)
	PPC_STORE_U32(ctx.r26.u32 + 20836, ctx.r28.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dfcf4
	if (!ctx.cr6.lt) goto loc_825DFCF4;
loc_825DFC9C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dfcf4
	if (ctx.cr6.eq) goto loc_825DFCF4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dfce4
	if (!ctx.cr0.lt) goto loc_825DFCE4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFCE4;
	sub_825D5398(ctx, base);
loc_825DFCE4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dfc9c
	if (ctx.cr6.gt) goto loc_825DFC9C;
loc_825DFCF4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dfd30
	if (!ctx.cr0.lt) goto loc_825DFD30;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFD30;
	sub_825D5398(ctx, base);
loc_825DFD30:
	// stw r30,20840(r26)
	PPC_STORE_U32(ctx.r26.u32 + 20840, ctx.r30.u32);
	// b 0x825dfdec
	goto loc_825DFDEC;
loc_825DFD38:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825dfdac
	if (!ctx.cr6.lt) goto loc_825DFDAC;
loc_825DFD54:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dfdac
	if (ctx.cr6.eq) goto loc_825DFDAC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825dfd9c
	if (!ctx.cr0.lt) goto loc_825DFD9C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFD9C;
	sub_825D5398(ctx, base);
loc_825DFD9C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dfd54
	if (ctx.cr6.gt) goto loc_825DFD54;
loc_825DFDAC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dfde8
	if (!ctx.cr0.lt) goto loc_825DFDE8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFDE8;
	sub_825D5398(ctx, base);
loc_825DFDE8:
	// stw r30,21164(r26)
	PPC_STORE_U32(ctx.r26.u32 + 21164, ctx.r30.u32);
loc_825DFDEC:
	// lwz r11,21372(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21372);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dfe08
	if (ctx.cr6.eq) goto loc_825DFE08;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x825dc298
	ctx.lr = 0x825DFE08;
	sub_825DC298(ctx, base);
loc_825DFE08:
	// lwz r11,14772(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 14772);
	// stw r24,21528(r26)
	PPC_STORE_U32(ctx.r26.u32 + 21528, ctx.r24.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825dfe20
	if (!ctx.cr6.gt) goto loc_825DFE20;
	// lwz r11,20980(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 20980);
	// stw r11,20972(r26)
	PPC_STORE_U32(ctx.r26.u32 + 20972, ctx.r11.u32);
loc_825DFE20:
	// addi r5,r1,96
	ctx.r5.s64 = ctx.r1.s64 + 96;
	// lwz r4,80(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x825cfff8
	ctx.lr = 0x825DFE30;
	sub_825CFFF8(ctx, base);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825DFE38:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dfe58
	if (ctx.cr6.eq) goto loc_825DFE58;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x825dfe58
	if (ctx.cr6.eq) goto loc_825DFE58;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825dfe58
	if (ctx.cr6.eq) goto loc_825DFE58;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825e1544
	if (!ctx.cr6.eq) goto loc_825E1544;
loc_825DFE58:
	// lwz r11,20848(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 20848);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dfee4
	if (ctx.cr6.eq) goto loc_825DFEE4;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825dfebc
	if (!ctx.cr6.lt) goto loc_825DFEBC;
loc_825DFE7C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dfebc
	if (ctx.cr6.eq) goto loc_825DFEBC;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825dfeac
	if (!ctx.cr0.lt) goto loc_825DFEAC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFEAC;
	sub_825D5398(ctx, base);
loc_825DFEAC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dfe7c
	if (ctx.cr6.gt) goto loc_825DFE7C;
loc_825DFEBC:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825dfee4
	if (!ctx.cr0.lt) goto loc_825DFEE4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFEE4;
	sub_825D5398(ctx, base);
loc_825DFEE4:
	// lwz r11,20832(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 20832);
	// li r30,1
	ctx.r30.s64 = 1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e012c
	if (ctx.cr6.eq) goto loc_825E012C;
	// lwz r11,21160(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21160);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e0078
	if (ctx.cr6.eq) goto loc_825E0078;
	// lwz r11,21548(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21548);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e0078
	if (!ctx.cr6.eq) goto loc_825E0078;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825dff80
	if (!ctx.cr6.lt) goto loc_825DFF80;
loc_825DFF28:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825dff80
	if (ctx.cr6.eq) goto loc_825DFF80;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825dff70
	if (!ctx.cr0.lt) goto loc_825DFF70;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFF70;
	sub_825D5398(ctx, base);
loc_825DFF70:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dff28
	if (ctx.cr6.gt) goto loc_825DFF28;
loc_825DFF80:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r27,r11,r28
	ctx.r27.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825dffbc
	if (!ctx.cr0.lt) goto loc_825DFFBC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825DFFBC;
	sub_825D5398(ctx, base);
loc_825DFFBC:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// stw r27,20836(r26)
	PPC_STORE_U32(ctx.r26.u32 + 20836, ctx.r27.u32);
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e0034
	if (!ctx.cr6.lt) goto loc_825E0034;
loc_825DFFDC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e0034
	if (ctx.cr6.eq) goto loc_825E0034;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e0024
	if (!ctx.cr0.lt) goto loc_825E0024;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0024;
	sub_825D5398(ctx, base);
loc_825E0024:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825dffdc
	if (ctx.cr6.gt) goto loc_825DFFDC;
loc_825E0034:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e0070
	if (!ctx.cr0.lt) goto loc_825E0070;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0070;
	sub_825D5398(ctx, base);
loc_825E0070:
	// stw r29,20840(r26)
	PPC_STORE_U32(ctx.r26.u32 + 20840, ctx.r29.u32);
	// b 0x825e012c
	goto loc_825E012C;
loc_825E0078:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r29,2
	ctx.r29.s64 = 2;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825e00ec
	if (!ctx.cr6.lt) goto loc_825E00EC;
loc_825E0094:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e00ec
	if (ctx.cr6.eq) goto loc_825E00EC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e00dc
	if (!ctx.cr0.lt) goto loc_825E00DC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E00DC;
	sub_825D5398(ctx, base);
loc_825E00DC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0094
	if (ctx.cr6.gt) goto loc_825E0094;
loc_825E00EC:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e0128
	if (!ctx.cr0.lt) goto loc_825E0128;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0128;
	sub_825D5398(ctx, base);
loc_825E0128:
	// stw r29,21164(r26)
	PPC_STORE_U32(ctx.r26.u32 + 21164, ctx.r29.u32);
loc_825E012C:
	// lwz r11,21372(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21372);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e0148
	if (ctx.cr6.eq) goto loc_825E0148;
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x825dc298
	ctx.lr = 0x825E0148;
	sub_825DC298(ctx, base);
loc_825E0148:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e01bc
	if (!ctx.cr6.lt) goto loc_825E01BC;
loc_825E0164:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e01bc
	if (ctx.cr6.eq) goto loc_825E01BC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e01ac
	if (!ctx.cr0.lt) goto loc_825E01AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E01AC;
	sub_825D5398(ctx, base);
loc_825E01AC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0164
	if (ctx.cr6.gt) goto loc_825E0164;
loc_825E01BC:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e01f8
	if (!ctx.cr0.lt) goto loc_825E01F8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E01F8;
	sub_825D5398(ctx, base);
loc_825E01F8:
	// lwz r11,21160(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21160);
	// stw r29,3904(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3904, ctx.r29.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e02bc
	if (ctx.cr6.eq) goto loc_825E02BC;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e027c
	if (!ctx.cr6.lt) goto loc_825E027C;
loc_825E0224:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e027c
	if (ctx.cr6.eq) goto loc_825E027C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e026c
	if (!ctx.cr0.lt) goto loc_825E026C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E026C;
	sub_825D5398(ctx, base);
loc_825E026C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0224
	if (ctx.cr6.gt) goto loc_825E0224;
loc_825E027C:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e02b8
	if (!ctx.cr0.lt) goto loc_825E02B8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E02B8;
	sub_825D5398(ctx, base);
loc_825E02B8:
	// stw r29,20972(r26)
	PPC_STORE_U32(ctx.r26.u32 + 20972, ctx.r29.u32);
loc_825E02BC:
	// lwz r11,3444(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3444);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e037c
	if (ctx.cr6.eq) goto loc_825E037C;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e033c
	if (!ctx.cr6.lt) goto loc_825E033C;
loc_825E02E4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e033c
	if (ctx.cr6.eq) goto loc_825E033C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e032c
	if (!ctx.cr0.lt) goto loc_825E032C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E032C;
	sub_825D5398(ctx, base);
loc_825E032C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e02e4
	if (ctx.cr6.gt) goto loc_825E02E4;
loc_825E033C:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e0378
	if (!ctx.cr0.lt) goto loc_825E0378;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0378;
	sub_825D5398(ctx, base);
loc_825E0378:
	// stw r29,3448(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3448, ctx.r29.u32);
loc_825E037C:
	// lwz r11,284(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825e0574
	if (!ctx.cr6.eq) goto loc_825E0574;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r29,3
	ctx.r29.s64 = 3;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825e03fc
	if (!ctx.cr6.lt) goto loc_825E03FC;
loc_825E03A4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e03fc
	if (ctx.cr6.eq) goto loc_825E03FC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e03ec
	if (!ctx.cr0.lt) goto loc_825E03EC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E03EC;
	sub_825D5398(ctx, base);
loc_825E03EC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e03a4
	if (ctx.cr6.gt) goto loc_825E03A4;
loc_825E03FC:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e0438
	if (!ctx.cr0.lt) goto loc_825E0438;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0438;
	sub_825D5398(ctx, base);
loc_825E0438:
	// cmpwi cr6,r29,7
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 7, ctx.xer);
	// bne cr6,0x825e0528
	if (!ctx.cr6.eq) goto loc_825E0528;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r29,4
	ctx.r29.s64 = 4;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bge cr6,0x825e04b4
	if (!ctx.cr6.lt) goto loc_825E04B4;
loc_825E045C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e04b4
	if (ctx.cr6.eq) goto loc_825E04B4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e04a4
	if (!ctx.cr0.lt) goto loc_825E04A4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E04A4;
	sub_825D5398(ctx, base);
loc_825E04A4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e045c
	if (ctx.cr6.gt) goto loc_825E045C;
loc_825E04B4:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e04f0
	if (!ctx.cr0.lt) goto loc_825E04F0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E04F0;
	sub_825D5398(ctx, base);
loc_825E04F0:
	// cmpwi cr6,r29,14
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 14, ctx.xer);
	// bge cr6,0x825e1544
	if (!ctx.cr6.lt) goto loc_825E1544;
	// addi r11,r29,112
	ctx.r11.s64 = ctx.r29.s64 + 112;
	// lwz r7,14772(r26)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r26.u32 + 14772);
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-112
	ctx.r11.s64 = ctx.r11.s64 + -112;
	// addi r10,r10,3568
	ctx.r10.s64 = ctx.r10.s64 + 3568;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// addi r9,r9,3624
	ctx.r9.s64 = ctx.r9.s64 + 3624;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	// stw r10,3392(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3392, ctx.r10.u32);
	// b 0x825e054c
	goto loc_825E054C;
loc_825E0528:
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// rlwinm r11,r29,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,3512
	ctx.r10.s64 = ctx.r10.s64 + 3512;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// addi r9,r9,3540
	ctx.r9.s64 = ctx.r9.s64 + 3540;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	// stw r10,3392(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3392, ctx.r10.u32);
	// lwz r10,14772(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 14772);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
loc_825E054C:
	// lwzx r11,r11,r9
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	// lis r8,-32244
	ctx.r8.s64 = -2113142784;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,-22904
	ctx.r8.s64 = ctx.r8.s64 + -22904;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r11,3388(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3388, ctx.r11.u32);
	// lwz r11,-4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// stw r11,14776(r26)
	PPC_STORE_U32(ctx.r26.u32 + 14776, ctx.r11.u32);
	// bne cr6,0x825e0574
	if (!ctx.cr6.eq) goto loc_825E0574;
	// stw r30,14772(r26)
	PPC_STORE_U32(ctx.r26.u32 + 14772, ctx.r30.u32);
loc_825E0574:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r29,5
	ctx.r29.s64 = 5;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825e05e8
	if (!ctx.cr6.lt) goto loc_825E05E8;
loc_825E0590:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e05e8
	if (ctx.cr6.eq) goto loc_825E05E8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e05d8
	if (!ctx.cr0.lt) goto loc_825E05D8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E05D8;
	sub_825D5398(ctx, base);
loc_825E05D8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0590
	if (ctx.cr6.gt) goto loc_825E0590;
loc_825E05E8:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e0624
	if (!ctx.cr0.lt) goto loc_825E0624;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0624;
	sub_825D5398(ctx, base);
loc_825E0624:
	// cmpwi cr6,r29,8
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 8, ctx.xer);
	// stw r29,3952(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3952, ctx.r29.u32);
	// bgt cr6,0x825e06e8
	if (ctx.cr6.gt) goto loc_825E06E8;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e06a4
	if (!ctx.cr6.lt) goto loc_825E06A4;
loc_825E064C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e06a4
	if (ctx.cr6.eq) goto loc_825E06A4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e0694
	if (!ctx.cr0.lt) goto loc_825E0694;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0694;
	sub_825D5398(ctx, base);
loc_825E0694:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e064c
	if (ctx.cr6.gt) goto loc_825E064C;
loc_825E06A4:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e06e0
	if (!ctx.cr0.lt) goto loc_825E06E0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E06E0;
	sub_825D5398(ctx, base);
loc_825E06E0:
	// stw r29,252(r26)
	PPC_STORE_U32(ctx.r26.u32 + 252, ctx.r29.u32);
	// b 0x825e06ec
	goto loc_825E06EC;
loc_825E06E8:
	// stw r24,252(r26)
	PPC_STORE_U32(ctx.r26.u32 + 252, ctx.r24.u32);
loc_825E06EC:
	// lwz r11,3440(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3440);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e07ac
	if (ctx.cr6.eq) goto loc_825E07AC;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e076c
	if (!ctx.cr6.lt) goto loc_825E076C;
loc_825E0714:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e076c
	if (ctx.cr6.eq) goto loc_825E076C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e075c
	if (!ctx.cr0.lt) goto loc_825E075C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E075C;
	sub_825D5398(ctx, base);
loc_825E075C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0714
	if (ctx.cr6.gt) goto loc_825E0714;
loc_825E076C:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e07a8
	if (!ctx.cr0.lt) goto loc_825E07A8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E07A8;
	sub_825D5398(ctx, base);
loc_825E07A8:
	// stw r29,3428(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3428, ctx.r29.u32);
loc_825E07AC:
	// lwz r11,3432(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3432);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e07ec
	if (!ctx.cr6.eq) goto loc_825E07EC;
	// lwz r11,3952(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3952);
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// bgt cr6,0x825e07d0
	if (ctx.cr6.gt) goto loc_825E07D0;
	// mr r27,r11
	ctx.r27.u64 = ctx.r11.u64;
	// stw r30,3428(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3428, ctx.r30.u32);
	// b 0x825e07f0
	goto loc_825E07F0;
loc_825E07D0:
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// stw r24,3428(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3428, ctx.r24.u32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,3680
	ctx.r10.s64 = ctx.r10.s64 + 3680;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r27,-4(r11)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// b 0x825e07f0
	goto loc_825E07F0;
loc_825E07EC:
	// lwz r27,3952(r26)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3952);
loc_825E07F0:
	// lwz r11,2972(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2972);
	// stw r27,248(r26)
	PPC_STORE_U32(ctx.r26.u32 + 248, ctx.r27.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r24,2968(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2968, ctx.r24.u32);
	// beq cr6,0x825e0838
	if (ctx.cr6.eq) goto loc_825E0838;
	// lwz r11,284(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825e0838
	if (ctx.cr6.eq) goto loc_825E0838;
	// cmpwi cr6,r27,9
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 9, ctx.xer);
	// blt cr6,0x825e0820
	if (ctx.cr6.lt) goto loc_825E0820;
	// stw r30,2968(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2968, ctx.r30.u32);
	// b 0x825e0838
	goto loc_825E0838;
loc_825E0820:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e0830
	if (ctx.cr6.eq) goto loc_825E0830;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bne cr6,0x825e0838
	if (!ctx.cr6.eq) goto loc_825E0838;
loc_825E0830:
	// li r11,7
	ctx.r11.s64 = 7;
	// stw r11,2968(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2968, ctx.r11.u32);
loc_825E0838:
	// lwz r11,20868(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 20868);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e08f8
	if (ctx.cr6.eq) goto loc_825E08F8;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r29,2
	ctx.r29.s64 = 2;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825e08b8
	if (!ctx.cr6.lt) goto loc_825E08B8;
loc_825E0860:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e08b8
	if (ctx.cr6.eq) goto loc_825E08B8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e08a8
	if (!ctx.cr0.lt) goto loc_825E08A8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E08A8;
	sub_825D5398(ctx, base);
loc_825E08A8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0860
	if (ctx.cr6.gt) goto loc_825E0860;
loc_825E08B8:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e08f4
	if (!ctx.cr0.lt) goto loc_825E08F4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E08F4;
	sub_825D5398(ctx, base);
loc_825E08F4:
	// stw r29,20872(r26)
	PPC_STORE_U32(ctx.r26.u32 + 20872, ctx.r29.u32);
loc_825E08F8:
	// lwz r11,284(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e090c
	if (ctx.cr6.eq) goto loc_825E090C;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bne cr6,0x825e0ba4
	if (!ctx.cr6.eq) goto loc_825E0BA4;
loc_825E090C:
	// lwz r11,21580(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21580);
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e0934
	if (ctx.cr6.eq) goto loc_825E0934;
	// bl 0x825eb218
	ctx.lr = 0x825E0924;
	sub_825EB218(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825e0940
	if (ctx.cr6.eq) goto loc_825E0940;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825E0934:
	// bl 0x825d75b8
	ctx.lr = 0x825E0938;
	sub_825D75B8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e1548
	if (!ctx.cr6.eq) goto loc_825E1548;
loc_825E0940:
	// lwz r11,20004(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 20004);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e0990
	if (ctx.cr6.eq) goto loc_825E0990;
	// lwz r11,144(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 144);
	// mr r9,r24
	ctx.r9.u64 = ctx.r24.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825e0990
	if (!ctx.cr6.gt) goto loc_825E0990;
	// mr r10,r24
	ctx.r10.u64 = ctx.r24.u64;
loc_825E0960:
	// lwz r11,268(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 268);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// rlwimi r7,r8,4,28,28
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 4) & 0x8) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFFF7);
	// rlwinm r8,r7,0,28,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFFFFFFFFEF;
	// stw r8,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
	// lwz r11,144(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 144);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825e0960
	if (ctx.cr6.lt) goto loc_825E0960;
loc_825E0990:
	// lwz r11,2968(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2968);
	// rlwinm r11,r11,0,30,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x2;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e0ba0
	if (ctx.cr6.eq) goto loc_825E0BA0;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e0a14
	if (!ctx.cr6.lt) goto loc_825E0A14;
loc_825E09BC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e0a14
	if (ctx.cr6.eq) goto loc_825E0A14;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e0a04
	if (!ctx.cr0.lt) goto loc_825E0A04;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0A04;
	sub_825D5398(ctx, base);
loc_825E0A04:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e09bc
	if (ctx.cr6.gt) goto loc_825E09BC;
loc_825E0A14:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e0a50
	if (!ctx.cr0.lt) goto loc_825E0A50;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0A50;
	sub_825D5398(ctx, base);
loc_825E0A50:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825e0a60
	if (!ctx.cr6.eq) goto loc_825E0A60;
	// stw r24,2968(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2968, ctx.r24.u32);
	// b 0x825e0ba0
	goto loc_825E0BA0;
loc_825E0A60:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e0ad4
	if (!ctx.cr6.lt) goto loc_825E0AD4;
loc_825E0A7C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e0ad4
	if (ctx.cr6.eq) goto loc_825E0AD4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e0ac4
	if (!ctx.cr0.lt) goto loc_825E0AC4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0AC4;
	sub_825D5398(ctx, base);
loc_825E0AC4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0a7c
	if (ctx.cr6.gt) goto loc_825E0A7C;
loc_825E0AD4:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e0b10
	if (!ctx.cr0.lt) goto loc_825E0B10;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0B10;
	sub_825D5398(ctx, base);
loc_825E0B10:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825e0b20
	if (!ctx.cr6.eq) goto loc_825E0B20;
	// stw r30,2968(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2968, ctx.r30.u32);
	// b 0x825e0ba0
	goto loc_825E0BA0;
loc_825E0B20:
	// lwz r11,21580(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21580);
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e0b48
	if (ctx.cr6.eq) goto loc_825E0B48;
	// bl 0x825eb218
	ctx.lr = 0x825E0B38;
	sub_825EB218(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825e0b54
	if (ctx.cr6.eq) goto loc_825E0B54;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825E0B48:
	// bl 0x825d75b8
	ctx.lr = 0x825E0B4C;
	sub_825D75B8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e1548
	if (!ctx.cr6.eq) goto loc_825E1548;
loc_825E0B54:
	// lwz r11,20940(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 20940);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e0ba0
	if (ctx.cr6.eq) goto loc_825E0BA0;
	// lwz r11,144(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 144);
	// mr r9,r24
	ctx.r9.u64 = ctx.r24.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825e0ba0
	if (!ctx.cr6.gt) goto loc_825E0BA0;
	// mr r10,r24
	ctx.r10.u64 = ctx.r24.u64;
loc_825E0B74:
	// lwz r11,268(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 268);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// rlwimi r7,r8,12,20,20
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 12) & 0x800) | (ctx.r7.u64 & 0xFFFFFFFFFFFFF7FF);
	// stw r7,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r7.u32);
	// lwz r11,144(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 144);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825e0b74
	if (ctx.cr6.lt) goto loc_825E0B74;
loc_825E0BA0:
	// stw r24,21528(r26)
	PPC_STORE_U32(ctx.r26.u32 + 21528, ctx.r24.u32);
loc_825E0BA4:
	// lwz r11,2968(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2968);
	// clrlwi r11,r11,31
	ctx.r11.u64 = ctx.r11.u32 & 0x1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e0bd8
	if (ctx.cr6.eq) goto loc_825E0BD8;
	// lwz r11,1900(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 1900);
	// sth r24,16(r11)
	PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r24.u16);
	// lwz r11,1900(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 1900);
	// sth r24,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r24.u16);
	// lwz r11,1904(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 1904);
	// sth r24,16(r11)
	PPC_STORE_U16(ctx.r11.u32 + 16, ctx.r24.u16);
	// lwz r11,1904(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 1904);
	// sth r24,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r24.u16);
	// b 0x825e0bfc
	goto loc_825E0BFC;
loc_825E0BD8:
	// lwz r10,1900(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 1900);
	// li r11,128
	ctx.r11.s64 = 128;
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r11.u16);
	// lwz r10,1900(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 1900);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r11.u16);
	// lwz r10,1904(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 1904);
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r11.u16);
	// lwz r10,1904(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 1904);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r11.u16);
loc_825E0BFC:
	// lwz r10,3428(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3428);
	// addi r11,r26,3988
	ctx.r11.s64 = ctx.r26.s64 + 3988;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e0c10
	if (!ctx.cr6.eq) goto loc_825E0C10;
	// addi r11,r26,5268
	ctx.r11.s64 = ctx.r26.s64 + 5268;
loc_825E0C10:
	// stw r11,6548(r26)
	PPC_STORE_U32(ctx.r26.u32 + 6548, ctx.r11.u32);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// addi r11,r26,6560
	ctx.r11.s64 = ctx.r26.s64 + 6560;
	// bne cr6,0x825e0c24
	if (!ctx.cr6.eq) goto loc_825E0C24;
	// addi r11,r26,10656
	ctx.r11.s64 = ctx.r26.s64 + 10656;
loc_825E0C24:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// stw r11,14752(r26)
	PPC_STORE_U32(ctx.r26.u32 + 14752, ctx.r11.u32);
	// stw r27,248(r26)
	PPC_STORE_U32(ctx.r26.u32 + 248, ctx.r27.u32);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e1544
	if (!ctx.cr6.eq) goto loc_825E1544;
	// addi r11,r27,-1
	ctx.r11.s64 = ctx.r27.s64 + -1;
	// stw r27,248(r26)
	PPC_STORE_U32(ctx.r26.u32 + 248, ctx.r27.u32);
	// cmplwi cr6,r11,30
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 30, ctx.xer);
	// bgt cr6,0x825e1544
	if (ctx.cr6.gt) goto loc_825E1544;
	// lwz r11,3952(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3952);
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// bgt cr6,0x825e0c68
	if (ctx.cr6.gt) goto loc_825E0C68;
	// addi r11,r26,2840
	ctx.r11.s64 = ctx.r26.s64 + 2840;
	// addi r10,r26,2800
	ctx.r10.s64 = ctx.r26.s64 + 2800;
	// stw r11,2904(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2904, ctx.r11.u32);
	// stw r10,2916(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2916, ctx.r10.u32);
loc_825E0C68:
	// lwz r11,284(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e1144
	if (ctx.cr6.eq) goto loc_825E1144;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825e0ee0
	if (ctx.cr6.eq) goto loc_825E0EE0;
	// lwz r11,20864(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 20864);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e0eb0
	if (ctx.cr6.eq) goto loc_825E0EB0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e0cf8
	if (!ctx.cr6.lt) goto loc_825E0CF8;
loc_825E0CA0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e0cf8
	if (ctx.cr6.eq) goto loc_825E0CF8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e0ce8
	if (!ctx.cr0.lt) goto loc_825E0CE8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0CE8;
	sub_825D5398(ctx, base);
loc_825E0CE8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0ca0
	if (ctx.cr6.gt) goto loc_825E0CA0;
loc_825E0CF8:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r27,r11,r28
	ctx.r27.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e0d34
	if (!ctx.cr0.lt) goto loc_825E0D34;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0D34;
	sub_825D5398(ctx, base);
loc_825E0D34:
	// mr r25,r27
	ctx.r25.u64 = ctx.r27.u64;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// beq cr6,0x825e0df4
	if (ctx.cr6.eq) goto loc_825E0DF4;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e0db4
	if (!ctx.cr6.lt) goto loc_825E0DB4;
loc_825E0D5C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e0db4
	if (ctx.cr6.eq) goto loc_825E0DB4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e0da4
	if (!ctx.cr0.lt) goto loc_825E0DA4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0DA4;
	sub_825D5398(ctx, base);
loc_825E0DA4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0d5c
	if (ctx.cr6.gt) goto loc_825E0D5C;
loc_825E0DB4:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e0df0
	if (!ctx.cr0.lt) goto loc_825E0DF0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0DF0;
	sub_825D5398(ctx, base);
loc_825E0DF0:
	// add r25,r29,r27
	ctx.r25.u64 = ctx.r29.u64 + ctx.r27.u64;
loc_825E0DF4:
	// cmpwi cr6,r25,2
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 2, ctx.xer);
	// bne cr6,0x825e0eb0
	if (!ctx.cr6.eq) goto loc_825E0EB0;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e0e70
	if (!ctx.cr6.lt) goto loc_825E0E70;
loc_825E0E18:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e0e70
	if (ctx.cr6.eq) goto loc_825E0E70;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e0e60
	if (!ctx.cr0.lt) goto loc_825E0E60;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0E60;
	sub_825D5398(ctx, base);
loc_825E0E60:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0e18
	if (ctx.cr6.gt) goto loc_825E0E18;
loc_825E0E70:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e0eac
	if (!ctx.cr0.lt) goto loc_825E0EAC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0EAC;
	sub_825D5398(ctx, base);
loc_825E0EAC:
	// addi r25,r29,2
	ctx.r25.s64 = ctx.r29.s64 + 2;
loc_825E0EB0:
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x825ed4d0
	ctx.lr = 0x825E0EBC;
	sub_825ED4D0(ctx, base);
	// lwz r11,284(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 284);
	// lwz r10,404(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 404);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825e0ed4
	if (!ctx.cr6.eq) goto loc_825E0ED4;
	// stw r10,21528(r26)
	PPC_STORE_U32(ctx.r26.u32 + 21528, ctx.r10.u32);
	// b 0x825e0ee0
	goto loc_825E0EE0;
loc_825E0ED4:
	// lwz r9,21528(r26)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21528);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x825e1544
	if (ctx.cr6.lt) goto loc_825E1544;
loc_825E0EE0:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e1144
	if (ctx.cr6.eq) goto loc_825E1144;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825e1144
	if (ctx.cr6.eq) goto loc_825E1144;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x825dec58
	ctx.lr = 0x825E0EF8;
	sub_825DEC58(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e1548
	if (!ctx.cr6.eq) goto loc_825E1548;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e0f74
	if (!ctx.cr6.lt) goto loc_825E0F74;
loc_825E0F1C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e0f74
	if (ctx.cr6.eq) goto loc_825E0F74;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e0f64
	if (!ctx.cr0.lt) goto loc_825E0F64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0F64;
	sub_825D5398(ctx, base);
loc_825E0F64:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0f1c
	if (ctx.cr6.gt) goto loc_825E0F1C;
loc_825E0F74:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e0fb0
	if (!ctx.cr0.lt) goto loc_825E0FB0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E0FB0;
	sub_825D5398(ctx, base);
loc_825E0FB0:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// stw r29,2928(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2928, ctx.r29.u32);
	// beq cr6,0x825e1078
	if (ctx.cr6.eq) goto loc_825E1078;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1030
	if (!ctx.cr6.lt) goto loc_825E1030;
loc_825E0FD8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1030
	if (ctx.cr6.eq) goto loc_825E1030;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e1020
	if (!ctx.cr0.lt) goto loc_825E1020;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1020;
	sub_825D5398(ctx, base);
loc_825E1020:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e0fd8
	if (ctx.cr6.gt) goto loc_825E0FD8;
loc_825E1030:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e106c
	if (!ctx.cr0.lt) goto loc_825E106C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E106C;
	sub_825D5398(ctx, base);
loc_825E106C:
	// lwz r11,2928(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2928);
	// add r11,r29,r11
	ctx.r11.u64 = ctx.r29.u64 + ctx.r11.u64;
	// stw r11,2928(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2928, ctx.r11.u32);
loc_825E1078:
	// lwz r11,2928(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2928);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// stw r11,2936(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2936, ctx.r11.u32);
	// stw r11,2932(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2932, ctx.r11.u32);
	// stw r11,2948(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2948, ctx.r11.u32);
	// stw r11,2944(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2944, ctx.r11.u32);
	// stw r11,2940(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2940, ctx.r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1100
	if (!ctx.cr6.lt) goto loc_825E1100;
loc_825E10A8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1100
	if (ctx.cr6.eq) goto loc_825E1100;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e10f0
	if (!ctx.cr0.lt) goto loc_825E10F0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E10F0;
	sub_825D5398(ctx, base);
loc_825E10F0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e10a8
	if (ctx.cr6.gt) goto loc_825E10A8;
loc_825E1100:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e113c
	if (!ctx.cr0.lt) goto loc_825E113C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E113C;
	sub_825D5398(ctx, base);
loc_825E113C:
	// stw r30,2088(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2088, ctx.r30.u32);
	// b 0x825e1534
	goto loc_825E1534;
loc_825E1144:
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// lwz r4,15464(r26)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r26.u32 + 15464);
	// bl 0x825dea98
	ctx.lr = 0x825E1150;
	sub_825DEA98(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e1548
	if (!ctx.cr6.eq) goto loc_825E1548;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e1544
	if (!ctx.cr6.eq) goto loc_825E1544;
	// stw r24,400(r26)
	PPC_STORE_U32(ctx.r26.u32 + 400, ctx.r24.u32);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e11dc
	if (!ctx.cr6.lt) goto loc_825E11DC;
loc_825E1184:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e11dc
	if (ctx.cr6.eq) goto loc_825E11DC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e11cc
	if (!ctx.cr0.lt) goto loc_825E11CC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E11CC;
	sub_825D5398(ctx, base);
loc_825E11CC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e1184
	if (ctx.cr6.gt) goto loc_825E1184;
loc_825E11DC:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e1218
	if (!ctx.cr0.lt) goto loc_825E1218;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1218;
	sub_825D5398(ctx, base);
loc_825E1218:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// stw r29,2928(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2928, ctx.r29.u32);
	// beq cr6,0x825e12e0
	if (ctx.cr6.eq) goto loc_825E12E0;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1298
	if (!ctx.cr6.lt) goto loc_825E1298;
loc_825E1240:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1298
	if (ctx.cr6.eq) goto loc_825E1298;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e1288
	if (!ctx.cr0.lt) goto loc_825E1288;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1288;
	sub_825D5398(ctx, base);
loc_825E1288:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e1240
	if (ctx.cr6.gt) goto loc_825E1240;
loc_825E1298:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e12d4
	if (!ctx.cr0.lt) goto loc_825E12D4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E12D4;
	sub_825D5398(ctx, base);
loc_825E12D4:
	// lwz r11,2928(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2928);
	// add r11,r29,r11
	ctx.r11.u64 = ctx.r29.u64 + ctx.r11.u64;
	// stw r11,2928(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2928, ctx.r11.u32);
loc_825E12E0:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1354
	if (!ctx.cr6.lt) goto loc_825E1354;
loc_825E12FC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1354
	if (ctx.cr6.eq) goto loc_825E1354;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e1344
	if (!ctx.cr0.lt) goto loc_825E1344;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1344;
	sub_825D5398(ctx, base);
loc_825E1344:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e12fc
	if (ctx.cr6.gt) goto loc_825E12FC;
loc_825E1354:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e1390
	if (!ctx.cr0.lt) goto loc_825E1390;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1390;
	sub_825D5398(ctx, base);
loc_825E1390:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// stw r29,2940(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2940, ctx.r29.u32);
	// beq cr6,0x825e1458
	if (ctx.cr6.eq) goto loc_825E1458;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
	// mr r28,r24
	ctx.r28.u64 = ctx.r24.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1410
	if (!ctx.cr6.lt) goto loc_825E1410;
loc_825E13B8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1410
	if (ctx.cr6.eq) goto loc_825E1410;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r29,r11,r29
	ctx.r29.s64 = ctx.r29.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r29
	ctx.r11.u64 = ctx.r29.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r29.u8 & 0x3F));
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// bge 0x825e1400
	if (!ctx.cr0.lt) goto loc_825E1400;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1400;
	sub_825D5398(ctx, base);
loc_825E1400:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r29,r11
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e13b8
	if (ctx.cr6.gt) goto loc_825E13B8;
loc_825E1410:
	// subfic r9,r29,64
	ctx.xer.ca = ctx.r29.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r29.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r29,32
	ctx.r8.u64 = ctx.r29.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r29,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r29.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r29,r11,r28
	ctx.r29.u64 = ctx.r11.u64 + ctx.r28.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e144c
	if (!ctx.cr0.lt) goto loc_825E144C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E144C;
	sub_825D5398(ctx, base);
loc_825E144C:
	// lwz r11,2940(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2940);
	// add r11,r29,r11
	ctx.r11.u64 = ctx.r29.u64 + ctx.r11.u64;
	// stw r11,2940(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2940, ctx.r11.u32);
loc_825E1458:
	// lwz r11,2940(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 2940);
	// mr r29,r24
	ctx.r29.u64 = ctx.r24.u64;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// stw r11,2948(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2948, ctx.r11.u32);
	// stw r11,2944(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2944, ctx.r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e14d4
	if (!ctx.cr6.lt) goto loc_825E14D4;
loc_825E147C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e14d4
	if (ctx.cr6.eq) goto loc_825E14D4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e14c4
	if (!ctx.cr0.lt) goto loc_825E14C4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E14C4;
	sub_825D5398(ctx, base);
loc_825E14C4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e147c
	if (ctx.cr6.gt) goto loc_825E147C;
loc_825E14D4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e1510
	if (!ctx.cr0.lt) goto loc_825E1510;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1510;
	sub_825D5398(ctx, base);
loc_825E1510:
	// lwz r11,3980(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3980);
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// stw r30,2088(r26)
	PPC_STORE_U32(ctx.r26.u32 + 2088, ctx.r30.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e1530
	if (ctx.cr6.eq) goto loc_825E1530;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825d8180
	ctx.lr = 0x825E152C;
	sub_825D8180(ctx, base);
	// b 0x825e1534
	goto loc_825E1534;
loc_825E1530:
	// bl 0x82601498
	ctx.lr = 0x825E1534;
	sub_82601498(ctx, base);
loc_825E1534:
	// lwz r11,84(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825dfe20
	if (ctx.cr6.eq) goto loc_825DFE20;
loc_825E1544:
	// li r3,4
	ctx.r3.s64 = 4;
loc_825E1548:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825E1550"))) PPC_WEAK_FUNC(sub_825E1550);
PPC_FUNC_IMPL(__imp__sub_825E1550) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba10
	ctx.lr = 0x825E1558;
	sub_8239BA10(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,14828(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// lwz r10,14824(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825e182c
	if (ctx.cr6.eq) goto loc_825E182C;
	// lwz r10,14772(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825e1588
	if (ctx.cr6.eq) goto loc_825E1588;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e1588
	if (!ctx.cr6.eq) goto loc_825E1588;
	// bl 0x825dd508
	ctx.lr = 0x825E1588;
	sub_825DD508(ctx, base);
loc_825E1588:
	// lwz r6,284(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// beq cr6,0x825e159c
	if (ctx.cr6.eq) goto loc_825E159C;
	// cmpwi cr6,r6,4
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 4, ctx.xer);
	// bne cr6,0x825e15c8
	if (!ctx.cr6.eq) goto loc_825E15C8;
loc_825E159C:
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// lwz r9,3724(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// lwz r8,3720(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// lwz r7,220(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// lwz r10,3728(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r9,3804(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3804, ctx.r9.u32);
	// stw r8,3800(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3800, ctx.r8.u32);
	// stw r11,3808(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3808, ctx.r11.u32);
loc_825E15C8:
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e15e4
	if (!ctx.cr6.eq) goto loc_825E15E4;
	// cmpwi cr6,r6,1
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 1, ctx.xer);
	// beq cr6,0x825e15e4
	if (ctx.cr6.eq) goto loc_825E15E4;
	// cmpwi cr6,r6,2
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 2, ctx.xer);
	// bne cr6,0x825e182c
	if (!ctx.cr6.eq) goto loc_825E182C;
loc_825E15E4:
	// lwz r10,14828(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// lwz r9,14824(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// mulli r11,r10,84
	ctx.r11.s64 = ctx.r10.s64 * 84;
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + ctx.r31.u64;
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// lwz r29,14900(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14900);
	// lwz r30,14904(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14904);
	// ble cr6,0x825e16f0
	if (!ctx.cr6.gt) goto loc_825E16F0;
	// lwz r10,152(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 152);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e16ac
	if (!ctx.cr6.eq) goto loc_825E16AC;
	// lwz r28,14884(r11)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14884);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r10,14856(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14856);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r9,14832(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14832);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// lwz r5,14840(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14840);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,3732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r28.u32);
	// lwz r28,19976(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// lwz r11,15856(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15856);
	// cntlzw r28,r28
	ctx.r28.u64 = ctx.r28.u32 == 0 ? 32 : __builtin_clz(ctx.r28.u32);
	// rlwinm r28,r28,27,31,31
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 27) & 0x1;
	// stw r28,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r28.u32);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x825E1654;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,14828(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// lwz r10,19976(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// li r9,1
	ctx.r9.s64 = 1;
	// mulli r11,r11,84
	ctx.r11.s64 = ctx.r11.s64 * 84;
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// lwz r3,3736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// lwz r28,15852(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15852);
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + ctx.r31.u64;
	// cntlzw r10,r10
	ctx.r10.u64 = ctx.r10.u32 == 0 ? 32 : __builtin_clz(ctx.r10.u32);
	// li r8,1
	ctx.r8.s64 = 1;
	// mr r7,r30
	ctx.r7.u64 = ctx.r30.u64;
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r27,14888(r11)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14888);
	// lwz r26,14860(r11)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14860);
	// lwz r6,14844(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14844);
	// stw r10,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r10.u32);
	// lwz r10,14836(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14836);
	// stw r27,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r27.u32);
	// stw r26,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r26.u32);
	// mtctr r28
	ctx.ctr.u64 = ctx.r28.u64;
	// bctrl 
	ctx.lr = 0x825E16AC;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_825E16AC:
	// lwz r8,220(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r7,3720(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lwz r8,3732(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// lwz r9,3728(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// lwz r10,3724(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// add r4,r8,r29
	ctx.r4.u64 = ctx.r8.u64 + ctx.r29.u64;
	// lwz r6,3740(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// lwz r5,3736(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
	// add r6,r30,r6
	ctx.r6.u64 = ctx.r30.u64 + ctx.r6.u64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + ctx.r30.u64;
	// bl 0x825f1df8
	ctx.lr = 0x825E16EC;
	sub_825F1DF8(ctx, base);
	// b 0x825e1730
	goto loc_825E1730;
loc_825E16F0:
	// lwz r8,220(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r7,3720(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3720);
	// lwz r11,224(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lwz r8,3732(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// lwz r9,3728(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// lwz r10,3724(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// add r4,r8,r29
	ctx.r4.u64 = ctx.r8.u64 + ctx.r29.u64;
	// lwz r6,3740(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// lwz r5,3736(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// add r8,r10,r11
	ctx.r8.u64 = ctx.r10.u64 + ctx.r11.u64;
	// add r6,r30,r6
	ctx.r6.u64 = ctx.r30.u64 + ctx.r6.u64;
	// add r5,r5,r30
	ctx.r5.u64 = ctx.r5.u64 + ctx.r30.u64;
	// bl 0x825f28f8
	ctx.lr = 0x825E1730;
	sub_825F28F8(ctx, base);
loc_825E1730:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825deb78
	ctx.lr = 0x825E173C;
	sub_825DEB78(ctx, base);
	// lwz r11,204(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// lwz r10,184(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 184);
	// li r8,1
	ctx.r8.s64 = 1;
	// lwz r9,164(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	// li r7,1
	ctx.r7.s64 = 1;
	// lwz r6,220(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// lwz r5,172(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 172);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// lwz r11,19976(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// lwz r3,3732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// lwz r30,15856(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15856);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// mtctr r30
	ctx.ctr.u64 = ctx.r30.u64;
	// bctrl 
	ctx.lr = 0x825E177C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,19976(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19976);
	// lwz r30,208(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// li r9,1
	ctx.r9.s64 = 1;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// lwz r29,196(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 196);
	// lwz r10,168(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	// li r8,1
	ctx.r8.s64 = 1;
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// lwz r7,224(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// lwz r6,176(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 176);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r4,3740(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3740);
	// lwz r3,3736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3736);
	// lwz r28,15852(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15852);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r30.u32);
	// stw r29,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r29.u32);
	// mtctr r28
	ctx.ctr.u64 = ctx.r28.u64;
	// bctrl 
	ctx.lr = 0x825E17C8;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// lwz r11,15564(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15564);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e182c
	if (!ctx.cr6.eq) goto loc_825E182C;
	// lwz r11,14828(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e182c
	if (!ctx.cr6.eq) goto loc_825E182C;
	// lwz r11,3704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r10,3688(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// stw r11,3688(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3688, ctx.r11.u32);
	// stw r10,3704(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3704, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r8,3720(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3720, ctx.r8.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r8,3724(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3724, ctx.r8.u32);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r11,3728(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3728, ctx.r11.u32);
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r11,3776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3776, ctx.r11.u32);
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r11,3780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3780, ctx.r11.u32);
	// lwz r11,8(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r9,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r9.u32);
	// stw r11,3784(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3784, ctx.r11.u32);
loc_825E182C:
	// lwz r11,14824(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// stw r11,14828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14828, ctx.r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
}

__attribute__((alias("__imp__sub_825E183C"))) PPC_WEAK_FUNC(sub_825E183C);
PPC_FUNC_IMPL(__imp__sub_825E183C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E1840"))) PPC_WEAK_FUNC(sub_825E1840);
PPC_FUNC_IMPL(__imp__sub_825E1840) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825E1848;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// li r25,0
	ctx.r25.s64 = 0;
	// mr r27,r25
	ctx.r27.u64 = ctx.r25.u64;
	// lwz r11,15472(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e1870
	if (!ctx.cr6.eq) goto loc_825E1870;
	// bl 0x825d62a8
	ctx.lr = 0x825E1868;
	sub_825D62A8(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825E1870:
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// stw r25,3400(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3400, ctx.r25.u32);
	// li r24,1
	ctx.r24.s64 = 1;
	// li r26,2
	ctx.r26.s64 = 2;
	// bne cr6,0x825e1a0c
	if (!ctx.cr6.eq) goto loc_825E1A0C;
	// lwz r11,3444(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3444);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e18c0
	if (ctx.cr6.eq) goto loc_825E18C0;
	// lwz r3,84(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e18bc
	if (!ctx.cr0.lt) goto loc_825E18BC;
	// bl 0x825d5398
	ctx.lr = 0x825E18BC;
	sub_825D5398(ctx, base);
loc_825E18BC:
	// stw r31,3448(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3448, ctx.r31.u32);
loc_825E18C0:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r26
	ctx.r30.u64 = ctx.r26.u64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825e1918
	if (!ctx.cr6.lt) goto loc_825E1918;
loc_825E18D8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1918
	if (ctx.cr6.eq) goto loc_825E1918;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e1908
	if (!ctx.cr0.lt) goto loc_825E1908;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1908;
	sub_825D5398(ctx, base);
loc_825E1908:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e18d8
	if (ctx.cr6.gt) goto loc_825E18D8;
loc_825E1918:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e1940
	if (!ctx.cr0.lt) goto loc_825E1940;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1940;
	sub_825D5398(ctx, base);
loc_825E1940:
	// lwz r11,15472(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// bne cr6,0x825e1a0c
	if (!ctx.cr6.eq) goto loc_825E1A0C;
	// lwz r11,14792(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 14792);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e1a0c
	if (ctx.cr6.eq) goto loc_825E1A0C;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e19cc
	if (!ctx.cr6.lt) goto loc_825E19CC;
loc_825E1974:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e19cc
	if (ctx.cr6.eq) goto loc_825E19CC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e19bc
	if (!ctx.cr0.lt) goto loc_825E19BC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E19BC;
	sub_825D5398(ctx, base);
loc_825E19BC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e1974
	if (ctx.cr6.gt) goto loc_825E1974;
loc_825E19CC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e1a08
	if (!ctx.cr0.lt) goto loc_825E1A08;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1A08;
	sub_825D5398(ctx, base);
loc_825E1A08:
	// stw r30,14796(r28)
	PPC_STORE_U32(ctx.r28.u32 + 14796, ctx.r30.u32);
loc_825E1A0C:
	// lwz r11,15472(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// bne cr6,0x825e21cc
	if (!ctx.cr6.eq) goto loc_825E21CC;
	// li r11,-1
	ctx.r11.s64 = -1;
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// stw r11,15196(r28)
	PPC_STORE_U32(ctx.r28.u32 + 15196, ctx.r11.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1a94
	if (!ctx.cr6.lt) goto loc_825E1A94;
loc_825E1A3C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1a94
	if (ctx.cr6.eq) goto loc_825E1A94;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e1a84
	if (!ctx.cr0.lt) goto loc_825E1A84;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1A84;
	sub_825D5398(ctx, base);
loc_825E1A84:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e1a3c
	if (ctx.cr6.gt) goto loc_825E1A3C;
loc_825E1A94:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e1ad0
	if (!ctx.cr0.lt) goto loc_825E1AD0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1AD0;
	sub_825D5398(ctx, base);
loc_825E1AD0:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// bne cr6,0x825e1ae0
	if (!ctx.cr6.eq) goto loc_825E1AE0;
	// stw r24,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r24.u32);
	// b 0x825e2298
	goto loc_825E2298;
loc_825E1AE0:
	// lwz r11,14772(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r11,15192(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15192);
	// bne cr6,0x825e1c94
	if (!ctx.cr6.eq) goto loc_825E1C94;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e1b00
	if (!ctx.cr6.eq) goto loc_825E1B00;
	// stw r25,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r25.u32);
	// b 0x825e2298
	goto loc_825E2298;
loc_825E1B00:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1b74
	if (!ctx.cr6.lt) goto loc_825E1B74;
loc_825E1B1C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1b74
	if (ctx.cr6.eq) goto loc_825E1B74;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e1b64
	if (!ctx.cr0.lt) goto loc_825E1B64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1B64;
	sub_825D5398(ctx, base);
loc_825E1B64:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e1b1c
	if (ctx.cr6.gt) goto loc_825E1B1C;
loc_825E1B74:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e1bb0
	if (!ctx.cr0.lt) goto loc_825E1BB0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1BB0;
	sub_825D5398(ctx, base);
loc_825E1BB0:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// bne cr6,0x825e1bc0
	if (!ctx.cr6.eq) goto loc_825E1BC0;
	// stw r25,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r25.u32);
	// b 0x825e2298
	goto loc_825E2298;
loc_825E1BC0:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1c34
	if (!ctx.cr6.lt) goto loc_825E1C34;
loc_825E1BDC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1c34
	if (ctx.cr6.eq) goto loc_825E1C34;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e1c24
	if (!ctx.cr0.lt) goto loc_825E1C24;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1C24;
	sub_825D5398(ctx, base);
loc_825E1C24:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e1bdc
	if (ctx.cr6.gt) goto loc_825E1BDC;
loc_825E1C34:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e1c70
	if (!ctx.cr0.lt) goto loc_825E1C70;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1C70;
	sub_825D5398(ctx, base);
loc_825E1C70:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// bne cr6,0x825e1c88
	if (!ctx.cr6.eq) goto loc_825E1C88;
	// lwz r3,15204(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15204);
	// stw r25,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r25.u32);
	// bl 0x82626248
	ctx.lr = 0x825E1C84;
	sub_82626248(ctx, base);
	// b 0x825e2298
	goto loc_825E2298;
loc_825E1C88:
	// mr r27,r24
	ctx.r27.u64 = ctx.r24.u64;
	// stw r24,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r24.u32);
	// b 0x825e2298
	goto loc_825E2298;
loc_825E1C94:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825e1d5c
	if (!ctx.cr6.eq) goto loc_825E1D5C;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1d10
	if (!ctx.cr6.lt) goto loc_825E1D10;
loc_825E1CB8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1d10
	if (ctx.cr6.eq) goto loc_825E1D10;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e1d00
	if (!ctx.cr0.lt) goto loc_825E1D00;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1D00;
	sub_825D5398(ctx, base);
loc_825E1D00:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e1cb8
	if (ctx.cr6.gt) goto loc_825E1CB8;
loc_825E1D10:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e1d4c
	if (!ctx.cr0.lt) goto loc_825E1D4C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1D4C;
	sub_825D5398(ctx, base);
loc_825E1D4C:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// bne cr6,0x825e1e00
	if (!ctx.cr6.eq) goto loc_825E1E00;
	// stw r25,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r25.u32);
	// b 0x825e1f98
	goto loc_825E1F98;
loc_825E1D5C:
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1dbc
	if (!ctx.cr6.lt) goto loc_825E1DBC;
loc_825E1D64:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1dbc
	if (ctx.cr6.eq) goto loc_825E1DBC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e1dac
	if (!ctx.cr0.lt) goto loc_825E1DAC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1DAC;
	sub_825D5398(ctx, base);
loc_825E1DAC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e1d64
	if (ctx.cr6.gt) goto loc_825E1D64;
loc_825E1DBC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e1df8
	if (!ctx.cr0.lt) goto loc_825E1DF8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1DF8;
	sub_825D5398(ctx, base);
loc_825E1DF8:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// bne cr6,0x825e1e08
	if (!ctx.cr6.eq) goto loc_825E1E08;
loc_825E1E00:
	// stw r26,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r26.u32);
	// b 0x825e1f98
	goto loc_825E1F98;
loc_825E1E08:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1e7c
	if (!ctx.cr6.lt) goto loc_825E1E7C;
loc_825E1E24:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1e7c
	if (ctx.cr6.eq) goto loc_825E1E7C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e1e6c
	if (!ctx.cr0.lt) goto loc_825E1E6C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1E6C;
	sub_825D5398(ctx, base);
loc_825E1E6C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e1e24
	if (ctx.cr6.gt) goto loc_825E1E24;
loc_825E1E7C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e1eb8
	if (!ctx.cr0.lt) goto loc_825E1EB8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1EB8;
	sub_825D5398(ctx, base);
loc_825E1EB8:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// bne cr6,0x825e1ec8
	if (!ctx.cr6.eq) goto loc_825E1EC8;
	// stw r25,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r25.u32);
	// b 0x825e1f98
	goto loc_825E1F98;
loc_825E1EC8:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e1f3c
	if (!ctx.cr6.lt) goto loc_825E1F3C;
loc_825E1EE4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e1f3c
	if (ctx.cr6.eq) goto loc_825E1F3C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e1f2c
	if (!ctx.cr0.lt) goto loc_825E1F2C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1F2C;
	sub_825D5398(ctx, base);
loc_825E1F2C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e1ee4
	if (ctx.cr6.gt) goto loc_825E1EE4;
loc_825E1F3C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e1f78
	if (!ctx.cr0.lt) goto loc_825E1F78;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E1F78;
	sub_825D5398(ctx, base);
loc_825E1F78:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// bne cr6,0x825e1f90
	if (!ctx.cr6.eq) goto loc_825E1F90;
	// lwz r3,15204(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15204);
	// stw r25,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r25.u32);
	// bl 0x82626248
	ctx.lr = 0x825E1F8C;
	sub_82626248(ctx, base);
	// b 0x825e1f98
	goto loc_825E1F98;
loc_825E1F90:
	// mr r27,r24
	ctx.r27.u64 = ctx.r24.u64;
	// stw r24,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r24.u32);
loc_825E1F98:
	// lwz r11,284(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825e2298
	if (!ctx.cr6.eq) goto loc_825E2298;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825e2018
	if (!ctx.cr6.lt) goto loc_825E2018;
loc_825E1FC0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e2018
	if (ctx.cr6.eq) goto loc_825E2018;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2008
	if (!ctx.cr0.lt) goto loc_825E2008;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2008;
	sub_825D5398(ctx, base);
loc_825E2008:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e1fc0
	if (ctx.cr6.gt) goto loc_825E1FC0;
loc_825E2018:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e2054
	if (!ctx.cr0.lt) goto loc_825E2054;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2054;
	sub_825D5398(ctx, base);
loc_825E2054:
	// cmpwi cr6,r30,7
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 7, ctx.xer);
	// bne cr6,0x825e217c
	if (!ctx.cr6.eq) goto loc_825E217C;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// li r30,4
	ctx.r30.s64 = 4;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bge cr6,0x825e20d0
	if (!ctx.cr6.lt) goto loc_825E20D0;
loc_825E2078:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e20d0
	if (ctx.cr6.eq) goto loc_825E20D0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e20c0
	if (!ctx.cr0.lt) goto loc_825E20C0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E20C0;
	sub_825D5398(ctx, base);
loc_825E20C0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2078
	if (ctx.cr6.gt) goto loc_825E2078;
loc_825E20D0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e210c
	if (!ctx.cr0.lt) goto loc_825E210C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E210C;
	sub_825D5398(ctx, base);
loc_825E210C:
	// cmpwi cr6,r30,14
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 14, ctx.xer);
	// beq cr6,0x825e37d4
	if (ctx.cr6.eq) goto loc_825E37D4;
	// cmpwi cr6,r30,15
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 15, ctx.xer);
	// bne cr6,0x825e2124
	if (!ctx.cr6.eq) goto loc_825E2124;
	// stw r24,3400(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3400, ctx.r24.u32);
	// b 0x825e2298
	goto loc_825E2298;
loc_825E2124:
	// addi r11,r30,112
	ctx.r11.s64 = ctx.r30.s64 + 112;
	// lwz r7,14772(r28)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r28.u32 + 14772);
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r11,r11,-112
	ctx.r11.s64 = ctx.r11.s64 + -112;
	// addi r10,r10,3568
	ctx.r10.s64 = ctx.r10.s64 + 3568;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// lis r8,-32244
	ctx.r8.s64 = -2113142784;
	// addi r9,r9,3624
	ctx.r9.s64 = ctx.r9.s64 + 3624;
	// addi r8,r8,-22904
	ctx.r8.s64 = ctx.r8.s64 + -22904;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// stw r10,3392(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3392, ctx.r10.u32);
	// lwzx r11,r11,r9
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r11,3388(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3388, ctx.r11.u32);
	// lwz r11,-4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// stw r11,14776(r28)
	PPC_STORE_U32(ctx.r28.u32 + 14776, ctx.r11.u32);
	// bne cr6,0x825e2298
	if (!ctx.cr6.eq) goto loc_825E2298;
	// stw r24,14772(r28)
	PPC_STORE_U32(ctx.r28.u32 + 14772, ctx.r24.u32);
	// b 0x825e2298
	goto loc_825E2298;
loc_825E217C:
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// lwz r7,14772(r28)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r28.u32 + 14772);
	// rlwinm r11,r30,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,3512
	ctx.r10.s64 = ctx.r10.s64 + 3512;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// lis r8,-32244
	ctx.r8.s64 = -2113142784;
	// addi r9,r9,3540
	ctx.r9.s64 = ctx.r9.s64 + 3540;
	// addi r8,r8,-22904
	ctx.r8.s64 = ctx.r8.s64 + -22904;
	// lwzx r10,r11,r10
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// stw r10,3392(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3392, ctx.r10.u32);
	// lwzx r11,r11,r9
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r9.u32);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// stw r11,3388(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3388, ctx.r11.u32);
	// lwz r11,-4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// stw r11,14776(r28)
	PPC_STORE_U32(ctx.r28.u32 + 14776, ctx.r11.u32);
	// bne cr6,0x825e2298
	if (!ctx.cr6.eq) goto loc_825E2298;
	// stw r24,14772(r28)
	PPC_STORE_U32(ctx.r28.u32 + 14772, ctx.r24.u32);
	// b 0x825e2298
	goto loc_825E2298;
loc_825E21CC:
	// addi r11,r11,-5
	ctx.r11.s64 = ctx.r11.s64 + -5;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// xori r11,r11,1
	ctx.r11.u64 = ctx.r11.u64 ^ 1;
	// addi r30,r11,1
	ctx.r30.s64 = ctx.r11.s64 + 1;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825e21f8
	if (!ctx.cr6.eq) goto loc_825E21F8;
	// mr r30,r25
	ctx.r30.u64 = ctx.r25.u64;
	// b 0x825e2294
	goto loc_825E2294;
loc_825E21F8:
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e2258
	if (!ctx.cr6.gt) goto loc_825E2258;
loc_825E2200:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e2258
	if (ctx.cr6.eq) goto loc_825E2258;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2248
	if (!ctx.cr0.lt) goto loc_825E2248;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2248;
	sub_825D5398(ctx, base);
loc_825E2248:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2200
	if (ctx.cr6.gt) goto loc_825E2200;
loc_825E2258:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e2294
	if (!ctx.cr0.lt) goto loc_825E2294;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2294;
	sub_825D5398(ctx, base);
loc_825E2294:
	// stw r30,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r30.u32);
loc_825E2298:
	// lwz r11,3400(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3400);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e22ac
	if (ctx.cr6.eq) goto loc_825E22AC;
	// li r11,4
	ctx.r11.s64 = 4;
	// stw r11,284(r28)
	PPC_STORE_U32(ctx.r28.u32 + 284, ctx.r11.u32);
loc_825E22AC:
	// lwz r11,284(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e22d0
	if (ctx.cr6.eq) goto loc_825E22D0;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// beq cr6,0x825e22d0
	if (ctx.cr6.eq) goto loc_825E22D0;
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825e22d0
	if (ctx.cr6.eq) goto loc_825E22D0;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bne cr6,0x825e37d4
	if (!ctx.cr6.eq) goto loc_825E37D4;
loc_825E22D0:
	// lwz r10,15472(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// bne cr6,0x825e22ec
	if (!ctx.cr6.eq) goto loc_825E22EC;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e22ec
	if (ctx.cr6.eq) goto loc_825E22EC;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825e37d4
	if (!ctx.cr6.eq) goto loc_825E37D4;
loc_825E22EC:
	// cmpwi cr6,r10,5
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 5, ctx.xer);
	// blt cr6,0x825e2394
	if (ctx.cr6.lt) goto loc_825E2394;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e2304
	if (ctx.cr6.eq) goto loc_825E2304;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// bne cr6,0x825e2394
	if (!ctx.cr6.eq) goto loc_825E2394;
loc_825E2304:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// li r30,7
	ctx.r30.s64 = 7;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,7
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 7, ctx.xer);
	// bge cr6,0x825e235c
	if (!ctx.cr6.lt) goto loc_825E235C;
loc_825E231C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e235c
	if (ctx.cr6.eq) goto loc_825E235C;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e234c
	if (!ctx.cr0.lt) goto loc_825E234C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E234C;
	sub_825D5398(ctx, base);
loc_825E234C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e231c
	if (ctx.cr6.gt) goto loc_825E231C;
loc_825E235C:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e2384
	if (!ctx.cr0.lt) goto loc_825E2384;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2384;
	sub_825D5398(ctx, base);
loc_825E2384:
	// lwz r11,84(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e37d4
	if (!ctx.cr6.eq) goto loc_825E37D4;
loc_825E2394:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// li r30,5
	ctx.r30.s64 = 5;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825e2408
	if (!ctx.cr6.lt) goto loc_825E2408;
loc_825E23B0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e2408
	if (ctx.cr6.eq) goto loc_825E2408;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e23f8
	if (!ctx.cr0.lt) goto loc_825E23F8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E23F8;
	sub_825D5398(ctx, base);
loc_825E23F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e23b0
	if (ctx.cr6.gt) goto loc_825E23B0;
loc_825E2408:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e2444
	if (!ctx.cr0.lt) goto loc_825E2444;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2444;
	sub_825D5398(ctx, base);
loc_825E2444:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e37d4
	if (!ctx.cr6.eq) goto loc_825E37D4;
	// lwz r10,15472(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// cmpwi cr6,r10,6
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 6, ctx.xer);
	// blt cr6,0x825e2708
	if (ctx.cr6.lt) goto loc_825E2708;
	// cmpwi cr6,r30,8
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 8, ctx.xer);
	// stw r30,3952(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3952, ctx.r30.u32);
	// bgt cr6,0x825e2524
	if (ctx.cr6.gt) goto loc_825E2524;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e24e0
	if (!ctx.cr6.lt) goto loc_825E24E0;
loc_825E2488:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e24e0
	if (ctx.cr6.eq) goto loc_825E24E0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e24d0
	if (!ctx.cr0.lt) goto loc_825E24D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E24D0;
	sub_825D5398(ctx, base);
loc_825E24D0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2488
	if (ctx.cr6.gt) goto loc_825E2488;
loc_825E24E0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e251c
	if (!ctx.cr0.lt) goto loc_825E251C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E251C;
	sub_825D5398(ctx, base);
loc_825E251C:
	// stw r30,252(r28)
	PPC_STORE_U32(ctx.r28.u32 + 252, ctx.r30.u32);
	// b 0x825e2528
	goto loc_825E2528;
loc_825E2524:
	// stw r25,252(r28)
	PPC_STORE_U32(ctx.r28.u32 + 252, ctx.r25.u32);
loc_825E2528:
	// lwz r11,3440(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3440);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e25e8
	if (ctx.cr6.eq) goto loc_825E25E8;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e25a8
	if (!ctx.cr6.lt) goto loc_825E25A8;
loc_825E2550:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e25a8
	if (ctx.cr6.eq) goto loc_825E25A8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2598
	if (!ctx.cr0.lt) goto loc_825E2598;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2598;
	sub_825D5398(ctx, base);
loc_825E2598:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2550
	if (ctx.cr6.gt) goto loc_825E2550;
loc_825E25A8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e25e4
	if (!ctx.cr0.lt) goto loc_825E25E4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E25E4;
	sub_825D5398(ctx, base);
loc_825E25E4:
	// stw r30,3428(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3428, ctx.r30.u32);
loc_825E25E8:
	// lwz r11,3432(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3432);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r11,3952(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3952);
	// bne cr6,0x825e2620
	if (!ctx.cr6.eq) goto loc_825E2620;
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// bgt cr6,0x825e2608
	if (ctx.cr6.gt) goto loc_825E2608;
	// stw r24,3428(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3428, ctx.r24.u32);
	// b 0x825e2620
	goto loc_825E2620;
loc_825E2608:
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// stw r25,3428(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3428, ctx.r25.u32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r10,3680
	ctx.r10.s64 = ctx.r10.s64 + 3680;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r11,-4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
loc_825E2620:
	// lwz r10,2972(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2972);
	// stw r11,248(r28)
	PPC_STORE_U32(ctx.r28.u32 + 248, ctx.r11.u32);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r25,2968(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2968, ctx.r25.u32);
	// beq cr6,0x825e2680
	if (ctx.cr6.eq) goto loc_825E2680;
	// lwz r10,284(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 284);
	// cmpwi cr6,r10,2
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 2, ctx.xer);
	// bne cr6,0x825e264c
	if (!ctx.cr6.eq) goto loc_825E264C;
	// lwz r9,3400(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3400);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x825e2680
	if (ctx.cr6.eq) goto loc_825E2680;
loc_825E264C:
	// cmpwi cr6,r11,9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 9, ctx.xer);
	// blt cr6,0x825e265c
	if (ctx.cr6.lt) goto loc_825E265C;
	// stw r24,2968(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2968, ctx.r24.u32);
	// b 0x825e2680
	goto loc_825E2680;
loc_825E265C:
	// lwz r9,20056(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 20056);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x825e2680
	if (ctx.cr6.eq) goto loc_825E2680;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825e2678
	if (ctx.cr6.eq) goto loc_825E2678;
	// cmpwi cr6,r10,4
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 4, ctx.xer);
	// bne cr6,0x825e2680
	if (!ctx.cr6.eq) goto loc_825E2680;
loc_825E2678:
	// li r10,7
	ctx.r10.s64 = 7;
	// stw r10,2968(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2968, ctx.r10.u32);
loc_825E2680:
	// lwz r10,2968(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2968);
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825e26b4
	if (ctx.cr6.eq) goto loc_825E26B4;
	// lwz r10,1900(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1900);
	// sth r25,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r25.u16);
	// lwz r10,1900(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1900);
	// sth r25,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r25.u16);
	// lwz r10,1904(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1904);
	// sth r25,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r25.u16);
	// lwz r10,1904(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1904);
	// sth r25,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r25.u16);
	// b 0x825e26d8
	goto loc_825E26D8;
loc_825E26B4:
	// lwz r9,1900(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1900);
	// li r10,128
	ctx.r10.s64 = 128;
	// sth r10,16(r9)
	PPC_STORE_U16(ctx.r9.u32 + 16, ctx.r10.u16);
	// lwz r9,1900(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1900);
	// sth r10,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r10.u16);
	// lwz r9,1904(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1904);
	// sth r10,16(r9)
	PPC_STORE_U16(ctx.r9.u32 + 16, ctx.r10.u16);
	// lwz r9,1904(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1904);
	// sth r10,0(r9)
	PPC_STORE_U16(ctx.r9.u32 + 0, ctx.r10.u16);
loc_825E26D8:
	// lwz r9,3428(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3428);
	// addi r10,r28,3988
	ctx.r10.s64 = ctx.r28.s64 + 3988;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x825e26ec
	if (!ctx.cr6.eq) goto loc_825E26EC;
	// addi r10,r28,5268
	ctx.r10.s64 = ctx.r28.s64 + 5268;
loc_825E26EC:
	// stw r10,6548(r28)
	PPC_STORE_U32(ctx.r28.u32 + 6548, ctx.r10.u32);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// addi r10,r28,6560
	ctx.r10.s64 = ctx.r28.s64 + 6560;
	// bne cr6,0x825e2700
	if (!ctx.cr6.eq) goto loc_825E2700;
	// addi r10,r28,10656
	ctx.r10.s64 = ctx.r28.s64 + 10656;
loc_825E2700:
	// stw r10,14752(r28)
	PPC_STORE_U32(ctx.r28.u32 + 14752, ctx.r10.u32);
	// b 0x825e2718
	goto loc_825E2718;
loc_825E2708:
	// addi r10,r28,5268
	ctx.r10.s64 = ctx.r28.s64 + 5268;
	// addi r9,r28,10656
	ctx.r9.s64 = ctx.r28.s64 + 10656;
	// stw r10,6548(r28)
	PPC_STORE_U32(ctx.r28.u32 + 6548, ctx.r10.u32);
	// stw r9,14752(r28)
	PPC_STORE_U32(ctx.r28.u32 + 14752, ctx.r9.u32);
loc_825E2718:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,248(r28)
	PPC_STORE_U32(ctx.r28.u32 + 248, ctx.r11.u32);
	// ble cr6,0x825e37d4
	if (!ctx.cr6.gt) goto loc_825E37D4;
	// cmpwi cr6,r11,31
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 31, ctx.xer);
	// bgt cr6,0x825e37d4
	if (ctx.cr6.gt) goto loc_825E37D4;
	// lwz r11,3952(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3952);
	// cmpwi cr6,r11,8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 8, ctx.xer);
	// bgt cr6,0x825e2754
	if (ctx.cr6.gt) goto loc_825E2754;
	// lwz r11,15472(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// bne cr6,0x825e2754
	if (!ctx.cr6.eq) goto loc_825E2754;
	// addi r11,r28,2840
	ctx.r11.s64 = ctx.r28.s64 + 2840;
	// addi r10,r28,2800
	ctx.r10.s64 = ctx.r28.s64 + 2800;
	// stw r11,2904(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2904, ctx.r11.u32);
	// stw r10,2916(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2916, ctx.r10.u32);
loc_825E2754:
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// beq cr6,0x825e281c
	if (ctx.cr6.eq) goto loc_825E281C;
	// lwz r11,15192(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15192);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e281c
	if (ctx.cr6.eq) goto loc_825E281C;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e27dc
	if (!ctx.cr6.lt) goto loc_825E27DC;
loc_825E2784:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e27dc
	if (ctx.cr6.eq) goto loc_825E27DC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e27cc
	if (!ctx.cr0.lt) goto loc_825E27CC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E27CC;
	sub_825D5398(ctx, base);
loc_825E27CC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2784
	if (ctx.cr6.gt) goto loc_825E2784;
loc_825E27DC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e2818
	if (!ctx.cr0.lt) goto loc_825E2818;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2818;
	sub_825D5398(ctx, base);
loc_825E2818:
	// stw r30,15196(r28)
	PPC_STORE_U32(ctx.r28.u32 + 15196, ctx.r30.u32);
loc_825E281C:
	// lwz r11,20864(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 20864);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e2a74
	if (ctx.cr6.eq) goto loc_825E2A74;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e289c
	if (!ctx.cr6.lt) goto loc_825E289C;
loc_825E2844:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e289c
	if (ctx.cr6.eq) goto loc_825E289C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e288c
	if (!ctx.cr0.lt) goto loc_825E288C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E288C;
	sub_825D5398(ctx, base);
loc_825E288C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2844
	if (ctx.cr6.gt) goto loc_825E2844;
loc_825E289C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e28d8
	if (!ctx.cr0.lt) goto loc_825E28D8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E28D8;
	sub_825D5398(ctx, base);
loc_825E28D8:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// stw r30,404(r28)
	PPC_STORE_U32(ctx.r28.u32 + 404, ctx.r30.u32);
	// beq cr6,0x825e29a0
	if (ctx.cr6.eq) goto loc_825E29A0;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e2958
	if (!ctx.cr6.lt) goto loc_825E2958;
loc_825E2900:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e2958
	if (ctx.cr6.eq) goto loc_825E2958;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2948
	if (!ctx.cr0.lt) goto loc_825E2948;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2948;
	sub_825D5398(ctx, base);
loc_825E2948:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2900
	if (ctx.cr6.gt) goto loc_825E2900;
loc_825E2958:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e2994
	if (!ctx.cr0.lt) goto loc_825E2994;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2994;
	sub_825D5398(ctx, base);
loc_825E2994:
	// lwz r11,404(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 404);
	// add r11,r30,r11
	ctx.r11.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stw r11,404(r28)
	PPC_STORE_U32(ctx.r28.u32 + 404, ctx.r11.u32);
loc_825E29A0:
	// lwz r11,404(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 404);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825e2a68
	if (!ctx.cr6.eq) goto loc_825E2A68;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e2a20
	if (!ctx.cr6.lt) goto loc_825E2A20;
loc_825E29C8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e2a20
	if (ctx.cr6.eq) goto loc_825E2A20;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2a10
	if (!ctx.cr0.lt) goto loc_825E2A10;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2A10;
	sub_825D5398(ctx, base);
loc_825E2A10:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e29c8
	if (ctx.cr6.gt) goto loc_825E29C8;
loc_825E2A20:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e2a5c
	if (!ctx.cr0.lt) goto loc_825E2A5C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2A5C;
	sub_825D5398(ctx, base);
loc_825E2A5C:
	// lwz r11,404(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 404);
	// add r11,r30,r11
	ctx.r11.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stw r11,404(r28)
	PPC_STORE_U32(ctx.r28.u32 + 404, ctx.r11.u32);
loc_825E2A68:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// lwz r4,404(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 404);
	// bl 0x825ed4d0
	ctx.lr = 0x825E2A74;
	sub_825ED4D0(ctx, base);
loc_825E2A74:
	// lwz r11,284(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 284);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// beq cr6,0x825e2bc8
	if (ctx.cr6.eq) goto loc_825E2BC8;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825e2bc8
	if (ctx.cr6.eq) goto loc_825E2BC8;
	// lwz r11,15472(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// bne cr6,0x825e2bc8
	if (!ctx.cr6.eq) goto loc_825E2BC8;
	// lwz r11,14820(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 14820);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e2bc8
	if (ctx.cr6.eq) goto loc_825E2BC8;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r11,3924(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825e2b24
	if (ctx.cr6.eq) goto loc_825E2B24;
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e2b88
	if (!ctx.cr6.lt) goto loc_825E2B88;
loc_825E2AC8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e2b88
	if (ctx.cr6.eq) goto loc_825E2B88;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2b10
	if (!ctx.cr0.lt) goto loc_825E2B10;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2B10;
	sub_825D5398(ctx, base);
loc_825E2B10:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2ac8
	if (ctx.cr6.gt) goto loc_825E2AC8;
	// b 0x825e2b88
	goto loc_825E2B88;
loc_825E2B24:
	// mr r30,r26
	ctx.r30.u64 = ctx.r26.u64;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825e2b88
	if (!ctx.cr6.lt) goto loc_825E2B88;
loc_825E2B30:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e2b88
	if (ctx.cr6.eq) goto loc_825E2B88;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2b78
	if (!ctx.cr0.lt) goto loc_825E2B78;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2B78;
	sub_825D5398(ctx, base);
loc_825E2B78:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2b30
	if (ctx.cr6.gt) goto loc_825E2B30;
loc_825E2B88:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2bc4
	if (!ctx.cr0.lt) goto loc_825E2BC4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2BC4;
	sub_825D5398(ctx, base);
loc_825E2BC4:
	// stw r30,14824(r28)
	PPC_STORE_U32(ctx.r28.u32 + 14824, ctx.r30.u32);
loc_825E2BC8:
	// lwz r4,14824(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 14824);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// beq cr6,0x825e2be4
	if (ctx.cr6.eq) goto loc_825E2BE4;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x825f1cd8
	ctx.lr = 0x825E2BDC;
	sub_825F1CD8(ctx, base);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x825f17a0
	ctx.lr = 0x825E2BE4;
	sub_825F17A0(ctx, base);
loc_825E2BE4:
	// lwz r11,3924(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e2d58
	if (ctx.cr6.eq) goto loc_825E2D58;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e2c64
	if (!ctx.cr6.lt) goto loc_825E2C64;
loc_825E2C0C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e2c64
	if (ctx.cr6.eq) goto loc_825E2C64;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2c54
	if (!ctx.cr0.lt) goto loc_825E2C54;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2C54;
	sub_825D5398(ctx, base);
loc_825E2C54:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2c0c
	if (ctx.cr6.gt) goto loc_825E2C0C;
loc_825E2C64:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e2ca0
	if (!ctx.cr0.lt) goto loc_825E2CA0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2CA0;
	sub_825D5398(ctx, base);
loc_825E2CA0:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825e2d1c
	if (ctx.cr6.eq) goto loc_825E2D1C;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x8263db28
	ctx.lr = 0x825E2CB0;
	sub_8263DB28(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x825d75b8
	ctx.lr = 0x825E2CBC;
	sub_825D75B8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e37d8
	if (!ctx.cr6.eq) goto loc_825E37D8;
	// lwz r11,348(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 348);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e2d58
	if (ctx.cr6.eq) goto loc_825E2D58;
	// lwz r11,144(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 144);
	// mr r9,r25
	ctx.r9.u64 = ctx.r25.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825e2d58
	if (!ctx.cr6.gt) goto loc_825E2D58;
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
loc_825E2CE4:
	// lwz r11,268(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 268);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// rlwimi r7,r8,10,22,22
	ctx.r7.u64 = (__builtin_rotateleft32(ctx.r8.u32, 10) & 0x200) | (ctx.r7.u64 & 0xFFFFFFFFFFFFFDFF);
	// rlwinm r8,r7,0,24,22
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFFFFFFFFFFFFFEFF;
	// rlwinm r8,r8,0,22,20
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFFFBFF;
	// stw r8,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r8.u32);
	// lwz r11,144(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 144);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825e2ce4
	if (ctx.cr6.lt) goto loc_825E2CE4;
	// b 0x825e2d58
	goto loc_825E2D58;
loc_825E2D1C:
	// lwz r11,144(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 144);
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825e2d58
	if (!ctx.cr6.gt) goto loc_825E2D58;
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_825E2D30:
	// lwz r9,268(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 268);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r11,r11,20
	ctx.r11.s64 = ctx.r11.s64 + 20;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r8,r8,0,24,20
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFFFFFFFF8FF;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// lwz r9,144(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 144);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x825e2d30
	if (ctx.cr6.lt) goto loc_825E2D30;
loc_825E2D58:
	// lwz r11,284(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e318c
	if (ctx.cr6.eq) goto loc_825E318C;
	// cmpwi cr6,r11,4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 4, ctx.xer);
	// beq cr6,0x825e318c
	if (ctx.cr6.eq) goto loc_825E318C;
	// lwz r11,15472(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// bne cr6,0x825e2d90
	if (!ctx.cr6.eq) goto loc_825E2D90;
	// bl 0x825dec58
	ctx.lr = 0x825E2D80;
	sub_825DEC58(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825e2d94
	if (ctx.cr6.eq) goto loc_825E2D94;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825E2D90:
	// bl 0x825dcbd8
	ctx.lr = 0x825E2D94;
	sub_825DCBD8(ctx, base);
loc_825E2D94:
	// lwz r11,15472(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// blt cr6,0x825e3170
	if (ctx.cr6.lt) goto loc_825E3170;
	// lwz r11,396(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 396);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e2e60
	if (ctx.cr6.eq) goto loc_825E2E60;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e2e20
	if (!ctx.cr6.lt) goto loc_825E2E20;
loc_825E2DC8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e2e20
	if (ctx.cr6.eq) goto loc_825E2E20;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2e10
	if (!ctx.cr0.lt) goto loc_825E2E10;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2E10;
	sub_825D5398(ctx, base);
loc_825E2E10:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2dc8
	if (ctx.cr6.gt) goto loc_825E2DC8;
loc_825E2E20:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e2e5c
	if (!ctx.cr0.lt) goto loc_825E2E5C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2E5C;
	sub_825D5398(ctx, base);
loc_825E2E5C:
	// stw r30,392(r28)
	PPC_STORE_U32(ctx.r28.u32 + 392, ctx.r30.u32);
loc_825E2E60:
	// lwz r11,392(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 392);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e2fe4
	if (!ctx.cr6.eq) goto loc_825E2FE4;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e2ee0
	if (!ctx.cr6.lt) goto loc_825E2EE0;
loc_825E2E88:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e2ee0
	if (ctx.cr6.eq) goto loc_825E2EE0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2ed0
	if (!ctx.cr0.lt) goto loc_825E2ED0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2ED0;
	sub_825D5398(ctx, base);
loc_825E2ED0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2e88
	if (ctx.cr6.gt) goto loc_825E2E88;
loc_825E2EE0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e2f1c
	if (!ctx.cr0.lt) goto loc_825E2F1C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2F1C;
	sub_825D5398(ctx, base);
loc_825E2F1C:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// stw r30,2928(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2928, ctx.r30.u32);
	// beq cr6,0x825e2fe4
	if (ctx.cr6.eq) goto loc_825E2FE4;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e2f9c
	if (!ctx.cr6.lt) goto loc_825E2F9C;
loc_825E2F44:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e2f9c
	if (ctx.cr6.eq) goto loc_825E2F9C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e2f8c
	if (!ctx.cr0.lt) goto loc_825E2F8C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2F8C;
	sub_825D5398(ctx, base);
loc_825E2F8C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e2f44
	if (ctx.cr6.gt) goto loc_825E2F44;
loc_825E2F9C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e2fd8
	if (!ctx.cr0.lt) goto loc_825E2FD8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E2FD8;
	sub_825D5398(ctx, base);
loc_825E2FD8:
	// lwz r11,2928(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2928);
	// add r11,r30,r11
	ctx.r11.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stw r11,2928(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2928, ctx.r11.u32);
loc_825E2FE4:
	// lwz r11,2928(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2928);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// stw r11,2936(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2936, ctx.r11.u32);
	// stw r11,2932(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2932, ctx.r11.u32);
	// stw r11,2948(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2948, ctx.r11.u32);
	// stw r11,2944(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2944, ctx.r11.u32);
	// stw r11,2940(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2940, ctx.r11.u32);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e3070
	if (!ctx.cr6.lt) goto loc_825E3070;
loc_825E3018:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e3070
	if (ctx.cr6.eq) goto loc_825E3070;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e3060
	if (!ctx.cr0.lt) goto loc_825E3060;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E3060;
	sub_825D5398(ctx, base);
loc_825E3060:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e3018
	if (ctx.cr6.gt) goto loc_825E3018;
loc_825E3070:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e30ac
	if (!ctx.cr0.lt) goto loc_825E30AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E30AC;
	sub_825D5398(ctx, base);
loc_825E30AC:
	// lwz r11,15472(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// stw r30,2088(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2088, ctx.r30.u32);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// bge cr6,0x825e3170
	if (!ctx.cr6.lt) goto loc_825E3170;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e3130
	if (!ctx.cr6.lt) goto loc_825E3130;
loc_825E30D8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e3130
	if (ctx.cr6.eq) goto loc_825E3130;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e3120
	if (!ctx.cr0.lt) goto loc_825E3120;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E3120;
	sub_825D5398(ctx, base);
loc_825E3120:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e30d8
	if (ctx.cr6.gt) goto loc_825E30D8;
loc_825E3130:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e316c
	if (!ctx.cr0.lt) goto loc_825E316C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E316C;
	sub_825D5398(ctx, base);
loc_825E316C:
	// stw r30,2036(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2036, ctx.r30.u32);
loc_825E3170:
	// lwz r11,284(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 284);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825e37b8
	if (!ctx.cr6.eq) goto loc_825E37B8;
	// lwz r11,3904(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3904);
	// xori r11,r11,1
	ctx.r11.u64 = ctx.r11.u64 ^ 1;
	// stw r11,3904(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3904, ctx.r11.u32);
	// b 0x825e37b8
	goto loc_825E37B8;
loc_825E318C:
	// lwz r11,15472(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// bge cr6,0x825e324c
	if (!ctx.cr6.lt) goto loc_825E324C;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// li r30,5
	ctx.r30.s64 = 5;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825e320c
	if (!ctx.cr6.lt) goto loc_825E320C;
loc_825E31B4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e320c
	if (ctx.cr6.eq) goto loc_825E320C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e31fc
	if (!ctx.cr0.lt) goto loc_825E31FC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E31FC;
	sub_825D5398(ctx, base);
loc_825E31FC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e31b4
	if (ctx.cr6.gt) goto loc_825E31B4;
loc_825E320C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3248
	if (!ctx.cr0.lt) goto loc_825E3248;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E3248;
	sub_825D5398(ctx, base);
loc_825E3248:
	// stw r30,15464(r28)
	PPC_STORE_U32(ctx.r28.u32 + 15464, ctx.r30.u32);
loc_825E324C:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// lwz r4,15464(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15464);
	// bl 0x825dea98
	ctx.lr = 0x825E3258;
	sub_825DEA98(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e37d8
	if (!ctx.cr6.eq) goto loc_825E37D8;
	// lwz r9,15472(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// cmpwi cr6,r9,4
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 4, ctx.xer);
	// blt cr6,0x825e32e8
	if (ctx.cr6.lt) goto loc_825E32E8;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x825d58a8
	ctx.lr = 0x825E3274;
	sub_825D58A8(ctx, base);
	// lwz r11,84(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e37d8
	if (!ctx.cr6.eq) goto loc_825E37D8;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e37d8
	if (!ctx.cr6.eq) goto loc_825E37D8;
	// lwz r9,15472(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15472);
	// cmpwi cr6,r9,5
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 5, ctx.xer);
	// blt cr6,0x825e32a0
	if (ctx.cr6.lt) goto loc_825E32A0;
	// stw r25,400(r28)
	PPC_STORE_U32(ctx.r28.u32 + 400, ctx.r25.u32);
	// b 0x825e32e8
	goto loc_825E32E8;
loc_825E32A0:
	// lwz r10,3660(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3660);
	// mr r11,r24
	ctx.r11.u64 = ctx.r24.u64;
	// cmpwi cr6,r10,50
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 50, ctx.xer);
	// bgt cr6,0x825e32b4
	if (ctx.cr6.gt) goto loc_825E32B4;
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_825E32B4:
	// cmpwi cr6,r10,128
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 128, ctx.xer);
	// stw r11,396(r28)
	PPC_STORE_U32(ctx.r28.u32 + 396, ctx.r11.u32);
	// bgt cr6,0x825e32e0
	if (ctx.cr6.gt) goto loc_825E32E0;
	// lwz r11,160(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 160);
	// lis r8,1
	ctx.r8.s64 = 65536;
	// lwz r10,156(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 156);
	// ori r8,r8,11264
	ctx.r8.u64 = ctx.r8.u64 | 11264;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// mr r11,r24
	ctx.r11.u64 = ctx.r24.u64;
	// blt cr6,0x825e32e4
	if (ctx.cr6.lt) goto loc_825E32E4;
loc_825E32E0:
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
loc_825E32E4:
	// stw r11,400(r28)
	PPC_STORE_U32(ctx.r28.u32 + 400, ctx.r11.u32);
loc_825E32E8:
	// lwz r11,3948(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3948);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e3778
	if (!ctx.cr6.eq) goto loc_825E3778;
	// cmpwi cr6,r9,3
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 3, ctx.xer);
	// blt cr6,0x825e3778
	if (ctx.cr6.lt) goto loc_825E3778;
	// lwz r11,396(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 396);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e33bc
	if (ctx.cr6.eq) goto loc_825E33BC;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e337c
	if (!ctx.cr6.lt) goto loc_825E337C;
loc_825E3324:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e337c
	if (ctx.cr6.eq) goto loc_825E337C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e336c
	if (!ctx.cr0.lt) goto loc_825E336C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E336C;
	sub_825D5398(ctx, base);
loc_825E336C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e3324
	if (ctx.cr6.gt) goto loc_825E3324;
loc_825E337C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e33b8
	if (!ctx.cr0.lt) goto loc_825E33B8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E33B8;
	sub_825D5398(ctx, base);
loc_825E33B8:
	// stw r30,392(r28)
	PPC_STORE_U32(ctx.r28.u32 + 392, ctx.r30.u32);
loc_825E33BC:
	// lwz r11,392(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 392);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e36c4
	if (!ctx.cr6.eq) goto loc_825E36C4;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e343c
	if (!ctx.cr6.lt) goto loc_825E343C;
loc_825E33E4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e343c
	if (ctx.cr6.eq) goto loc_825E343C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e342c
	if (!ctx.cr0.lt) goto loc_825E342C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E342C;
	sub_825D5398(ctx, base);
loc_825E342C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e33e4
	if (ctx.cr6.gt) goto loc_825E33E4;
loc_825E343C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3478
	if (!ctx.cr0.lt) goto loc_825E3478;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E3478;
	sub_825D5398(ctx, base);
loc_825E3478:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// stw r30,2928(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2928, ctx.r30.u32);
	// beq cr6,0x825e3540
	if (ctx.cr6.eq) goto loc_825E3540;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e34f8
	if (!ctx.cr6.lt) goto loc_825E34F8;
loc_825E34A0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e34f8
	if (ctx.cr6.eq) goto loc_825E34F8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e34e8
	if (!ctx.cr0.lt) goto loc_825E34E8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E34E8;
	sub_825D5398(ctx, base);
loc_825E34E8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e34a0
	if (ctx.cr6.gt) goto loc_825E34A0;
loc_825E34F8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3534
	if (!ctx.cr0.lt) goto loc_825E3534;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E3534;
	sub_825D5398(ctx, base);
loc_825E3534:
	// lwz r11,2928(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2928);
	// add r11,r30,r11
	ctx.r11.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stw r11,2928(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2928, ctx.r11.u32);
loc_825E3540:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e35b4
	if (!ctx.cr6.lt) goto loc_825E35B4;
loc_825E355C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e35b4
	if (ctx.cr6.eq) goto loc_825E35B4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e35a4
	if (!ctx.cr0.lt) goto loc_825E35A4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E35A4;
	sub_825D5398(ctx, base);
loc_825E35A4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e355c
	if (ctx.cr6.gt) goto loc_825E355C;
loc_825E35B4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e35f0
	if (!ctx.cr0.lt) goto loc_825E35F0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E35F0;
	sub_825D5398(ctx, base);
loc_825E35F0:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// stw r30,2940(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2940, ctx.r30.u32);
	// beq cr6,0x825e36b8
	if (ctx.cr6.eq) goto loc_825E36B8;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e3670
	if (!ctx.cr6.lt) goto loc_825E3670;
loc_825E3618:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e3670
	if (ctx.cr6.eq) goto loc_825E3670;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e3660
	if (!ctx.cr0.lt) goto loc_825E3660;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E3660;
	sub_825D5398(ctx, base);
loc_825E3660:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e3618
	if (ctx.cr6.gt) goto loc_825E3618;
loc_825E3670:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e36ac
	if (!ctx.cr0.lt) goto loc_825E36AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E36AC;
	sub_825D5398(ctx, base);
loc_825E36AC:
	// lwz r11,2940(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2940);
	// add r11,r30,r11
	ctx.r11.u64 = ctx.r30.u64 + ctx.r11.u64;
	// stw r11,2940(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2940, ctx.r11.u32);
loc_825E36B8:
	// lwz r11,2940(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 2940);
	// stw r11,2948(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2948, ctx.r11.u32);
	// stw r11,2944(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2944, ctx.r11.u32);
loc_825E36C4:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r30,r24
	ctx.r30.u64 = ctx.r24.u64;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e3738
	if (!ctx.cr6.lt) goto loc_825E3738;
loc_825E36E0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e3738
	if (ctx.cr6.eq) goto loc_825E3738;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e3728
	if (!ctx.cr0.lt) goto loc_825E3728;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E3728;
	sub_825D5398(ctx, base);
loc_825E3728:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e36e0
	if (ctx.cr6.gt) goto loc_825E36E0;
loc_825E3738:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3774
	if (!ctx.cr0.lt) goto loc_825E3774;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E3774;
	sub_825D5398(ctx, base);
loc_825E3774:
	// stw r30,2088(r28)
	PPC_STORE_U32(ctx.r28.u32 + 2088, ctx.r30.u32);
loc_825E3778:
	// lwz r11,20056(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 20056);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e3790
	if (!ctx.cr6.eq) goto loc_825E3790;
	// lwz r11,3924(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e37ac
	if (ctx.cr6.eq) goto loc_825E37AC;
loc_825E3790:
	// lwz r11,3980(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3980);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e37ac
	if (ctx.cr6.eq) goto loc_825E37AC;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x825d8180
	ctx.lr = 0x825E37A8;
	sub_825D8180(ctx, base);
	// b 0x825e37b4
	goto loc_825E37B4;
loc_825E37AC:
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82601498
	ctx.lr = 0x825E37B4;
	sub_82601498(ctx, base);
loc_825E37B4:
	// stw r24,3904(r28)
	PPC_STORE_U32(ctx.r28.u32 + 3904, ctx.r24.u32);
loc_825E37B8:
	// lwz r11,84(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// subfic r11,r11,0
	ctx.xer.ca = ctx.r11.u32 <= 0;
	ctx.r11.s64 = 0 - ctx.r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// rlwinm r3,r11,0,29,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x4;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825E37D4:
	// li r3,4
	ctx.r3.s64 = 4;
loc_825E37D8:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825E37E0"))) PPC_WEAK_FUNC(sub_825E37E0);
PPC_FUNC_IMPL(__imp__sub_825E37E0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825E37E8;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// subfic r11,r6,64
	ctx.xer.ca = ctx.r6.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r6.s64;
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r11,r5
	ctx.r30.u64 = ctx.r11.u64 + ctx.r5.u64;
	// lbz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r30.u32 + 0);
	// bl 0x825d5468
	ctx.lr = 0x825E381C;
	sub_825D5468(ctx, base);
	// lbz r11,1(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 1);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e3830
	if (!ctx.cr6.eq) goto loc_825E3830;
	// li r10,3
	ctx.r10.s64 = 3;
	// stw r10,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r10.u32);
loc_825E3830:
	// lwz r31,84(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// li r26,0
	ctx.r26.s64 = 0;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e3914
	if (!ctx.cr6.eq) goto loc_825E3914;
	// extsb r27,r11
	ctx.r27.s64 = ctx.r11.s8;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// beq cr6,0x825e3978
	if (ctx.cr6.eq) goto loc_825E3978;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e38c4
	if (!ctx.cr6.gt) goto loc_825E38C4;
loc_825E386C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e38c4
	if (ctx.cr6.eq) goto loc_825E38C4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e38b4
	if (!ctx.cr0.lt) goto loc_825E38B4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E38B4;
	sub_825D5398(ctx, base);
loc_825E38B4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e386c
	if (ctx.cr6.gt) goto loc_825E386C;
loc_825E38C4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e3900
	if (!ctx.cr0.lt) goto loc_825E3900;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E3900;
	sub_825D5398(ctx, base);
loc_825E3900:
	// lwz r10,84(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 84);
	// clrlwi r11,r30,24
	ctx.r11.u64 = ctx.r30.u32 & 0xFF;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825e392c
	if (ctx.cr6.eq) goto loc_825E392C;
loc_825E3914:
	// li r11,4
	ctx.r11.s64 = 4;
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
	// lwz r11,1760(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1760);
	// stw r26,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r26.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E392C:
	// addi r9,r27,-1
	ctx.r9.s64 = ctx.r27.s64 + -1;
	// stw r26,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r26.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// slw r9,r10,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r9.u8 & 0x3F));
	// and r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 & ctx.r11.u64;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x825e395c
	if (ctx.cr6.eq) goto loc_825E395C;
	// lwz r10,1760(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1760);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E395C:
	// slw r10,r10,r27
	ctx.r10.u64 = ctx.r27.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r27.u8 & 0x3F));
	// lwz r9,1760(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1760);
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E3978:
	// li r26,0
	ctx.r26.s64 = 0;
	// stw r26,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r26.u32);
	// lwz r11,1760(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 1760);
	// stw r26,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r26.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825E3990"))) PPC_WEAK_FUNC(sub_825E3990);
PPC_FUNC_IMPL(__imp__sub_825E3990) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825E3998;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// addi r31,r11,14880
	ctx.r31.s64 = ctx.r11.s64 + 14880;
	// lwz r30,84(r29)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// rldicl r11,r11,3,61
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 3) & 0x7;
	// rlwinm r27,r11,1,0,30
	ctx.r27.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r27,r31
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r27.u32 + ctx.r31.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E39C8;
	sub_825D5468(ctx, base);
	// addi r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 1;
	// li r31,3
	ctx.r31.s64 = 3;
	// lbzx r11,r27,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r27.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e39e0
	if (!ctx.cr6.eq) goto loc_825E39E0;
	// stw r31,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r31.u32);
loc_825E39E0:
	// lwz r3,84(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	// clrlwi r25,r11,24
	ctx.r25.u64 = ctx.r11.u32 & 0xFF;
	// lwz r11,20(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e3af0
	if (!ctx.cr6.eq) goto loc_825E3AF0;
	// cmplwi cr6,r25,3
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 3, ctx.xer);
	// bgt cr6,0x825e3af0
	if (ctx.cr6.gt) goto loc_825E3AF0;
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e3a24
	if (!ctx.cr0.lt) goto loc_825E3A24;
	// bl 0x825d5398
	ctx.lr = 0x825E3A24;
	sub_825D5398(ctx, base);
loc_825E3A24:
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// clrlwi r10,r30,24
	ctx.r10.u64 = ctx.r30.u32 & 0xFF;
	// rlwimi r11,r10,3,27,28
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 3) & 0x18) | (ctx.r11.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r11,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r11.u32);
	// lwz r10,84(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e3af0
	if (!ctx.cr6.eq) goto loc_825E3AF0;
	// clrlwi r10,r11,1
	ctx.r10.u64 = ctx.r11.u32 & 0x7FFFFFFF;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// rlwinm r10,r10,0,15,13
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// addi r27,r11,14752
	ctx.r27.s64 = ctx.r11.s64 + 14752;
	// stw r10,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r10.u32);
	// lwz r30,84(r29)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// rldicl r11,r11,6,58
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 6) & 0x3F;
	// rlwinm r26,r11,1,0,30
	ctx.r26.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r26,r27
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r26.u32 + ctx.r27.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E3A74;
	sub_825D5468(ctx, base);
	// addi r11,r27,1
	ctx.r11.s64 = ctx.r27.s64 + 1;
	// lbzx r11,r26,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r26.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e3a88
	if (!ctx.cr6.eq) goto loc_825E3A88;
	// stw r31,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r31.u32);
loc_825E3A88:
	// lwz r10,84(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	// clrlwi r29,r11,24
	ctx.r29.u64 = ctx.r11.u32 & 0xFF;
	// lwz r11,20(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e3af0
	if (!ctx.cr6.eq) goto loc_825E3AF0;
	// li r4,5
	ctx.r4.s64 = 5;
	// srawi r5,r25,1
	ctx.xer.ca = (ctx.r25.s32 < 0) & ((ctx.r25.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r25.s32 >> 1;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82622d20
	ctx.lr = 0x825E3AAC;
	sub_82622D20(ctx, base);
	// clrlwi r5,r25,31
	ctx.r5.u64 = ctx.r25.u32 & 0x1;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82622d20
	ctx.lr = 0x825E3ABC;
	sub_82622D20(ctx, base);
	// li r30,1
	ctx.r30.s64 = 1;
loc_825E3AC0:
	// sraw r11,r29,r31
	temp.u32 = ctx.r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r29.s32 < 0) & (((ctx.r29.s32 >> temp.u32) << temp.u32) != ctx.r29.s32);
	ctx.r11.s64 = ctx.r29.s32 >> temp.u32;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// clrlwi r5,r11,31
	ctx.r5.u64 = ctx.r11.u32 & 0x1;
	// bl 0x82622d20
	ctx.lr = 0x825E3AD4;
	sub_82622D20(ctx, base);
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// bge cr6,0x825e3ac0
	if (!ctx.cr6.lt) goto loc_825E3AC0;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E3AF0:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825E3AFC"))) PPC_WEAK_FUNC(sub_825E3AFC);
PPC_FUNC_IMPL(__imp__sub_825E3AFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E3B00"))) PPC_WEAK_FUNC(sub_825E3B00);
PPC_FUNC_IMPL(__imp__sub_825E3B00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825E3B08;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// lwz r11,448(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 448);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e3b98
	if (ctx.cr6.eq) goto loc_825E3B98;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e3b4c
	if (!ctx.cr0.lt) goto loc_825E3B4C;
	// bl 0x825d5398
	ctx.lr = 0x825E3B4C;
	sub_825D5398(ctx, base);
loc_825E3B4C:
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// rlwimi r11,r31,31,0,0
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r31.u32, 31) & 0x80000000) | (ctx.r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r11.u32);
	// lwz r10,84(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e3dc0
	if (!ctx.cr6.eq) goto loc_825E3DC0;
	// rlwinm r11,r11,0,0,0
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e3ba4
	if (ctx.cr6.eq) goto loc_825E3BA4;
	// li r11,0
	ctx.r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,12(r27)
	PPC_STORE_U32(ctx.r27.u32 + 12, ctx.r11.u32);
	// sth r11,16(r27)
	PPC_STORE_U16(ctx.r27.u32 + 16, ctx.r11.u16);
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// oris r11,r11,2
	ctx.r11.u64 = ctx.r11.u64 | 131072;
	// stw r11,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E3B98:
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// clrlwi r11,r11,1
	ctx.r11.u64 = ctx.r11.u32 & 0x7FFFFFFF;
	// stw r11,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r11.u32);
loc_825E3BA4:
	// lwz r30,84(r26)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// addi r31,r11,14896
	ctx.r31.s64 = ctx.r11.s64 + 14896;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// rldicl r11,r11,7,57
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 7) & 0x7F;
	// rlwinm r29,r11,1,0,30
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r29,r31
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r29.u32 + ctx.r31.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E3BC8;
	sub_825D5468(ctx, base);
	// addi r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 1;
	// li r31,3
	ctx.r31.s64 = 3;
	// lbzx r11,r29,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r29.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e3be0
	if (!ctx.cr6.eq) goto loc_825E3BE0;
	// stw r31,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r31.u32);
loc_825E3BE0:
	// lwz r10,84(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e3dc0
	if (!ctx.cr6.eq) goto loc_825E3DC0;
	// cmplwi cr6,r11,7
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 7, ctx.xer);
	// bgt cr6,0x825e3dc0
	if (ctx.cr6.gt) goto loc_825E3DC0;
	// clrlwi r25,r11,30
	ctx.r25.u64 = ctx.r11.u32 & 0x3;
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// blt cr6,0x825e3ca4
	if (ctx.cr6.lt) goto loc_825E3CA4;
	// bne cr6,0x825e3dc0
	if (!ctx.cr6.eq) goto loc_825E3DC0;
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// rlwinm r11,r11,0,15,13
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// stw r11,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r11.u32);
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e3c48
	if (!ctx.cr0.lt) goto loc_825E3C48;
	// bl 0x825d5398
	ctx.lr = 0x825E3C48;
	sub_825D5398(ctx, base);
loc_825E3C48:
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// clrlwi r11,r30,24
	ctx.r11.u64 = ctx.r30.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r10.u32);
	// lwz r30,84(r26)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r11,20(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e3dc0
	if (!ctx.cr6.eq) goto loc_825E3DC0;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// addi r29,r11,14752
	ctx.r29.s64 = ctx.r11.s64 + 14752;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// rldicl r11,r11,6,58
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 6) & 0x3F;
	// rlwinm r28,r11,1,0,30
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r28,r29
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r29.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E3C88;
	sub_825D5468(ctx, base);
	// addi r11,r29,1
	ctx.r11.s64 = ctx.r29.s64 + 1;
	// lbzx r11,r28,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e3c9c
	if (!ctx.cr6.eq) goto loc_825E3C9C;
	// stw r31,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r31.u32);
loc_825E3C9C:
	// clrlwi r29,r11,24
	ctx.r29.u64 = ctx.r11.u32 & 0xFF;
	// b 0x825e3d3c
	goto loc_825E3D3C;
loc_825E3CA4:
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// cmpwi cr6,r25,3
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 3, ctx.xer);
	// oris r11,r11,2
	ctx.r11.u64 = ctx.r11.u64 | 131072;
	// stw r11,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r11.u32);
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lwz r30,84(r26)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// addi r29,r11,14752
	ctx.r29.s64 = ctx.r11.s64 + 14752;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// rldicl r11,r11,6,58
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 6) & 0x3F;
	// rlwinm r28,r11,1,0,30
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r28,r29
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r29.u32);
	// bne cr6,0x825e3cf8
	if (!ctx.cr6.eq) goto loc_825E3CF8;
	// bl 0x825d5468
	ctx.lr = 0x825E3CDC;
	sub_825D5468(ctx, base);
	// addi r11,r29,1
	ctx.r11.s64 = ctx.r29.s64 + 1;
	// lbzx r11,r28,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e3cf0
	if (!ctx.cr6.eq) goto loc_825E3CF0;
	// stw r31,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r31.u32);
loc_825E3CF0:
	// clrlwi r29,r11,24
	ctx.r29.u64 = ctx.r11.u32 & 0xFF;
	// b 0x825e3d2c
	goto loc_825E3D2C;
loc_825E3CF8:
	// bl 0x825d5468
	ctx.lr = 0x825E3CFC;
	sub_825D5468(ctx, base);
	// addi r11,r29,1
	ctx.r11.s64 = ctx.r29.s64 + 1;
	// lbzx r11,r28,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e3d10
	if (!ctx.cr6.eq) goto loc_825E3D10;
	// stw r31,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r31.u32);
loc_825E3D10:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// subfic r29,r11,15
	ctx.xer.ca = ctx.r11.u32 <= 15;
	ctx.r29.s64 = 15 - ctx.r11.s64;
	// bne cr6,0x825e3d2c
	if (!ctx.cr6.eq) goto loc_825E3D2C;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// beq cr6,0x825e3d30
	if (ctx.cr6.eq) goto loc_825E3D30;
loc_825E3D2C:
	// li r11,0
	ctx.r11.s64 = 0;
loc_825E3D30:
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// rlwimi r10,r11,30,1,1
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r11.u32, 30) & 0x40000000) | (ctx.r10.u64 & 0xFFFFFFFFBFFFFFFF);
	// stw r10,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r10.u32);
loc_825E3D3C:
	// lwz r11,84(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e3dc0
	if (!ctx.cr6.eq) goto loc_825E3DC0;
	// cmplwi cr6,r29,15
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 15, ctx.xer);
	// bgt cr6,0x825e3dc0
	if (ctx.cr6.gt) goto loc_825E3DC0;
	// li r4,5
	ctx.r4.s64 = 5;
	// srawi r5,r25,1
	ctx.xer.ca = (ctx.r25.s32 < 0) & ((ctx.r25.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r25.s32 >> 1;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x82622d20
	ctx.lr = 0x825E3D64;
	sub_82622D20(ctx, base);
	// clrlwi r5,r25,31
	ctx.r5.u64 = ctx.r25.u32 & 0x1;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x82622d20
	ctx.lr = 0x825E3D74;
	sub_82622D20(ctx, base);
	// li r30,1
	ctx.r30.s64 = 1;
loc_825E3D78:
	// sraw r11,r29,r31
	temp.u32 = ctx.r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r29.s32 < 0) & (((ctx.r29.s32 >> temp.u32) << temp.u32) != ctx.r29.s32);
	ctx.r11.s64 = ctx.r29.s32 >> temp.u32;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// clrlwi r5,r11,31
	ctx.r5.u64 = ctx.r11.u32 & 0x1;
	// bl 0x82622d20
	ctx.lr = 0x825E3D8C;
	sub_82622D20(ctx, base);
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// bge cr6,0x825e3d78
	if (!ctx.cr6.lt) goto loc_825E3D78;
	// addi r3,r27,12
	ctx.r3.s64 = ctx.r27.s64 + 12;
	// bl 0x825d5668
	ctx.lr = 0x825E3DA4;
	sub_825D5668(ctx, base);
	// lwz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwimi r11,r10,19,12,13
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 19) & 0xC0000) | (ctx.r11.u64 & 0xFFFFFFFFFFF3FFFF);
	// stw r11,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E3DC0:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825E3DCC"))) PPC_WEAK_FUNC(sub_825E3DCC);
PPC_FUNC_IMPL(__imp__sub_825E3DCC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E3DD0"))) PPC_WEAK_FUNC(sub_825E3DD0);
PPC_FUNC_IMPL(__imp__sub_825E3DD0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825E3DD8;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// addi r31,r11,15152
	ctx.r31.s64 = ctx.r11.s64 + 15152;
	// lwz r30,84(r29)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// rldicl r11,r11,9,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 9) & 0x1FF;
	// rlwinm r27,r11,1,0,30
	ctx.r27.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r27,r31
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r27.u32 + ctx.r31.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E3E08;
	sub_825D5468(ctx, base);
	// addi r11,r31,1
	ctx.r11.s64 = ctx.r31.s64 + 1;
	// li r31,3
	ctx.r31.s64 = 3;
	// lbzx r11,r27,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r27.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e3e20
	if (!ctx.cr6.eq) goto loc_825E3E20;
	// stw r31,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r31.u32);
loc_825E3E20:
	// lwz r3,84(r29)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	// clrlwi r25,r11,24
	ctx.r25.u64 = ctx.r11.u32 & 0xFF;
	// lwz r11,20(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e3f30
	if (!ctx.cr6.eq) goto loc_825E3F30;
	// cmplwi cr6,r25,20
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 20, ctx.xer);
	// bgt cr6,0x825e3f30
	if (ctx.cr6.gt) goto loc_825E3F30;
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e3e64
	if (!ctx.cr0.lt) goto loc_825E3E64;
	// bl 0x825d5398
	ctx.lr = 0x825E3E64;
	sub_825D5398(ctx, base);
loc_825E3E64:
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// clrlwi r10,r30,24
	ctx.r10.u64 = ctx.r30.u32 & 0xFF;
	// rlwimi r11,r10,3,27,28
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 3) & 0x18) | (ctx.r11.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r11,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r11.u32);
	// lwz r10,84(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e3f30
	if (!ctx.cr6.eq) goto loc_825E3F30;
	// clrlwi r10,r11,1
	ctx.r10.u64 = ctx.r11.u32 & 0x7FFFFFFF;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// rlwinm r10,r10,0,15,13
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// addi r27,r11,14752
	ctx.r27.s64 = ctx.r11.s64 + 14752;
	// stw r10,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r10.u32);
	// lwz r30,84(r29)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// rldicl r11,r11,6,58
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 6) & 0x3F;
	// rlwinm r26,r11,1,0,30
	ctx.r26.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r26,r27
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r26.u32 + ctx.r27.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E3EB4;
	sub_825D5468(ctx, base);
	// addi r11,r27,1
	ctx.r11.s64 = ctx.r27.s64 + 1;
	// lbzx r11,r26,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r26.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e3ec8
	if (!ctx.cr6.eq) goto loc_825E3EC8;
	// stw r31,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r31.u32);
loc_825E3EC8:
	// lwz r10,84(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 84);
	// clrlwi r29,r11,24
	ctx.r29.u64 = ctx.r11.u32 & 0xFF;
	// lwz r11,20(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e3f30
	if (!ctx.cr6.eq) goto loc_825E3F30;
	// li r4,5
	ctx.r4.s64 = 5;
	// srawi r5,r25,1
	ctx.xer.ca = (ctx.r25.s32 < 0) & ((ctx.r25.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r25.s32 >> 1;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82622d20
	ctx.lr = 0x825E3EEC;
	sub_82622D20(ctx, base);
	// clrlwi r5,r25,31
	ctx.r5.u64 = ctx.r25.u32 & 0x1;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82622d20
	ctx.lr = 0x825E3EFC;
	sub_82622D20(ctx, base);
	// li r30,1
	ctx.r30.s64 = 1;
loc_825E3F00:
	// sraw r11,r29,r31
	temp.u32 = ctx.r31.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r29.s32 < 0) & (((ctx.r29.s32 >> temp.u32) << temp.u32) != ctx.r29.s32);
	ctx.r11.s64 = ctx.r29.s32 >> temp.u32;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// clrlwi r5,r11,31
	ctx.r5.u64 = ctx.r11.u32 & 0x1;
	// bl 0x82622d20
	ctx.lr = 0x825E3F14;
	sub_82622D20(ctx, base);
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// bge cr6,0x825e3f00
	if (!ctx.cr6.lt) goto loc_825E3F00;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E3F30:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825E3F3C"))) PPC_WEAK_FUNC(sub_825E3F3C);
PPC_FUNC_IMPL(__imp__sub_825E3F3C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E3F40"))) PPC_WEAK_FUNC(sub_825E3F40);
PPC_FUNC_IMPL(__imp__sub_825E3F40) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825E3F48;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e3f80
	if (!ctx.cr0.lt) goto loc_825E3F80;
	// bl 0x825d5398
	ctx.lr = 0x825E3F80;
	sub_825D5398(ctx, base);
loc_825E3F80:
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// rlwimi r11,r30,31,0,0
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r30.u32, 31) & 0x80000000) | (ctx.r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// lwz r10,84(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e41cc
	if (!ctx.cr6.eq) goto loc_825E41CC;
	// rlwinm r10,r11,0,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0x80000000;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825e3fcc
	if (ctx.cr6.eq) goto loc_825E3FCC;
	// li r11,0
	ctx.r11.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r11.u32);
	// sth r11,16(r31)
	PPC_STORE_U16(ctx.r31.u32 + 16, ctx.r11.u16);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// oris r11,r11,2
	ctx.r11.u64 = ctx.r11.u64 | 131072;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E3FCC:
	// clrlwi r10,r11,1
	ctx.r10.u64 = ctx.r11.u32 & 0x7FFFFFFF;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// addi r30,r11,16176
	ctx.r30.s64 = ctx.r11.s64 + 16176;
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// lwz r29,84(r26)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// ld r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r29.u32 + 0);
	// rldicl r11,r11,9,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 9) & 0x1FF;
	// rlwinm r28,r11,1,0,30
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r28,r30
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r30.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E3FF8;
	sub_825D5468(ctx, base);
	// addi r11,r30,1
	ctx.r11.s64 = ctx.r30.s64 + 1;
	// li r30,3
	ctx.r30.s64 = 3;
	// lbzx r11,r28,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e4010
	if (!ctx.cr6.eq) goto loc_825E4010;
	// stw r30,20(r29)
	PPC_STORE_U32(ctx.r29.u32 + 20, ctx.r30.u32);
loc_825E4010:
	// lwz r10,84(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e41cc
	if (!ctx.cr6.eq) goto loc_825E41CC;
	// cmplwi cr6,r11,20
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 20, ctx.xer);
	// bgt cr6,0x825e41cc
	if (ctx.cr6.gt) goto loc_825E41CC;
	// clrlwi r25,r11,30
	ctx.r25.u64 = ctx.r11.u32 & 0x3;
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e40d8
	if (ctx.cr6.eq) goto loc_825E40D8;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// bne cr6,0x825e41cc
	if (!ctx.cr6.eq) goto loc_825E41CC;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// rlwinm r11,r11,0,15,13
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFDFFFF;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r29,r8,0
	ctx.r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e407c
	if (!ctx.cr0.lt) goto loc_825E407C;
	// bl 0x825d5398
	ctx.lr = 0x825E407C;
	sub_825D5398(ctx, base);
loc_825E407C:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// clrlwi r11,r29,24
	ctx.r11.u64 = ctx.r29.u32 & 0xFF;
	// rlwimi r10,r11,3,27,28
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r11.u32, 3) & 0x18) | (ctx.r10.u64 & 0xFFFFFFFFFFFFFFE7);
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// lwz r29,84(r26)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r11,20(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e41cc
	if (!ctx.cr6.eq) goto loc_825E41CC;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// addi r28,r11,14752
	ctx.r28.s64 = ctx.r11.s64 + 14752;
	// ld r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r29.u32 + 0);
	// rldicl r11,r11,6,58
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 6) & 0x3F;
	// rlwinm r27,r11,1,0,30
	ctx.r27.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r27,r28
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r27.u32 + ctx.r28.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E40BC;
	sub_825D5468(ctx, base);
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// lbzx r11,r27,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r27.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e40d0
	if (!ctx.cr6.eq) goto loc_825E40D0;
	// stw r30,20(r29)
	PPC_STORE_U32(ctx.r29.u32 + 20, ctx.r30.u32);
loc_825E40D0:
	// clrlwi r28,r11,24
	ctx.r28.u64 = ctx.r11.u32 & 0xFF;
	// b 0x825e4148
	goto loc_825E4148;
loc_825E40D8:
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// addi r28,r11,14752
	ctx.r28.s64 = ctx.r11.s64 + 14752;
	// oris r11,r10,2
	ctx.r11.u64 = ctx.r10.u64 | 131072;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// lwz r29,84(r26)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// ld r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r29.u32 + 0);
	// rldicl r11,r11,6,58
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 6) & 0x3F;
	// rlwinm r27,r11,1,0,30
	ctx.r27.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r27,r28
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r27.u32 + ctx.r28.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E4108;
	sub_825D5468(ctx, base);
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// lbzx r11,r27,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r27.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e411c
	if (!ctx.cr6.eq) goto loc_825E411C;
	// stw r30,20(r29)
	PPC_STORE_U32(ctx.r29.u32 + 20, ctx.r30.u32);
loc_825E411C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// subfic r28,r11,15
	ctx.xer.ca = ctx.r11.u32 <= 15;
	ctx.r28.s64 = 15 - ctx.r11.s64;
	// bne cr6,0x825e4138
	if (!ctx.cr6.eq) goto loc_825E4138;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// beq cr6,0x825e413c
	if (ctx.cr6.eq) goto loc_825E413C;
loc_825E4138:
	// li r11,0
	ctx.r11.s64 = 0;
loc_825E413C:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// rlwimi r10,r11,30,1,1
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r11.u32, 30) & 0x40000000) | (ctx.r10.u64 & 0xFFFFFFFFBFFFFFFF);
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
loc_825E4148:
	// lwz r11,84(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e41cc
	if (!ctx.cr6.eq) goto loc_825E41CC;
	// cmplwi cr6,r28,15
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 15, ctx.xer);
	// bgt cr6,0x825e41cc
	if (ctx.cr6.gt) goto loc_825E41CC;
	// li r4,5
	ctx.r4.s64 = 5;
	// srawi r5,r25,1
	ctx.xer.ca = (ctx.r25.s32 < 0) & ((ctx.r25.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r25.s32 >> 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82622d20
	ctx.lr = 0x825E4170;
	sub_82622D20(ctx, base);
	// clrlwi r5,r25,31
	ctx.r5.u64 = ctx.r25.u32 & 0x1;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82622d20
	ctx.lr = 0x825E4180;
	sub_82622D20(ctx, base);
	// li r29,1
	ctx.r29.s64 = 1;
loc_825E4184:
	// sraw r11,r28,r30
	temp.u32 = ctx.r30.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r28.s32 < 0) & (((ctx.r28.s32 >> temp.u32) << temp.u32) != ctx.r28.s32);
	ctx.r11.s64 = ctx.r28.s32 >> temp.u32;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// clrlwi r5,r11,31
	ctx.r5.u64 = ctx.r11.u32 & 0x1;
	// bl 0x82622d20
	ctx.lr = 0x825E4198;
	sub_82622D20(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bge cr6,0x825e4184
	if (!ctx.cr6.lt) goto loc_825E4184;
	// addi r3,r31,12
	ctx.r3.s64 = ctx.r31.s64 + 12;
	// bl 0x825d5668
	ctx.lr = 0x825E41B0;
	sub_825D5668(ctx, base);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// li r10,1
	ctx.r10.s64 = 1;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwimi r11,r10,19,12,13
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 19) & 0xC0000) | (ctx.r11.u64 & 0xFFFFFFFFFFF3FFFF);
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E41CC:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825E41D8"))) PPC_WEAK_FUNC(sub_825E41D8);
PPC_FUNC_IMPL(__imp__sub_825E41D8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,3636(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3636);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e4250
	if (!ctx.cr6.eq) goto loc_825E4250;
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// bl 0x825d4d40
	ctx.lr = 0x825E4204;
	sub_825D4D40(ctx, base);
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r10,1
	ctx.r10.s64 = 1;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// slw r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r11.u8 & 0x3F));
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// bne cr6,0x825e4250
	if (!ctx.cr6.eq) goto loc_825E4250;
	// lwz r11,3556(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3556);
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// addi r4,r11,16
	ctx.r4.s64 = ctx.r11.s64 + 16;
	// bl 0x825d51c0
	ctx.lr = 0x825E4230;
	sub_825D51C0(ctx, base);
	// addi r11,r3,-1
	ctx.r11.s64 = ctx.r3.s64 + -1;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825E4250:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E4268"))) PPC_WEAK_FUNC(sub_825E4268);
PPC_FUNC_IMPL(__imp__sub_825E4268) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba10
	ctx.lr = 0x825E4270;
	sub_8239BA10(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// li r4,8
	ctx.r4.s64 = 8;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,29
	ctx.r11.u64 = ctx.r11.u32 & 0x7;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e4298
	if (ctx.cr6.eq) goto loc_825E4298;
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
loc_825E4298:
	// bl 0x825d5468
	ctx.lr = 0x825E429C;
	sub_825D5468(ctx, base);
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r11,3556(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3556);
	// addi r30,r11,16
	ctx.r30.s64 = ctx.r11.s64 + 16;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// beq cr6,0x825e4328
	if (ctx.cr6.eq) goto loc_825E4328;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e4300
	if (!ctx.cr6.gt) goto loc_825E4300;
loc_825E42C0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e4300
	if (ctx.cr6.eq) goto loc_825E4300;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e42f0
	if (!ctx.cr0.lt) goto loc_825E42F0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E42F0;
	sub_825D5398(ctx, base);
loc_825E42F0:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e42c0
	if (ctx.cr6.gt) goto loc_825E42C0;
loc_825E4300:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e4328
	if (!ctx.cr0.lt) goto loc_825E4328;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4328;
	sub_825D5398(ctx, base);
loc_825E4328:
	// lwz r11,140(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 140);
	// li r30,0
	ctx.r30.s64 = 0;
	// lwz r10,136(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// mullw r10,r11,r10
	ctx.r10.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// addi r11,r10,-1
	ctx.r11.s64 = ctx.r10.s64 + -1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e4354
	if (ctx.cr6.eq) goto loc_825E4354;
loc_825E4344:
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e4344
	if (!ctx.cr6.eq) goto loc_825E4344;
loc_825E4354:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// ble cr6,0x825e4420
	if (!ctx.cr6.gt) goto loc_825E4420;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825e4380
	if (!ctx.cr6.eq) goto loc_825E4380;
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x825e4420
	goto loc_825E4420;
loc_825E4380:
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e43e0
	if (!ctx.cr6.gt) goto loc_825E43E0;
loc_825E4388:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e43e0
	if (ctx.cr6.eq) goto loc_825E43E0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e43d0
	if (!ctx.cr0.lt) goto loc_825E43D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E43D0;
	sub_825D5398(ctx, base);
loc_825E43D0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e4388
	if (ctx.cr6.gt) goto loc_825E4388;
loc_825E43E0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e441c
	if (!ctx.cr0.lt) goto loc_825E441C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E441C;
	sub_825D5398(ctx, base);
loc_825E441C:
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_825E4420:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,5
	ctx.r30.s64 = 5;
	// stw r11,3592(r26)
	PPC_STORE_U32(ctx.r26.u32 + 3592, ctx.r11.u32);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825e4498
	if (!ctx.cr6.lt) goto loc_825E4498;
loc_825E4440:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e4498
	if (ctx.cr6.eq) goto loc_825E4498;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e4488
	if (!ctx.cr0.lt) goto loc_825E4488;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4488;
	sub_825D5398(ctx, base);
loc_825E4488:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e4440
	if (ctx.cr6.gt) goto loc_825E4440;
loc_825E4498:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e44d4
	if (!ctx.cr0.lt) goto loc_825E44D4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E44D4;
	sub_825D5398(ctx, base);
loc_825E44D4:
	// stw r28,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r28.u32);
	// li r30,1
	ctx.r30.s64 = 1;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e454c
	if (!ctx.cr6.lt) goto loc_825E454C;
loc_825E44F4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e454c
	if (ctx.cr6.eq) goto loc_825E454C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e453c
	if (!ctx.cr0.lt) goto loc_825E453C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E453C;
	sub_825D5398(ctx, base);
loc_825E453C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e44f4
	if (ctx.cr6.gt) goto loc_825E44F4;
loc_825E454C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4588
	if (!ctx.cr0.lt) goto loc_825E4588;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4588;
	sub_825D5398(ctx, base);
loc_825E4588:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e495c
	if (ctx.cr6.eq) goto loc_825E495C;
loc_825E4590:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e4604
	if (!ctx.cr6.lt) goto loc_825E4604;
loc_825E45AC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e4604
	if (ctx.cr6.eq) goto loc_825E4604;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e45f4
	if (!ctx.cr0.lt) goto loc_825E45F4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E45F4;
	sub_825D5398(ctx, base);
loc_825E45F4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e45ac
	if (ctx.cr6.gt) goto loc_825E45AC;
loc_825E4604:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4640
	if (!ctx.cr0.lt) goto loc_825E4640;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4640;
	sub_825D5398(ctx, base);
loc_825E4640:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825e4590
	if (!ctx.cr6.eq) goto loc_825E4590;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e46a0
	if (!ctx.cr6.lt) goto loc_825E46A0;
loc_825E4660:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e46a0
	if (ctx.cr6.eq) goto loc_825E46A0;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e4690
	if (!ctx.cr0.lt) goto loc_825E4690;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4690;
	sub_825D5398(ctx, base);
loc_825E4690:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e4660
	if (ctx.cr6.gt) goto loc_825E4660;
loc_825E46A0:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e46c8
	if (!ctx.cr0.lt) goto loc_825E46C8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E46C8;
	sub_825D5398(ctx, base);
loc_825E46C8:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r30,3632(r26)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3632);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// beq cr6,0x825e4750
	if (ctx.cr6.eq) goto loc_825E4750;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e4728
	if (!ctx.cr6.gt) goto loc_825E4728;
loc_825E46E8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e4728
	if (ctx.cr6.eq) goto loc_825E4728;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e4718
	if (!ctx.cr0.lt) goto loc_825E4718;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4718;
	sub_825D5398(ctx, base);
loc_825E4718:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e46e8
	if (ctx.cr6.gt) goto loc_825E46E8;
loc_825E4728:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e4750
	if (!ctx.cr0.lt) goto loc_825E4750;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4750;
	sub_825D5398(ctx, base);
loc_825E4750:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e47a8
	if (!ctx.cr6.lt) goto loc_825E47A8;
loc_825E4768:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e47a8
	if (ctx.cr6.eq) goto loc_825E47A8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e4798
	if (!ctx.cr0.lt) goto loc_825E4798;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4798;
	sub_825D5398(ctx, base);
loc_825E4798:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e4768
	if (ctx.cr6.gt) goto loc_825E4768;
loc_825E47A8:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e47d0
	if (!ctx.cr0.lt) goto loc_825E47D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E47D0;
	sub_825D5398(ctx, base);
loc_825E47D0:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825e4828
	if (!ctx.cr6.lt) goto loc_825E4828;
loc_825E47E8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e4828
	if (ctx.cr6.eq) goto loc_825E4828;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e4818
	if (!ctx.cr0.lt) goto loc_825E4818;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4818;
	sub_825D5398(ctx, base);
loc_825E4818:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e47e8
	if (ctx.cr6.gt) goto loc_825E47E8;
loc_825E4828:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e4850
	if (!ctx.cr0.lt) goto loc_825E4850;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4850;
	sub_825D5398(ctx, base);
loc_825E4850:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825e48a8
	if (!ctx.cr6.lt) goto loc_825E48A8;
loc_825E4868:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e48a8
	if (ctx.cr6.eq) goto loc_825E48A8;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e4898
	if (!ctx.cr0.lt) goto loc_825E4898;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4898;
	sub_825D5398(ctx, base);
loc_825E4898:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e4868
	if (ctx.cr6.gt) goto loc_825E4868;
loc_825E48A8:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e48d0
	if (!ctx.cr0.lt) goto loc_825E48D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E48D0;
	sub_825D5398(ctx, base);
loc_825E48D0:
	// lwz r11,284(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 284);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825e495c
	if (!ctx.cr6.eq) goto loc_825E495C;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825e4934
	if (!ctx.cr6.lt) goto loc_825E4934;
loc_825E48F4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e4934
	if (ctx.cr6.eq) goto loc_825E4934;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e4924
	if (!ctx.cr0.lt) goto loc_825E4924;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4924;
	sub_825D5398(ctx, base);
loc_825E4924:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e48f4
	if (ctx.cr6.gt) goto loc_825E48F4;
loc_825E4934:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e495c
	if (!ctx.cr0.lt) goto loc_825E495C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E495C;
	sub_825D5398(ctx, base);
loc_825E495C:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
}

__attribute__((alias("__imp__sub_825E4968"))) PPC_WEAK_FUNC(sub_825E4968);
PPC_FUNC_IMPL(__imp__sub_825E4968) {
	PPC_FUNC_PROLOGUE();
	// lbz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 0);
	// neg r9,r4
	ctx.r9.s64 = -ctx.r4.s64;
	// extsb r11,r11
	ctx.r11.s64 = ctx.r11.s8;
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// bge cr6,0x825e4988
	if (!ctx.cr6.lt) goto loc_825E4988;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// b 0x825e4998
	goto loc_825E4998;
loc_825E4988:
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// blt cr6,0x825e499c
	if (ctx.cr6.lt) goto loc_825E499C;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
loc_825E4998:
	// stb r11,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r11.u8);
loc_825E499C:
	// lbz r11,1(r3)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r3.u32 + 1);
	// extsb r11,r11
	ctx.r11.s64 = ctx.r11.s8;
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// bge cr6,0x825e49bc
	if (!ctx.cr6.lt) goto loc_825E49BC;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stb r11,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, ctx.r11.u8);
	// blr 
	return;
loc_825E49BC:
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// bltlr cr6
	if (ctx.cr6.lt) return;
	// rlwinm r10,r4,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// stb r11,1(r3)
	PPC_STORE_U8(ctx.r3.u32 + 1, ctx.r11.u8);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E49D4"))) PPC_WEAK_FUNC(sub_825E49D4);
PPC_FUNC_IMPL(__imp__sub_825E49D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E49D8"))) PPC_WEAK_FUNC(sub_825E49D8);
PPC_FUNC_IMPL(__imp__sub_825E49D8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825E49E0;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// addi r27,r11,17968
	ctx.r27.s64 = ctx.r11.s64 + 17968;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// rldicl r11,r11,13,51
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 13) & 0x1FFF;
	// rlwinm r30,r11,1,0,30
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r30,r27
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r30.u32 + ctx.r27.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E4A10;
	sub_825D5468(ctx, base);
	// addi r11,r27,1
	ctx.r11.s64 = ctx.r27.s64 + 1;
	// li r25,3
	ctx.r25.s64 = 3;
	// lbzx r11,r30,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e4a28
	if (!ctx.cr6.eq) goto loc_825E4A28;
	// stw r25,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r25.u32);
loc_825E4A28:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// addi r28,r11,-32
	ctx.r28.s64 = ctx.r11.s64 + -32;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq cr6,0x825e4afc
	if (ctx.cr6.eq) goto loc_825E4AFC;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,3556(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3556);
	// addi r30,r11,-1
	ctx.r30.s64 = ctx.r11.s64 + -1;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825e4afc
	if (ctx.cr6.eq) goto loc_825E4AFC;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e4ab8
	if (!ctx.cr6.gt) goto loc_825E4AB8;
loc_825E4A60:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e4ab8
	if (ctx.cr6.eq) goto loc_825E4AB8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e4aa8
	if (!ctx.cr0.lt) goto loc_825E4AA8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4AA8;
	sub_825D5398(ctx, base);
loc_825E4AA8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e4a60
	if (ctx.cr6.gt) goto loc_825E4A60;
loc_825E4AB8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4af4
	if (!ctx.cr0.lt) goto loc_825E4AF4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4AF4;
	sub_825D5398(ctx, base);
loc_825E4AF4:
	// mr r9,r30
	ctx.r9.u64 = ctx.r30.u64;
	// b 0x825e4b00
	goto loc_825E4B00;
loc_825E4AFC:
	// li r9,0
	ctx.r9.s64 = 0;
loc_825E4B00:
	// lwz r10,3564(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3564);
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// bne cr6,0x825e4b1c
	if (!ctx.cr6.eq) goto loc_825E4B1C;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x825e4b1c
	if (!ctx.cr6.eq) goto loc_825E4B1C;
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x825e4b58
	goto loc_825E4B58;
loc_825E4B1C:
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// beq cr6,0x825e4b58
	if (ctx.cr6.eq) goto loc_825E4B58;
	// srawi r8,r11,31
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r11.s32 >> 31;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// xor r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 ^ ctx.r8.u64;
	// subf r11,r8,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r8.s64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x825e4b54
	if (ctx.cr6.gt) goto loc_825E4B54;
	// li r11,-1
	ctx.r11.s64 = -1;
loc_825E4B54:
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
loc_825E4B58:
	// stb r11,0(r24)
	PPC_STORE_U8(ctx.r24.u32 + 0, ctx.r11.u8);
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// rldicl r11,r11,13,51
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 13) & 0x1FFF;
	// rlwinm r30,r11,1,0,30
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r30,r27
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r30.u32 + ctx.r27.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E4B78;
	sub_825D5468(ctx, base);
	// addi r11,r27,1
	ctx.r11.s64 = ctx.r27.s64 + 1;
	// lbzx r11,r30,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e4b8c
	if (!ctx.cr6.eq) goto loc_825E4B8C;
	// stw r25,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r25.u32);
loc_825E4B8C:
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// addi r28,r11,-32
	ctx.r28.s64 = ctx.r11.s64 + -32;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq cr6,0x825e4c60
	if (ctx.cr6.eq) goto loc_825E4C60;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,3556(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3556);
	// addi r30,r11,-1
	ctx.r30.s64 = ctx.r11.s64 + -1;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// beq cr6,0x825e4c60
	if (ctx.cr6.eq) goto loc_825E4C60;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e4c1c
	if (!ctx.cr6.gt) goto loc_825E4C1C;
loc_825E4BC4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e4c1c
	if (ctx.cr6.eq) goto loc_825E4C1C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e4c0c
	if (!ctx.cr0.lt) goto loc_825E4C0C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4C0C;
	sub_825D5398(ctx, base);
loc_825E4C0C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e4bc4
	if (ctx.cr6.gt) goto loc_825E4BC4;
loc_825E4C1C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4c58
	if (!ctx.cr0.lt) goto loc_825E4C58;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4C58;
	sub_825D5398(ctx, base);
loc_825E4C58:
	// mr r9,r30
	ctx.r9.u64 = ctx.r30.u64;
	// b 0x825e4c64
	goto loc_825E4C64;
loc_825E4C60:
	// li r9,0
	ctx.r9.s64 = 0;
loc_825E4C64:
	// lwz r10,3564(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 3564);
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// bne cr6,0x825e4c88
	if (!ctx.cr6.eq) goto loc_825E4C88;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x825e4c88
	if (!ctx.cr6.eq) goto loc_825E4C88;
	// li r11,0
	ctx.r11.s64 = 0;
	// stb r11,1(r24)
	PPC_STORE_U8(ctx.r24.u32 + 1, ctx.r11.u8);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825E4C88:
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x825e4c9c
	if (!ctx.cr6.eq) goto loc_825E4C9C;
	// stb r28,1(r24)
	PPC_STORE_U8(ctx.r24.u32 + 1, ctx.r28.u8);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825E4C9C:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// srawi r8,r11,31
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r8.s64 = ctx.r11.s32 >> 31;
	// xor r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 ^ ctx.r8.u64;
	// subf r11,r8,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r8.s64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// li r11,1
	ctx.r11.s64 = 1;
	// bgt cr6,0x825e4ccc
	if (ctx.cr6.gt) goto loc_825E4CCC;
	// li r11,-1
	ctx.r11.s64 = -1;
loc_825E4CCC:
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// stb r11,1(r24)
	PPC_STORE_U8(ctx.r24.u32 + 1, ctx.r11.u8);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825E4CDC"))) PPC_WEAK_FUNC(sub_825E4CDC);
PPC_FUNC_IMPL(__imp__sub_825E4CDC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E4CE0"))) PPC_WEAK_FUNC(sub_825E4CE0);
PPC_FUNC_IMPL(__imp__sub_825E4CE0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825E4CE8;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// subfic r11,r6,64
	ctx.xer.ca = ctx.r6.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r6.s64;
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r30,r11,r5
	ctx.r30.u64 = ctx.r11.u64 + ctx.r5.u64;
	// lbz r4,0(r30)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r30.u32 + 0);
	// bl 0x825d5468
	ctx.lr = 0x825E4D1C;
	sub_825D5468(ctx, base);
	// lbz r11,1(r30)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r30.u32 + 1);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e4d30
	if (!ctx.cr6.eq) goto loc_825E4D30;
	// li r10,3
	ctx.r10.s64 = 3;
	// stw r10,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r10.u32);
loc_825E4D30:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// li r26,0
	ctx.r26.s64 = 0;
	// lwz r10,20(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e4f1c
	if (!ctx.cr6.eq) goto loc_825E4F1C;
	// extsb r28,r11
	ctx.r28.s64 = ctx.r11.s8;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// beq cr6,0x825e4fa8
	if (ctx.cr6.eq) goto loc_825E4FA8;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpwi cr6,r28,8
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 8, ctx.xer);
	// mr r30,r28
	ctx.r30.u64 = ctx.r28.u64;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// bgt cr6,0x825e4e70
	if (ctx.cr6.gt) goto loc_825E4E70;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e4dcc
	if (!ctx.cr6.gt) goto loc_825E4DCC;
loc_825E4D74:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e4dcc
	if (ctx.cr6.eq) goto loc_825E4DCC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e4dbc
	if (!ctx.cr0.lt) goto loc_825E4DBC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4DBC;
	sub_825D5398(ctx, base);
loc_825E4DBC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e4d74
	if (ctx.cr6.gt) goto loc_825E4D74;
loc_825E4DCC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4e08
	if (!ctx.cr0.lt) goto loc_825E4E08;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4E08;
	sub_825D5398(ctx, base);
loc_825E4E08:
	// lwz r10,84(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// clrlwi r11,r30,24
	ctx.r11.u64 = ctx.r30.u32 & 0xFF;
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e4f1c
	if (!ctx.cr6.eq) goto loc_825E4F1C;
	// addi r10,r28,-1
	ctx.r10.s64 = ctx.r28.s64 + -1;
	// stw r26,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r26.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// slw r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r10.u8 & 0x3F));
	// and r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 & ctx.r11.u64;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e4e60
	if (!ctx.cr6.eq) goto loc_825E4E60;
	// subfic r10,r28,8
	ctx.xer.ca = ctx.r28.u32 <= 8;
	ctx.r10.s64 = 8 - ctx.r28.s64;
	// lwz r8,1760(r27)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// li r9,255
	ctx.r9.s64 = 255;
	// sraw r10,r9,r10
	temp.u32 = ctx.r10.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r9.s32 < 0) & (((ctx.r9.s32 >> temp.u32) << temp.u32) != ctx.r9.s32);
	ctx.r10.s64 = ctx.r9.s32 >> temp.u32;
	// andc r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 & ~ctx.r11.u64;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E4E60:
	// lwz r10,1760(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// stw r11,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E4E70:
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e4ed0
	if (!ctx.cr6.gt) goto loc_825E4ED0;
loc_825E4E78:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e4ed0
	if (ctx.cr6.eq) goto loc_825E4ED0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e4ec0
	if (!ctx.cr0.lt) goto loc_825E4EC0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4EC0;
	sub_825D5398(ctx, base);
loc_825E4EC0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e4e78
	if (ctx.cr6.gt) goto loc_825E4E78;
loc_825E4ED0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e4f0c
	if (!ctx.cr0.lt) goto loc_825E4F0C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E4F0C;
	sub_825D5398(ctx, base);
loc_825E4F0C:
	// lwz r11,84(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e4f34
	if (ctx.cr6.eq) goto loc_825E4F34;
loc_825E4F1C:
	// li r11,4
	ctx.r11.s64 = 4;
	// stw r11,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r11.u32);
	// lwz r11,1760(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// stw r26,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r26.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E4F34:
	// addi r11,r28,-1
	ctx.r11.s64 = ctx.r28.s64 + -1;
	// stw r26,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r26.u32);
	// li r10,1
	ctx.r10.s64 = 1;
	// slw r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r11.u8 & 0x3F));
	// and r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 & ctx.r30.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825e4f74
	if (!ctx.cr6.eq) goto loc_825E4F74;
	// lis r10,0
	ctx.r10.s64 = 0;
	// lwz r9,1760(r27)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// subfic r11,r28,16
	ctx.xer.ca = ctx.r28.u32 <= 16;
	ctx.r11.s64 = 16 - ctx.r28.s64;
	// ori r10,r10,65535
	ctx.r10.u64 = ctx.r10.u64 | 65535;
	// sraw r11,r10,r11
	temp.u32 = ctx.r11.u32 & 0x3F;
	if (temp.u32 > 0x1F) temp.u32 = 0x1F;
	ctx.xer.ca = (ctx.r10.s32 < 0) & (((ctx.r10.s32 >> temp.u32) << temp.u32) != ctx.r10.s32);
	ctx.r11.s64 = ctx.r10.s32 >> temp.u32;
	// andc r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 & ~ctx.r30.u64;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r11.u32);
	// b 0x825e4f7c
	goto loc_825E4F7C;
loc_825E4F74:
	// lwz r11,1760(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// stw r30,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r30.u32);
loc_825E4F7C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r11,r11,1,62
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r11.u64);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addic. r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e4fb8
	if (!ctx.cr0.lt) goto loc_825E4FB8;
	// bl 0x825d5398
	ctx.lr = 0x825E4FA0;
	sub_825D5398(ctx, base);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825E4FA8:
	// li r26,0
	ctx.r26.s64 = 0;
	// stw r26,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r26.u32);
	// lwz r11,1760(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// stw r26,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r26.u32);
loc_825E4FB8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825E4FC0"))) PPC_WEAK_FUNC(sub_825E4FC0);
PPC_FUNC_IMPL(__imp__sub_825E4FC0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9e0
	ctx.lr = 0x825E4FC8;
	sub_8239B9E0(ctx, base);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// mr r20,r6
	ctx.r20.u64 = ctx.r6.u64;
	// mr r18,r7
	ctx.r18.u64 = ctx.r7.u64;
	// li r25,0
	ctx.r25.s64 = 0;
	// li r21,1
	ctx.r21.s64 = 1;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// lwz r17,312(r27)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r27.u32 + 312);
	// addi r23,r10,1
	ctx.r23.s64 = ctx.r10.s64 + 1;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r16,316(r27)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r27.u32 + 316);
	// lwz r24,0(r11)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r19,28(r11)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// lwz r22,32(r11)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// lwz r14,24(r11)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// lwz r15,4(r11)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// beq cr6,0x825e5844
	if (ctx.cr6.eq) goto loc_825E5844;
	// lis r11,0
	ctx.r11.s64 = 0;
	// ori r26,r11,32768
	ctx.r26.u64 = ctx.r11.u64 | 32768;
loc_825E5030:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r24.u32 + 8);
	// lwz r29,0(r24)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// subfic r11,r4,64
	ctx.xer.ca = ctx.r4.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e5120
	if (ctx.cr6.lt) goto loc_825E5120;
	// clrlwi r11,r30,28
	ctx.r11.u64 = ctx.r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// sld r10,r10,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825e5118
	if (!ctx.cr6.lt) goto loc_825E5118;
loc_825E5080:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825e50ac
	if (ctx.cr6.lt) goto loc_825E50AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825E509C;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825e5080
	if (ctx.cr6.eq) goto loc_825E5080;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e515c
	goto loc_825E515C;
loc_825E50AC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = ctx.r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// rldicr r11,r9,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
loc_825E5118:
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e515c
	goto loc_825E515C;
loc_825E5120:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E5128;
	sub_825D5468(ctx, base);
loc_825E5128:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// add r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E5144;
	sub_825D5468(ctx, base);
	// add r11,r30,r26
	ctx.r11.u64 = ctx.r30.u64 + ctx.r26.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e5128
	if (ctx.cr6.lt) goto loc_825E5128;
loc_825E515C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// clrlwi r11,r30,24
	ctx.r11.u64 = ctx.r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e5870
	if (!ctx.cr6.eq) goto loc_825E5870;
	// clrlwi r31,r11,24
	ctx.r31.u64 = ctx.r11.u32 & 0xFF;
	// cmpw cr6,r31,r15
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r15.s32, ctx.xer);
	// bgt cr6,0x825e5870
	if (ctx.cr6.gt) goto loc_825E5870;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// beq cr6,0x825e51d8
	if (ctx.cr6.eq) goto loc_825E51D8;
	// subfc r11,r23,r31
	ctx.xer.ca = ctx.r31.u32 >= ctx.r23.u32;
	ctx.r11.s64 = ctx.r31.s64 - ctx.r23.s64;
	// lbzx r28,r31,r22
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r31.u32 + ctx.r22.u32);
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addi r25,r11,1
	ctx.r25.s64 = ctx.r11.s64 + 1;
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e51b8
	if (!ctx.cr0.lt) goto loc_825E51B8;
	// bl 0x825d5398
	ctx.lr = 0x825E51B8;
	sub_825D5398(ctx, base);
loc_825E51B8:
	// lbzx r11,r31,r19
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + ctx.r19.u32);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e51d0
	if (ctx.cr6.eq) goto loc_825E51D0;
	// extsb r11,r11
	ctx.r11.s64 = ctx.r11.s8;
	// neg r31,r11
	ctx.r31.s64 = -ctx.r11.s64;
	// b 0x825e57a4
	goto loc_825E57A4;
loc_825E51D0:
	// extsb r31,r11
	ctx.r31.s64 = ctx.r11.s8;
	// b 0x825e57a4
	goto loc_825E57A4;
loc_825E51D8:
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e51fc
	if (!ctx.cr0.lt) goto loc_825E51FC;
	// bl 0x825d5398
	ctx.lr = 0x825E51FC;
	sub_825D5398(ctx, base);
loc_825E51FC:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825e53c4
	if (!ctx.cr6.eq) goto loc_825E53C4;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e5870
	if (!ctx.cr6.eq) goto loc_825E5870;
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r24.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subfic r11,r4,64
	ctx.xer.ca = ctx.r4.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r24)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e5300
	if (ctx.cr6.lt) goto loc_825E5300;
	// clrlwi r11,r30,28
	ctx.r11.u64 = ctx.r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// sld r10,r10,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825e52f8
	if (!ctx.cr6.lt) goto loc_825E52F8;
loc_825E5260:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825e528c
	if (ctx.cr6.lt) goto loc_825E528C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825E527C;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825e5260
	if (ctx.cr6.eq) goto loc_825E5260;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e533c
	goto loc_825E533C;
loc_825E528C:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = ctx.r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// rldicr r11,r9,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
loc_825E52F8:
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e533c
	goto loc_825E533C;
loc_825E5300:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E5308;
	sub_825D5468(ctx, base);
loc_825E5308:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// add r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E5324;
	sub_825D5468(ctx, base);
	// add r11,r30,r26
	ctx.r11.u64 = ctx.r30.u64 + ctx.r26.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e5308
	if (ctx.cr6.lt) goto loc_825E5308;
loc_825E533C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// clrlwi r11,r30,24
	ctx.r11.u64 = ctx.r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e5870
	if (!ctx.cr6.eq) goto loc_825E5870;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmpw cr6,r11,r15
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r15.s32, ctx.xer);
	// beq cr6,0x825e5870
	if (ctx.cr6.eq) goto loc_825E5870;
	// lbzx r10,r11,r19
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r19.u32);
	// cmplw cr6,r11,r23
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r23.u32, ctx.xer);
	// lbzx r28,r11,r22
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r22.u32);
	// extsb r11,r10
	ctx.r11.s64 = ctx.r10.s8;
	// blt cr6,0x825e537c
	if (ctx.cr6.lt) goto loc_825E537C;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r25,1
	ctx.r25.s64 = 1;
	// b 0x825e5380
	goto loc_825E5380;
loc_825E537C:
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
loc_825E5380:
	// lbzx r10,r28,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r10.u32);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r31,r10,r11
	ctx.r31.u64 = ctx.r10.u64 + ctx.r11.u64;
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e53b4
	if (!ctx.cr0.lt) goto loc_825E53B4;
	// bl 0x825d5398
	ctx.lr = 0x825E53B4;
	sub_825D5398(ctx, base);
loc_825E53B4:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e57a4
	if (ctx.cr6.eq) goto loc_825E57A4;
	// neg r31,r31
	ctx.r31.s64 = -ctx.r31.s64;
	// b 0x825e57a4
	goto loc_825E57A4;
loc_825E53C4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e53f0
	if (!ctx.cr0.lt) goto loc_825E53F0;
	// bl 0x825d5398
	ctx.lr = 0x825E53F0;
	sub_825D5398(ctx, base);
loc_825E53F0:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825e55b8
	if (!ctx.cr6.eq) goto loc_825E55B8;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e5870
	if (!ctx.cr6.eq) goto loc_825E5870;
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r24.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subfic r11,r4,64
	ctx.xer.ca = ctx.r4.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r24)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e54f4
	if (ctx.cr6.lt) goto loc_825E54F4;
	// clrlwi r11,r30,28
	ctx.r11.u64 = ctx.r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// sld r10,r10,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825e54ec
	if (!ctx.cr6.lt) goto loc_825E54EC;
loc_825E5454:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825e5480
	if (ctx.cr6.lt) goto loc_825E5480;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825E5470;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825e5454
	if (ctx.cr6.eq) goto loc_825E5454;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e5530
	goto loc_825E5530;
loc_825E5480:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = ctx.r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// rldicr r11,r9,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
loc_825E54EC:
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e5530
	goto loc_825E5530;
loc_825E54F4:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E54FC;
	sub_825D5468(ctx, base);
loc_825E54FC:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// add r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E5518;
	sub_825D5468(ctx, base);
	// add r11,r30,r26
	ctx.r11.u64 = ctx.r30.u64 + ctx.r26.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e54fc
	if (ctx.cr6.lt) goto loc_825E54FC;
loc_825E5530:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// clrlwi r11,r30,24
	ctx.r11.u64 = ctx.r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e5870
	if (!ctx.cr6.eq) goto loc_825E5870;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmpw cr6,r11,r15
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r15.s32, ctx.xer);
	// beq cr6,0x825e5870
	if (ctx.cr6.eq) goto loc_825E5870;
	// lbzx r9,r11,r19
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r19.u32);
	// cmplw cr6,r11,r23
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r23.u32, ctx.xer);
	// lbzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r22.u32);
	// extsb r31,r9
	ctx.r31.s64 = ctx.r9.s8;
	// blt cr6,0x825e5570
	if (ctx.cr6.lt) goto loc_825E5570;
	// lbzx r11,r31,r14
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + ctx.r14.u32);
	// li r25,1
	ctx.r25.s64 = 1;
	// b 0x825e5578
	goto loc_825E5578;
loc_825E5570:
	// lwz r11,88(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
	// lbzx r11,r31,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + ctx.r11.u32);
loc_825E5578:
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r28,r11,1
	ctx.r28.s64 = ctx.r11.s64 + 1;
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e55a8
	if (!ctx.cr0.lt) goto loc_825E55A8;
	// bl 0x825d5398
	ctx.lr = 0x825E55A8;
	sub_825D5398(ctx, base);
loc_825E55A8:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e57a4
	if (ctx.cr6.eq) goto loc_825E57A4;
	// neg r31,r31
	ctx.r31.s64 = -ctx.r31.s64;
	// b 0x825e57a4
	goto loc_825E57A4;
loc_825E55B8:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e55e4
	if (!ctx.cr0.lt) goto loc_825E55E4;
	// bl 0x825d5398
	ctx.lr = 0x825E55E4;
	sub_825D5398(ctx, base);
loc_825E55E4:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// mr r25,r30
	ctx.r25.u64 = ctx.r30.u64;
	// li r30,6
	ctx.r30.s64 = 6;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825e565c
	if (!ctx.cr6.lt) goto loc_825E565C;
loc_825E5604:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e565c
	if (ctx.cr6.eq) goto loc_825E565C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e564c
	if (!ctx.cr0.lt) goto loc_825E564C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E564C;
	sub_825D5398(ctx, base);
loc_825E564C:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e5604
	if (ctx.cr6.gt) goto loc_825E5604;
loc_825E565C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e5698
	if (!ctx.cr0.lt) goto loc_825E5698;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E5698;
	sub_825D5398(ctx, base);
loc_825E5698:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// mr r28,r30
	ctx.r28.u64 = ctx.r30.u64;
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r11,r11,1,62
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r11.u64);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addic. r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e56c0
	if (!ctx.cr0.lt) goto loc_825E56C0;
	// bl 0x825d5398
	ctx.lr = 0x825E56C0;
	sub_825D5398(ctx, base);
loc_825E56C0:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,12
	ctx.r30.s64 = 12;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,12
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 12, ctx.xer);
	// bge cr6,0x825e5734
	if (!ctx.cr6.lt) goto loc_825E5734;
loc_825E56DC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e5734
	if (ctx.cr6.eq) goto loc_825E5734;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e5724
	if (!ctx.cr0.lt) goto loc_825E5724;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E5724;
	sub_825D5398(ctx, base);
loc_825E5724:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e56dc
	if (ctx.cr6.gt) goto loc_825E56DC;
loc_825E5734:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e5770
	if (!ctx.cr0.lt) goto loc_825E5770;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E5770;
	sub_825D5398(ctx, base);
loc_825E5770:
	// mr r31,r30
	ctx.r31.u64 = ctx.r30.u64;
	// cmpwi cr6,r30,2047
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 2047, ctx.xer);
	// ble cr6,0x825e5780
	if (!ctx.cr6.gt) goto loc_825E5780;
	// addi r31,r30,-4096
	ctx.r31.s64 = ctx.r30.s64 + -4096;
loc_825E5780:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r11,r11,1,62
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r11.u64);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addic. r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e57a4
	if (!ctx.cr0.lt) goto loc_825E57A4;
	// bl 0x825d5398
	ctx.lr = 0x825E57A4;
	sub_825D5398(ctx, base);
loc_825E57A4:
	// lwz r11,84(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e5870
	if (!ctx.cr6.eq) goto loc_825E5870;
	// add r9,r28,r21
	ctx.r9.u64 = ctx.r28.u64 + ctx.r21.u64;
	// cmplwi cr6,r9,64
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 64, ctx.xer);
	// bge cr6,0x825e5870
	if (!ctx.cr6.lt) goto loc_825E5870;
	// lbzx r11,r9,r18
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r18.u32);
	// rlwinm r10,r11,0,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFF8;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e57e8
	if (!ctx.cr6.eq) goto loc_825E57E8;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// rotlwi r11,r11,1
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
	// lhzx r10,r11,r20
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r20.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// sthx r10,r11,r20
	PPC_STORE_U16(ctx.r11.u32 + ctx.r20.u32, ctx.r10.u16);
	// b 0x825e5838
	goto loc_825E5838;
loc_825E57E8:
	// clrlwi r10,r11,29
	ctx.r10.u64 = ctx.r11.u32 & 0x7;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e5814
	if (!ctx.cr6.eq) goto loc_825E5814;
	// lbzx r11,r9,r18
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r18.u32);
	// rlwinm r11,r11,29,3,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 29) & 0x1FFFFFFF;
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r11,r20
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r20.u32);
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// sthx r10,r11,r20
	PPC_STORE_U16(ctx.r11.u32 + ctx.r20.u32, ctx.r10.u16);
	// b 0x825e5838
	goto loc_825E5838;
loc_825E5814:
	// lwz r8,1760(r27)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// mullw r10,r31,r17
	ctx.r10.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r17.s32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// ble cr6,0x825e5830
	if (!ctx.cr6.gt) goto loc_825E5830;
	// add r10,r10,r16
	ctx.r10.u64 = ctx.r10.u64 + ctx.r16.u64;
	// b 0x825e5834
	goto loc_825E5834;
loc_825E5830:
	// subf r10,r16,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r16.s64;
loc_825E5834:
	// stwx r10,r11,r8
	PPC_STORE_U32(ctx.r11.u32 + ctx.r8.u32, ctx.r10.u32);
loc_825E5838:
	// addi r21,r9,1
	ctx.r21.s64 = ctx.r9.s64 + 1;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// beq cr6,0x825e5030
	if (ctx.cr6.eq) goto loc_825E5030;
loc_825E5844:
	// li r9,32
	ctx.r9.s64 = 32;
	// li r10,4
	ctx.r10.s64 = 4;
	// addi r8,r20,2
	ctx.r8.s64 = ctx.r20.s64 + 2;
	// li r7,0
	ctx.r7.s64 = 0;
loc_825E5854:
	// lhz r11,0(r8)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r8.u32 + 0);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e587c
	if (!ctx.cr6.eq) goto loc_825E587C;
	// lwz r11,1760(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// stwx r7,r10,r11
	PPC_STORE_U32(ctx.r10.u32 + ctx.r11.u32, ctx.r7.u32);
	// b 0x825e5898
	goto loc_825E5898;
loc_825E5870:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825E587C:
	// lwz r6,1760(r27)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// mullw r11,r11,r17
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r17.s32);
	// ble cr6,0x825e5890
	if (!ctx.cr6.gt) goto loc_825E5890;
	// add r11,r11,r16
	ctx.r11.u64 = ctx.r11.u64 + ctx.r16.u64;
	// b 0x825e5894
	goto loc_825E5894;
loc_825E5890:
	// subf r11,r16,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r16.s64;
loc_825E5894:
	// stwx r11,r10,r6
	PPC_STORE_U32(ctx.r10.u32 + ctx.r6.u32, ctx.r11.u32);
loc_825E5898:
	// lhz r11,16(r8)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r8.u32 + 16);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e58b4
	if (!ctx.cr6.eq) goto loc_825E58B4;
	// lwz r11,1760(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// stwx r7,r9,r11
	PPC_STORE_U32(ctx.r9.u32 + ctx.r11.u32, ctx.r7.u32);
	// b 0x825e58d0
	goto loc_825E58D0;
loc_825E58B4:
	// lwz r6,1760(r27)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// mullw r11,r11,r17
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r17.s32);
	// ble cr6,0x825e58c8
	if (!ctx.cr6.gt) goto loc_825E58C8;
	// add r11,r11,r16
	ctx.r11.u64 = ctx.r11.u64 + ctx.r16.u64;
	// b 0x825e58cc
	goto loc_825E58CC;
loc_825E58C8:
	// subf r11,r16,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r16.s64;
loc_825E58CC:
	// stwx r11,r9,r6
	PPC_STORE_U32(ctx.r9.u32 + ctx.r6.u32, ctx.r11.u32);
loc_825E58D0:
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// addi r8,r8,2
	ctx.r8.s64 = ctx.r8.s64 + 2;
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// cmpwi cr6,r10,32
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 32, ctx.xer);
	// blt cr6,0x825e5854
	if (ctx.cr6.lt) goto loc_825E5854;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825E58F0"))) PPC_WEAK_FUNC(sub_825E58F0);
PPC_FUNC_IMPL(__imp__sub_825E58F0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9e0
	ctx.lr = 0x825E58F8;
	sub_8239B9E0(ctx, base);
	// stwu r1,-256(r1)
	ea = -256 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// li r23,0
	ctx.r23.s64 = 0;
	// mr r22,r5
	ctx.r22.u64 = ctx.r5.u64;
	// li r5,256
	ctx.r5.s64 = 256;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r10,8(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mr r25,r23
	ctx.r25.u64 = ctx.r23.u64;
	// lwz r18,312(r27)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r27.u32 + 312);
	// addi r19,r10,1
	ctx.r19.s64 = ctx.r10.s64 + 1;
	// lwz r10,12(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 12);
	// lwz r17,316(r27)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r27.u32 + 316);
	// lwz r24,0(r11)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r20,28(r11)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r11.u32 + 28);
	// lwz r21,32(r11)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r11.u32 + 32);
	// stw r10,88(r1)
	PPC_STORE_U32(ctx.r1.u32 + 88, ctx.r10.u32);
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// lwz r15,24(r11)
	ctx.r15.u64 = PPC_LOAD_U32(ctx.r11.u32 + 24);
	// lwz r16,4(r11)
	ctx.r16.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r3,1760(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// stw r23,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r23.u32);
	// stw r10,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r10.u32);
	// lwz r10,20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// stw r10,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r10.u32);
	// bl 0x8239ca70
	ctx.lr = 0x825E5960;
	sub_8239CA70(ctx, base);
	// lis r11,0
	ctx.r11.s64 = 0;
	// li r14,1
	ctx.r14.s64 = 1;
	// ori r26,r11,32768
	ctx.r26.u64 = ctx.r11.u64 | 32768;
loc_825E596C:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r24.u32 + 8);
	// lwz r29,0(r24)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// subfic r11,r4,64
	ctx.xer.ca = ctx.r4.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e5a5c
	if (ctx.cr6.lt) goto loc_825E5A5C;
	// clrlwi r11,r30,28
	ctx.r11.u64 = ctx.r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// sld r10,r10,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825e5a54
	if (!ctx.cr6.lt) goto loc_825E5A54;
loc_825E59BC:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825e59e8
	if (ctx.cr6.lt) goto loc_825E59E8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825E59D8;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825e59bc
	if (ctx.cr6.eq) goto loc_825E59BC;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e5a98
	goto loc_825E5A98;
loc_825E59E8:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = ctx.r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// rldicr r11,r9,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
loc_825E5A54:
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e5a98
	goto loc_825E5A98;
loc_825E5A5C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E5A64;
	sub_825D5468(ctx, base);
loc_825E5A64:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// add r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E5A80;
	sub_825D5468(ctx, base);
	// add r11,r30,r26
	ctx.r11.u64 = ctx.r30.u64 + ctx.r26.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e5a64
	if (ctx.cr6.lt) goto loc_825E5A64;
loc_825E5A98:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// clrlwi r11,r30,24
	ctx.r11.u64 = ctx.r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e61c0
	if (!ctx.cr6.eq) goto loc_825E61C0;
	// clrlwi r31,r11,24
	ctx.r31.u64 = ctx.r11.u32 & 0xFF;
	// cmpw cr6,r31,r16
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r16.s32, ctx.xer);
	// beq cr6,0x825e5b10
	if (ctx.cr6.eq) goto loc_825E5B10;
	// cmplw cr6,r31,r19
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r19.u32, ctx.xer);
	// blt cr6,0x825e5ac4
	if (ctx.cr6.lt) goto loc_825E5AC4;
	// mr r25,r14
	ctx.r25.u64 = ctx.r14.u64;
loc_825E5AC4:
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// lbzx r28,r31,r21
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r31.u32 + ctx.r21.u32);
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e5af0
	if (!ctx.cr0.lt) goto loc_825E5AF0;
	// bl 0x825d5398
	ctx.lr = 0x825E5AF0;
	sub_825D5398(ctx, base);
loc_825E5AF0:
	// lbzx r11,r31,r20
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + ctx.r20.u32);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e5b08
	if (ctx.cr6.eq) goto loc_825E5B08;
	// extsb r11,r11
	ctx.r11.s64 = ctx.r11.s8;
	// neg r31,r11
	ctx.r31.s64 = -ctx.r11.s64;
	// b 0x825e60e0
	goto loc_825E60E0;
loc_825E5B08:
	// extsb r31,r11
	ctx.r31.s64 = ctx.r11.s8;
	// b 0x825e60e0
	goto loc_825E60E0;
loc_825E5B10:
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e5b38
	if (!ctx.cr0.lt) goto loc_825E5B38;
	// bl 0x825d5398
	ctx.lr = 0x825E5B38;
	sub_825D5398(ctx, base);
loc_825E5B38:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825e5d00
	if (!ctx.cr6.eq) goto loc_825E5D00;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e61c0
	if (!ctx.cr6.eq) goto loc_825E61C0;
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r24.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subfic r11,r4,64
	ctx.xer.ca = ctx.r4.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r24)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e5c3c
	if (ctx.cr6.lt) goto loc_825E5C3C;
	// clrlwi r11,r30,28
	ctx.r11.u64 = ctx.r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// sld r10,r10,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825e5c34
	if (!ctx.cr6.lt) goto loc_825E5C34;
loc_825E5B9C:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825e5bc8
	if (ctx.cr6.lt) goto loc_825E5BC8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825E5BB8;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825e5b9c
	if (ctx.cr6.eq) goto loc_825E5B9C;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e5c78
	goto loc_825E5C78;
loc_825E5BC8:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = ctx.r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// rldicr r11,r9,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
loc_825E5C34:
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e5c78
	goto loc_825E5C78;
loc_825E5C3C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E5C44;
	sub_825D5468(ctx, base);
loc_825E5C44:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// add r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E5C60;
	sub_825D5468(ctx, base);
	// add r11,r30,r26
	ctx.r11.u64 = ctx.r30.u64 + ctx.r26.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e5c44
	if (ctx.cr6.lt) goto loc_825E5C44;
loc_825E5C78:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// clrlwi r11,r30,24
	ctx.r11.u64 = ctx.r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e61c0
	if (!ctx.cr6.eq) goto loc_825E61C0;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmpw cr6,r11,r16
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r16.s32, ctx.xer);
	// beq cr6,0x825e61c0
	if (ctx.cr6.eq) goto loc_825E61C0;
	// lbzx r10,r11,r20
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r20.u32);
	// cmplw cr6,r11,r19
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r19.u32, ctx.xer);
	// lbzx r28,r11,r21
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r21.u32);
	// extsb r11,r10
	ctx.r11.s64 = ctx.r10.s8;
	// blt cr6,0x825e5cb8
	if (ctx.cr6.lt) goto loc_825E5CB8;
	// lwz r10,84(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// mr r25,r14
	ctx.r25.u64 = ctx.r14.u64;
	// b 0x825e5cbc
	goto loc_825E5CBC;
loc_825E5CB8:
	// lwz r10,88(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 88);
loc_825E5CBC:
	// lbzx r10,r28,r10
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r10.u32);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r31,r10,r11
	ctx.r31.u64 = ctx.r10.u64 + ctx.r11.u64;
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e5cf0
	if (!ctx.cr0.lt) goto loc_825E5CF0;
	// bl 0x825d5398
	ctx.lr = 0x825E5CF0;
	sub_825D5398(ctx, base);
loc_825E5CF0:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e60e0
	if (ctx.cr6.eq) goto loc_825E60E0;
	// neg r31,r31
	ctx.r31.s64 = -ctx.r31.s64;
	// b 0x825e60e0
	goto loc_825E60E0;
loc_825E5D00:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e5d2c
	if (!ctx.cr0.lt) goto loc_825E5D2C;
	// bl 0x825d5398
	ctx.lr = 0x825E5D2C;
	sub_825D5398(ctx, base);
loc_825E5D2C:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825e5ef4
	if (!ctx.cr6.eq) goto loc_825E5EF4;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,20(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e61c0
	if (!ctx.cr6.eq) goto loc_825E61C0;
	// lbz r4,8(r24)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r24.u32 + 8);
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subfic r11,r4,64
	ctx.xer.ca = ctx.r4.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r4.s64;
	// lwz r29,0(r24)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r24.u32 + 0);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e5e30
	if (ctx.cr6.lt) goto loc_825E5E30;
	// clrlwi r11,r30,28
	ctx.r11.u64 = ctx.r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// sld r10,r10,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825e5e28
	if (!ctx.cr6.lt) goto loc_825E5E28;
loc_825E5D90:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825e5dbc
	if (ctx.cr6.lt) goto loc_825E5DBC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825E5DAC;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825e5d90
	if (ctx.cr6.eq) goto loc_825E5D90;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e5e6c
	goto loc_825E5E6C;
loc_825E5DBC:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = ctx.r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// rldicr r11,r9,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
loc_825E5E28:
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e5e6c
	goto loc_825E5E6C;
loc_825E5E30:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E5E38;
	sub_825D5468(ctx, base);
loc_825E5E38:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// add r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E5E54;
	sub_825D5468(ctx, base);
	// add r11,r30,r26
	ctx.r11.u64 = ctx.r30.u64 + ctx.r26.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e5e38
	if (ctx.cr6.lt) goto loc_825E5E38;
loc_825E5E6C:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// clrlwi r11,r30,24
	ctx.r11.u64 = ctx.r30.u32 & 0xFF;
	// lwz r10,20(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e61c0
	if (!ctx.cr6.eq) goto loc_825E61C0;
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// cmpw cr6,r11,r16
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r16.s32, ctx.xer);
	// beq cr6,0x825e61c0
	if (ctx.cr6.eq) goto loc_825E61C0;
	// lbzx r9,r11,r20
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r20.u32);
	// cmplw cr6,r11,r19
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r19.u32, ctx.xer);
	// lbzx r10,r11,r21
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r21.u32);
	// extsb r31,r9
	ctx.r31.s64 = ctx.r9.s8;
	// blt cr6,0x825e5eac
	if (ctx.cr6.lt) goto loc_825E5EAC;
	// lbzx r11,r31,r15
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + ctx.r15.u32);
	// mr r25,r14
	ctx.r25.u64 = ctx.r14.u64;
	// b 0x825e5eb4
	goto loc_825E5EB4;
loc_825E5EAC:
	// lwz r11,92(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// lbzx r11,r31,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r31.u32 + ctx.r11.u32);
loc_825E5EB4:
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addi r28,r11,1
	ctx.r28.s64 = ctx.r11.s64 + 1;
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e5ee4
	if (!ctx.cr0.lt) goto loc_825E5EE4;
	// bl 0x825d5398
	ctx.lr = 0x825E5EE4;
	sub_825D5398(ctx, base);
loc_825E5EE4:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e60e0
	if (ctx.cr6.eq) goto loc_825E60E0;
	// neg r31,r31
	ctx.r31.s64 = -ctx.r31.s64;
	// b 0x825e60e0
	goto loc_825E60E0;
loc_825E5EF4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e5f20
	if (!ctx.cr0.lt) goto loc_825E5F20;
	// bl 0x825d5398
	ctx.lr = 0x825E5F20;
	sub_825D5398(ctx, base);
loc_825E5F20:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// mr r25,r30
	ctx.r25.u64 = ctx.r30.u64;
	// li r30,6
	ctx.r30.s64 = 6;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,6
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 6, ctx.xer);
	// bge cr6,0x825e5f98
	if (!ctx.cr6.lt) goto loc_825E5F98;
loc_825E5F40:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e5f98
	if (ctx.cr6.eq) goto loc_825E5F98;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e5f88
	if (!ctx.cr0.lt) goto loc_825E5F88;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E5F88;
	sub_825D5398(ctx, base);
loc_825E5F88:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e5f40
	if (ctx.cr6.gt) goto loc_825E5F40;
loc_825E5F98:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e5fd4
	if (!ctx.cr0.lt) goto loc_825E5FD4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E5FD4;
	sub_825D5398(ctx, base);
loc_825E5FD4:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// mr r28,r30
	ctx.r28.u64 = ctx.r30.u64;
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r11,r11,1,62
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r11.u64);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addic. r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e5ffc
	if (!ctx.cr0.lt) goto loc_825E5FFC;
	// bl 0x825d5398
	ctx.lr = 0x825E5FFC;
	sub_825D5398(ctx, base);
loc_825E5FFC:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,12
	ctx.r30.s64 = 12;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,12
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 12, ctx.xer);
	// bge cr6,0x825e6070
	if (!ctx.cr6.lt) goto loc_825E6070;
loc_825E6018:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6070
	if (ctx.cr6.eq) goto loc_825E6070;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6060
	if (!ctx.cr0.lt) goto loc_825E6060;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6060;
	sub_825D5398(ctx, base);
loc_825E6060:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6018
	if (ctx.cr6.gt) goto loc_825E6018;
loc_825E6070:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e60ac
	if (!ctx.cr0.lt) goto loc_825E60AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E60AC;
	sub_825D5398(ctx, base);
loc_825E60AC:
	// mr r31,r30
	ctx.r31.u64 = ctx.r30.u64;
	// cmpwi cr6,r30,2047
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 2047, ctx.xer);
	// ble cr6,0x825e60bc
	if (!ctx.cr6.gt) goto loc_825E60BC;
	// addi r31,r30,-4096
	ctx.r31.s64 = ctx.r30.s64 + -4096;
loc_825E60BC:
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rldicr r11,r11,1,62
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// std r11,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r11.u64);
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// addic. r11,r11,-1
	ctx.xer.ca = ctx.r11.u32 > 0;
	ctx.r11.s64 = ctx.r11.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e60e0
	if (!ctx.cr0.lt) goto loc_825E60E0;
	// bl 0x825d5398
	ctx.lr = 0x825E60E0;
	sub_825D5398(ctx, base);
loc_825E60E0:
	// lwz r11,84(r27)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r11,20(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 20);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e61c0
	if (!ctx.cr6.eq) goto loc_825E61C0;
	// add r11,r28,r23
	ctx.r11.u64 = ctx.r28.u64 + ctx.r23.u64;
	// cmplwi cr6,r11,64
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 64, ctx.xer);
	// bge cr6,0x825e61c0
	if (!ctx.cr6.lt) goto loc_825E61C0;
	// lwz r10,1828(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1828);
	// lbzx r10,r10,r11
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r11.u32);
	// clrlwi r9,r10,29
	ctx.r9.u64 = ctx.r10.u32 & 0x7;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x825e6128
	if (ctx.cr6.eq) goto loc_825E6128;
	// srawi r10,r10,3
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// clrlwi r10,r10,29
	ctx.r10.u64 = ctx.r10.u32 & 0x7;
	// slw r10,r14,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x20 ? 0 : (ctx.r14.u32 << (ctx.r10.u8 & 0x3F));
	// or r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 | ctx.r9.u64;
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
loc_825E6128:
	// cmpwi cr6,r31,1
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 1, ctx.xer);
	// bne cr6,0x825e6148
	if (!ctx.cr6.eq) goto loc_825E6148;
	// lbzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r22.u32);
	// lwz r9,1760(r27)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// lwz r8,304(r27)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r27.u32 + 304);
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
	// b 0x825e61a0
	goto loc_825E61A0;
loc_825E6148:
	// cmpwi cr6,r31,-1
	ctx.cr6.compare<int32_t>(ctx.r31.s32, -1, ctx.xer);
	// bne cr6,0x825e6168
	if (!ctx.cr6.eq) goto loc_825E6168;
	// lbzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r22.u32);
	// lwz r9,1760(r27)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// lwz r8,308(r27)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r27.u32 + 308);
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// stwx r8,r10,r9
	PPC_STORE_U32(ctx.r10.u32 + ctx.r9.u32, ctx.r8.u32);
	// b 0x825e61a0
	goto loc_825E61A0;
loc_825E6168:
	// lwz r8,1760(r27)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r27.u32 + 1760);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// ble cr6,0x825e618c
	if (!ctx.cr6.gt) goto loc_825E618C;
	// lbzx r9,r11,r22
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r22.u32);
	// mullw r10,r31,r18
	ctx.r10.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r18.s32);
	// add r10,r10,r17
	ctx.r10.u64 = ctx.r10.u64 + ctx.r17.u64;
	// rotlwi r9,r9,2
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 2);
	// stwx r10,r9,r8
	PPC_STORE_U32(ctx.r9.u32 + ctx.r8.u32, ctx.r10.u32);
	// b 0x825e61a0
	goto loc_825E61A0;
loc_825E618C:
	// lbzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r22.u32);
	// mullw r9,r31,r18
	ctx.r9.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r18.s32);
	// subf r9,r17,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r17.s64;
	// rotlwi r10,r10,2
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 2);
	// stwx r9,r10,r8
	PPC_STORE_U32(ctx.r10.u32 + ctx.r8.u32, ctx.r9.u32);
loc_825E61A0:
	// addi r23,r11,1
	ctx.r23.s64 = ctx.r11.s64 + 1;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// beq cr6,0x825e596c
	if (ctx.cr6.eq) goto loc_825E596C;
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,1940(r27)
	PPC_STORE_U32(ctx.r27.u32 + 1940, ctx.r11.u32);
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825E61C0:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,256
	ctx.r1.s64 = ctx.r1.s64 + 256;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825E61CC"))) PPC_WEAK_FUNC(sub_825E61CC);
PPC_FUNC_IMPL(__imp__sub_825E61CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E61D0"))) PPC_WEAK_FUNC(sub_825E61D0);
PPC_FUNC_IMPL(__imp__sub_825E61D0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba14
	ctx.lr = 0x825E61D8;
	sub_8239BA14(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// li r30,24
	ctx.r30.s64 = 24;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,24
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 24, ctx.xer);
	// bge cr6,0x825e6254
	if (!ctx.cr6.lt) goto loc_825E6254;
loc_825E61FC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6254
	if (ctx.cr6.eq) goto loc_825E6254;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6244
	if (!ctx.cr0.lt) goto loc_825E6244;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6244;
	sub_825D5398(ctx, base);
loc_825E6244:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e61fc
	if (ctx.cr6.gt) goto loc_825E61FC;
loc_825E6254:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6290
	if (!ctx.cr0.lt) goto loc_825E6290;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6290;
	sub_825D5398(ctx, base);
loc_825E6290:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// beq cr6,0x825e62a4
	if (ctx.cr6.eq) goto loc_825E62A4;
loc_825E6298:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
loc_825E62A4:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,3
	ctx.r30.s64 = 3;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825e6318
	if (!ctx.cr6.lt) goto loc_825E6318;
loc_825E62C0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6318
	if (ctx.cr6.eq) goto loc_825E6318;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6308
	if (!ctx.cr0.lt) goto loc_825E6308;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6308;
	sub_825D5398(ctx, base);
loc_825E6308:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e62c0
	if (ctx.cr6.gt) goto loc_825E62C0;
loc_825E6318:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6354
	if (!ctx.cr0.lt) goto loc_825E6354;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6354;
	sub_825D5398(ctx, base);
loc_825E6354:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,5
	ctx.r30.s64 = 5;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,5
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 5, ctx.xer);
	// bge cr6,0x825e63b4
	if (!ctx.cr6.lt) goto loc_825E63B4;
loc_825E6374:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e63b4
	if (ctx.cr6.eq) goto loc_825E63B4;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e63a4
	if (!ctx.cr0.lt) goto loc_825E63A4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E63A4;
	sub_825D5398(ctx, base);
loc_825E63A4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6374
	if (ctx.cr6.gt) goto loc_825E6374;
loc_825E63B4:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e63dc
	if (!ctx.cr0.lt) goto loc_825E63DC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E63DC;
	sub_825D5398(ctx, base);
loc_825E63DC:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,24
	ctx.r30.s64 = 24;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,24
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 24, ctx.xer);
	// bge cr6,0x825e6450
	if (!ctx.cr6.lt) goto loc_825E6450;
loc_825E63F8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6450
	if (ctx.cr6.eq) goto loc_825E6450;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6440
	if (!ctx.cr0.lt) goto loc_825E6440;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6440;
	sub_825D5398(ctx, base);
loc_825E6440:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e63f8
	if (ctx.cr6.gt) goto loc_825E63F8;
loc_825E6450:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e648c
	if (!ctx.cr0.lt) goto loc_825E648C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E648C;
	sub_825D5398(ctx, base);
loc_825E648C:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,4
	ctx.r30.s64 = 4;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bge cr6,0x825e6508
	if (!ctx.cr6.lt) goto loc_825E6508;
loc_825E64B0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6508
	if (ctx.cr6.eq) goto loc_825E6508;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e64f8
	if (!ctx.cr0.lt) goto loc_825E64F8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E64F8;
	sub_825D5398(ctx, base);
loc_825E64F8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e64b0
	if (ctx.cr6.gt) goto loc_825E64B0;
loc_825E6508:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6544
	if (!ctx.cr0.lt) goto loc_825E6544;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6544;
	sub_825D5398(ctx, base);
loc_825E6544:
	// cmplwi cr6,r30,2
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 2, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,4
	ctx.r30.s64 = 4;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bge cr6,0x825e65a4
	if (!ctx.cr6.lt) goto loc_825E65A4;
loc_825E6564:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e65a4
	if (ctx.cr6.eq) goto loc_825E65A4;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e6594
	if (!ctx.cr0.lt) goto loc_825E6594;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6594;
	sub_825D5398(ctx, base);
loc_825E6594:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6564
	if (ctx.cr6.gt) goto loc_825E6564;
loc_825E65A4:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e65cc
	if (!ctx.cr0.lt) goto loc_825E65CC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E65CC;
	sub_825D5398(ctx, base);
loc_825E65CC:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e6640
	if (!ctx.cr6.lt) goto loc_825E6640;
loc_825E65E8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6640
	if (ctx.cr6.eq) goto loc_825E6640;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6630
	if (!ctx.cr0.lt) goto loc_825E6630;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6630;
	sub_825D5398(ctx, base);
loc_825E6630:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e65e8
	if (ctx.cr6.gt) goto loc_825E65E8;
loc_825E6640:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e667c
	if (!ctx.cr0.lt) goto loc_825E667C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E667C;
	sub_825D5398(ctx, base);
loc_825E667C:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,8
	ctx.r30.s64 = 8;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 8, ctx.xer);
	// bge cr6,0x825e66dc
	if (!ctx.cr6.lt) goto loc_825E66DC;
loc_825E669C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e66dc
	if (ctx.cr6.eq) goto loc_825E66DC;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e66cc
	if (!ctx.cr0.lt) goto loc_825E66CC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E66CC;
	sub_825D5398(ctx, base);
loc_825E66CC:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e669c
	if (ctx.cr6.gt) goto loc_825E669C;
loc_825E66DC:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e6704
	if (!ctx.cr0.lt) goto loc_825E6704;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6704;
	sub_825D5398(ctx, base);
loc_825E6704:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e6778
	if (!ctx.cr6.lt) goto loc_825E6778;
loc_825E6720:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6778
	if (ctx.cr6.eq) goto loc_825E6778;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6768
	if (!ctx.cr0.lt) goto loc_825E6768;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6768;
	sub_825D5398(ctx, base);
loc_825E6768:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6720
	if (ctx.cr6.gt) goto loc_825E6720;
loc_825E6778:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e67b4
	if (!ctx.cr0.lt) goto loc_825E67B4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E67B4;
	sub_825D5398(ctx, base);
loc_825E67B4:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,4
	ctx.r30.s64 = 4;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bge cr6,0x825e6830
	if (!ctx.cr6.lt) goto loc_825E6830;
loc_825E67D8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6830
	if (ctx.cr6.eq) goto loc_825E6830;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6820
	if (!ctx.cr0.lt) goto loc_825E6820;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6820;
	sub_825D5398(ctx, base);
loc_825E6820:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e67d8
	if (ctx.cr6.gt) goto loc_825E67D8;
loc_825E6830:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e686c
	if (!ctx.cr0.lt) goto loc_825E686C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E686C;
	sub_825D5398(ctx, base);
loc_825E686C:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e68e4
	if (!ctx.cr6.lt) goto loc_825E68E4;
loc_825E688C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e68e4
	if (ctx.cr6.eq) goto loc_825E68E4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e68d4
	if (!ctx.cr0.lt) goto loc_825E68D4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E68D4;
	sub_825D5398(ctx, base);
loc_825E68D4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e688c
	if (ctx.cr6.gt) goto loc_825E688C;
loc_825E68E4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6920
	if (!ctx.cr0.lt) goto loc_825E6920;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6920;
	sub_825D5398(ctx, base);
loc_825E6920:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825e6980
	if (!ctx.cr6.lt) goto loc_825E6980;
loc_825E6940:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6980
	if (ctx.cr6.eq) goto loc_825E6980;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r11,32
	ctx.r8.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r8.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// bge 0x825e6970
	if (!ctx.cr0.lt) goto loc_825E6970;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6970;
	sub_825D5398(ctx, base);
loc_825E6970:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6940
	if (ctx.cr6.gt) goto loc_825E6940;
loc_825E6980:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e69a8
	if (!ctx.cr0.lt) goto loc_825E69A8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E69A8;
	sub_825D5398(ctx, base);
loc_825E69A8:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e6a1c
	if (!ctx.cr6.lt) goto loc_825E6A1C;
loc_825E69C4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6a1c
	if (ctx.cr6.eq) goto loc_825E6A1C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6a0c
	if (!ctx.cr0.lt) goto loc_825E6A0C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6A0C;
	sub_825D5398(ctx, base);
loc_825E6A0C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e69c4
	if (ctx.cr6.gt) goto loc_825E69C4;
loc_825E6A1C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6a58
	if (!ctx.cr0.lt) goto loc_825E6A58;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6A58;
	sub_825D5398(ctx, base);
loc_825E6A58:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,16
	ctx.r30.s64 = 16;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,16
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 16, ctx.xer);
	// bge cr6,0x825e6ad4
	if (!ctx.cr6.lt) goto loc_825E6AD4;
loc_825E6A7C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6ad4
	if (ctx.cr6.eq) goto loc_825E6AD4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6ac4
	if (!ctx.cr0.lt) goto loc_825E6AC4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6AC4;
	sub_825D5398(ctx, base);
loc_825E6AC4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6a7c
	if (ctx.cr6.gt) goto loc_825E6A7C;
loc_825E6AD4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6b10
	if (!ctx.cr0.lt) goto loc_825E6B10;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6B10;
	sub_825D5398(ctx, base);
loc_825E6B10:
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e6298
	if (ctx.cr6.eq) goto loc_825E6298;
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r30,3568(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3568, ctx.r30.u32);
	// stw r10,3632(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3632, ctx.r10.u32);
loc_825E6B28:
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// beq cr6,0x825e6b4c
	if (ctx.cr6.eq) goto loc_825E6B4C;
	// lwz r10,3632(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 3632);
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stw r10,3632(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3632, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// cmpwi cr6,r10,16
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 16, ctx.xer);
	// blt cr6,0x825e6b28
	if (ctx.cr6.lt) goto loc_825E6B28;
loc_825E6B4C:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e6bc0
	if (!ctx.cr6.lt) goto loc_825E6BC0;
loc_825E6B68:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6bc0
	if (ctx.cr6.eq) goto loc_825E6BC0;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6bb0
	if (!ctx.cr0.lt) goto loc_825E6BB0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6BB0;
	sub_825D5398(ctx, base);
loc_825E6BB0:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6b68
	if (ctx.cr6.gt) goto loc_825E6B68;
loc_825E6BC0:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6bfc
	if (!ctx.cr0.lt) goto loc_825E6BFC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6BFC;
	sub_825D5398(ctx, base);
loc_825E6BFC:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e6c74
	if (!ctx.cr6.lt) goto loc_825E6C74;
loc_825E6C1C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6c74
	if (ctx.cr6.eq) goto loc_825E6C74;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6c64
	if (!ctx.cr0.lt) goto loc_825E6C64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6C64;
	sub_825D5398(ctx, base);
loc_825E6C64:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6c1c
	if (ctx.cr6.gt) goto loc_825E6C1C;
loc_825E6C74:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6cb0
	if (!ctx.cr0.lt) goto loc_825E6CB0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6CB0;
	sub_825D5398(ctx, base);
loc_825E6CB0:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e6d2c
	if (!ctx.cr6.lt) goto loc_825E6D2C;
loc_825E6CD4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6d2c
	if (ctx.cr6.eq) goto loc_825E6D2C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6d1c
	if (!ctx.cr0.lt) goto loc_825E6D1C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6D1C;
	sub_825D5398(ctx, base);
loc_825E6D1C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6cd4
	if (ctx.cr6.gt) goto loc_825E6CD4;
loc_825E6D2C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6d68
	if (!ctx.cr0.lt) goto loc_825E6D68;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6D68;
	sub_825D5398(ctx, base);
loc_825E6D68:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,13
	ctx.r30.s64 = 13;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,13
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 13, ctx.xer);
	// bge cr6,0x825e6de4
	if (!ctx.cr6.lt) goto loc_825E6DE4;
loc_825E6D8C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6de4
	if (ctx.cr6.eq) goto loc_825E6DE4;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6dd4
	if (!ctx.cr0.lt) goto loc_825E6DD4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6DD4;
	sub_825D5398(ctx, base);
loc_825E6DD4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6d8c
	if (ctx.cr6.gt) goto loc_825E6D8C;
loc_825E6DE4:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6e20
	if (!ctx.cr0.lt) goto loc_825E6E20;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6E20;
	sub_825D5398(ctx, base);
loc_825E6E20:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,156(r27)
	PPC_STORE_U32(ctx.r27.u32 + 156, ctx.r28.u32);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e6e98
	if (!ctx.cr6.lt) goto loc_825E6E98;
loc_825E6E40:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6e98
	if (ctx.cr6.eq) goto loc_825E6E98;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6e88
	if (!ctx.cr0.lt) goto loc_825E6E88;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6E88;
	sub_825D5398(ctx, base);
loc_825E6E88:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6e40
	if (ctx.cr6.gt) goto loc_825E6E40;
loc_825E6E98:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6ed4
	if (!ctx.cr0.lt) goto loc_825E6ED4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6ED4;
	sub_825D5398(ctx, base);
loc_825E6ED4:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,13
	ctx.r30.s64 = 13;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,13
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 13, ctx.xer);
	// bge cr6,0x825e6f50
	if (!ctx.cr6.lt) goto loc_825E6F50;
loc_825E6EF8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e6f50
	if (ctx.cr6.eq) goto loc_825E6F50;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6f40
	if (!ctx.cr0.lt) goto loc_825E6F40;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6F40;
	sub_825D5398(ctx, base);
loc_825E6F40:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6ef8
	if (ctx.cr6.gt) goto loc_825E6EF8;
loc_825E6F50:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r29
	ctx.r28.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e6f8c
	if (!ctx.cr0.lt) goto loc_825E6F8C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6F8C;
	sub_825D5398(ctx, base);
loc_825E6F8C:
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// stw r28,160(r27)
	PPC_STORE_U32(ctx.r27.u32 + 160, ctx.r28.u32);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e7004
	if (!ctx.cr6.lt) goto loc_825E7004;
loc_825E6FAC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e7004
	if (ctx.cr6.eq) goto loc_825E7004;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e6ff4
	if (!ctx.cr0.lt) goto loc_825E6FF4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E6FF4;
	sub_825D5398(ctx, base);
loc_825E6FF4:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e6fac
	if (ctx.cr6.gt) goto loc_825E6FAC;
loc_825E7004:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7040
	if (!ctx.cr0.lt) goto loc_825E7040;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E7040;
	sub_825D5398(ctx, base);
loc_825E7040:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e70b8
	if (!ctx.cr6.lt) goto loc_825E70B8;
loc_825E7060:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e70b8
	if (ctx.cr6.eq) goto loc_825E70B8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e70a8
	if (!ctx.cr0.lt) goto loc_825E70A8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E70A8;
	sub_825D5398(ctx, base);
loc_825E70A8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e7060
	if (ctx.cr6.gt) goto loc_825E7060;
loc_825E70B8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e70f4
	if (!ctx.cr0.lt) goto loc_825E70F4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E70F4;
	sub_825D5398(ctx, base);
loc_825E70F4:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e7170
	if (!ctx.cr6.lt) goto loc_825E7170;
loc_825E7118:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e7170
	if (ctx.cr6.eq) goto loc_825E7170;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e7160
	if (!ctx.cr0.lt) goto loc_825E7160;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E7160;
	sub_825D5398(ctx, base);
loc_825E7160:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e7118
	if (ctx.cr6.gt) goto loc_825E7118;
loc_825E7170:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e71ac
	if (!ctx.cr0.lt) goto loc_825E71AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E71AC;
	sub_825D5398(ctx, base);
loc_825E71AC:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e7224
	if (!ctx.cr6.lt) goto loc_825E7224;
loc_825E71CC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e7224
	if (ctx.cr6.eq) goto loc_825E7224;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e7214
	if (!ctx.cr0.lt) goto loc_825E7214;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E7214;
	sub_825D5398(ctx, base);
loc_825E7214:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e71cc
	if (ctx.cr6.gt) goto loc_825E71CC;
loc_825E7224:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7260
	if (!ctx.cr0.lt) goto loc_825E7260;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E7260;
	sub_825D5398(ctx, base);
loc_825E7260:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e72dc
	if (!ctx.cr6.lt) goto loc_825E72DC;
loc_825E7284:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e72dc
	if (ctx.cr6.eq) goto loc_825E72DC;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e72cc
	if (!ctx.cr0.lt) goto loc_825E72CC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E72CC;
	sub_825D5398(ctx, base);
loc_825E72CC:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e7284
	if (ctx.cr6.gt) goto loc_825E7284;
loc_825E72DC:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7318
	if (!ctx.cr0.lt) goto loc_825E7318;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E7318;
	sub_825D5398(ctx, base);
loc_825E7318:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825e6298
	if (!ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e7394
	if (!ctx.cr6.lt) goto loc_825E7394;
loc_825E733C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e7394
	if (ctx.cr6.eq) goto loc_825E7394;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e7384
	if (!ctx.cr0.lt) goto loc_825E7384;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E7384;
	sub_825D5398(ctx, base);
loc_825E7384:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e733c
	if (ctx.cr6.gt) goto loc_825E733C;
loc_825E7394:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e73d0
	if (!ctx.cr0.lt) goto loc_825E73D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E73D0;
	sub_825D5398(ctx, base);
loc_825E73D0:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// beq cr6,0x825e6298
	if (ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e744c
	if (!ctx.cr6.lt) goto loc_825E744C;
loc_825E73F4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e744c
	if (ctx.cr6.eq) goto loc_825E744C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e743c
	if (!ctx.cr0.lt) goto loc_825E743C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E743C;
	sub_825D5398(ctx, base);
loc_825E743C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e73f4
	if (ctx.cr6.gt) goto loc_825E73F4;
loc_825E744C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e7488
	if (!ctx.cr0.lt) goto loc_825E7488;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E7488;
	sub_825D5398(ctx, base);
loc_825E7488:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825e6298
	if (ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e7504
	if (!ctx.cr6.lt) goto loc_825E7504;
loc_825E74AC:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e7504
	if (ctx.cr6.eq) goto loc_825E7504;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e74f4
	if (!ctx.cr0.lt) goto loc_825E74F4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E74F4;
	sub_825D5398(ctx, base);
loc_825E74F4:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e74ac
	if (ctx.cr6.gt) goto loc_825E74AC;
loc_825E7504:
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r10,r30,32
	ctx.r10.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf. r11,r30,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// sld r10,r9,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r10.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// bge 0x825e752c
	if (!ctx.cr0.lt) goto loc_825E752C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E752C;
	sub_825D5398(ctx, base);
loc_825E752C:
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// stw r11,3636(r27)
	PPC_STORE_U32(ctx.r27.u32 + 3636, ctx.r11.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e75a8
	if (!ctx.cr6.lt) goto loc_825E75A8;
loc_825E7550:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e75a8
	if (ctx.cr6.eq) goto loc_825E75A8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e7598
	if (!ctx.cr0.lt) goto loc_825E7598;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E7598;
	sub_825D5398(ctx, base);
loc_825E7598:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e7550
	if (ctx.cr6.gt) goto loc_825E7550;
loc_825E75A8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e75e4
	if (!ctx.cr0.lt) goto loc_825E75E4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E75E4;
	sub_825D5398(ctx, base);
loc_825E75E4:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// beq cr6,0x825e6298
	if (ctx.cr6.eq) goto loc_825E6298;
	// lwz r31,84(r27)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e7660
	if (!ctx.cr6.lt) goto loc_825E7660;
loc_825E7608:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e7660
	if (ctx.cr6.eq) goto loc_825E7660;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e7650
	if (!ctx.cr0.lt) goto loc_825E7650;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E7650;
	sub_825D5398(ctx, base);
loc_825E7650:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e7608
	if (ctx.cr6.gt) goto loc_825E7608;
loc_825E7660:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e769c
	if (!ctx.cr0.lt) goto loc_825E769C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E769C;
	sub_825D5398(ctx, base);
loc_825E769C:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// beq cr6,0x825e6298
	if (ctx.cr6.eq) goto loc_825E6298;
	// lwz r3,84(r27)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r27.u32 + 84);
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r11,8(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// clrlwi r11,r11,29
	ctx.r11.u64 = ctx.r11.u32 & 0x7;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e76c0
	if (ctx.cr6.eq) goto loc_825E76C0;
	// mr r4,r11
	ctx.r4.u64 = ctx.r11.u64;
loc_825E76C0:
	// bl 0x825d5468
	ctx.lr = 0x825E76C4;
	sub_825D5468(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x825dea98
	ctx.lr = 0x825E76D0;
	sub_825DEA98(ctx, base);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
}

__attribute__((alias("__imp__sub_825E76D8"))) PPC_WEAK_FUNC(sub_825E76D8);
PPC_FUNC_IMPL(__imp__sub_825E76D8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9e8
	ctx.lr = 0x825E76E0;
	sub_8239B9E8(ctx, base);
	// stwu r1,-272(r1)
	ea = -272 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r17,364(r1)
	ctx.r17.u64 = PPC_LOAD_U32(ctx.r1.u32 + 364);
	// lwz r18,356(r1)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r1.u32 + 356);
	// mr r21,r10
	ctx.r21.u64 = ctx.r10.u64;
	// mr r22,r4
	ctx.r22.u64 = ctx.r4.u64;
	// mr r28,r5
	ctx.r28.u64 = ctx.r5.u64;
	// mr r19,r6
	ctx.r19.u64 = ctx.r6.u64;
	// lwz r11,392(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 392);
	// mr r16,r7
	ctx.r16.u64 = ctx.r7.u64;
	// mr r30,r8
	ctx.r30.u64 = ctx.r8.u64;
	// stw r17,112(r1)
	PPC_STORE_U32(ctx.r1.u32 + 112, ctx.r17.u32);
	// mr r26,r9
	ctx.r26.u64 = ctx.r9.u64;
	// stw r18,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r18.u32);
	// stw r21,120(r1)
	PPC_STORE_U32(ctx.r1.u32 + 120, ctx.r21.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r17,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r17.u32);
	// beq cr6,0x825e7750
	if (ctx.cr6.eq) goto loc_825E7750;
	// lwz r11,0(r22)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r22.u32 + 0);
	// rlwinm r11,r11,10,30,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 10) & 0x3;
	// addi r10,r11,726
	ctx.r10.s64 = ctx.r11.s64 + 726;
	// addi r11,r11,729
	ctx.r11.s64 = ctx.r11.s64 + 729;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r10,r31
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r31.u32);
	// stw r10,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r10.u32);
	// lwzx r11,r11,r31
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r31.u32);
	// b 0x825e775c
	goto loc_825E775C;
loc_825E7750:
	// lwz r11,2880(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2880);
	// stw r11,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r11.u32);
	// lwz r11,2892(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2892);
loc_825E775C:
	// stw r11,104(r1)
	PPC_STORE_U32(ctx.r1.u32 + 104, ctx.r11.u32);
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// li r20,0
	ctx.r20.s64 = 0;
	// addi r24,r11,-21616
	ctx.r24.s64 = ctx.r11.s64 + -21616;
	// lis r11,-32138
	ctx.r11.s64 = -2106195968;
	// mr r29,r20
	ctx.r29.u64 = ctx.r20.u64;
	// addi r27,r11,-31184
	ctx.r27.s64 = ctx.r11.s64 + -31184;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r23,2
	ctx.r23.s64 = 131072;
	// addi r25,r11,17200
	ctx.r25.s64 = ctx.r11.s64 + 17200;
loc_825E7784:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// blt cr6,0x825e77ac
	if (ctx.cr6.lt) goto loc_825E77AC;
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,296(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 296);
	// lwz r5,2092(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2092);
	// bl 0x8263f000
	ctx.lr = 0x825E77A8;
	sub_8263F000(ctx, base);
	// b 0x825e77d0
	goto loc_825E77D0;
loc_825E77AC:
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825e77c4
	if (!ctx.cr6.eq) goto loc_825E77C4;
	// li r6,7
	ctx.r6.s64 = 7;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// bl 0x825e37e0
	ctx.lr = 0x825E77C0;
	sub_825E37E0(ctx, base);
	// b 0x825e77d0
	goto loc_825E77D0;
loc_825E77C4:
	// li r6,11
	ctx.r6.s64 = 11;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// bl 0x825e4ce0
	ctx.lr = 0x825E77D0;
	sub_825E4CE0(ctx, base);
loc_825E77D0:
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e7bbc
	if (!ctx.cr6.eq) goto loc_825E7BBC;
	// rlwinm r11,r29,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r1,112
	ctx.r10.s64 = ctx.r1.s64 + 112;
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// mr r6,r18
	ctx.r6.u64 = ctx.r18.u64;
	// mr r5,r21
	ctx.r5.u64 = ctx.r21.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwzx r7,r11,r10
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r10.u32);
	// bl 0x82632f68
	ctx.lr = 0x825E7800;
	sub_82632F68(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lhz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// beq cr6,0x825e7840
	if (ctx.cr6.eq) goto loc_825E7840;
	// lwz r9,1760(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(ctx.r30.u32 + 0, ctx.r11.u16);
	// lwz r11,296(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 296);
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// mullw r11,r11,r9
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r9.s32);
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r11.u32);
	// b 0x825e7890
	goto loc_825E7890;
loc_825E7840:
	// lwz r9,296(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 296);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwzx r8,r7,r24
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r24.u32);
	// mullw r11,r8,r11
	ctx.r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
	// add r11,r11,r23
	ctx.r11.u64 = ctx.r11.u64 + ctx.r23.u64;
	// srawi r11,r11,18
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3FFFF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 18;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(ctx.r30.u32 + 0, ctx.r11.u16);
	// lwz r11,296(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 296);
	// mullw r11,r9,r11
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(ctx.r30.u32 + 0, ctx.r11.u16);
	// lwz r11,1760(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
loc_825E7890:
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r9,r30
	ctx.r9.u64 = ctx.r30.u64;
	// mr r8,r29
	ctx.r8.u64 = ctx.r29.u64;
	// lwz r7,204(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r20.u32);
	// mr r5,r22
	ctx.r5.u64 = ctx.r22.u64;
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// bl 0x82632ac8
	ctx.lr = 0x825E78BC;
	sub_82632AC8(ctx, base);
	// stw r3,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e7bbc
	if (!ctx.cr6.eq) goto loc_825E7BBC;
	// cmplwi cr6,r29,1
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 1, ctx.xer);
	// li r11,8
	ctx.r11.s64 = 8;
	// bne cr6,0x825e78d8
	if (!ctx.cr6.eq) goto loc_825E78D8;
	// lwz r11,236(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 236);
loc_825E78D8:
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// add r28,r11,r28
	ctx.r28.u64 = ctx.r11.u64 + ctx.r28.u64;
	// addi r30,r30,32
	ctx.r30.s64 = ctx.r30.s64 + 32;
	// addi r26,r26,24
	ctx.r26.s64 = ctx.r26.s64 + 24;
	// cmplwi cr6,r29,4
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 4, ctx.xer);
	// blt cr6,0x825e7784
	if (ctx.cr6.lt) goto loc_825E7784;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// addi r28,r10,17456
	ctx.r28.s64 = ctx.r10.s64 + 17456;
	// lis r10,-32138
	ctx.r10.s64 = -2106195968;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// addi r27,r10,-27088
	ctx.r27.s64 = ctx.r10.s64 + -27088;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// blt cr6,0x825e7928
	if (ctx.cr6.lt) goto loc_825E7928;
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,300(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	// lwz r5,2096(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2096);
	// bl 0x8263f000
	ctx.lr = 0x825E7924;
	sub_8263F000(ctx, base);
	// b 0x825e794c
	goto loc_825E794C;
loc_825E7928:
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825e7940
	if (!ctx.cr6.eq) goto loc_825E7940;
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// bl 0x825e37e0
	ctx.lr = 0x825E793C;
	sub_825E37E0(ctx, base);
	// b 0x825e794c
	goto loc_825E794C;
loc_825E7940:
	// li r6,12
	ctx.r6.s64 = 12;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// bl 0x825e4ce0
	ctx.lr = 0x825E794C;
	sub_825E4CE0(ctx, base);
loc_825E794C:
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e7bbc
	if (!ctx.cr6.eq) goto loc_825E7BBC;
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// mr r7,r17
	ctx.r7.u64 = ctx.r17.u64;
	// mr r6,r18
	ctx.r6.u64 = ctx.r18.u64;
	// mr r5,r21
	ctx.r5.u64 = ctx.r21.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82632f68
	ctx.lr = 0x825E7974;
	sub_82632F68(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lhz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// beq cr6,0x825e79b4
	if (ctx.cr6.eq) goto loc_825E79B4;
	// lwz r9,1760(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(ctx.r30.u32 + 0, ctx.r11.u16);
	// lwz r11,300(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// mullw r11,r9,r11
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r11.u32);
	// b 0x825e7a04
	goto loc_825E7A04;
loc_825E79B4:
	// lwz r9,300(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// rlwinm r7,r9,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwzx r8,r7,r24
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r24.u32);
	// mullw r11,r8,r11
	ctx.r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
	// add r11,r11,r23
	ctx.r11.u64 = ctx.r11.u64 + ctx.r23.u64;
	// srawi r11,r11,18
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3FFFF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 18;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(ctx.r30.u32 + 0, ctx.r11.u16);
	// lwz r11,300(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	// mullw r11,r9,r11
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(ctx.r30.u32 + 0, ctx.r11.u16);
	// lwz r11,1760(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
loc_825E7A04:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// mr r9,r30
	ctx.r9.u64 = ctx.r30.u64;
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// li r8,4
	ctx.r8.s64 = 4;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r6,r19
	ctx.r6.u64 = ctx.r19.u64;
	// mr r5,r22
	ctx.r5.u64 = ctx.r22.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// beq cr6,0x825e7a40
	if (ctx.cr6.eq) goto loc_825E7A40;
	// addi r4,r1,108
	ctx.r4.s64 = ctx.r1.s64 + 108;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r20.u32);
	// bl 0x82632ac8
	ctx.lr = 0x825E7A3C;
	sub_82632AC8(ctx, base);
	// b 0x825e7a50
	goto loc_825E7A50;
loc_825E7A40:
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r20.u32);
	// bl 0x82632ac8
	ctx.lr = 0x825E7A50;
	sub_82632AC8(ctx, base);
loc_825E7A50:
	// stw r3,96(r1)
	PPC_STORE_U32(ctx.r1.u32 + 96, ctx.r3.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e7bbc
	if (!ctx.cr6.eq) goto loc_825E7BBC;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// addi r30,r30,32
	ctx.r30.s64 = ctx.r30.s64 + 32;
	// addi r29,r26,24
	ctx.r29.s64 = ctx.r26.s64 + 24;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// addi r4,r1,96
	ctx.r4.s64 = ctx.r1.s64 + 96;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// blt cr6,0x825e7a8c
	if (ctx.cr6.lt) goto loc_825E7A8C;
	// li r6,119
	ctx.r6.s64 = 119;
	// lwz r7,300(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	// lwz r5,2096(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2096);
	// bl 0x8263f000
	ctx.lr = 0x825E7A88;
	sub_8263F000(ctx, base);
	// b 0x825e7ab0
	goto loc_825E7AB0;
loc_825E7A8C:
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bne cr6,0x825e7aa4
	if (!ctx.cr6.eq) goto loc_825E7AA4;
	// li r6,8
	ctx.r6.s64 = 8;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// bl 0x825e37e0
	ctx.lr = 0x825E7AA0;
	sub_825E37E0(ctx, base);
	// b 0x825e7ab0
	goto loc_825E7AB0;
loc_825E7AA4:
	// li r6,12
	ctx.r6.s64 = 12;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// bl 0x825e4ce0
	ctx.lr = 0x825E7AB0;
	sub_825E4CE0(ctx, base);
loc_825E7AB0:
	// lwz r3,96(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 96);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e7bbc
	if (!ctx.cr6.eq) goto loc_825E7BBC;
	// addi r8,r1,100
	ctx.r8.s64 = ctx.r1.s64 + 100;
	// mr r7,r17
	ctx.r7.u64 = ctx.r17.u64;
	// mr r6,r18
	ctx.r6.u64 = ctx.r18.u64;
	// mr r5,r21
	ctx.r5.u64 = ctx.r21.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82632f68
	ctx.lr = 0x825E7AD8;
	sub_82632F68(ctx, base);
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e7b18
	if (ctx.cr6.eq) goto loc_825E7B18;
	// lwz r9,1760(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// lhz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(ctx.r30.u32 + 0, ctx.r11.u16);
	// lwz r11,300(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// mullw r11,r9,r11
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// stw r11,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r11.u32);
	// b 0x825e7b6c
	goto loc_825E7B6C;
loc_825E7B18:
	// lwz r11,300(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	// lhz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r10.u32 + 0);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r8,1760(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// extsh r7,r9
	ctx.r7.s64 = ctx.r9.s16;
	// lwzx r11,r11,r24
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r24.u32);
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// mullw r11,r11,r7
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r7.s32);
	// add r11,r11,r23
	ctx.r11.u64 = ctx.r11.u64 + ctx.r23.u64;
	// srawi r11,r11,18
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3FFFF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 18;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(ctx.r30.u32 + 0, ctx.r11.u16);
	// lwz r11,300(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	// mullw r11,r9,r11
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// extsh r11,r11
	ctx.r11.s64 = ctx.r11.s16;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// sth r11,0(r30)
	PPC_STORE_U16(ctx.r30.u32 + 0, ctx.r11.u16);
	// lwz r11,1760(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1760);
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
loc_825E7B6C:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// mr r9,r30
	ctx.r9.u64 = ctx.r30.u64;
	// lwz r7,208(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// li r8,5
	ctx.r8.s64 = 5;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r11,100(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r6,r16
	ctx.r6.u64 = ctx.r16.u64;
	// mr r5,r22
	ctx.r5.u64 = ctx.r22.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// beq cr6,0x825e7bac
	if (ctx.cr6.eq) goto loc_825E7BAC;
	// addi r4,r1,108
	ctx.r4.s64 = ctx.r1.s64 + 108;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r20.u32);
	// bl 0x82632ac8
	ctx.lr = 0x825E7BA4;
	sub_82632AC8(ctx, base);
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239ba38
	// ERROR 8239BA38
	return;
loc_825E7BAC:
	// addi r4,r1,104
	ctx.r4.s64 = ctx.r1.s64 + 104;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// stw r20,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r20.u32);
	// bl 0x82632ac8
	ctx.lr = 0x825E7BBC;
	sub_82632AC8(ctx, base);
loc_825E7BBC:
	// addi r1,r1,272
	ctx.r1.s64 = ctx.r1.s64 + 272;
	// b 0x8239ba38
	// ERROR 8239BA38
	return;
}

__attribute__((alias("__imp__sub_825E7BC4"))) PPC_WEAK_FUNC(sub_825E7BC4);
PPC_FUNC_IMPL(__imp__sub_825E7BC4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E7BC8"))) PPC_WEAK_FUNC(sub_825E7BC8);
PPC_FUNC_IMPL(__imp__sub_825E7BC8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba10
	ctx.lr = 0x825E7BD0;
	sub_8239BA10(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// mr r8,r7
	ctx.r8.u64 = ctx.r7.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// stw r11,452(r31)
	PPC_STORE_U32(ctx.r31.u32 + 452, ctx.r11.u32);
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// stw r11,332(r31)
	PPC_STORE_U32(ctx.r31.u32 + 332, ctx.r11.u32);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// bl 0x82623e30
	ctx.lr = 0x825E7C04;
	sub_82623E30(ctx, base);
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e7d38
	if (ctx.cr6.eq) goto loc_825E7D38;
	// lwz r30,84(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// addi r28,r11,17968
	ctx.r28.s64 = ctx.r11.s64 + 17968;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// rldicl r11,r11,13,51
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 13) & 0x1FFF;
	// rlwinm r29,r11,1,0,30
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r29,r28
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r29.u32 + ctx.r28.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E7C34;
	sub_825D5468(ctx, base);
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// li r26,3
	ctx.r26.s64 = 3;
	// lbzx r11,r29,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r29.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e7c4c
	if (!ctx.cr6.eq) goto loc_825E7C4C;
	// stw r26,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r26.u32);
loc_825E7C4C:
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825e7c68
	if (ctx.cr6.eq) goto loc_825E7C68;
loc_825E7C5C:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
loc_825E7C68:
	// lbz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// lwz r10,244(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 244);
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// ble cr6,0x825e7c90
	if (!ctx.cr6.gt) goto loc_825E7C90;
	// addi r11,r11,-64
	ctx.r11.s64 = ctx.r11.s64 + -64;
	// b 0x825e7ca0
	goto loc_825E7CA0;
loc_825E7C90:
	// lwz r10,240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 240);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x825e7ca0
	if (!ctx.cr6.lt) goto loc_825E7CA0;
	// addi r11,r11,64
	ctx.r11.s64 = ctx.r11.s64 + 64;
loc_825E7CA0:
	// stb r11,0(r27)
	PPC_STORE_U8(ctx.r27.u32 + 0, ctx.r11.u8);
	// lwz r30,84(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// rldicl r11,r11,13,51
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 13) & 0x1FFF;
	// rlwinm r29,r11,1,0,30
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r4,r29,r28
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r29.u32 + ctx.r28.u32);
	// bl 0x825d5468
	ctx.lr = 0x825E7CC0;
	sub_825D5468(ctx, base);
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// lbzx r11,r29,r11
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r29.u32 + ctx.r11.u32);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// bne cr6,0x825e7cd4
	if (!ctx.cr6.eq) goto loc_825E7CD4;
	// stw r26,20(r30)
	PPC_STORE_U32(ctx.r30.u32 + 20, ctx.r26.u32);
loc_825E7CD4:
	// lwz r10,84(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// lwz r10,20(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 20);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne cr6,0x825e7c5c
	if (!ctx.cr6.eq) goto loc_825E7C5C;
	// lbz r10,81(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// clrlwi r11,r11,24
	ctx.r11.u64 = ctx.r11.u32 & 0xFF;
	// lwz r9,244(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 244);
	// extsb r10,r10
	ctx.r10.s64 = ctx.r10.s8;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r11,r11,-32
	ctx.r11.s64 = ctx.r11.s64 + -32;
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// ble cr6,0x825e7d18
	if (!ctx.cr6.gt) goto loc_825E7D18;
	// addi r11,r11,-64
	ctx.r11.s64 = ctx.r11.s64 + -64;
	// li r3,0
	ctx.r3.s64 = 0;
	// stb r11,1(r27)
	PPC_STORE_U8(ctx.r27.u32 + 1, ctx.r11.u8);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
loc_825E7D18:
	// lwz r10,240(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 240);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bge cr6,0x825e7d28
	if (!ctx.cr6.lt) goto loc_825E7D28;
	// addi r11,r11,64
	ctx.r11.s64 = ctx.r11.s64 + 64;
loc_825E7D28:
	// stb r11,1(r27)
	PPC_STORE_U8(ctx.r27.u32 + 1, ctx.r11.u8);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
loc_825E7D38:
	// addi r4,r1,82
	ctx.r4.s64 = ctx.r1.s64 + 82;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825e49d8
	ctx.lr = 0x825E7D44;
	sub_825E49D8(ctx, base);
	// lbz r8,80(r1)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// lbz r10,81(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// addi r3,r1,80
	ctx.r3.s64 = ctx.r1.s64 + 80;
	// lbz r9,82(r1)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r1.u32 + 82);
	// lbz r11,83(r1)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 83);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lwz r4,3560(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3560);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stb r9,80(r1)
	PPC_STORE_U8(ctx.r1.u32 + 80, ctx.r9.u8);
	// stb r11,81(r1)
	PPC_STORE_U8(ctx.r1.u32 + 81, ctx.r11.u8);
	// bl 0x825e4968
	ctx.lr = 0x825E7D70;
	sub_825E4968(ctx, base);
	// lbz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r1.u32 + 80);
	// li r3,0
	ctx.r3.s64 = 0;
	// lbz r10,81(r1)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r1.u32 + 81);
	// stb r11,0(r27)
	PPC_STORE_U8(ctx.r27.u32 + 0, ctx.r11.u8);
	// stb r10,1(r27)
	PPC_STORE_U8(ctx.r27.u32 + 1, ctx.r10.u8);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
}

__attribute__((alias("__imp__sub_825E7D8C"))) PPC_WEAK_FUNC(sub_825E7D8C);
PPC_FUNC_IMPL(__imp__sub_825E7D8C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E7D90"))) PPC_WEAK_FUNC(sub_825E7D90);
PPC_FUNC_IMPL(__imp__sub_825E7D90) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba18
	ctx.lr = 0x825E7D98;
	sub_8239BA18(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r11,r5,4096
	ctx.r11.s64 = ctx.r5.s64 + 4096;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// li r28,0
	ctx.r28.s64 = 0;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// mr r29,r28
	ctx.r29.u64 = ctx.r28.u64;
	// stw r11,21324(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21324, ctx.r11.u32);
	// beq cr6,0x825e7df0
	if (ctx.cr6.eq) goto loc_825E7DF0;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825edb18
	ctx.lr = 0x825E7DC8;
	sub_825EDB18(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825e7de0
	if (!ctx.cr6.eq) goto loc_825E7DE0;
loc_825E7DD4:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
loc_825E7DE0:
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r4,21320(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21320);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825E7DF0;
	sub_8239CB70(ctx, base);
loc_825E7DF0:
	// lwz r3,21320(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21320);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825e7e04
	if (ctx.cr6.eq) goto loc_825E7E04;
	// bl 0x825edb28
	ctx.lr = 0x825E7E00;
	sub_825EDB28(ctx, base);
	// stw r28,21320(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21320, ctx.r28.u32);
loc_825E7E04:
	// lwz r3,21316(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21316);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825e7e18
	if (ctx.cr6.eq) goto loc_825E7E18;
	// bl 0x825edb28
	ctx.lr = 0x825E7E14;
	sub_825EDB28(ctx, base);
	// stw r28,21316(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21316, ctx.r28.u32);
loc_825E7E18:
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,21324(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21324);
	// bl 0x825edb18
	ctx.lr = 0x825E7E24;
	sub_825EDB18(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,21316(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21316, ctx.r3.u32);
	// beq cr6,0x825e7dd4
	if (ctx.cr6.eq) goto loc_825E7DD4;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,21324(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21324);
	// bl 0x825edb18
	ctx.lr = 0x825E7E3C;
	sub_825EDB18(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,21320(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21320, ctx.r3.u32);
	// beq cr6,0x825e7dd4
	if (ctx.cr6.eq) goto loc_825E7DD4;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e7e5c
	if (ctx.cr6.eq) goto loc_825E7E5C;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825E7E5C;
	sub_8239CB70(ctx, base);
loc_825E7E5C:
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x825e7e6c
	if (ctx.cr6.eq) goto loc_825E7E6C;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x825edb28
	ctx.lr = 0x825E7E6C;
	sub_825EDB28(ctx, base);
loc_825E7E6C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
}

__attribute__((alias("__imp__sub_825E7E78"))) PPC_WEAK_FUNC(sub_825E7E78);
PPC_FUNC_IMPL(__imp__sub_825E7E78) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba10
	ctx.lr = 0x825E7E80;
	sub_8239BA10(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r6
	ctx.r30.u64 = ctx.r6.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// mr r26,r7
	ctx.r26.u64 = ctx.r7.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825e7ea8
	if (!ctx.cr6.eq) goto loc_825E7EA8;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// bne cr6,0x825e7f60
	if (!ctx.cr6.eq) goto loc_825E7F60;
loc_825E7EA8:
	// lwz r11,21304(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21304);
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r11,r30
	ctx.r3.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825edb18
	ctx.lr = 0x825E7EB8;
	sub_825EDB18(ctx, base);
	// lwz r5,21304(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21304);
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// ble cr6,0x825e7ed8
	if (!ctx.cr6.gt) goto loc_825E7ED8;
	// lwz r4,21308(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21308);
	// bl 0x8239cb70
	ctx.lr = 0x825E7ED0;
	sub_8239CB70(ctx, base);
	// lwz r3,21308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21308);
	// bl 0x825edb28
	ctx.lr = 0x825E7ED8;
	sub_825EDB28(ctx, base);
loc_825E7ED8:
	// lwz r11,21304(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21304);
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bne cr6,0x825e7f0c
	if (!ctx.cr6.eq) goto loc_825E7F0C;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// bne cr6,0x825e7f04
	if (!ctx.cr6.eq) goto loc_825E7F04;
	// li r7,3
	ctx.r7.s64 = 3;
	// b 0x825e7f1c
	goto loc_825E7F1C;
loc_825E7F04:
	// li r7,1
	ctx.r7.s64 = 1;
	// b 0x825e7f1c
	goto loc_825E7F1C;
loc_825E7F0C:
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// li r7,2
	ctx.r7.s64 = 2;
	// beq cr6,0x825e7f1c
	if (ctx.cr6.eq) goto loc_825E7F1C;
	// li r7,0
	ctx.r7.s64 = 0;
loc_825E7F1C:
	// bl 0x825d0118
	ctx.lr = 0x825E7F20;
	sub_825D0118(ctx, base);
	// lwz r11,21304(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21304);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// add r3,r11,r27
	ctx.r3.u64 = ctx.r11.u64 + ctx.r27.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825E7F34;
	sub_8239CB70(ctx, base);
	// lwz r11,21304(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21304);
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// stw r27,21308(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21308, ctx.r27.u32);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// stw r11,21304(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21304, ctx.r11.u32);
	// bne cr6,0x825e7f60
	if (!ctx.cr6.eq) goto loc_825E7F60;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x825edb28
	ctx.lr = 0x825E7F54;
	sub_825EDB28(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,21304(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21304, ctx.r11.u32);
	// stw r11,21308(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21308, ctx.r11.u32);
loc_825E7F60:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
}

__attribute__((alias("__imp__sub_825E7F6C"))) PPC_WEAK_FUNC(sub_825E7F6C);
PPC_FUNC_IMPL(__imp__sub_825E7F6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E7F70"))) PPC_WEAK_FUNC(sub_825E7F70);
PPC_FUNC_IMPL(__imp__sub_825E7F70) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9e0
	ctx.lr = 0x825E7F78;
	sub_8239B9E0(ctx, base);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r15,r8
	ctx.r15.u64 = ctx.r8.u64;
	// stw r4,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r4.u32);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// stw r5,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r5.u32);
	// li r24,0
	ctx.r24.s64 = 0;
	// li r22,1
	ctx.r22.s64 = 1;
	// mr r28,r7
	ctx.r28.u64 = ctx.r7.u64;
	// lwz r11,0(r15)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r15.u32 + 0);
	// mr r14,r6
	ctx.r14.u64 = ctx.r6.u64;
	// lwz r26,21320(r30)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21320);
	// mr r21,r24
	ctx.r21.u64 = ctx.r24.u64;
	// lwz r23,21316(r30)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21316);
	// mr r25,r24
	ctx.r25.u64 = ctx.r24.u64;
	// stw r24,21344(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21344, ctx.r24.u32);
	// mr r17,r24
	ctx.r17.u64 = ctx.r24.u64;
	// stw r28,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, ctx.r28.u32);
	// mr r19,r24
	ctx.r19.u64 = ctx.r24.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// mr r31,r24
	ctx.r31.u64 = ctx.r24.u64;
	// lwz r11,80(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 80);
	// mr r16,r24
	ctx.r16.u64 = ctx.r24.u64;
	// stw r22,32(r11)
	PPC_STORE_U32(ctx.r11.u32 + 32, ctx.r22.u32);
	// lwz r11,3676(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 3676);
	// stw r24,21348(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21348, ctx.r24.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e8004
	if (!ctx.cr6.eq) goto loc_825E8004;
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r7,21344(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21344);
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// stw r24,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r24.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825cedb0
	ctx.lr = 0x825E7FFC;
	sub_825CEDB0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e8924
	if (!ctx.cr6.eq) goto loc_825E8924;
loc_825E8004:
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x825e8920
	if (ctx.cr6.eq) goto loc_825E8920;
	// cmplwi cr6,r14,0
	ctx.cr6.compare<uint32_t>(ctx.r14.u32, 0, ctx.xer);
	// beq cr6,0x825e8920
	if (ctx.cr6.eq) goto loc_825E8920;
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// beq cr6,0x825e8920
	if (ctx.cr6.eq) goto loc_825E8920;
	// lwz r29,276(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// cmplwi cr6,r29,4
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 4, ctx.xer);
	// bge cr6,0x825e8044
	if (!ctx.cr6.lt) goto loc_825E8044;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825E8038;
	sub_8239CB70(ctx, base);
	// lwz r29,276(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mr r31,r29
	ctx.r31.u64 = ctx.r29.u64;
loc_825E8044:
	// lwz r11,21324(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21324);
	// add r5,r31,r29
	ctx.r5.u64 = ctx.r31.u64 + ctx.r29.u64;
	// cmplw cr6,r5,r11
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e8078
	if (!ctx.cr6.gt) goto loc_825E8078;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7d90
	ctx.lr = 0x825E8060;
	sub_825E7D90(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e8924
	if (!ctx.cr6.eq) goto loc_825E8924;
	// lwz r26,21320(r30)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21320);
	// lwz r23,21316(r30)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21316);
	// lwz r29,276(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
loc_825E8078:
	// cmplwi cr6,r29,4
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 4, ctx.xer);
	// bge cr6,0x825e8114
	if (!ctx.cr6.lt) goto loc_825E8114;
loc_825E8080:
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e813c
	if (ctx.cr6.eq) goto loc_825E813C;
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// lwz r3,3340(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 3340);
	// addi r7,r1,276
	ctx.r7.s64 = ctx.r1.s64 + 276;
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,268
	ctx.r5.s64 = ctx.r1.s64 + 268;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8248c788
	ctx.lr = 0x825E80A8;
	sub_8248C788(ctx, base);
	// lwz r11,80(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 80);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r10,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r10.u32);
	// lwz r11,276(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// add r5,r31,r11
	ctx.r5.u64 = ctx.r31.u64 + ctx.r11.u64;
	// lwz r11,21324(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21324);
	// cmplw cr6,r5,r11
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e80e4
	if (!ctx.cr6.gt) goto loc_825E80E4;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7d90
	ctx.lr = 0x825E80D4;
	sub_825E7D90(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e8924
	if (!ctx.cr6.eq) goto loc_825E8924;
	// lwz r26,21320(r30)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21320);
	// lwz r23,21316(r30)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21316);
loc_825E80E4:
	// add r3,r31,r26
	ctx.r3.u64 = ctx.r31.u64 + ctx.r26.u64;
	// lwz r5,276(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// bl 0x8239cb70
	ctx.lr = 0x825E80F4;
	sub_8239CB70(ctx, base);
	// lwz r11,276(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// add r31,r31,r11
	ctx.r31.u64 = ctx.r31.u64 + ctx.r11.u64;
	// mr r29,r31
	ctx.r29.u64 = ctx.r31.u64;
	// cmplwi cr6,r31,4
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 4, ctx.xer);
	// stw r4,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r4.u32);
	// stw r29,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r29.u32);
	// blt cr6,0x825e8080
	if (ctx.cr6.lt) goto loc_825E8080;
loc_825E8114:
	// lbz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825e813c
	if (!ctx.cr6.eq) goto loc_825E813C;
	// lbz r11,1(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825e813c
	if (!ctx.cr6.eq) goto loc_825E813C;
	// lbz r11,2(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// mr r11,r24
	ctx.r11.u64 = ctx.r24.u64;
	// beq cr6,0x825e8140
	if (ctx.cr6.eq) goto loc_825E8140;
loc_825E813C:
	// mr r11,r22
	ctx.r11.u64 = ctx.r22.u64;
loc_825E8140:
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r11,21292(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21292, ctx.r11.u32);
	// stw r24,21340(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21340, ctx.r24.u32);
	// beq cr6,0x825e816c
	if (ctx.cr6.eq) goto loc_825E816C;
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r29,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r29.u32);
	// stw r4,0(r14)
	PPC_STORE_U32(ctx.r14.u32 + 0, ctx.r4.u32);
	// stw r11,0(r15)
	PPC_STORE_U32(ctx.r15.u32 + 0, ctx.r11.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825E816C:
	// li r20,3
	ctx.r20.s64 = 3;
	// li r18,2
	ctx.r18.s64 = 2;
	// cmplwi cr6,r29,4
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 4, ctx.xer);
	// blt cr6,0x825e8840
	if (ctx.cr6.lt) goto loc_825E8840;
loc_825E817C:
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// bne cr6,0x825e883c
	if (!ctx.cr6.eq) goto loc_825E883C;
loc_825E8184:
	// addi r11,r4,4
	ctx.r11.s64 = ctx.r4.s64 + 4;
	// lbz r3,3(r4)
	ctx.r3.u64 = PPC_LOAD_U8(ctx.r4.u32 + 3);
	// cmplwi cr6,r29,8
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 8, ctx.xer);
	// mr r27,r11
	ctx.r27.u64 = ctx.r11.u64;
	// blt cr6,0x825e8274
	if (ctx.cr6.lt) goto loc_825E8274;
	// clrlwi r9,r4,31
	ctx.r9.u64 = ctx.r4.u32 & 0x1;
	// li r10,4
	ctx.r10.s64 = 4;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x825e81dc
	if (ctx.cr6.eq) goto loc_825E81DC;
	// lbz r11,0(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825e81d4
	if (!ctx.cr6.eq) goto loc_825E81D4;
	// lbz r11,5(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 5);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825e81d4
	if (!ctx.cr6.eq) goto loc_825E81D4;
	// lbz r11,6(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 6);
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bne cr6,0x825e81d4
	if (!ctx.cr6.eq) goto loc_825E81D4;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// b 0x825e826c
	goto loc_825E826C;
loc_825E81D4:
	// addi r11,r4,5
	ctx.r11.s64 = ctx.r4.s64 + 5;
	// li r10,5
	ctx.r10.s64 = 5;
loc_825E81DC:
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// addi r5,r29,-1
	ctx.r5.s64 = ctx.r29.s64 + -1;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmplw cr6,r10,r5
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, ctx.xer);
	// bge cr6,0x825e8274
	if (!ctx.cr6.lt) goto loc_825E8274;
	// addi r6,r10,2
	ctx.r6.s64 = ctx.r10.s64 + 2;
loc_825E81F8:
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// and r31,r9,r7
	ctx.r31.u64 = ctx.r9.u64 & ctx.r7.u64;
	// clrlwi r31,r31,16
	ctx.r31.u64 = ctx.r31.u32 & 0xFFFF;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825e8244
	if (!ctx.cr6.eq) goto loc_825E8244;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825e8228
	if (!ctx.cr6.eq) goto loc_825E8228;
	// rlwinm r31,r7,0,16,23
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFF00;
	// cmplwi cr6,r31,256
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 256, ctx.xer);
	// beq cr6,0x825e8260
	if (ctx.cr6.eq) goto loc_825E8260;
loc_825E8228:
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825e8244
	if (!ctx.cr6.eq) goto loc_825E8244;
	// cmplwi cr6,r7,1
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 1, ctx.xer);
	// bne cr6,0x825e8244
	if (!ctx.cr6.eq) goto loc_825E8244;
	// cmplw cr6,r29,r6
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, ctx.r6.u32, ctx.xer);
	// bgt cr6,0x825e8268
	if (ctx.cr6.gt) goto loc_825E8268;
loc_825E8244:
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmplw cr6,r10,r5
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, ctx.xer);
	// blt cr6,0x825e81f8
	if (ctx.cr6.lt) goto loc_825E81F8;
	// b 0x825e8274
	goto loc_825E8274;
loc_825E8260:
	// addi r28,r11,-2
	ctx.r28.s64 = ctx.r11.s64 + -2;
	// b 0x825e826c
	goto loc_825E826C;
loc_825E8268:
	// addi r28,r11,-1
	ctx.r28.s64 = ctx.r11.s64 + -1;
loc_825E826C:
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// bne cr6,0x825e827c
	if (!ctx.cr6.eq) goto loc_825E827C;
loc_825E8274:
	// add r28,r4,r29
	ctx.r28.u64 = ctx.r4.u64 + ctx.r29.u64;
	// mr r25,r22
	ctx.r25.u64 = ctx.r22.u64;
loc_825E827C:
	// clrlwi r11,r3,24
	ctx.r11.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r11,13
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 13, ctx.xer);
	// stw r11,21296(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21296, ctx.r11.u32);
	// beq cr6,0x825e829c
	if (ctx.cr6.eq) goto loc_825E829C;
	// cmplwi cr6,r11,12
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 12, ctx.xer);
	// beq cr6,0x825e829c
	if (ctx.cr6.eq) goto loc_825E829C;
	// cmplwi cr6,r11,11
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 11, ctx.xer);
	// bne cr6,0x825e82a0
	if (!ctx.cr6.eq) goto loc_825E82A0;
loc_825E829C:
	// mr r17,r22
	ctx.r17.u64 = ctx.r22.u64;
loc_825E82A0:
	// cmpwi cr6,r25,1
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 1, ctx.xer);
	// bne cr6,0x825e8350
	if (!ctx.cr6.eq) goto loc_825E8350;
	// cmpwi cr6,r17,0
	ctx.cr6.compare<int32_t>(ctx.r17.s32, 0, ctx.xer);
	// bne cr6,0x825e8350
	if (!ctx.cr6.eq) goto loc_825E8350;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// bne cr6,0x825e8354
	if (!ctx.cr6.eq) goto loc_825E8354;
	// subf r31,r4,r28
	ctx.r31.s64 = ctx.r28.s64 - ctx.r4.s64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825E82CC;
	sub_8239CB70(ctx, base);
	// addi r8,r1,80
	ctx.r8.s64 = ctx.r1.s64 + 80;
	// addi r7,r1,276
	ctx.r7.s64 = ctx.r1.s64 + 276;
	// lwz r3,3340(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 3340);
	// li r6,4
	ctx.r6.s64 = 4;
	// addi r5,r1,268
	ctx.r5.s64 = ctx.r1.s64 + 268;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8248c788
	ctx.lr = 0x825E82E8;
	sub_8248C788(ctx, base);
	// lwz r11,80(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 80);
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// stw r10,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r10.u32);
	// lwz r11,276(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// add r5,r31,r11
	ctx.r5.u64 = ctx.r31.u64 + ctx.r11.u64;
	// lwz r11,21324(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21324);
	// cmplw cr6,r5,r11
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e8324
	if (!ctx.cr6.gt) goto loc_825E8324;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7d90
	ctx.lr = 0x825E8314;
	sub_825E7D90(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e8924
	if (!ctx.cr6.eq) goto loc_825E8924;
	// lwz r26,21320(r30)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21320);
	// lwz r23,21316(r30)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21316);
loc_825E8324:
	// add r3,r31,r26
	ctx.r3.u64 = ctx.r31.u64 + ctx.r26.u64;
	// lwz r5,276(r1)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// bl 0x8239cb70
	ctx.lr = 0x825E8334;
	sub_8239CB70(ctx, base);
	// lwz r11,276(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// add r29,r31,r11
	ctx.r29.u64 = ctx.r31.u64 + ctx.r11.u64;
	// mr r25,r24
	ctx.r25.u64 = ctx.r24.u64;
	// stw r4,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r4.u32);
	// stw r29,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r29.u32);
	// b 0x825e8184
	goto loc_825E8184;
loc_825E8350:
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_825E8354:
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x825e840c
	if (ctx.cr6.eq) goto loc_825E840C;
	// cmpwi cr6,r25,1
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 1, ctx.xer);
	// bne cr6,0x825e840c
	if (!ctx.cr6.eq) goto loc_825E840C;
	// subf r11,r27,r28
	ctx.r11.s64 = ctx.r28.s64 - ctx.r27.s64;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// ble cr6,0x825e83a8
	if (!ctx.cr6.gt) goto loc_825E83A8;
	// lbz r10,-1(r28)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + -1);
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// bne cr6,0x825e83a8
	if (!ctx.cr6.eq) goto loc_825E83A8;
	// lbz r10,-2(r28)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + -2);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e83a8
	if (!ctx.cr6.eq) goto loc_825E83A8;
	// lbz r10,-3(r28)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + -3);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e83a8
	if (!ctx.cr6.eq) goto loc_825E83A8;
	// stb r24,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r24.u8);
	// stb r24,21313(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21313, ctx.r24.u8);
	// stb r22,21314(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21314, ctx.r22.u8);
	// stw r20,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r20.u32);
	// b 0x825e83fc
	goto loc_825E83FC;
loc_825E83A8:
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// ble cr6,0x825e83d8
	if (!ctx.cr6.gt) goto loc_825E83D8;
	// lbz r10,-1(r28)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + -1);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e83d8
	if (!ctx.cr6.eq) goto loc_825E83D8;
	// lbz r10,-2(r28)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + -2);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e83d8
	if (!ctx.cr6.eq) goto loc_825E83D8;
	// stb r24,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r24.u8);
	// stb r24,21313(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21313, ctx.r24.u8);
	// stw r18,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r18.u32);
	// b 0x825e83fc
	goto loc_825E83FC;
loc_825E83D8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e83f8
	if (ctx.cr6.eq) goto loc_825E83F8;
	// lbz r10,-1(r28)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + -1);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e83f8
	if (!ctx.cr6.eq) goto loc_825E83F8;
	// stb r24,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r24.u8);
	// stw r22,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r22.u32);
	// b 0x825e83fc
	goto loc_825E83FC;
loc_825E83F8:
	// stw r24,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r24.u32);
loc_825E83FC:
	// lwz r10,21300(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// mr r19,r22
	ctx.r19.u64 = ctx.r22.u64;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// add r28,r11,r27
	ctx.r28.u64 = ctx.r11.u64 + ctx.r27.u64;
loc_825E840C:
	// clrlwi r10,r3,24
	ctx.r10.u64 = ctx.r3.u32 & 0xFF;
	// cmplwi cr6,r10,13
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 13, ctx.xer);
	// beq cr6,0x825e86ac
	if (ctx.cr6.eq) goto loc_825E86AC;
	// cmplwi cr6,r10,12
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 12, ctx.xer);
	// beq cr6,0x825e86ac
	if (ctx.cr6.eq) goto loc_825E86AC;
	// cmplwi cr6,r10,11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 11, ctx.xer);
	// beq cr6,0x825e86ac
	if (ctx.cr6.eq) goto loc_825E86AC;
	// addi r11,r10,-10
	ctx.r11.s64 = ctx.r10.s64 + -10;
	// subf r5,r27,r28
	ctx.r5.s64 = ctx.r28.s64 - ctx.r27.s64;
	// cmplwi cr6,r11,54
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 54, ctx.xer);
	// bgt cr6,0x825e86a4
	if (ctx.cr6.gt) goto loc_825E86A4;
	// lis r12,-32161
	ctx.r12.s64 = -2107703296;
	// addi r12,r12,-31664
	ctx.r12.s64 = ctx.r12.s64 + -31664;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr 
	switch (ctx.r11.u64) {
	case 0:
		goto loc_825E869C;
	case 1:
		goto loc_825E86A4;
	case 2:
		goto loc_825E86A4;
	case 3:
		goto loc_825E86A4;
	case 4:
		goto loc_825E856C;
	case 5:
		goto loc_825E852C;
	case 6:
		goto loc_825E86A4;
	case 7:
		goto loc_825E86A4;
	case 8:
		goto loc_825E86A4;
	case 9:
		goto loc_825E86A4;
	case 10:
		goto loc_825E86A4;
	case 11:
		goto loc_825E86A4;
	case 12:
		goto loc_825E86A4;
	case 13:
		goto loc_825E86A4;
	case 14:
		goto loc_825E86A4;
	case 15:
		goto loc_825E86A4;
	case 16:
		goto loc_825E86A4;
	case 17:
		goto loc_825E866C;
	case 18:
		goto loc_825E863C;
	case 19:
		goto loc_825E860C;
	case 20:
		goto loc_825E85DC;
	case 21:
		goto loc_825E85AC;
	case 22:
		goto loc_825E8820;
	case 23:
		goto loc_825E8820;
	case 24:
		goto loc_825E8820;
	case 25:
		goto loc_825E8820;
	case 26:
		goto loc_825E8820;
	case 27:
		goto loc_825E8820;
	case 28:
		goto loc_825E8820;
	case 29:
		goto loc_825E8820;
	case 30:
		goto loc_825E8820;
	case 31:
		goto loc_825E8820;
	case 32:
		goto loc_825E8820;
	case 33:
		goto loc_825E8820;
	case 34:
		goto loc_825E8820;
	case 35:
		goto loc_825E8820;
	case 36:
		goto loc_825E8820;
	case 37:
		goto loc_825E8820;
	case 38:
		goto loc_825E8820;
	case 39:
		goto loc_825E8820;
	case 40:
		goto loc_825E8820;
	case 41:
		goto loc_825E8820;
	case 42:
		goto loc_825E8820;
	case 43:
		goto loc_825E8820;
	case 44:
		goto loc_825E8820;
	case 45:
		goto loc_825E8820;
	case 46:
		goto loc_825E8820;
	case 47:
		goto loc_825E8820;
	case 48:
		goto loc_825E8820;
	case 49:
		goto loc_825E8820;
	case 50:
		goto loc_825E8820;
	case 51:
		goto loc_825E8820;
	case 52:
		goto loc_825E8820;
	case 53:
		goto loc_825E8820;
	case 54:
		goto loc_825E8820;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-31076(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31076);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31380(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31380);
	// lwz r18,-31444(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31444);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31068(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31068);
	// lwz r18,-31124(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31124);
	// lwz r18,-31172(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31172);
	// lwz r18,-31220(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31220);
	// lwz r18,-31268(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31268);
	// lwz r18,-31316(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -31316);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
	// lwz r18,-30688(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -30688);
loc_825E852C:
	// lwz r11,15472(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 15472);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// lwz r3,80(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 80);
	// addi r11,r11,-7
	ctx.r11.s64 = ctx.r11.s64 + -7;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// bl 0x825d5100
	ctx.lr = 0x825E854C;
	sub_825D5100(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825ed0a8
	ctx.lr = 0x825E8558;
	sub_825ED0A8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825e8818
	if (ctx.cr6.eq) goto loc_825E8818;
	// stw r22,3676(r30)
	PPC_STORE_U32(ctx.r30.u32 + 3676, ctx.r22.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825E856C:
	// lwz r11,15472(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 15472);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// lwz r3,80(r30)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r30.u32 + 80);
	// addi r11,r11,-7
	ctx.r11.s64 = ctx.r11.s64 + -7;
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// rlwinm r7,r11,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// bl 0x825d5100
	ctx.lr = 0x825E858C;
	sub_825D5100(ctx, base);
	// lwz r11,3676(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 3676);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e88a0
	if (!ctx.cr6.eq) goto loc_825E88A0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d3160
	ctx.lr = 0x825E85A0;
	sub_825D3160(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e8924
	if (!ctx.cr6.eq) goto loc_825E8924;
	// b 0x825e8818
	goto loc_825E8818;
loc_825E85AC:
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// bne cr6,0x825e85c0
	if (!ctx.cr6.eq) goto loc_825E85C0;
	// cmpwi cr6,r25,1
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 1, ctx.xer);
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
	// beq cr6,0x825e85c4
	if (ctx.cr6.eq) goto loc_825E85C4;
loc_825E85C0:
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
loc_825E85C4:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7e78
	ctx.lr = 0x825E85D8;
	sub_825E7E78(ctx, base);
	// b 0x825e8818
	goto loc_825E8818;
loc_825E85DC:
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// bne cr6,0x825e85f0
	if (!ctx.cr6.eq) goto loc_825E85F0;
	// cmpwi cr6,r25,1
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 1, ctx.xer);
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
	// beq cr6,0x825e85f4
	if (ctx.cr6.eq) goto loc_825E85F4;
loc_825E85F0:
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
loc_825E85F4:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7e78
	ctx.lr = 0x825E8608;
	sub_825E7E78(ctx, base);
	// b 0x825e8818
	goto loc_825E8818;
loc_825E860C:
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// bne cr6,0x825e8620
	if (!ctx.cr6.eq) goto loc_825E8620;
	// cmpwi cr6,r25,1
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 1, ctx.xer);
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
	// beq cr6,0x825e8624
	if (ctx.cr6.eq) goto loc_825E8624;
loc_825E8620:
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
loc_825E8624:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7e78
	ctx.lr = 0x825E8638;
	sub_825E7E78(ctx, base);
	// b 0x825e8818
	goto loc_825E8818;
loc_825E863C:
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// bne cr6,0x825e8650
	if (!ctx.cr6.eq) goto loc_825E8650;
	// cmpwi cr6,r25,1
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 1, ctx.xer);
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
	// beq cr6,0x825e8654
	if (ctx.cr6.eq) goto loc_825E8654;
loc_825E8650:
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
loc_825E8654:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7e78
	ctx.lr = 0x825E8668;
	sub_825E7E78(ctx, base);
	// b 0x825e8818
	goto loc_825E8818;
loc_825E866C:
	// cmpwi cr6,r9,1
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 1, ctx.xer);
	// bne cr6,0x825e8680
	if (!ctx.cr6.eq) goto loc_825E8680;
	// cmpwi cr6,r25,1
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 1, ctx.xer);
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
	// beq cr6,0x825e8684
	if (ctx.cr6.eq) goto loc_825E8684;
loc_825E8680:
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
loc_825E8684:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7e78
	ctx.lr = 0x825E8698;
	sub_825E7E78(ctx, base);
	// b 0x825e8818
	goto loc_825E8818;
loc_825E869C:
	// stw r22,3680(r30)
	PPC_STORE_U32(ctx.r30.u32 + 3680, ctx.r22.u32);
	// b 0x825e8820
	goto loc_825E8820;
loc_825E86A4:
	// li r16,4
	ctx.r16.s64 = 4;
	// b 0x825e8820
	goto loc_825E8820;
loc_825E86AC:
	// subf r31,r27,r28
	ctx.r31.s64 = ctx.r28.s64 - ctx.r27.s64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825e86dc
	if (ctx.cr6.eq) goto loc_825E86DC;
	// add r11,r31,r27
	ctx.r11.u64 = ctx.r31.u64 + ctx.r27.u64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
loc_825E86C0:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825e86dc
	if (!ctx.cr6.eq) goto loc_825E86DC;
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825e86c0
	if (!ctx.cr6.eq) goto loc_825E86C0;
loc_825E86DC:
	// lwz r11,3676(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 3676);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e8804
	if (!ctx.cr6.eq) goto loc_825E8804;
	// cmplwi cr6,r10,12
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 12, ctx.xer);
	// bne cr6,0x825e8704
	if (!ctx.cr6.eq) goto loc_825E8704;
	// lwz r11,21244(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21244);
	// stw r31,21260(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21260, ctx.r31.u32);
	// stw r22,21344(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21344, ctx.r22.u32);
	// stw r21,21348(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21348, ctx.r21.u32);
	// stw r11,21248(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21248, ctx.r11.u32);
loc_825E8704:
	// cmplwi cr6,r10,13
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 13, ctx.xer);
	// bne cr6,0x825e871c
	if (!ctx.cr6.eq) goto loc_825E871C;
	// lwz r11,21288(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21288);
	// stw r31,21256(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21256, ctx.r31.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,21288(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21288, ctx.r11.u32);
loc_825E871C:
	// cmplwi cr6,r10,11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 11, ctx.xer);
	// bne cr6,0x825e8804
	if (!ctx.cr6.eq) goto loc_825E8804;
	// cmplwi cr6,r31,1
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 1, ctx.xer);
	// ble cr6,0x825e87a0
	if (!ctx.cr6.gt) goto loc_825E87A0;
	// addi r29,r27,1
	ctx.r29.s64 = ctx.r27.s64 + 1;
	// lbz r11,0(r27)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r27.u32 + 0);
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// lwz r7,21344(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21344);
	// rotlwi r11,r11,1
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
	// addi r5,r1,84
	ctx.r5.s64 = ctx.r1.s64 + 84;
	// li r4,1
	ctx.r4.s64 = 1;
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r29.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// rlwinm r10,r10,25,7,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 25) & 0x1FFFFFF;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// bl 0x825cedb0
	ctx.lr = 0x825E8760;
	sub_825CEDB0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825e8804
	if (ctx.cr6.eq) goto loc_825E8804;
	// cmpwi cr6,r3,4
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 4, ctx.xer);
	// bne cr6,0x825e8898
	if (!ctx.cr6.eq) goto loc_825E8898;
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// li r16,4
	ctx.r16.s64 = 4;
	// addi r31,r31,-2
	ctx.r31.s64 = ctx.r31.s64 + -2;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r27,r27,2
	ctx.r27.s64 = ctx.r27.s64 + 2;
	// stbx r11,r23,r21
	PPC_STORE_U8(ctx.r23.u32 + ctx.r21.u32, ctx.r11.u8);
	// addi r11,r21,1
	ctx.r11.s64 = ctx.r21.s64 + 1;
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r29.u32 + 0);
	// addi r21,r11,1
	ctx.r21.s64 = ctx.r11.s64 + 1;
	// clrlwi r10,r10,25
	ctx.r10.u64 = ctx.r10.u32 & 0x7F;
	// stbx r10,r23,r11
	PPC_STORE_U8(ctx.r23.u32 + ctx.r11.u32, ctx.r10.u8);
	// b 0x825e8804
	goto loc_825E8804;
loc_825E87A0:
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825e8804
	if (!ctx.cr6.eq) goto loc_825E8804;
	// cmplwi cr6,r31,1
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 1, ctx.xer);
	// stw r22,21340(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21340, ctx.r22.u32);
	// bne cr6,0x825e8804
	if (!ctx.cr6.eq) goto loc_825E8804;
	// lwz r11,21300(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bgt cr6,0x825e88a0
	if (ctx.cr6.gt) goto loc_825E88A0;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e87ec
	if (ctx.cr6.eq) goto loc_825E87EC;
	// add r10,r11,r30
	ctx.r10.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addi r10,r10,21312
	ctx.r10.s64 = ctx.r10.s64 + 21312;
loc_825E87D4:
	// lbz r9,-1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// bne cr6,0x825e87d4
	if (!ctx.cr6.eq) goto loc_825E87D4;
loc_825E87EC:
	// lwz r11,21300(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// mr r31,r24
	ctx.r31.u64 = ctx.r24.u64;
	// lbz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r27.u32 + 0);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stb r10,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r10.u8);
	// stw r11,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r11.u32);
loc_825E8804:
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// add r3,r23,r21
	ctx.r3.u64 = ctx.r23.u64 + ctx.r21.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825E8814;
	sub_8239CB70(ctx, base);
	// add r21,r31,r21
	ctx.r21.u64 = ctx.r31.u64 + ctx.r21.u64;
loc_825E8818:
	// lwz r4,268(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// lwz r29,276(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
loc_825E8820:
	// subf r11,r28,r4
	ctx.r11.s64 = ctx.r4.s64 - ctx.r28.s64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// cmplwi cr6,r29,4
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 4, ctx.xer);
	// stw r4,268(r1)
	PPC_STORE_U32(ctx.r1.u32 + 268, ctx.r4.u32);
	// stw r29,276(r1)
	PPC_STORE_U32(ctx.r1.u32 + 276, ctx.r29.u32);
	// bge cr6,0x825e817c
	if (!ctx.cr6.lt) goto loc_825E817C;
loc_825E883C:
	// lwz r28,292(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
loc_825E8840:
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825e8908
	if (ctx.cr6.eq) goto loc_825E8908;
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// bne cr6,0x825e8908
	if (!ctx.cr6.eq) goto loc_825E8908;
	// add r11,r23,r21
	ctx.r11.u64 = ctx.r23.u64 + ctx.r21.u64;
	// cmplwi cr6,r21,2
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 2, ctx.xer);
	// ble cr6,0x825e88ac
	if (!ctx.cr6.gt) goto loc_825E88AC;
	// lbz r9,-1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + -1);
	// cmplwi cr6,r9,1
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 1, ctx.xer);
	// bne cr6,0x825e88ac
	if (!ctx.cr6.eq) goto loc_825E88AC;
	// lbz r9,-2(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825e88ac
	if (!ctx.cr6.eq) goto loc_825E88AC;
	// lbz r9,-3(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + -3);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825e88ac
	if (!ctx.cr6.eq) goto loc_825E88AC;
	// stb r24,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r24.u8);
	// stb r24,21313(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21313, ctx.r24.u8);
	// stb r22,21314(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21314, ctx.r22.u8);
	// stw r20,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r20.u32);
	// b 0x825e8900
	goto loc_825E8900;
loc_825E8898:
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// bne cr6,0x825e8924
	if (!ctx.cr6.eq) goto loc_825E8924;
loc_825E88A0:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825E88AC:
	// cmplwi cr6,r21,1
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 1, ctx.xer);
	// ble cr6,0x825e88dc
	if (!ctx.cr6.gt) goto loc_825E88DC;
	// lbz r9,-1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + -1);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825e88dc
	if (!ctx.cr6.eq) goto loc_825E88DC;
	// lbz r9,-2(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825e88dc
	if (!ctx.cr6.eq) goto loc_825E88DC;
	// stb r24,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r24.u8);
	// stb r24,21313(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21313, ctx.r24.u8);
	// stw r18,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r18.u32);
	// b 0x825e8900
	goto loc_825E8900;
loc_825E88DC:
	// cmplwi cr6,r21,0
	ctx.cr6.compare<uint32_t>(ctx.r21.u32, 0, ctx.xer);
	// beq cr6,0x825e88fc
	if (ctx.cr6.eq) goto loc_825E88FC;
	// lbz r11,-1(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + -1);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825e88fc
	if (!ctx.cr6.eq) goto loc_825E88FC;
	// stb r24,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r24.u8);
	// stw r22,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r22.u32);
	// b 0x825e8900
	goto loc_825E8900;
loc_825E88FC:
	// stw r24,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r24.u32);
loc_825E8900:
	// lwz r11,21300(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// subf r21,r11,r21
	ctx.r21.s64 = ctx.r21.s64 - ctx.r11.s64;
loc_825E8908:
	// stw r23,0(r14)
	PPC_STORE_U32(ctx.r14.u32 + 0, ctx.r23.u32);
	// mr r3,r16
	ctx.r3.u64 = ctx.r16.u64;
	// stw r21,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r21.u32);
	// stw r10,0(r15)
	PPC_STORE_U32(ctx.r15.u32 + 0, ctx.r10.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825E8920:
	// li r3,7
	ctx.r3.s64 = 7;
loc_825E8924:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825E892C"))) PPC_WEAK_FUNC(sub_825E892C);
PPC_FUNC_IMPL(__imp__sub_825E892C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E8930"))) PPC_WEAK_FUNC(sub_825E8930);
PPC_FUNC_IMPL(__imp__sub_825E8930) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9e0
	ctx.lr = 0x825E8938;
	sub_8239B9E0(ctx, base);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// mr r29,r7
	ctx.r29.u64 = ctx.r7.u64;
	// li r22,0
	ctx.r22.s64 = 0;
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// mr r26,r5
	ctx.r26.u64 = ctx.r5.u64;
	// lwz r31,21320(r30)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21320);
	// mr r14,r6
	ctx.r14.u64 = ctx.r6.u64;
	// lwz r21,21316(r30)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21316);
	// mr r15,r8
	ctx.r15.u64 = ctx.r8.u64;
	// stw r29,292(r1)
	PPC_STORE_U32(ctx.r1.u32 + 292, ctx.r29.u32);
	// mr r23,r22
	ctx.r23.u64 = ctx.r22.u64;
	// mr r19,r22
	ctx.r19.u64 = ctx.r22.u64;
	// mr r20,r22
	ctx.r20.u64 = ctx.r22.u64;
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// beq cr6,0x825e9140
	if (ctx.cr6.eq) goto loc_825E9140;
	// cmplwi cr6,r14,0
	ctx.cr6.compare<uint32_t>(ctx.r14.u32, 0, ctx.xer);
	// beq cr6,0x825e9140
	if (ctx.cr6.eq) goto loc_825E9140;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x825e9140
	if (ctx.cr6.eq) goto loc_825E9140;
	// lwz r11,21300(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// lwz r10,21324(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21324);
	// add r5,r11,r26
	ctx.r5.u64 = ctx.r11.u64 + ctx.r26.u64;
	// cmplw cr6,r5,r10
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r10.u32, ctx.xer);
	// ble cr6,0x825e89b4
	if (!ctx.cr6.gt) goto loc_825E89B4;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x825e7d90
	ctx.lr = 0x825E89A4;
	sub_825E7D90(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e9140
	if (!ctx.cr6.eq) goto loc_825E9140;
	// lwz r31,21320(r30)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21320);
	// lwz r21,21316(r30)
	ctx.r21.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21316);
loc_825E89B4:
	// lwz r11,21292(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21292);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e89d4
	if (ctx.cr6.eq) goto loc_825E89D4;
	// stw r26,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r25,0(r14)
	PPC_STORE_U32(ctx.r14.u32 + 0, ctx.r25.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825E89D4:
	// lwz r10,21300(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825e8a24
	if (!ctx.cr6.gt) goto loc_825E8A24;
	// mr r11,r22
	ctx.r11.u64 = ctx.r22.u64;
	// addi r10,r30,21312
	ctx.r10.s64 = ctx.r30.s64 + 21312;
loc_825E89E8:
	// lbzx r9,r10,r11
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r11.u32);
	// stbx r9,r11,r31
	PPC_STORE_U8(ctx.r11.u32 + ctx.r31.u32, ctx.r9.u8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lwz r9,21300(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x825e89e8
	if (ctx.cr6.lt) goto loc_825E89E8;
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// add r3,r11,r31
	ctx.r3.u64 = ctx.r11.u64 + ctx.r31.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825E8A14;
	sub_8239CB70(ctx, base);
	// lwz r11,21300(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// mr r25,r31
	ctx.r25.u64 = ctx.r31.u64;
	// stw r22,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r22.u32);
	// add r26,r11,r26
	ctx.r26.u64 = ctx.r11.u64 + ctx.r26.u64;
loc_825E8A24:
	// lwz r11,21296(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21296);
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// li r24,1
	ctx.r24.s64 = 1;
	// clrlwi r18,r11,24
	ctx.r18.u64 = ctx.r11.u32 & 0xFF;
	// li r17,3
	ctx.r17.s64 = 3;
	// li r16,2
	ctx.r16.s64 = 2;
loc_825E8A3C:
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// bne cr6,0x825e9068
	if (!ctx.cr6.eq) goto loc_825E9068;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x825e8a50
	if (ctx.cr6.eq) goto loc_825E8A50;
	// lbz r18,3(r25)
	ctx.r18.u64 = PPC_LOAD_U8(ctx.r25.u32 + 3);
loc_825E8A50:
	// add r11,r4,r25
	ctx.r11.u64 = ctx.r4.u64 + ctx.r25.u64;
	// addi r10,r4,4
	ctx.r10.s64 = ctx.r4.s64 + 4;
	// mr r28,r11
	ctx.r28.u64 = ctx.r11.u64;
	// cmplw cr6,r26,r10
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825e8b40
	if (ctx.cr6.lt) goto loc_825E8B40;
	// clrlwi r9,r25,31
	ctx.r9.u64 = ctx.r25.u32 & 0x1;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x825e8aa8
	if (ctx.cr6.eq) goto loc_825E8AA8;
	// lbz r10,4(r25)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r25.u32 + 4);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e8aa0
	if (!ctx.cr6.eq) goto loc_825E8AA0;
	// lbz r10,5(r25)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r25.u32 + 5);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e8aa0
	if (!ctx.cr6.eq) goto loc_825E8AA0;
	// lbz r10,6(r25)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r25.u32 + 6);
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// bne cr6,0x825e8aa0
	if (!ctx.cr6.eq) goto loc_825E8AA0;
	// mr r27,r25
	ctx.r27.u64 = ctx.r25.u64;
	// b 0x825e8b38
	goto loc_825E8B38;
loc_825E8AA0:
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r4,1
	ctx.r10.s64 = ctx.r4.s64 + 1;
loc_825E8AA8:
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lhz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// addi r5,r26,-1
	ctx.r5.s64 = ctx.r26.s64 + -1;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmplw cr6,r10,r5
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, ctx.xer);
	// bge cr6,0x825e8b40
	if (!ctx.cr6.lt) goto loc_825E8B40;
	// addi r6,r10,2
	ctx.r6.s64 = ctx.r10.s64 + 2;
loc_825E8AC4:
	// lhz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U16(ctx.r11.u32 + 0);
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// mr r7,r8
	ctx.r7.u64 = ctx.r8.u64;
	// and r3,r9,r7
	ctx.r3.u64 = ctx.r9.u64 & ctx.r7.u64;
	// clrlwi r3,r3,16
	ctx.r3.u64 = ctx.r3.u32 & 0xFFFF;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x825e8b10
	if (!ctx.cr6.eq) goto loc_825E8B10;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825e8af4
	if (!ctx.cr6.eq) goto loc_825E8AF4;
	// rlwinm r3,r7,0,16,23
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 0) & 0xFF00;
	// cmplwi cr6,r3,256
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 256, ctx.xer);
	// beq cr6,0x825e8b2c
	if (ctx.cr6.eq) goto loc_825E8B2C;
loc_825E8AF4:
	// clrlwi r9,r9,24
	ctx.r9.u64 = ctx.r9.u32 & 0xFF;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825e8b10
	if (!ctx.cr6.eq) goto loc_825E8B10;
	// cmplwi cr6,r7,1
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 1, ctx.xer);
	// bne cr6,0x825e8b10
	if (!ctx.cr6.eq) goto loc_825E8B10;
	// cmplw cr6,r26,r6
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, ctx.r6.u32, ctx.xer);
	// bgt cr6,0x825e8b34
	if (ctx.cr6.gt) goto loc_825E8B34;
loc_825E8B10:
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// mr r9,r8
	ctx.r9.u64 = ctx.r8.u64;
	// addi r6,r6,2
	ctx.r6.s64 = ctx.r6.s64 + 2;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmplw cr6,r10,r5
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r5.u32, ctx.xer);
	// blt cr6,0x825e8ac4
	if (ctx.cr6.lt) goto loc_825E8AC4;
	// b 0x825e8b40
	goto loc_825E8B40;
loc_825E8B2C:
	// addi r27,r11,-2
	ctx.r27.s64 = ctx.r11.s64 + -2;
	// b 0x825e8b38
	goto loc_825E8B38;
loc_825E8B34:
	// addi r27,r11,-1
	ctx.r27.s64 = ctx.r11.s64 + -1;
loc_825E8B38:
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// bne cr6,0x825e8b50
	if (!ctx.cr6.eq) goto loc_825E8B50;
loc_825E8B40:
	// clrlwi r11,r18,24
	ctx.r11.u64 = ctx.r18.u32 & 0xFF;
	// add r27,r25,r26
	ctx.r27.u64 = ctx.r25.u64 + ctx.r26.u64;
	// mr r19,r24
	ctx.r19.u64 = ctx.r24.u64;
	// stw r11,21296(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21296, ctx.r11.u32);
loc_825E8B50:
	// cmpwi cr6,r15,0
	ctx.cr6.compare<int32_t>(ctx.r15.s32, 0, ctx.xer);
	// beq cr6,0x825e8c08
	if (ctx.cr6.eq) goto loc_825E8C08;
	// cmpwi cr6,r19,1
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 1, ctx.xer);
	// bne cr6,0x825e8c08
	if (!ctx.cr6.eq) goto loc_825E8C08;
	// subf r11,r28,r27
	ctx.r11.s64 = ctx.r27.s64 - ctx.r28.s64;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// ble cr6,0x825e8ba4
	if (!ctx.cr6.gt) goto loc_825E8BA4;
	// lbz r10,-1(r27)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r27.u32 + -1);
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// bne cr6,0x825e8ba4
	if (!ctx.cr6.eq) goto loc_825E8BA4;
	// lbz r10,-2(r27)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r27.u32 + -2);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e8ba4
	if (!ctx.cr6.eq) goto loc_825E8BA4;
	// lbz r10,-3(r27)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r27.u32 + -3);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e8ba4
	if (!ctx.cr6.eq) goto loc_825E8BA4;
	// stw r17,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r17.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r22.u8);
	// stb r22,21313(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21313, ctx.r22.u8);
	// stb r24,21314(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21314, ctx.r24.u8);
	// b 0x825e8bf8
	goto loc_825E8BF8;
loc_825E8BA4:
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// ble cr6,0x825e8bd4
	if (!ctx.cr6.gt) goto loc_825E8BD4;
	// lbz r10,-1(r27)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r27.u32 + -1);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e8bd4
	if (!ctx.cr6.eq) goto loc_825E8BD4;
	// lbz r10,-2(r27)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r27.u32 + -2);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e8bd4
	if (!ctx.cr6.eq) goto loc_825E8BD4;
	// stw r16,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r16.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r22.u8);
	// stb r22,21313(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21313, ctx.r22.u8);
	// b 0x825e8bf8
	goto loc_825E8BF8;
loc_825E8BD4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e8bf4
	if (ctx.cr6.eq) goto loc_825E8BF4;
	// lbz r10,-1(r27)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r27.u32 + -1);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e8bf4
	if (!ctx.cr6.eq) goto loc_825E8BF4;
	// stw r24,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r24.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r22.u8);
	// b 0x825e8bf8
	goto loc_825E8BF8;
loc_825E8BF4:
	// stw r22,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r22.u32);
loc_825E8BF8:
	// lwz r10,21300(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// mr r20,r24
	ctx.r20.u64 = ctx.r24.u64;
	// subf r11,r10,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r10.s64;
	// add r27,r11,r28
	ctx.r27.u64 = ctx.r11.u64 + ctx.r28.u64;
loc_825E8C08:
	// clrlwi r10,r18,24
	ctx.r10.u64 = ctx.r18.u32 & 0xFF;
	// cmplwi cr6,r10,13
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 13, ctx.xer);
	// beq cr6,0x825e8e04
	if (ctx.cr6.eq) goto loc_825E8E04;
	// cmplwi cr6,r10,12
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 12, ctx.xer);
	// beq cr6,0x825e8e04
	if (ctx.cr6.eq) goto loc_825E8E04;
	// cmplwi cr6,r10,11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 11, ctx.xer);
	// beq cr6,0x825e8e04
	if (ctx.cr6.eq) goto loc_825E8E04;
	// addi r11,r10,-10
	ctx.r11.s64 = ctx.r10.s64 + -10;
	// subf r6,r28,r27
	ctx.r6.s64 = ctx.r27.s64 - ctx.r28.s64;
	// cmplwi cr6,r11,54
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 54, ctx.xer);
	// bgt cr6,0x825e9050
	if (ctx.cr6.gt) goto loc_825E9050;
	// lis r12,-32161
	ctx.r12.s64 = -2107703296;
	// addi r12,r12,-29620
	ctx.r12.s64 = ctx.r12.s64 + -29620;
	// rlwinm r0,r11,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr 
	switch (ctx.r11.u64) {
	case 0:
		goto loc_825E9050;
	case 1:
		goto loc_825E9050;
	case 2:
		goto loc_825E9050;
	case 3:
		goto loc_825E9050;
	case 4:
		goto loc_825E9050;
	case 5:
		goto loc_825E9050;
	case 6:
		goto loc_825E9050;
	case 7:
		goto loc_825E9050;
	case 8:
		goto loc_825E9050;
	case 9:
		goto loc_825E9050;
	case 10:
		goto loc_825E9050;
	case 11:
		goto loc_825E9050;
	case 12:
		goto loc_825E9050;
	case 13:
		goto loc_825E9050;
	case 14:
		goto loc_825E9050;
	case 15:
		goto loc_825E9050;
	case 16:
		goto loc_825E9050;
	case 17:
		goto loc_825E8DD8;
	case 18:
		goto loc_825E8DAC;
	case 19:
		goto loc_825E8D80;
	case 20:
		goto loc_825E8D54;
	case 21:
		goto loc_825E8D28;
	case 22:
		goto loc_825E9050;
	case 23:
		goto loc_825E9050;
	case 24:
		goto loc_825E9050;
	case 25:
		goto loc_825E9050;
	case 26:
		goto loc_825E9050;
	case 27:
		goto loc_825E9050;
	case 28:
		goto loc_825E9050;
	case 29:
		goto loc_825E9050;
	case 30:
		goto loc_825E9050;
	case 31:
		goto loc_825E9050;
	case 32:
		goto loc_825E9050;
	case 33:
		goto loc_825E9050;
	case 34:
		goto loc_825E9050;
	case 35:
		goto loc_825E9050;
	case 36:
		goto loc_825E9050;
	case 37:
		goto loc_825E9050;
	case 38:
		goto loc_825E9050;
	case 39:
		goto loc_825E9050;
	case 40:
		goto loc_825E9050;
	case 41:
		goto loc_825E9050;
	case 42:
		goto loc_825E9050;
	case 43:
		goto loc_825E9050;
	case 44:
		goto loc_825E9050;
	case 45:
		goto loc_825E9050;
	case 46:
		goto loc_825E9050;
	case 47:
		goto loc_825E9050;
	case 48:
		goto loc_825E9050;
	case 49:
		goto loc_825E9050;
	case 50:
		goto loc_825E9050;
	case 51:
		goto loc_825E9050;
	case 52:
		goto loc_825E9050;
	case 53:
		goto loc_825E9050;
	case 54:
		goto loc_825E9050;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-29224(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -29224);
	// lwz r18,-29268(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -29268);
	// lwz r18,-29312(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -29312);
	// lwz r18,-29356(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -29356);
	// lwz r18,-29400(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -29400);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
	// lwz r18,-28592(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -28592);
loc_825E8D28:
	// cmpwi cr6,r15,1
	ctx.cr6.compare<int32_t>(ctx.r15.s32, 1, ctx.xer);
	// bne cr6,0x825e8d3c
	if (!ctx.cr6.eq) goto loc_825E8D3C;
	// cmpwi cr6,r19,1
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 1, ctx.xer);
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
	// beq cr6,0x825e8d40
	if (ctx.cr6.eq) goto loc_825E8D40;
loc_825E8D3C:
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
loc_825E8D40:
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// li r4,4
	ctx.r4.s64 = 4;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7e78
	ctx.lr = 0x825E8D50;
	sub_825E7E78(ctx, base);
	// b 0x825e9050
	goto loc_825E9050;
loc_825E8D54:
	// cmpwi cr6,r15,1
	ctx.cr6.compare<int32_t>(ctx.r15.s32, 1, ctx.xer);
	// bne cr6,0x825e8d68
	if (!ctx.cr6.eq) goto loc_825E8D68;
	// cmpwi cr6,r19,1
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 1, ctx.xer);
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
	// beq cr6,0x825e8d6c
	if (ctx.cr6.eq) goto loc_825E8D6C;
loc_825E8D68:
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
loc_825E8D6C:
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// li r4,5
	ctx.r4.s64 = 5;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7e78
	ctx.lr = 0x825E8D7C;
	sub_825E7E78(ctx, base);
	// b 0x825e9050
	goto loc_825E9050;
loc_825E8D80:
	// cmpwi cr6,r15,1
	ctx.cr6.compare<int32_t>(ctx.r15.s32, 1, ctx.xer);
	// bne cr6,0x825e8d94
	if (!ctx.cr6.eq) goto loc_825E8D94;
	// cmpwi cr6,r19,1
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 1, ctx.xer);
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
	// beq cr6,0x825e8d98
	if (ctx.cr6.eq) goto loc_825E8D98;
loc_825E8D94:
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
loc_825E8D98:
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// li r4,6
	ctx.r4.s64 = 6;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7e78
	ctx.lr = 0x825E8DA8;
	sub_825E7E78(ctx, base);
	// b 0x825e9050
	goto loc_825E9050;
loc_825E8DAC:
	// cmpwi cr6,r15,1
	ctx.cr6.compare<int32_t>(ctx.r15.s32, 1, ctx.xer);
	// bne cr6,0x825e8dc0
	if (!ctx.cr6.eq) goto loc_825E8DC0;
	// cmpwi cr6,r19,1
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 1, ctx.xer);
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
	// beq cr6,0x825e8dc4
	if (ctx.cr6.eq) goto loc_825E8DC4;
loc_825E8DC0:
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
loc_825E8DC4:
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// li r4,7
	ctx.r4.s64 = 7;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7e78
	ctx.lr = 0x825E8DD4;
	sub_825E7E78(ctx, base);
	// b 0x825e9050
	goto loc_825E9050;
loc_825E8DD8:
	// cmpwi cr6,r15,1
	ctx.cr6.compare<int32_t>(ctx.r15.s32, 1, ctx.xer);
	// bne cr6,0x825e8dec
	if (!ctx.cr6.eq) goto loc_825E8DEC;
	// cmpwi cr6,r19,1
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 1, ctx.xer);
	// mr r7,r24
	ctx.r7.u64 = ctx.r24.u64;
	// beq cr6,0x825e8df0
	if (ctx.cr6.eq) goto loc_825E8DF0;
loc_825E8DEC:
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
loc_825E8DF0:
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// li r4,8
	ctx.r4.s64 = 8;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825e7e78
	ctx.lr = 0x825E8E00;
	sub_825E7E78(ctx, base);
	// b 0x825e9050
	goto loc_825E9050;
loc_825E8E04:
	// subf r31,r28,r27
	ctx.r31.s64 = ctx.r27.s64 - ctx.r28.s64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825e8e2c
	if (ctx.cr6.eq) goto loc_825E8E2C;
	// addi r11,r28,-1
	ctx.r11.s64 = ctx.r28.s64 + -1;
loc_825E8E14:
	// lbzx r9,r11,r31
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r31.u32);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825e8e2c
	if (!ctx.cr6.eq) goto loc_825E8E2C;
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825e8e14
	if (!ctx.cr6.eq) goto loc_825E8E14;
loc_825E8E2C:
	// cmplwi cr6,r4,4
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 4, ctx.xer);
	// beq cr6,0x825e8e40
	if (ctx.cr6.eq) goto loc_825E8E40;
	// lwz r11,21340(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21340);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825e9038
	if (!ctx.cr6.eq) goto loc_825E9038;
loc_825E8E40:
	// cmplwi cr6,r10,12
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 12, ctx.xer);
	// bne cr6,0x825e8e5c
	if (!ctx.cr6.eq) goto loc_825E8E5C;
	// lwz r11,21244(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21244);
	// stw r31,21260(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21260, ctx.r31.u32);
	// stw r24,21344(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21344, ctx.r24.u32);
	// stw r23,21348(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21348, ctx.r23.u32);
	// stw r11,21248(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21248, ctx.r11.u32);
loc_825E8E5C:
	// cmplwi cr6,r10,13
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 13, ctx.xer);
	// bne cr6,0x825e8e74
	if (!ctx.cr6.eq) goto loc_825E8E74;
	// lwz r11,21288(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21288);
	// stw r31,21256(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21256, ctx.r31.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,21288(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21288, ctx.r11.u32);
loc_825E8E74:
	// cmplwi cr6,r10,11
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 11, ctx.xer);
	// bne cr6,0x825e9038
	if (!ctx.cr6.eq) goto loc_825E9038;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// bne cr6,0x825e8f5c
	if (!ctx.cr6.eq) goto loc_825E8F5C;
	// lwz r11,21340(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21340);
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// bne cr6,0x825e9038
	if (!ctx.cr6.eq) goto loc_825E9038;
	// cmplwi cr6,r31,1
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 1, ctx.xer);
	// ble cr6,0x825e8f0c
	if (!ctx.cr6.gt) goto loc_825E8F0C;
	// addi r29,r28,1
	ctx.r29.s64 = ctx.r28.s64 + 1;
	// lbz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r28.u32 + 0);
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// lwz r7,21344(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21344);
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// lbz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r29.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// rlwinm r11,r11,25,7,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 25) & 0x1FFFFFF;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// bl 0x825cedb0
	ctx.lr = 0x825E8ECC;
	sub_825CEDB0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825e8fd0
	if (ctx.cr6.eq) goto loc_825E8FD0;
	// cmpwi cr6,r3,4
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 4, ctx.xer);
	// bne cr6,0x825e90bc
	if (!ctx.cr6.eq) goto loc_825E90BC;
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r31,r31,-2
	ctx.r31.s64 = ctx.r31.s64 + -2;
	// addi r28,r28,2
	ctx.r28.s64 = ctx.r28.s64 + 2;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// stbx r11,r21,r23
	PPC_STORE_U8(ctx.r21.u32 + ctx.r23.u32, ctx.r11.u8);
	// addi r11,r23,1
	ctx.r11.s64 = ctx.r23.s64 + 1;
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r29.u32 + 0);
	// addi r23,r11,1
	ctx.r23.s64 = ctx.r11.s64 + 1;
	// clrlwi r10,r10,25
	ctx.r10.u64 = ctx.r10.u32 & 0x7F;
	// stbx r10,r21,r11
	PPC_STORE_U8(ctx.r21.u32 + ctx.r11.u32, ctx.r10.u8);
	// stw r22,21340(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21340, ctx.r22.u32);
	// b 0x825e9038
	goto loc_825E9038;
loc_825E8F0C:
	// cmpwi cr6,r15,1
	ctx.cr6.compare<int32_t>(ctx.r15.s32, 1, ctx.xer);
	// bne cr6,0x825e9038
	if (!ctx.cr6.eq) goto loc_825E9038;
	// cmplwi cr6,r31,1
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 1, ctx.xer);
	// bne cr6,0x825e9038
	if (!ctx.cr6.eq) goto loc_825E9038;
	// lwz r11,21300(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bgt cr6,0x825e90c4
	if (ctx.cr6.gt) goto loc_825E90C4;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825e8f50
	if (!ctx.cr6.gt) goto loc_825E8F50;
	// add r10,r11,r30
	ctx.r10.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addi r10,r10,21312
	ctx.r10.s64 = ctx.r10.s64 + 21312;
loc_825E8F38:
	// lbz r9,-1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// bgt cr6,0x825e8f38
	if (ctx.cr6.gt) goto loc_825E8F38;
loc_825E8F50:
	// lbz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + 0);
	// stw r24,21340(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21340, ctx.r24.u32);
	// b 0x825e9024
	goto loc_825E9024;
loc_825E8F5C:
	// cmplwi cr6,r31,1
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 1, ctx.xer);
	// ble cr6,0x825e8fd8
	if (!ctx.cr6.gt) goto loc_825E8FD8;
	// addi r29,r28,1
	ctx.r29.s64 = ctx.r28.s64 + 1;
	// lbz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r28.u32 + 0);
	// mr r6,r31
	ctx.r6.u64 = ctx.r31.u64;
	// lwz r7,21344(r30)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21344);
	// rotlwi r10,r11,1
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r11.u32, 1);
	// addi r5,r1,80
	ctx.r5.s64 = ctx.r1.s64 + 80;
	// li r4,1
	ctx.r4.s64 = 1;
	// lbz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r29.u32 + 0);
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// rlwinm r11,r11,25,7,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 25) & 0x1FFFFFF;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r11,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r11.u32);
	// bl 0x825cedb0
	ctx.lr = 0x825E8F98;
	sub_825CEDB0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825e8fd0
	if (ctx.cr6.eq) goto loc_825E8FD0;
	// cmpwi cr6,r3,4
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 4, ctx.xer);
	// bne cr6,0x825e90bc
	if (!ctx.cr6.eq) goto loc_825E90BC;
	// lwz r11,80(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// addi r31,r31,-2
	ctx.r31.s64 = ctx.r31.s64 + -2;
	// addi r28,r28,2
	ctx.r28.s64 = ctx.r28.s64 + 2;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// stbx r11,r21,r23
	PPC_STORE_U8(ctx.r21.u32 + ctx.r23.u32, ctx.r11.u8);
	// addi r11,r23,1
	ctx.r11.s64 = ctx.r23.s64 + 1;
	// lbz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r29.u32 + 0);
	// addi r23,r11,1
	ctx.r23.s64 = ctx.r11.s64 + 1;
	// clrlwi r10,r10,25
	ctx.r10.u64 = ctx.r10.u32 & 0x7F;
	// stbx r10,r21,r11
	PPC_STORE_U8(ctx.r21.u32 + ctx.r11.u32, ctx.r10.u8);
loc_825E8FD0:
	// stw r22,21340(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21340, ctx.r22.u32);
	// b 0x825e9038
	goto loc_825E9038;
loc_825E8FD8:
	// cmpwi cr6,r15,1
	ctx.cr6.compare<int32_t>(ctx.r15.s32, 1, ctx.xer);
	// bne cr6,0x825e9038
	if (!ctx.cr6.eq) goto loc_825E9038;
	// cmplwi cr6,r31,1
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 1, ctx.xer);
	// stw r24,21340(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21340, ctx.r24.u32);
	// bne cr6,0x825e9038
	if (!ctx.cr6.eq) goto loc_825E9038;
	// lwz r11,21300(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// cmpwi cr6,r11,2
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 2, ctx.xer);
	// bgt cr6,0x825e90c4
	if (ctx.cr6.gt) goto loc_825E90C4;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825e9020
	if (!ctx.cr6.gt) goto loc_825E9020;
	// add r10,r11,r30
	ctx.r10.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addi r10,r10,21312
	ctx.r10.s64 = ctx.r10.s64 + 21312;
loc_825E9008:
	// lbz r9,-1(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stb r9,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r9.u8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// bgt cr6,0x825e9008
	if (ctx.cr6.gt) goto loc_825E9008;
loc_825E9020:
	// lbz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r28.u32 + 0);
loc_825E9024:
	// lwz r11,21300(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// mr r31,r22
	ctx.r31.u64 = ctx.r22.u64;
	// stb r10,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r10.u8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// stw r11,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r11.u32);
loc_825E9038:
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// add r3,r21,r23
	ctx.r3.u64 = ctx.r21.u64 + ctx.r23.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825E9048;
	sub_8239CB70(ctx, base);
	// lwz r29,292(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// add r23,r31,r23
	ctx.r23.u64 = ctx.r31.u64 + ctx.r23.u64;
loc_825E9050:
	// subf r11,r27,r25
	ctx.r11.s64 = ctx.r25.s64 - ctx.r27.s64;
	// li r4,4
	ctx.r4.s64 = 4;
	// add r26,r11,r26
	ctx.r26.u64 = ctx.r11.u64 + ctx.r26.u64;
	// mr r25,r27
	ctx.r25.u64 = ctx.r27.u64;
	// cmplwi cr6,r26,4
	ctx.cr6.compare<uint32_t>(ctx.r26.u32, 4, ctx.xer);
	// bge cr6,0x825e8a3c
	if (!ctx.cr6.lt) goto loc_825E8A3C;
loc_825E9068:
	// cmpwi cr6,r15,0
	ctx.cr6.compare<int32_t>(ctx.r15.s32, 0, ctx.xer);
	// beq cr6,0x825e912c
	if (ctx.cr6.eq) goto loc_825E912C;
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// bne cr6,0x825e912c
	if (!ctx.cr6.eq) goto loc_825E912C;
	// add r11,r21,r23
	ctx.r11.u64 = ctx.r21.u64 + ctx.r23.u64;
	// cmplwi cr6,r23,2
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 2, ctx.xer);
	// ble cr6,0x825e90d0
	if (!ctx.cr6.gt) goto loc_825E90D0;
	// lbz r10,-1(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -1);
	// cmplwi cr6,r10,1
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 1, ctx.xer);
	// bne cr6,0x825e90d0
	if (!ctx.cr6.eq) goto loc_825E90D0;
	// lbz r10,-2(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e90d0
	if (!ctx.cr6.eq) goto loc_825E90D0;
	// lbz r10,-3(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -3);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e90d0
	if (!ctx.cr6.eq) goto loc_825E90D0;
	// stw r17,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r17.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r22.u8);
	// stb r22,21313(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21313, ctx.r22.u8);
	// stb r24,21314(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21314, ctx.r24.u8);
	// b 0x825e9124
	goto loc_825E9124;
loc_825E90BC:
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// bne cr6,0x825e9144
	if (!ctx.cr6.eq) goto loc_825E9144;
loc_825E90C4:
	// li r3,4
	ctx.r3.s64 = 4;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825E90D0:
	// cmplwi cr6,r23,1
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 1, ctx.xer);
	// ble cr6,0x825e9100
	if (!ctx.cr6.gt) goto loc_825E9100;
	// lbz r10,-1(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -1);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e9100
	if (!ctx.cr6.eq) goto loc_825E9100;
	// lbz r10,-2(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825e9100
	if (!ctx.cr6.eq) goto loc_825E9100;
	// stw r16,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r16.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r22.u8);
	// stb r22,21313(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21313, ctx.r22.u8);
	// b 0x825e9124
	goto loc_825E9124;
loc_825E9100:
	// cmplwi cr6,r23,0
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 0, ctx.xer);
	// beq cr6,0x825e9120
	if (ctx.cr6.eq) goto loc_825E9120;
	// lbz r11,-1(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + -1);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825e9120
	if (!ctx.cr6.eq) goto loc_825E9120;
	// stw r24,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r24.u32);
	// stb r22,21312(r30)
	PPC_STORE_U8(ctx.r30.u32 + 21312, ctx.r22.u8);
	// b 0x825e9124
	goto loc_825E9124;
loc_825E9120:
	// stw r22,21300(r30)
	PPC_STORE_U32(ctx.r30.u32 + 21300, ctx.r22.u32);
loc_825E9124:
	// lwz r11,21300(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 21300);
	// subf r23,r11,r23
	ctx.r23.s64 = ctx.r23.s64 - ctx.r11.s64;
loc_825E912C:
	// stw r21,0(r14)
	PPC_STORE_U32(ctx.r14.u32 + 0, ctx.r21.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r23,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r23.u32);
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825E9140:
	// li r3,2
	ctx.r3.s64 = 2;
loc_825E9144:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825E914C"))) PPC_WEAK_FUNC(sub_825E914C);
PPC_FUNC_IMPL(__imp__sub_825E914C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E9150"))) PPC_WEAK_FUNC(sub_825E9150);
PPC_FUNC_IMPL(__imp__sub_825E9150) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r30,0(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi r30,0
	ctx.cr0.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq 0x825e9194
	if (ctx.cr0.eq) goto loc_825E9194;
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// lwz r11,4(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 4);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x825E9190;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
loc_825E9194:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E91AC"))) PPC_WEAK_FUNC(sub_825E91AC);
PPC_FUNC_IMPL(__imp__sub_825E91AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825E91B0"))) PPC_WEAK_FUNC(sub_825E91B0);
PPC_FUNC_IMPL(__imp__sub_825E91B0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,21592(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21592);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e9224
	if (ctx.cr6.eq) goto loc_825E9224;
	// lwz r11,21584(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21584);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e91f4
	if (!ctx.cr6.eq) goto loc_825E91F4;
	// lwz r11,21588(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21588);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825e91f4
	if (!ctx.cr6.eq) goto loc_825E91F4;
	// li r11,-1
	ctx.r11.s64 = -1;
	// stw r11,15200(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15200, ctx.r11.u32);
loc_825E91F4:
	// addi r4,r31,3688
	ctx.r4.s64 = ctx.r31.s64 + 3688;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// li r5,0
	ctx.r5.s64 = 0;
	// lwz r30,0(r4)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// bl 0x82626170
	ctx.lr = 0x825E9208;
	sub_82626170(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// bl 0x826261c0
	ctx.lr = 0x825E9214;
	sub_826261C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825e9270
	if (ctx.cr6.eq) goto loc_825E9270;
	// li r3,-100
	ctx.r3.s64 = -100;
	// b 0x825e92b0
	goto loc_825E92B0;
loc_825E9224:
	// lwz r11,15200(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15200);
	// cmpwi cr6,r11,-1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, -1, ctx.xer);
	// beq cr6,0x825e9270
	if (ctx.cr6.eq) goto loc_825E9270;
	// addi r10,r11,-1
	ctx.r10.s64 = ctx.r11.s64 + -1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r10,15200(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15200, ctx.r10.u32);
	// bne cr6,0x825e9270
	if (!ctx.cr6.eq) goto loc_825E9270;
	// addi r4,r31,3696
	ctx.r4.s64 = ctx.r31.s64 + 3696;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r30,0(r4)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// bl 0x82626170
	ctx.lr = 0x825E9254;
	sub_82626170(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// bl 0x826261c0
	ctx.lr = 0x825E9260;
	sub_826261C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825e9270
	if (ctx.cr6.eq) goto loc_825E9270;
	// li r3,-100
	ctx.r3.s64 = -100;
	// b 0x825e92b0
	goto loc_825E92B0;
loc_825E9270:
	// lwz r11,284(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 284);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e9288
	if (ctx.cr6.eq) goto loc_825E9288;
	// lwz r10,21584(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21584);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825e92ac
	if (ctx.cr6.eq) goto loc_825E92AC;
loc_825E9288:
	// li r9,1
	ctx.r9.s64 = 1;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r9,15200(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15200, ctx.r9.u32);
	// stw r10,21584(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21584, ctx.r10.u32);
	// bne cr6,0x825e92a8
	if (!ctx.cr6.eq) goto loc_825E92A8;
	// stw r9,21588(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21588, ctx.r9.u32);
	// b 0x825e92ac
	goto loc_825E92AC;
loc_825E92A8:
	// stw r10,21588(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21588, ctx.r10.u32);
loc_825E92AC:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825E92B0:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E92C8"))) PPC_WEAK_FUNC(sub_825E92C8);
PPC_FUNC_IMPL(__imp__sub_825E92C8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r11,21592(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21592);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825e9334
	if (ctx.cr6.eq) goto loc_825E9334;
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r10,21588(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21588);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r11,21592(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21592, ctx.r11.u32);
	// bne cr6,0x825e9308
	if (!ctx.cr6.eq) goto loc_825E9308;
	// li r11,-1
	ctx.r11.s64 = -1;
	// stw r11,15200(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15200, ctx.r11.u32);
loc_825E9308:
	// addi r4,r31,3696
	ctx.r4.s64 = ctx.r31.s64 + 3696;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r30,0(r4)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// bl 0x82626170
	ctx.lr = 0x825E931C;
	sub_82626170(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// bl 0x826261c0
	ctx.lr = 0x825E9328;
	sub_826261C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// li r3,-100
	ctx.r3.s64 = -100;
	// bne cr6,0x825e9338
	if (!ctx.cr6.eq) goto loc_825E9338;
loc_825E9334:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825E9338:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825E9350"))) PPC_WEAK_FUNC(sub_825E9350);
PPC_FUNC_IMPL(__imp__sub_825E9350) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9f4
	ctx.lr = 0x825E9358;
	sub_8239B9F4(ctx, base);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r22,r3
	ctx.r22.u64 = ctx.r3.u64;
	// lis r11,21845
	ctx.r11.s64 = 1431633920;
	// li r19,0
	ctx.r19.s64 = 0;
	// ori r11,r11,21846
	ctx.r11.u64 = ctx.r11.u64 | 21846;
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// lwz r9,140(r22)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r22.u32 + 140);
	// mr r20,r19
	ctx.r20.u64 = ctx.r19.u64;
	// lwz r10,136(r22)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r22.u32 + 136);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// rlwinm r23,r9,31,1,31
	ctx.r23.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r30,r10,31,1,31
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// mulhw r10,r23,r11
	ctx.r10.s64 = (int64_t(ctx.r23.s32) * int64_t(ctx.r11.s32)) >> 32;
	// rlwinm r9,r10,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf. r10,r10,r23
	ctx.r10.s64 = ctx.r23.s64 - ctx.r10.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// bne 0x825e949c
	if (!ctx.cr0.eq) goto loc_825E949C;
	// mulhw r10,r30,r11
	ctx.r10.s64 = (int64_t(ctx.r30.s32) * int64_t(ctx.r11.s32)) >> 32;
	// rlwinm r9,r10,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf. r10,r10,r30
	ctx.r10.s64 = ctx.r30.s64 - ctx.r10.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x825e949c
	if (ctx.cr0.eq) goto loc_825E949C;
	// clrlwi r21,r30,31
	ctx.r21.u64 = ctx.r30.u32 & 0x1;
	// mr r25,r19
	ctx.r25.u64 = ctx.r19.u64;
	// cmpwi cr6,r23,0
	ctx.cr6.compare<int32_t>(ctx.r23.s32, 0, ctx.xer);
	// ble cr6,0x825e9584
	if (!ctx.cr6.gt) goto loc_825E9584;
	// rlwinm r11,r30,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r26,r21
	ctx.r26.u64 = ctx.r21.u64;
	// add r24,r30,r11
	ctx.r24.u64 = ctx.r30.u64 + ctx.r11.u64;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// addi r27,r11,4088
	ctx.r27.s64 = ctx.r11.s64 + 4088;
loc_825E93E8:
	// mr r28,r21
	ctx.r28.u64 = ctx.r21.u64;
	// cmpw cr6,r21,r30
	ctx.cr6.compare<int32_t>(ctx.r21.s32, ctx.r30.s32, ctx.xer);
	// bge cr6,0x825e9488
	if (!ctx.cr6.lt) goto loc_825E9488;
	// mr r31,r26
	ctx.r31.u64 = ctx.r26.u64;
loc_825E93F8:
	// addi r5,r27,64
	ctx.r5.s64 = ctx.r27.s64 + 64;
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r22.u32 + 84);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x825d6da0
	ctx.lr = 0x825E940C;
	sub_825D6DA0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e9710
	if (!ctx.cr6.eq) goto loc_825E9710;
	// lwz r9,80(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r5,r31,r29
	ctx.r5.u64 = ctx.r31.u64 + ctx.r29.u64;
	// add r11,r30,r29
	ctx.r11.u64 = ctx.r30.u64 + ctx.r29.u64;
	// clrlwi r8,r9,31
	ctx.r8.u64 = ctx.r9.u32 & 0x1;
	// srawi r9,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// rlwinm r10,r30,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// clrlwi r7,r9,31
	ctx.r7.u64 = ctx.r9.u32 & 0x1;
	// srawi r9,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// stbx r8,r31,r29
	PPC_STORE_U8(ctx.r31.u32 + ctx.r29.u32, ctx.r8.u8);
	// add r10,r10,r29
	ctx.r10.u64 = ctx.r10.u64 + ctx.r29.u64;
	// clrlwi r8,r9,31
	ctx.r8.u64 = ctx.r9.u32 & 0x1;
	// srawi r9,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// stb r7,1(r5)
	PPC_STORE_U8(ctx.r5.u32 + 1, ctx.r7.u8);
	// add r7,r31,r11
	ctx.r7.u64 = ctx.r31.u64 + ctx.r11.u64;
	// clrlwi r6,r9,31
	ctx.r6.u64 = ctx.r9.u32 & 0x1;
	// srawi r9,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// stbx r8,r31,r11
	PPC_STORE_U8(ctx.r31.u32 + ctx.r11.u32, ctx.r8.u8);
	// add r11,r31,r10
	ctx.r11.u64 = ctx.r31.u64 + ctx.r10.u64;
	// clrlwi r4,r9,31
	ctx.r4.u64 = ctx.r9.u32 & 0x1;
	// srawi r9,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 1;
	// addi r28,r28,2
	ctx.r28.s64 = ctx.r28.s64 + 2;
	// stb r6,1(r7)
	PPC_STORE_U8(ctx.r7.u32 + 1, ctx.r6.u8);
	// cmpw cr6,r28,r30
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r30.s32, ctx.xer);
	// stbx r4,r31,r10
	PPC_STORE_U8(ctx.r31.u32 + ctx.r10.u32, ctx.r4.u8);
	// addi r31,r31,2
	ctx.r31.s64 = ctx.r31.s64 + 2;
	// stw r9,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r9.u32);
	// clrlwi r9,r9,31
	ctx.r9.u64 = ctx.r9.u32 & 0x1;
	// stb r9,1(r11)
	PPC_STORE_U8(ctx.r11.u32 + 1, ctx.r9.u8);
	// blt cr6,0x825e93f8
	if (ctx.cr6.lt) goto loc_825E93F8;
loc_825E9488:
	// addi r25,r25,3
	ctx.r25.s64 = ctx.r25.s64 + 3;
	// add r26,r24,r26
	ctx.r26.u64 = ctx.r24.u64 + ctx.r26.u64;
	// cmpw cr6,r25,r23
	ctx.cr6.compare<int32_t>(ctx.r25.s32, ctx.r23.s32, ctx.xer);
	// blt cr6,0x825e93e8
	if (ctx.cr6.lt) goto loc_825E93E8;
	// b 0x825e9584
	goto loc_825E9584;
loc_825E949C:
	// mulhw r11,r30,r11
	ctx.r11.s64 = (int64_t(ctx.r30.s32) * int64_t(ctx.r11.s32)) >> 32;
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0x1;
	// clrlwi r20,r23,31
	ctx.r20.u64 = ctx.r23.u32 & 0x1;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// mr r25,r20
	ctx.r25.u64 = ctx.r20.u64;
	// cmpw cr6,r20,r23
	ctx.cr6.compare<int32_t>(ctx.r20.s32, ctx.r23.s32, ctx.xer);
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// subf r21,r11,r30
	ctx.r21.s64 = ctx.r30.s64 - ctx.r11.s64;
	// bge cr6,0x825e9584
	if (!ctx.cr6.lt) goto loc_825E9584;
	// mullw r11,r30,r20
	ctx.r11.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r20.s32);
	// add r26,r11,r21
	ctx.r26.u64 = ctx.r11.u64 + ctx.r21.u64;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// rlwinm r24,r30,1,0,30
	ctx.r24.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r27,r11,4088
	ctx.r27.s64 = ctx.r11.s64 + 4088;
loc_825E94D8:
	// mr r28,r21
	ctx.r28.u64 = ctx.r21.u64;
	// cmpw cr6,r21,r30
	ctx.cr6.compare<int32_t>(ctx.r21.s32, ctx.r30.s32, ctx.xer);
	// bge cr6,0x825e9574
	if (!ctx.cr6.lt) goto loc_825E9574;
	// mr r31,r26
	ctx.r31.u64 = ctx.r26.u64;
loc_825E94E8:
	// addi r5,r27,64
	ctx.r5.s64 = ctx.r27.s64 + 64;
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r22.u32 + 84);
	// addi r6,r1,80
	ctx.r6.s64 = ctx.r1.s64 + 80;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x825d6da0
	ctx.lr = 0x825E94FC;
	sub_825D6DA0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825e9710
	if (!ctx.cr6.eq) goto loc_825E9710;
	// lwz r10,80(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// add r6,r31,r29
	ctx.r6.u64 = ctx.r31.u64 + ctx.r29.u64;
	// add r5,r31,r29
	ctx.r5.u64 = ctx.r31.u64 + ctx.r29.u64;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r11,r30,r29
	ctx.r11.u64 = ctx.r30.u64 + ctx.r29.u64;
	// clrlwi r8,r10,31
	ctx.r8.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stbx r9,r31,r29
	PPC_STORE_U8(ctx.r31.u32 + ctx.r29.u32, ctx.r9.u8);
	// addi r28,r28,3
	ctx.r28.s64 = ctx.r28.s64 + 3;
	// clrlwi r9,r10,31
	ctx.r9.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stb r8,1(r6)
	PPC_STORE_U8(ctx.r6.u32 + 1, ctx.r8.u8);
	// cmpw cr6,r28,r30
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r30.s32, ctx.xer);
	// clrlwi r7,r10,31
	ctx.r7.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stb r9,2(r5)
	PPC_STORE_U8(ctx.r5.u32 + 2, ctx.r9.u8);
	// add r9,r31,r11
	ctx.r9.u64 = ctx.r31.u64 + ctx.r11.u64;
	// clrlwi r4,r10,31
	ctx.r4.u64 = ctx.r10.u32 & 0x1;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stbx r7,r31,r11
	PPC_STORE_U8(ctx.r31.u32 + ctx.r11.u32, ctx.r7.u8);
	// add r11,r31,r11
	ctx.r11.u64 = ctx.r31.u64 + ctx.r11.u64;
	// addi r31,r31,3
	ctx.r31.s64 = ctx.r31.s64 + 3;
	// stb r4,1(r9)
	PPC_STORE_U8(ctx.r9.u32 + 1, ctx.r4.u8);
	// stw r10,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r10.u32);
	// clrlwi r10,r10,31
	ctx.r10.u64 = ctx.r10.u32 & 0x1;
	// stb r10,2(r11)
	PPC_STORE_U8(ctx.r11.u32 + 2, ctx.r10.u8);
	// blt cr6,0x825e94e8
	if (ctx.cr6.lt) goto loc_825E94E8;
loc_825E9574:
	// addi r25,r25,2
	ctx.r25.s64 = ctx.r25.s64 + 2;
	// add r26,r26,r24
	ctx.r26.u64 = ctx.r26.u64 + ctx.r24.u64;
	// cmpw cr6,r25,r23
	ctx.cr6.compare<int32_t>(ctx.r25.s32, ctx.r23.s32, ctx.xer);
	// blt cr6,0x825e94d8
	if (ctx.cr6.lt) goto loc_825E94D8;
loc_825E9584:
	// cmpwi cr6,r21,0
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 0, ctx.xer);
	// ble cr6,0x825e9650
	if (!ctx.cr6.gt) goto loc_825E9650;
	// mr r26,r29
	ctx.r26.u64 = ctx.r29.u64;
	// mr r25,r21
	ctx.r25.u64 = ctx.r21.u64;
loc_825E9594:
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r22.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e95c0
	if (!ctx.cr0.lt) goto loc_825E95C0;
	// bl 0x825d5398
	ctx.lr = 0x825E95C0;
	sub_825D5398(ctx, base);
loc_825E95C0:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825e961c
	if (ctx.cr6.eq) goto loc_825E961C;
	// cmpwi cr6,r23,0
	ctx.cr6.compare<int32_t>(ctx.r23.s32, 0, ctx.xer);
	// ble cr6,0x825e9640
	if (!ctx.cr6.gt) goto loc_825E9640;
	// mr r28,r26
	ctx.r28.u64 = ctx.r26.u64;
	// mr r31,r23
	ctx.r31.u64 = ctx.r23.u64;
loc_825E95D8:
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r22.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r27,r8,0
	ctx.r27.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e9604
	if (!ctx.cr0.lt) goto loc_825E9604;
	// bl 0x825d5398
	ctx.lr = 0x825E9604;
	sub_825D5398(ctx, base);
loc_825E9604:
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// stb r27,0(r28)
	PPC_STORE_U8(ctx.r28.u32 + 0, ctx.r27.u8);
	// add r28,r28,r30
	ctx.r28.u64 = ctx.r28.u64 + ctx.r30.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825e95d8
	if (!ctx.cr6.eq) goto loc_825E95D8;
	// b 0x825e9640
	goto loc_825E9640;
loc_825E961C:
	// cmpwi cr6,r23,0
	ctx.cr6.compare<int32_t>(ctx.r23.s32, 0, ctx.xer);
	// ble cr6,0x825e9640
	if (!ctx.cr6.gt) goto loc_825E9640;
	// mr r10,r26
	ctx.r10.u64 = ctx.r26.u64;
	// mr r11,r23
	ctx.r11.u64 = ctx.r23.u64;
loc_825E962C:
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stb r19,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r19.u8);
	// add r10,r10,r30
	ctx.r10.u64 = ctx.r10.u64 + ctx.r30.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825e962c
	if (!ctx.cr6.eq) goto loc_825E962C;
loc_825E9640:
	// addi r25,r25,-1
	ctx.r25.s64 = ctx.r25.s64 + -1;
	// addi r26,r26,1
	ctx.r26.s64 = ctx.r26.s64 + 1;
	// cmplwi cr6,r25,0
	ctx.cr6.compare<uint32_t>(ctx.r25.u32, 0, ctx.xer);
	// bne cr6,0x825e9594
	if (!ctx.cr6.eq) goto loc_825E9594;
loc_825E9650:
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// beq cr6,0x825e970c
	if (ctx.cr6.eq) goto loc_825E970C;
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r22.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e9684
	if (!ctx.cr0.lt) goto loc_825E9684;
	// bl 0x825d5398
	ctx.lr = 0x825E9684;
	sub_825D5398(ctx, base);
loc_825E9684:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825e96e0
	if (ctx.cr6.eq) goto loc_825E96E0;
	// mr r31,r21
	ctx.r31.u64 = ctx.r21.u64;
	// cmpw cr6,r21,r30
	ctx.cr6.compare<int32_t>(ctx.r21.s32, ctx.r30.s32, ctx.xer);
	// bge cr6,0x825e970c
	if (!ctx.cr6.lt) goto loc_825E970C;
loc_825E9698:
	// lwz r3,84(r22)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r22.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r28,r8,0
	ctx.r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825e96c4
	if (!ctx.cr0.lt) goto loc_825E96C4;
	// bl 0x825d5398
	ctx.lr = 0x825E96C4;
	sub_825D5398(ctx, base);
loc_825E96C4:
	// stbx r28,r31,r29
	PPC_STORE_U8(ctx.r31.u32 + ctx.r29.u32, ctx.r28.u8);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// cmpw cr6,r31,r30
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r30.s32, ctx.xer);
	// blt cr6,0x825e9698
	if (ctx.cr6.lt) goto loc_825E9698;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239ba44
	// ERROR 8239BA44
	return;
loc_825E96E0:
	// cmpw cr6,r21,r30
	ctx.cr6.compare<int32_t>(ctx.r21.s32, ctx.r30.s32, ctx.xer);
	// bge cr6,0x825e970c
	if (!ctx.cr6.lt) goto loc_825E970C;
	// subf r10,r21,r30
	ctx.r10.s64 = ctx.r30.s64 - ctx.r21.s64;
	// add r11,r21,r29
	ctx.r11.u64 = ctx.r21.u64 + ctx.r29.u64;
	// mr r9,r19
	ctx.r9.u64 = ctx.r19.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825e970c
	if (ctx.cr6.eq) goto loc_825E970C;
	// mtctr r10
	ctx.ctr.u64 = ctx.r10.u64;
loc_825E9700:
	// stb r9,0(r11)
	PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r9.u8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// bdnz 0x825e9700
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_825E9700;
loc_825E970C:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825E9710:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239ba44
	// ERROR 8239BA44
	return;
}

__attribute__((alias("__imp__sub_825E9718"))) PPC_WEAK_FUNC(sub_825E9718);
PPC_FUNC_IMPL(__imp__sub_825E9718) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9f8
	ctx.lr = 0x825E9720;
	sub_8239B9F8(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
	// mr r23,r3
	ctx.r23.u64 = ctx.r3.u64;
	// mr r20,r5
	ctx.r20.u64 = ctx.r5.u64;
	// lbz r4,8(r11)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r11.u32 + 8);
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// lwz r29,0(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// subfic r11,r4,64
	ctx.xer.ca = ctx.r4.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r4.s64;
	// clrldi r10,r11,32
	ctx.r10.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// srd r10,r11,r10
	ctx.r10.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r10.u8 & 0x7F));
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r10,r10,r29
	ctx.r10.u64 = PPC_LOAD_U16(ctx.r10.u32 + ctx.r29.u32);
	// extsh r30,r10
	ctx.r30.s64 = ctx.r10.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e9820
	if (ctx.cr6.lt) goto loc_825E9820;
	// clrlwi r10,r30,28
	ctx.r10.u64 = ctx.r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r9,r10
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r10.s32, ctx.xer);
	// sld r11,r11,r10
	ctx.r11.u64 = ctx.r10.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r10.u8 & 0x7F));
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// subf r11,r10,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// bge cr6,0x825e9818
	if (!ctx.cr6.lt) goto loc_825E9818;
loc_825E9780:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825e97ac
	if (ctx.cr6.lt) goto loc_825E97AC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825E979C;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825e9780
	if (ctx.cr6.eq) goto loc_825E9780;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e9864
	goto loc_825E9864;
loc_825E97AC:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = ctx.r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// rldicr r11,r9,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
loc_825E9818:
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825e9864
	goto loc_825E9864;
loc_825E9820:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E9828;
	sub_825D5468(ctx, base);
	// lis r11,0
	ctx.r11.s64 = 0;
	// ori r28,r11,32768
	ctx.r28.u64 = ctx.r11.u64 | 32768;
loc_825E9830:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// add r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825d5468
	ctx.lr = 0x825E984C;
	sub_825D5468(ctx, base);
	// add r11,r30,r28
	ctx.r11.u64 = ctx.r30.u64 + ctx.r28.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825e9830
	if (ctx.cr6.lt) goto loc_825E9830;
loc_825E9864:
	// addi r24,r30,1
	ctx.r24.s64 = ctx.r30.s64 + 1;
	// lwz r11,0(r20)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	// cmpwi cr6,r24,38
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 38, ctx.xer);
	// blt cr6,0x825e9880
	if (ctx.cr6.lt) goto loc_825E9880;
	// addi r24,r24,-38
	ctx.r24.s64 = ctx.r24.s64 + -38;
	// ori r11,r11,8
	ctx.r11.u64 = ctx.r11.u64 | 8;
	// b 0x825e9884
	goto loc_825E9884;
loc_825E9880:
	// rlwinm r11,r11,0,29,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFF7;
loc_825E9884:
	// stw r11,0(r20)
	PPC_STORE_U32(ctx.r20.u32 + 0, ctx.r11.u32);
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// cmpwi cr6,r24,0
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// rlwinm r11,r11,0,30,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFFFFFB;
	// stw r11,0(r20)
	PPC_STORE_U32(ctx.r20.u32 + 0, ctx.r11.u32);
	// beq cr6,0x825ea39c
	if (ctx.cr6.eq) goto loc_825EA39C;
	// cmpwi cr6,r24,36
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 36, ctx.xer);
	// beq cr6,0x825ea1d8
	if (ctx.cr6.eq) goto loc_825EA1D8;
	// cmpwi cr6,r24,37
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 37, ctx.xer);
	// beq cr6,0x825ea1b8
	if (ctx.cr6.eq) goto loc_825EA1B8;
	// lwz r10,1972(r23)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r23.u32 + 1972);
	// lis r11,10922
	ctx.r11.s64 = 715784192;
	// li r21,1
	ctx.r21.s64 = 1;
	// ori r25,r11,43691
	ctx.r25.u64 = ctx.r11.u64 | 43691;
	// mulhw r11,r24,r25
	ctx.r11.s64 = (int64_t(ctx.r24.s32) * int64_t(ctx.r25.s32)) >> 32;
	// lwz r10,76(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// rlwinm r10,r11,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0x1;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r26,r11,r24
	ctx.r26.s64 = ctx.r24.s64 - ctx.r11.s64;
	// beq cr6,0x825e98f4
	if (ctx.cr6.eq) goto loc_825E98F4;
	// cmpwi cr6,r26,5
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 5, ctx.xer);
	// bne cr6,0x825e98f4
	if (!ctx.cr6.eq) goto loc_825E98F4;
	// mr r10,r21
	ctx.r10.u64 = ctx.r21.u64;
	// b 0x825e9900
	goto loc_825E9900;
loc_825E98F4:
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r26,1
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 1, ctx.xer);
	// ble cr6,0x825e9c38
	if (!ctx.cr6.gt) goto loc_825E9C38;
loc_825E9900:
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// addi r9,r26,-2
	ctx.r9.s64 = ctx.r26.s64 + -2;
	// addi r22,r11,4008
	ctx.r22.s64 = ctx.r11.s64 + 4008;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r11,r22,272
	ctx.r11.s64 = ctx.r22.s64 + 272;
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwzx r11,r9,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r11.u32);
	// subf r27,r10,r11
	ctx.r27.s64 = ctx.r11.s64 - ctx.r10.s64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e9990
	if (!ctx.cr6.lt) goto loc_825E9990;
loc_825E9938:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e9990
	if (ctx.cr6.eq) goto loc_825E9990;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e9980
	if (!ctx.cr0.lt) goto loc_825E9980;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9980;
	sub_825D5398(ctx, base);
loc_825E9980:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e9938
	if (ctx.cr6.gt) goto loc_825E9938;
loc_825E9990:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e99cc
	if (!ctx.cr0.lt) goto loc_825E99CC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E99CC;
	sub_825D5398(ctx, base);
loc_825E99CC:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e99dc
	if (ctx.cr6.eq) goto loc_825E99DC;
	// li r28,0
	ctx.r28.s64 = 0;
	// b 0x825e9b58
	goto loc_825E9B58;
loc_825E99DC:
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e9a50
	if (!ctx.cr6.lt) goto loc_825E9A50;
loc_825E99F8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e9a50
	if (ctx.cr6.eq) goto loc_825E9A50;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e9a40
	if (!ctx.cr0.lt) goto loc_825E9A40;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9A40;
	sub_825D5398(ctx, base);
loc_825E9A40:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e99f8
	if (ctx.cr6.gt) goto loc_825E99F8;
loc_825E9A50:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e9a8c
	if (!ctx.cr0.lt) goto loc_825E9A8C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9A8C;
	sub_825D5398(ctx, base);
loc_825E9A8C:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e9a9c
	if (ctx.cr6.eq) goto loc_825E9A9C;
	// mr r28,r21
	ctx.r28.u64 = ctx.r21.u64;
	// b 0x825e9b58
	goto loc_825E9B58;
loc_825E9A9C:
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e9b10
	if (!ctx.cr6.lt) goto loc_825E9B10;
loc_825E9AB8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e9b10
	if (ctx.cr6.eq) goto loc_825E9B10;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e9b00
	if (!ctx.cr0.lt) goto loc_825E9B00;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9B00;
	sub_825D5398(ctx, base);
loc_825E9B00:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e9ab8
	if (ctx.cr6.gt) goto loc_825E9AB8;
loc_825E9B10:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e9b4c
	if (!ctx.cr0.lt) goto loc_825E9B4C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9B4C;
	sub_825D5398(ctx, base);
loc_825E9B4C:
	// cntlzw r11,r30
	ctx.r11.u64 = ctx.r30.u32 == 0 ? 32 : __builtin_clz(ctx.r30.u32);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// addi r28,r11,2
	ctx.r28.s64 = ctx.r11.s64 + 2;
loc_825E9B58:
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825e9b88
	if (!ctx.cr6.eq) goto loc_825E9B88;
	// slw r11,r21,r27
	ctx.r11.u64 = ctx.r27.u8 & 0x20 ? 0 : (ctx.r21.u32 << (ctx.r27.u8 & 0x3F));
	// li r10,0
	ctx.r10.s64 = 0;
	// mullw r11,r11,r28
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r28.s32);
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// b 0x825e9d08
	goto loc_825E9D08;
loc_825E9B88:
	// cmplw cr6,r27,r11
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e9be8
	if (!ctx.cr6.gt) goto loc_825E9BE8;
loc_825E9B90:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e9be8
	if (ctx.cr6.eq) goto loc_825E9BE8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e9bd8
	if (!ctx.cr0.lt) goto loc_825E9BD8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9BD8;
	sub_825D5398(ctx, base);
loc_825E9BD8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e9b90
	if (ctx.cr6.gt) goto loc_825E9B90;
loc_825E9BE8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e9c24
	if (!ctx.cr0.lt) goto loc_825E9C24;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9C24;
	sub_825D5398(ctx, base);
loc_825E9C24:
	// slw r11,r21,r27
	ctx.r11.u64 = ctx.r27.u8 & 0x20 ? 0 : (ctx.r21.u32 << (ctx.r27.u8 & 0x3F));
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// mullw r11,r11,r28
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r28.s32);
	// add r10,r11,r10
	ctx.r10.u64 = ctx.r11.u64 + ctx.r10.u64;
	// b 0x825e9d08
	goto loc_825E9D08;
loc_825E9C38:
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// li r29,0
	ctx.r29.s64 = 0;
	// addi r22,r11,4008
	ctx.r22.s64 = ctx.r11.s64 + 4008;
	// rlwinm r11,r26,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// lwzx r30,r11,r22
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825e9c68
	if (!ctx.cr6.eq) goto loc_825E9C68;
	// li r10,0
	ctx.r10.s64 = 0;
	// b 0x825e9d08
	goto loc_825E9D08;
loc_825E9C68:
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825e9cc8
	if (!ctx.cr6.gt) goto loc_825E9CC8;
loc_825E9C70:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e9cc8
	if (ctx.cr6.eq) goto loc_825E9CC8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e9cb8
	if (!ctx.cr0.lt) goto loc_825E9CB8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9CB8;
	sub_825D5398(ctx, base);
loc_825E9CB8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e9c70
	if (ctx.cr6.gt) goto loc_825E9C70;
loc_825E9CC8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e9d04
	if (!ctx.cr0.lt) goto loc_825E9D04;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9D04;
	sub_825D5398(ctx, base);
loc_825E9D04:
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
loc_825E9D08:
	// rlwinm r7,r26,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r22,24
	ctx.r8.s64 = ctx.r22.s64 + 24;
	// clrlwi r11,r10,31
	ctx.r11.u64 = ctx.r10.u32 & 0x1;
	// srawi r9,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r10.s32 >> 1;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
	// lwzx r10,r7,r8
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r8.u32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// mulhw r10,r24,r25
	ctx.r10.s64 = (int64_t(ctx.r24.s32) * int64_t(ctx.r25.s32)) >> 32;
	// xor r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 ^ ctx.r11.u64;
	// sth r9,0(r20)
	PPC_STORE_U16(ctx.r20.u32 + 0, ctx.r9.u16);
	// clrlwi r9,r9,16
	ctx.r9.u64 = ctx.r9.u32 & 0xFFFF;
	// subf r11,r11,r9
	ctx.r11.s64 = ctx.r9.s64 - ctx.r11.s64;
	// sth r11,0(r20)
	PPC_STORE_U16(ctx.r20.u32 + 0, ctx.r11.u16);
	// lwz r11,1972(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 1972);
	// lwz r11,76(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rlwinm r11,r10,1,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// add r26,r10,r11
	ctx.r26.u64 = ctx.r10.u64 + ctx.r11.u64;
	// beq cr6,0x825e9d64
	if (ctx.cr6.eq) goto loc_825E9D64;
	// cmpwi cr6,r26,5
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 5, ctx.xer);
	// bne cr6,0x825e9d64
	if (!ctx.cr6.eq) goto loc_825E9D64;
	// mr r11,r21
	ctx.r11.u64 = ctx.r21.u64;
	// b 0x825e9d70
	goto loc_825E9D70;
loc_825E9D64:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r26,1
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 1, ctx.xer);
	// ble cr6,0x825ea0a0
	if (!ctx.cr6.gt) goto loc_825EA0A0;
loc_825E9D70:
	// addi r10,r26,-2
	ctx.r10.s64 = ctx.r26.s64 + -2;
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// addi r9,r22,272
	ctx.r9.s64 = ctx.r22.s64 + 272;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwzx r10,r10,r9
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r9.u32);
	// subf r27,r11,r10
	ctx.r27.s64 = ctx.r10.s64 - ctx.r11.s64;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e9df8
	if (!ctx.cr6.lt) goto loc_825E9DF8;
loc_825E9DA0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e9df8
	if (ctx.cr6.eq) goto loc_825E9DF8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// srd r8,r9,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r9.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r9,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r9.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r9,r8,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r9,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e9de8
	if (!ctx.cr0.lt) goto loc_825E9DE8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9DE8;
	sub_825D5398(ctx, base);
loc_825E9DE8:
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r10,16
	ctx.r11.s64 = ctx.r10.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e9da0
	if (ctx.cr6.gt) goto loc_825E9DA0;
loc_825E9DF8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e9e34
	if (!ctx.cr0.lt) goto loc_825E9E34;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9E34;
	sub_825D5398(ctx, base);
loc_825E9E34:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e9e44
	if (ctx.cr6.eq) goto loc_825E9E44;
	// li r28,0
	ctx.r28.s64 = 0;
	// b 0x825e9fc0
	goto loc_825E9FC0;
loc_825E9E44:
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e9eb8
	if (!ctx.cr6.lt) goto loc_825E9EB8;
loc_825E9E60:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e9eb8
	if (ctx.cr6.eq) goto loc_825E9EB8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e9ea8
	if (!ctx.cr0.lt) goto loc_825E9EA8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9EA8;
	sub_825D5398(ctx, base);
loc_825E9EA8:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e9e60
	if (ctx.cr6.gt) goto loc_825E9E60;
loc_825E9EB8:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e9ef4
	if (!ctx.cr0.lt) goto loc_825E9EF4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9EF4;
	sub_825D5398(ctx, base);
loc_825E9EF4:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825e9f04
	if (ctx.cr6.eq) goto loc_825E9F04;
	// mr r28,r21
	ctx.r28.u64 = ctx.r21.u64;
	// b 0x825e9fc0
	goto loc_825E9FC0;
loc_825E9F04:
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// mr r30,r21
	ctx.r30.u64 = ctx.r21.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,1
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 1, ctx.xer);
	// bge cr6,0x825e9f78
	if (!ctx.cr6.lt) goto loc_825E9F78;
loc_825E9F20:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825e9f78
	if (ctx.cr6.eq) goto loc_825E9F78;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825e9f68
	if (!ctx.cr0.lt) goto loc_825E9F68;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9F68;
	sub_825D5398(ctx, base);
loc_825E9F68:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e9f20
	if (ctx.cr6.gt) goto loc_825E9F20;
loc_825E9F78:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825e9fb4
	if (!ctx.cr0.lt) goto loc_825E9FB4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825E9FB4;
	sub_825D5398(ctx, base);
loc_825E9FB4:
	// cntlzw r11,r30
	ctx.r11.u64 = ctx.r30.u32 == 0 ? 32 : __builtin_clz(ctx.r30.u32);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// addi r28,r11,2
	ctx.r28.s64 = ctx.r11.s64 + 2;
loc_825E9FC0:
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// mr r30,r27
	ctx.r30.u64 = ctx.r27.u64;
	// li r29,0
	ctx.r29.s64 = 0;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825e9ff0
	if (!ctx.cr6.eq) goto loc_825E9FF0;
	// slw r11,r21,r27
	ctx.r11.u64 = ctx.r27.u8 & 0x20 ? 0 : (ctx.r21.u32 << (ctx.r27.u8 & 0x3F));
	// li r10,0
	ctx.r10.s64 = 0;
	// mullw r11,r11,r28
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r28.s32);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// b 0x825ea168
	goto loc_825EA168;
loc_825E9FF0:
	// cmplw cr6,r27,r11
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825ea050
	if (!ctx.cr6.gt) goto loc_825EA050;
loc_825E9FF8:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ea050
	if (ctx.cr6.eq) goto loc_825EA050;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825ea040
	if (!ctx.cr0.lt) goto loc_825EA040;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EA040;
	sub_825D5398(ctx, base);
loc_825EA040:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825e9ff8
	if (ctx.cr6.gt) goto loc_825E9FF8;
loc_825EA050:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ea08c
	if (!ctx.cr0.lt) goto loc_825EA08C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EA08C;
	sub_825D5398(ctx, base);
loc_825EA08C:
	// slw r11,r21,r27
	ctx.r11.u64 = ctx.r27.u8 & 0x20 ? 0 : (ctx.r21.u32 << (ctx.r27.u8 & 0x3F));
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// mullw r11,r11,r28
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r28.s32);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// b 0x825ea168
	goto loc_825EA168;
loc_825EA0A0:
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// rlwinm r11,r26,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwzx r30,r11,r22
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// bne cr6,0x825ea0c8
	if (!ctx.cr6.eq) goto loc_825EA0C8;
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x825ea168
	goto loc_825EA168;
loc_825EA0C8:
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825ea128
	if (!ctx.cr6.gt) goto loc_825EA128;
loc_825EA0D0:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ea128
	if (ctx.cr6.eq) goto loc_825EA128;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825ea118
	if (!ctx.cr0.lt) goto loc_825EA118;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EA118;
	sub_825D5398(ctx, base);
loc_825EA118:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825ea0d0
	if (ctx.cr6.gt) goto loc_825EA0D0;
loc_825EA128:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ea164
	if (!ctx.cr0.lt) goto loc_825EA164;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EA164;
	sub_825D5398(ctx, base);
loc_825EA164:
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_825EA168:
	// addi r9,r22,24
	ctx.r9.s64 = ctx.r22.s64 + 24;
	// lwz r8,0(r20)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	// rlwinm r7,r26,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r10,r11,31
	ctx.r10.u64 = ctx.r11.u32 & 0x1;
	// neg r10,r10
	ctx.r10.s64 = -ctx.r10.s64;
	// lwzx r9,r7,r9
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r7.u32 + ctx.r9.u32);
	// rlwinm r9,r9,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r9,r11
	ctx.r11.u64 = ctx.r9.u64 + ctx.r11.u64;
	// rlwinm r9,r10,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r10,r10,4,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// xor r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 ^ ctx.r9.u64;
	// rlwimi r11,r8,0,28,15
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r8.u32, 0) & 0xFFFFFFFFFFFF000F) | (ctx.r11.u64 & 0xFFF0);
	// extsh r9,r11
	ctx.r9.s64 = ctx.r11.s16;
	// rlwinm r9,r9,0,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF0;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// rlwimi r10,r11,0,28,15
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r11.u32, 0) & 0xFFFFFFFFFFFF000F) | (ctx.r10.u64 & 0xFFF0);
	// stw r10,0(r20)
	PPC_STORE_U32(ctx.r20.u32 + 0, ctx.r10.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825EA1B8:
	// li r10,1
	ctx.r10.s64 = 1;
	// li r9,0
	ctx.r9.s64 = 0;
	// rlwimi r11,r10,2,29,29
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 2) & 0x4) | (ctx.r11.u64 & 0xFFFFFFFFFFFFFFFB);
	// rlwimi r11,r10,2,16,27
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 2) & 0xFFF0) | (ctx.r11.u64 & 0xFFFFFFFFFFFF000F);
	// stw r11,0(r20)
	PPC_STORE_U32(ctx.r20.u32 + 0, ctx.r11.u32);
	// sth r9,0(r20)
	PPC_STORE_U16(ctx.r20.u32 + 0, ctx.r9.u16);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825EA1D8:
	// lwz r10,1972(r23)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r23.u32 + 1972);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// lwz r11,408(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 408);
	// lwz r10,76(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 76);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf r30,r10,r11
	ctx.r30.s64 = ctx.r11.s64 - ctx.r10.s64;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825ea208
	if (!ctx.cr6.eq) goto loc_825EA208;
	// li r11,0
	ctx.r11.s64 = 0;
	// b 0x825ea2a8
	goto loc_825EA2A8;
loc_825EA208:
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825ea268
	if (!ctx.cr6.gt) goto loc_825EA268;
loc_825EA210:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ea268
	if (ctx.cr6.eq) goto loc_825EA268;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825ea258
	if (!ctx.cr0.lt) goto loc_825EA258;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EA258;
	sub_825D5398(ctx, base);
loc_825EA258:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825ea210
	if (ctx.cr6.gt) goto loc_825EA210;
loc_825EA268:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ea2a4
	if (!ctx.cr0.lt) goto loc_825EA2A4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EA2A4;
	sub_825D5398(ctx, base);
loc_825EA2A4:
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
loc_825EA2A8:
	// sth r11,0(r20)
	PPC_STORE_U16(ctx.r20.u32 + 0, ctx.r11.u16);
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r11,1972(r23)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r23.u32 + 1972);
	// lwz r31,84(r23)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r23.u32 + 84);
	// lwz r10,412(r23)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r23.u32 + 412);
	// lwz r11,76(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 76);
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// subf r30,r11,r10
	ctx.r30.s64 = ctx.r10.s64 - ctx.r11.s64;
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825ea2ec
	if (!ctx.cr6.eq) goto loc_825EA2EC;
	// lwz r11,0(r20)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	// li r30,0
	ctx.r30.s64 = 0;
	// rlwimi r11,r30,4,16,27
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r30.u32, 4) & 0xFFF0) | (ctx.r11.u64 & 0xFFFFFFFFFFFF000F);
	// stw r11,0(r20)
	PPC_STORE_U32(ctx.r20.u32 + 0, ctx.r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825EA2EC:
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// ble cr6,0x825ea34c
	if (!ctx.cr6.gt) goto loc_825EA34C;
loc_825EA2F4:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ea34c
	if (ctx.cr6.eq) goto loc_825EA34C;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825ea33c
	if (!ctx.cr0.lt) goto loc_825EA33C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EA33C;
	sub_825D5398(ctx, base);
loc_825EA33C:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825ea2f4
	if (ctx.cr6.gt) goto loc_825EA2F4;
loc_825EA34C:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ea388
	if (!ctx.cr0.lt) goto loc_825EA388;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EA388;
	sub_825D5398(ctx, base);
loc_825EA388:
	// lwz r11,0(r20)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r20.u32 + 0);
	// rlwimi r11,r30,4,16,27
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r30.u32, 4) & 0xFFF0) | (ctx.r11.u64 & 0xFFFFFFFFFFFF000F);
	// stw r11,0(r20)
	PPC_STORE_U32(ctx.r20.u32 + 0, ctx.r11.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825EA39C:
	// rlwinm r11,r11,0,28,15
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFFFFFFF000F;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r11,0(r20)
	PPC_STORE_U32(ctx.r20.u32 + 0, ctx.r11.u32);
	// sth r10,0(r20)
	PPC_STORE_U16(ctx.r20.u32 + 0, ctx.r10.u16);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
}

__attribute__((alias("__imp__sub_825EA3B4"))) PPC_WEAK_FUNC(sub_825EA3B4);
PPC_FUNC_IMPL(__imp__sub_825EA3B4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EA3B8"))) PPC_WEAK_FUNC(sub_825EA3B8);
PPC_FUNC_IMPL(__imp__sub_825EA3B8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9f4
	ctx.lr = 0x825EA3C0;
	sub_8239B9F4(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r3
	ctx.r26.u64 = ctx.r3.u64;
	// li r25,0
	ctx.r25.s64 = 0;
	// mr r21,r25
	ctx.r21.u64 = ctx.r25.u64;
	// lwz r11,140(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 140);
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lwz r10,136(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// lwz r22,268(r26)
	ctx.r22.u64 = PPC_LOAD_U32(ctx.r26.u32 + 268);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// lwz r28,21636(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21636);
	// rlwinm r20,r11,31,1,31
	ctx.r20.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// rlwinm r24,r10,31,1,31
	ctx.r24.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// mullw r23,r20,r24
	ctx.r23.s64 = int64_t(ctx.r20.s32) * int64_t(ctx.r24.s32);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// rotlwi r19,r8,0
	ctx.r19.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// bge 0x825ea420
	if (!ctx.cr0.lt) goto loc_825EA420;
	// bl 0x825d5398
	ctx.lr = 0x825EA420;
	sub_825D5398(ctx, base);
loc_825EA420:
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// li r30,2
	ctx.r30.s64 = 2;
	// mr r29,r25
	ctx.r29.u64 = ctx.r25.u64;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bge cr6,0x825ea494
	if (!ctx.cr6.lt) goto loc_825EA494;
loc_825EA43C:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ea494
	if (ctx.cr6.eq) goto loc_825EA494;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r30,r11,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r30
	ctx.r11.u64 = ctx.r30.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r30.u8 & 0x3F));
	// add r29,r11,r29
	ctx.r29.u64 = ctx.r11.u64 + ctx.r29.u64;
	// bge 0x825ea484
	if (!ctx.cr0.lt) goto loc_825EA484;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EA484;
	sub_825D5398(ctx, base);
loc_825EA484:
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825ea43c
	if (ctx.cr6.gt) goto loc_825EA43C;
loc_825EA494:
	// subfic r9,r30,64
	ctx.xer.ca = ctx.r30.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r30.s64;
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r8,r30,32
	ctx.r8.u64 = ctx.r30.u64 & 0xFFFFFFFF;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r30,r11,r29
	ctx.r30.u64 = ctx.r11.u64 + ctx.r29.u64;
	// std r8,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r8.u64);
	// bge 0x825ea4d0
	if (!ctx.cr0.lt) goto loc_825EA4D0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EA4D0;
	sub_825D5398(ctx, base);
loc_825EA4D0:
	// cmplwi cr6,r30,1
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 1, ctx.xer);
	// beq cr6,0x825ea804
	if (ctx.cr6.eq) goto loc_825EA804;
	// cmplwi cr6,r30,2
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 2, ctx.xer);
	// beq cr6,0x825ea6c4
	if (ctx.cr6.eq) goto loc_825EA6C4;
	// cmplwi cr6,r30,3
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 3, ctx.xer);
	// beq cr6,0x825ea6a8
	if (ctx.cr6.eq) goto loc_825EA6A8;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea514
	if (!ctx.cr0.lt) goto loc_825EA514;
	// bl 0x825d5398
	ctx.lr = 0x825EA514;
	sub_825D5398(ctx, base);
loc_825EA514:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825ea660
	if (ctx.cr6.eq) goto loc_825EA660;
	// clrlwi r31,r23,31
	ctx.r31.u64 = ctx.r23.u32 & 0x1;
	// li r29,1
	ctx.r29.s64 = 1;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// mr r21,r29
	ctx.r21.u64 = ctx.r29.u64;
	// beq cr6,0x825ea564
	if (ctx.cr6.eq) goto loc_825EA564;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea55c
	if (!ctx.cr0.lt) goto loc_825EA55C;
	// bl 0x825d5398
	ctx.lr = 0x825EA55C;
	sub_825D5398(ctx, base);
loc_825EA55C:
	// stb r30,0(r28)
	PPC_STORE_U8(ctx.r28.u32 + 0, ctx.r30.u8);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
loc_825EA564:
	// cmpw cr6,r31,r23
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r23.s32, ctx.xer);
	// bge cr6,0x825ea9c8
	if (!ctx.cr6.lt) goto loc_825EA9C8;
	// subf r11,r31,r23
	ctx.r11.s64 = ctx.r23.s64 - ctx.r31.s64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	ctx.r30.s64 = ctx.r11.s64 + 1;
loc_825EA57C:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea5a8
	if (!ctx.cr0.lt) goto loc_825EA5A8;
	// bl 0x825d5398
	ctx.lr = 0x825EA5A8;
	sub_825D5398(ctx, base);
loc_825EA5A8:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825ea640
	if (ctx.cr6.eq) goto loc_825EA640;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea5dc
	if (!ctx.cr0.lt) goto loc_825EA5DC;
	// bl 0x825d5398
	ctx.lr = 0x825EA5DC;
	sub_825D5398(ctx, base);
loc_825EA5DC:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825ea5f4
	if (ctx.cr6.eq) goto loc_825EA5F4;
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// stb r29,0(r28)
	PPC_STORE_U8(ctx.r28.u32 + 0, ctx.r29.u8);
	// stb r29,0(r11)
	PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r29.u8);
	// b 0x825ea64c
	goto loc_825EA64C;
loc_825EA5F4:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea620
	if (!ctx.cr0.lt) goto loc_825EA620;
	// bl 0x825d5398
	ctx.lr = 0x825EA620;
	sub_825D5398(ctx, base);
loc_825EA620:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// beq cr6,0x825ea638
	if (ctx.cr6.eq) goto loc_825EA638;
	// stb r25,0(r28)
	PPC_STORE_U8(ctx.r28.u32 + 0, ctx.r25.u8);
	// stb r29,0(r11)
	PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r29.u8);
	// b 0x825ea64c
	goto loc_825EA64C;
loc_825EA638:
	// stb r29,0(r28)
	PPC_STORE_U8(ctx.r28.u32 + 0, ctx.r29.u8);
	// b 0x825ea648
	goto loc_825EA648;
loc_825EA640:
	// stb r25,0(r28)
	PPC_STORE_U8(ctx.r28.u32 + 0, ctx.r25.u8);
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
loc_825EA648:
	// stb r25,0(r11)
	PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r25.u8);
loc_825EA64C:
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r28,r11,1
	ctx.r28.s64 = ctx.r11.s64 + 1;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825ea57c
	if (!ctx.cr6.eq) goto loc_825EA57C;
	// b 0x825ea9c8
	goto loc_825EA9C8;
loc_825EA660:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea68c
	if (!ctx.cr0.lt) goto loc_825EA68C;
	// bl 0x825d5398
	ctx.lr = 0x825EA68C;
	sub_825D5398(ctx, base);
loc_825EA68C:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825ea9c8
	if (ctx.cr6.eq) goto loc_825EA9C8;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// li r21,1
	ctx.r21.s64 = 1;
	// bl 0x825e9350
	ctx.lr = 0x825EA6A4;
	sub_825E9350(ctx, base);
	// b 0x825ea9c8
	goto loc_825EA9C8;
loc_825EA6A8:
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x825e9350
	ctx.lr = 0x825EA6B4;
	sub_825E9350(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825ea9c8
	if (ctx.cr6.eq) goto loc_825EA9C8;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba44
	// ERROR 8239BA44
	return;
loc_825EA6C4:
	// clrlwi r30,r23,31
	ctx.r30.u64 = ctx.r23.u32 & 0x1;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825ea704
	if (ctx.cr6.eq) goto loc_825EA704;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea6fc
	if (!ctx.cr0.lt) goto loc_825EA6FC;
	// bl 0x825d5398
	ctx.lr = 0x825EA6FC;
	sub_825D5398(ctx, base);
loc_825EA6FC:
	// stb r31,0(r28)
	PPC_STORE_U8(ctx.r28.u32 + 0, ctx.r31.u8);
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
loc_825EA704:
	// cmpw cr6,r30,r23
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r23.s32, ctx.xer);
	// bge cr6,0x825ea9c8
	if (!ctx.cr6.lt) goto loc_825EA9C8;
	// subf r11,r30,r23
	ctx.r11.s64 = ctx.r23.s64 - ctx.r30.s64;
	// li r29,1
	ctx.r29.s64 = 1;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r30,r11,1
	ctx.r30.s64 = ctx.r11.s64 + 1;
loc_825EA720:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea74c
	if (!ctx.cr0.lt) goto loc_825EA74C;
	// bl 0x825d5398
	ctx.lr = 0x825EA74C;
	sub_825D5398(ctx, base);
loc_825EA74C:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825ea7e4
	if (ctx.cr6.eq) goto loc_825EA7E4;
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea780
	if (!ctx.cr0.lt) goto loc_825EA780;
	// bl 0x825d5398
	ctx.lr = 0x825EA780;
	sub_825D5398(ctx, base);
loc_825EA780:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825ea798
	if (ctx.cr6.eq) goto loc_825EA798;
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// stb r29,0(r28)
	PPC_STORE_U8(ctx.r28.u32 + 0, ctx.r29.u8);
	// stb r29,0(r11)
	PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r29.u8);
	// b 0x825ea7f0
	goto loc_825EA7F0;
loc_825EA798:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea7c4
	if (!ctx.cr0.lt) goto loc_825EA7C4;
	// bl 0x825d5398
	ctx.lr = 0x825EA7C4;
	sub_825D5398(ctx, base);
loc_825EA7C4:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
	// beq cr6,0x825ea7dc
	if (ctx.cr6.eq) goto loc_825EA7DC;
	// stb r25,0(r28)
	PPC_STORE_U8(ctx.r28.u32 + 0, ctx.r25.u8);
	// stb r29,0(r11)
	PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r29.u8);
	// b 0x825ea7f0
	goto loc_825EA7F0;
loc_825EA7DC:
	// stb r29,0(r28)
	PPC_STORE_U8(ctx.r28.u32 + 0, ctx.r29.u8);
	// b 0x825ea7ec
	goto loc_825EA7EC;
loc_825EA7E4:
	// stb r25,0(r28)
	PPC_STORE_U8(ctx.r28.u32 + 0, ctx.r25.u8);
	// addi r11,r28,1
	ctx.r11.s64 = ctx.r28.s64 + 1;
loc_825EA7EC:
	// stb r25,0(r11)
	PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r25.u8);
loc_825EA7F0:
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r28,r11,1
	ctx.r28.s64 = ctx.r11.s64 + 1;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825ea720
	if (!ctx.cr6.eq) goto loc_825EA720;
	// b 0x825ea9c8
	goto loc_825EA9C8;
loc_825EA804:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea830
	if (!ctx.cr0.lt) goto loc_825EA830;
	// bl 0x825d5398
	ctx.lr = 0x825EA830;
	sub_825D5398(ctx, base);
loc_825EA830:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825ea904
	if (ctx.cr6.eq) goto loc_825EA904;
	// cmpwi cr6,r24,0
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// ble cr6,0x825ea9c8
	if (!ctx.cr6.gt) goto loc_825EA9C8;
	// mr r27,r24
	ctx.r27.u64 = ctx.r24.u64;
loc_825EA844:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea870
	if (!ctx.cr0.lt) goto loc_825EA870;
	// bl 0x825d5398
	ctx.lr = 0x825EA870;
	sub_825D5398(ctx, base);
loc_825EA870:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825ea8cc
	if (ctx.cr6.eq) goto loc_825EA8CC;
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x825ea8f0
	if (!ctx.cr6.gt) goto loc_825EA8F0;
	// mr r30,r28
	ctx.r30.u64 = ctx.r28.u64;
	// mr r31,r20
	ctx.r31.u64 = ctx.r20.u64;
loc_825EA888:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r29,r8,0
	ctx.r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea8b4
	if (!ctx.cr0.lt) goto loc_825EA8B4;
	// bl 0x825d5398
	ctx.lr = 0x825EA8B4;
	sub_825D5398(ctx, base);
loc_825EA8B4:
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// stb r29,0(r30)
	PPC_STORE_U8(ctx.r30.u32 + 0, ctx.r29.u8);
	// add r30,r30,r24
	ctx.r30.u64 = ctx.r30.u64 + ctx.r24.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825ea888
	if (!ctx.cr6.eq) goto loc_825EA888;
	// b 0x825ea8f0
	goto loc_825EA8F0;
loc_825EA8CC:
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x825ea8f0
	if (!ctx.cr6.gt) goto loc_825EA8F0;
	// mr r10,r28
	ctx.r10.u64 = ctx.r28.u64;
	// mr r11,r20
	ctx.r11.u64 = ctx.r20.u64;
loc_825EA8DC:
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// stb r25,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r25.u8);
	// add r10,r10,r24
	ctx.r10.u64 = ctx.r10.u64 + ctx.r24.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825ea8dc
	if (!ctx.cr6.eq) goto loc_825EA8DC;
loc_825EA8F0:
	// addi r27,r27,-1
	ctx.r27.s64 = ctx.r27.s64 + -1;
	// addi r28,r28,1
	ctx.r28.s64 = ctx.r28.s64 + 1;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// bne cr6,0x825ea844
	if (!ctx.cr6.eq) goto loc_825EA844;
	// b 0x825ea9c8
	goto loc_825EA9C8;
loc_825EA904:
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x825ea9c8
	if (!ctx.cr6.gt) goto loc_825EA9C8;
	// mr r29,r20
	ctx.r29.u64 = ctx.r20.u64;
loc_825EA910:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r31,r8,0
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea93c
	if (!ctx.cr0.lt) goto loc_825EA93C;
	// bl 0x825d5398
	ctx.lr = 0x825EA93C;
	sub_825D5398(ctx, base);
loc_825EA93C:
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825ea990
	if (ctx.cr6.eq) goto loc_825EA990;
	// mr r31,r25
	ctx.r31.u64 = ctx.r25.u64;
	// cmpwi cr6,r24,0
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// ble cr6,0x825ea9b8
	if (!ctx.cr6.gt) goto loc_825EA9B8;
loc_825EA950:
	// lwz r3,84(r26)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825ea97c
	if (!ctx.cr0.lt) goto loc_825EA97C;
	// bl 0x825d5398
	ctx.lr = 0x825EA97C;
	sub_825D5398(ctx, base);
loc_825EA97C:
	// stbx r30,r28,r31
	PPC_STORE_U8(ctx.r28.u32 + ctx.r31.u32, ctx.r30.u8);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// cmpw cr6,r31,r24
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r24.s32, ctx.xer);
	// blt cr6,0x825ea950
	if (ctx.cr6.lt) goto loc_825EA950;
	// b 0x825ea9b8
	goto loc_825EA9B8;
loc_825EA990:
	// cmpwi cr6,r24,0
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// ble cr6,0x825ea9b8
	if (!ctx.cr6.gt) goto loc_825EA9B8;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// beq cr6,0x825ea9b8
	if (ctx.cr6.eq) goto loc_825EA9B8;
	// mtctr r24
	ctx.ctr.u64 = ctx.r24.u64;
loc_825EA9AC:
	// stb r10,0(r11)
	PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r10.u8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// bdnz 0x825ea9ac
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_825EA9AC;
loc_825EA9B8:
	// addi r29,r29,-1
	ctx.r29.s64 = ctx.r29.s64 + -1;
	// add r28,r28,r24
	ctx.r28.u64 = ctx.r28.u64 + ctx.r24.u64;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825ea910
	if (!ctx.cr6.eq) goto loc_825EA910;
loc_825EA9C8:
	// lwz r10,21636(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21636);
	// cmpwi cr6,r21,0
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 0, ctx.xer);
	// beq cr6,0x825eaa70
	if (ctx.cr6.eq) goto loc_825EAA70;
	// mr r7,r25
	ctx.r7.u64 = ctx.r25.u64;
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x825eaaa0
	if (!ctx.cr6.gt) goto loc_825EAAA0;
loc_825EA9E0:
	// mr r9,r25
	ctx.r9.u64 = ctx.r25.u64;
	// cmpwi cr6,r24,0
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// ble cr6,0x825eaa60
	if (!ctx.cr6.gt) goto loc_825EAA60;
	// subfic r8,r24,1
	ctx.xer.ca = ctx.r24.u32 <= 1;
	ctx.r8.s64 = 1 - ctx.r24.s64;
loc_825EA9F0:
	// add. r11,r9,r7
	ctx.r11.u64 = ctx.r9.u64 + ctx.r7.u64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x825eaa40
	if (ctx.cr0.eq) goto loc_825EAA40;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// bne cr6,0x825eaa0c
	if (!ctx.cr6.eq) goto loc_825EAA0C;
	// lbz r11,-1(r10)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// extsb r11,r11
	ctx.r11.s64 = ctx.r11.s8;
	// b 0x825eaa44
	goto loc_825EAA44;
loc_825EAA0C:
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x825eaa24
	if (!ctx.cr6.eq) goto loc_825EAA24;
	// add r11,r8,r10
	ctx.r11.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lbz r11,-1(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + -1);
	// extsb r11,r11
	ctx.r11.s64 = ctx.r11.s8;
	// b 0x825eaa44
	goto loc_825EAA44;
loc_825EAA24:
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// lbz r11,-1(r10)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r10.u32 + -1);
	// extsb r11,r11
	ctx.r11.s64 = ctx.r11.s8;
	// lbz r6,-1(r6)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r6.u32 + -1);
	// extsb r6,r6
	ctx.r6.s64 = ctx.r6.s8;
	// cmpw cr6,r11,r6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r6.s32, ctx.xer);
	// beq cr6,0x825eaa44
	if (ctx.cr6.eq) goto loc_825EAA44;
loc_825EAA40:
	// mr r11,r19
	ctx.r11.u64 = ctx.r19.u64;
loc_825EAA44:
	// lbz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// xor r11,r6,r11
	ctx.r11.u64 = ctx.r6.u64 ^ ctx.r11.u64;
	// cmpw cr6,r9,r24
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r24.s32, ctx.xer);
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r11.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// blt cr6,0x825ea9f0
	if (ctx.cr6.lt) goto loc_825EA9F0;
loc_825EAA60:
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// cmpw cr6,r7,r20
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r20.s32, ctx.xer);
	// blt cr6,0x825ea9e0
	if (ctx.cr6.lt) goto loc_825EA9E0;
	// b 0x825eaaa0
	goto loc_825EAAA0;
loc_825EAA70:
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// beq cr6,0x825eaaa0
	if (ctx.cr6.eq) goto loc_825EAAA0;
	// mr r11,r25
	ctx.r11.u64 = ctx.r25.u64;
	// cmpwi cr6,r23,0
	ctx.cr6.compare<int32_t>(ctx.r23.s32, 0, ctx.xer);
	// ble cr6,0x825eaaa0
	if (!ctx.cr6.gt) goto loc_825EAAA0;
loc_825EAA84:
	// add r9,r11,r10
	ctx.r9.u64 = ctx.r11.u64 + ctx.r10.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpw cr6,r11,r23
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r23.s32, ctx.xer);
	// lbz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// xori r8,r8,1
	ctx.r8.u64 = ctx.r8.u64 ^ 1;
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// blt cr6,0x825eaa84
	if (ctx.cr6.lt) goto loc_825EAA84;
loc_825EAAA0:
	// lwz r10,136(r26)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// lwz r11,140(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 140);
	// clrlwi r8,r10,31
	ctx.r8.u64 = ctx.r10.u32 & 0x1;
	// lwz r20,21636(r26)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21636);
	// clrlwi r9,r11,31
	ctx.r9.u64 = ctx.r11.u32 & 0x1;
	// subf r21,r8,r10
	ctx.r21.s64 = ctx.r10.s64 - ctx.r8.s64;
	// subf r11,r9,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r9.s64;
	// lis r10,0
	ctx.r10.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ori r24,r10,32768
	ctx.r24.u64 = ctx.r10.u64 | 32768;
	// ble cr6,0x825eaed8
	if (!ctx.cr6.gt) goto loc_825EAED8;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r23,r11,1
	ctx.r23.s64 = ctx.r11.s64 + 1;
loc_825EAAD8:
	// mr r27,r25
	ctx.r27.u64 = ctx.r25.u64;
	// cmpwi cr6,r21,0
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 0, ctx.xer);
	// ble cr6,0x825eacf8
	if (!ctx.cr6.gt) goto loc_825EACF8;
	// mr r29,r22
	ctx.r29.u64 = ctx.r22.u64;
loc_825EAAE8:
	// lbz r11,0(r20)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r20.u32 + 0);
	// addi r20,r20,1
	ctx.r20.s64 = ctx.r20.s64 + 1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825eac8c
	if (!ctx.cr6.eq) goto loc_825EAC8C;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lbz r4,21648(r26)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r26.u32 + 21648);
	// lwz r28,21640(r26)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21640);
	// subfic r11,r4,64
	ctx.xer.ca = ctx.r4.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r28.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825eabe8
	if (ctx.cr6.lt) goto loc_825EABE8;
	// clrlwi r11,r30,28
	ctx.r11.u64 = ctx.r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// sld r10,r10,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825eabe0
	if (!ctx.cr6.lt) goto loc_825EABE0;
loc_825EAB48:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825eab74
	if (ctx.cr6.lt) goto loc_825EAB74;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825EAB64;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825eab48
	if (ctx.cr6.eq) goto loc_825EAB48;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825eac24
	goto loc_825EAC24;
loc_825EAB74:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = ctx.r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// rldicr r11,r9,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
loc_825EABE0:
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825eac24
	goto loc_825EAC24;
loc_825EABE8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5468
	ctx.lr = 0x825EABF0;
	sub_825D5468(ctx, base);
loc_825EABF0:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// add r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825d5468
	ctx.lr = 0x825EAC0C;
	sub_825D5468(ctx, base);
	// add r11,r30,r24
	ctx.r11.u64 = ctx.r30.u64 + ctx.r24.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r28
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r28.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825eabf0
	if (ctx.cr6.lt) goto loc_825EABF0;
loc_825EAC24:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// srawi r10,r30,1
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r30.s32 >> 1;
	// srawi r9,r30,2
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r30.s32 >> 2;
	// rlwimi r11,r30,31,0,0
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r30.u32, 31) & 0x80000000) | (ctx.r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// srawi r8,r30,3
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r30.s32 >> 3;
	// stw r11,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r11.u32);
	// lwz r11,20(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 20);
	// rlwimi r11,r10,31,0,0
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 31) & 0x80000000) | (ctx.r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,20(r29)
	PPC_STORE_U32(ctx.r29.u32 + 20, ctx.r11.u32);
	// lwz r11,136(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// rlwimi r10,r9,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r22
	PPC_STORE_U32(ctx.r11.u32 + ctx.r22.u32, ctx.r10.u32);
	// lwz r11,136(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// rlwimi r10,r8,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r8.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// b 0x825eace4
	goto loc_825EACE4;
loc_825EAC8C:
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// oris r11,r11,32768
	ctx.r11.u64 = ctx.r11.u64 | 2147483648;
	// stw r11,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r11.u32);
	// lwz r11,20(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 20);
	// oris r11,r11,32768
	ctx.r11.u64 = ctx.r11.u64 | 2147483648;
	// stw r11,20(r29)
	PPC_STORE_U32(ctx.r29.u32 + 20, ctx.r11.u32);
	// lwz r11,136(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stwx r10,r11,r22
	PPC_STORE_U32(ctx.r11.u32 + ctx.r22.u32, ctx.r10.u32);
	// lwz r11,136(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
loc_825EACE4:
	// addi r27,r27,2
	ctx.r27.s64 = ctx.r27.s64 + 2;
	// stwx r10,r11,r22
	PPC_STORE_U32(ctx.r11.u32 + ctx.r22.u32, ctx.r10.u32);
	// addi r29,r29,40
	ctx.r29.s64 = ctx.r29.s64 + 40;
	// cmpw cr6,r27,r21
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r21.s32, ctx.xer);
	// blt cr6,0x825eaae8
	if (ctx.cr6.lt) goto loc_825EAAE8;
loc_825EACF8:
	// lwz r11,136(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// clrlwi r11,r11,31
	ctx.r11.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825eaeb8
	if (ctx.cr6.eq) goto loc_825EAEB8;
	// lbz r11,0(r20)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r20.u32 + 0);
	// addi r20,r20,1
	ctx.r20.s64 = ctx.r20.s64 + 1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825eae80
	if (!ctx.cr6.eq) goto loc_825EAE80;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lbz r4,21648(r26)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r26.u32 + 21648);
	// lwz r29,21640(r26)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21640);
	// subfic r11,r4,64
	ctx.xer.ca = ctx.r4.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825eae08
	if (ctx.cr6.lt) goto loc_825EAE08;
	// clrlwi r11,r30,28
	ctx.r11.u64 = ctx.r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// sld r10,r10,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825eae00
	if (!ctx.cr6.lt) goto loc_825EAE00;
loc_825EAD68:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825ead94
	if (ctx.cr6.lt) goto loc_825EAD94;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825EAD84;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825ead68
	if (ctx.cr6.eq) goto loc_825EAD68;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825eae44
	goto loc_825EAE44;
loc_825EAD94:
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = ctx.r11.s64 + 6;
	// lbz r9,1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r8,r8,8,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// rldicr r11,r9,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
loc_825EAE00:
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825eae44
	goto loc_825EAE44;
loc_825EAE08:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5468
	ctx.lr = 0x825EAE10;
	sub_825D5468(ctx, base);
loc_825EAE10:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// add r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825d5468
	ctx.lr = 0x825EAE2C;
	sub_825D5468(ctx, base);
	// add r11,r30,r24
	ctx.r11.u64 = ctx.r30.u64 + ctx.r24.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825eae10
	if (ctx.cr6.lt) goto loc_825EAE10;
loc_825EAE44:
	// rlwinm r11,r27,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r9,r30,2
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r30.s32 >> 2;
	// add r11,r27,r11
	ctx.r11.u64 = ctx.r27.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// rlwimi r10,r30,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r30.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r22
	PPC_STORE_U32(ctx.r11.u32 + ctx.r22.u32, ctx.r10.u32);
	// lwz r11,136(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// rlwimi r10,r9,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// b 0x825eaeb4
	goto loc_825EAEB4;
loc_825EAE80:
	// rlwinm r11,r27,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r27,r11
	ctx.r11.u64 = ctx.r27.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stwx r10,r11,r22
	PPC_STORE_U32(ctx.r11.u32 + ctx.r22.u32, ctx.r10.u32);
	// lwz r11,136(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
loc_825EAEB4:
	// stwx r10,r11,r22
	PPC_STORE_U32(ctx.r11.u32 + ctx.r22.u32, ctx.r10.u32);
loc_825EAEB8:
	// lwz r11,136(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// addi r23,r23,-1
	ctx.r23.s64 = ctx.r23.s64 + -1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmplwi cr6,r23,0
	ctx.cr6.compare<uint32_t>(ctx.r23.u32, 0, ctx.xer);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r22,r11,r22
	ctx.r22.u64 = ctx.r11.u64 + ctx.r22.u64;
	// bne cr6,0x825eaad8
	if (!ctx.cr6.eq) goto loc_825EAAD8;
loc_825EAED8:
	// lwz r11,140(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 140);
	// clrlwi r11,r11,31
	ctx.r11.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825eb208
	if (ctx.cr6.eq) goto loc_825EB208;
	// cmpwi cr6,r21,0
	ctx.cr6.compare<int32_t>(ctx.r21.s32, 0, ctx.xer);
	// ble cr6,0x825eb084
	if (!ctx.cr6.gt) goto loc_825EB084;
	// addi r11,r21,-1
	ctx.r11.s64 = ctx.r21.s64 + -1;
	// mr r28,r22
	ctx.r28.u64 = ctx.r22.u64;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r27,r11,1
	ctx.r27.s64 = ctx.r11.s64 + 1;
	// rlwinm r25,r27,1,0,30
	ctx.r25.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 1) & 0xFFFFFFFE;
loc_825EAF04:
	// lbz r11,0(r20)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r20.u32 + 0);
	// addi r20,r20,1
	ctx.r20.s64 = ctx.r20.s64 + 1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825eb05c
	if (!ctx.cr6.eq) goto loc_825EB05C;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lbz r4,21648(r26)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r26.u32 + 21648);
	// lwz r29,21640(r26)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21640);
	// subfic r11,r4,64
	ctx.xer.ca = ctx.r4.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825eb004
	if (ctx.cr6.lt) goto loc_825EB004;
	// clrlwi r11,r30,28
	ctx.r11.u64 = ctx.r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// sld r10,r10,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825eaffc
	if (!ctx.cr6.lt) goto loc_825EAFFC;
loc_825EAF64:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825eaf90
	if (ctx.cr6.lt) goto loc_825EAF90;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825EAF80;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825eaf64
	if (ctx.cr6.eq) goto loc_825EAF64;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825eb040
	goto loc_825EB040;
loc_825EAF90:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = ctx.r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// rldicr r11,r9,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
loc_825EAFFC:
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825eb040
	goto loc_825EB040;
loc_825EB004:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5468
	ctx.lr = 0x825EB00C;
	sub_825D5468(ctx, base);
loc_825EB00C:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// add r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825d5468
	ctx.lr = 0x825EB028;
	sub_825D5468(ctx, base);
	// add r11,r30,r24
	ctx.r11.u64 = ctx.r30.u64 + ctx.r24.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825eb00c
	if (ctx.cr6.lt) goto loc_825EB00C;
loc_825EB040:
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// srawi r10,r30,1
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r30.s32 >> 1;
	// rlwimi r11,r30,31,0,0
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r30.u32, 31) & 0x80000000) | (ctx.r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// stw r11,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r11.u32);
	// lwz r11,20(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 20);
	// rlwimi r11,r10,31,0,0
	ctx.r11.u64 = (__builtin_rotateleft32(ctx.r10.u32, 31) & 0x80000000) | (ctx.r11.u64 & 0xFFFFFFFF7FFFFFFF);
	// b 0x825eb070
	goto loc_825EB070;
loc_825EB05C:
	// lwz r11,0(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// oris r11,r11,32768
	ctx.r11.u64 = ctx.r11.u64 | 2147483648;
	// stw r11,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r11.u32);
	// lwz r11,20(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 20);
	// oris r11,r11,32768
	ctx.r11.u64 = ctx.r11.u64 | 2147483648;
loc_825EB070:
	// addi r27,r27,-1
	ctx.r27.s64 = ctx.r27.s64 + -1;
	// stw r11,20(r28)
	PPC_STORE_U32(ctx.r28.u32 + 20, ctx.r11.u32);
	// addi r28,r28,40
	ctx.r28.s64 = ctx.r28.s64 + 40;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// bne cr6,0x825eaf04
	if (!ctx.cr6.eq) goto loc_825EAF04;
loc_825EB084:
	// lwz r11,136(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 136);
	// clrlwi r11,r11,31
	ctx.r11.u64 = ctx.r11.u32 & 0x1;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825eb208
	if (ctx.cr6.eq) goto loc_825EB208;
	// lbz r11,0(r20)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r20.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825eb1f0
	if (!ctx.cr6.eq) goto loc_825EB1F0;
	// lwz r31,84(r26)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r26.u32 + 84);
	// lbz r4,21648(r26)
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r26.u32 + 21648);
	// lwz r29,21640(r26)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r26.u32 + 21640);
	// subfic r11,r4,64
	ctx.xer.ca = ctx.r4.u32 <= 64;
	ctx.r11.s64 = 64 - ctx.r4.s64;
	// ld r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// srd r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r11.u8 & 0x7F));
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825eb190
	if (ctx.cr6.lt) goto loc_825EB190;
	// clrlwi r11,r30,28
	ctx.r11.u64 = ctx.r30.u32 & 0xF;
	// lwz r9,8(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// cmpw cr6,r9,r11
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r11.s32, ctx.xer);
	// sld r10,r10,r11
	ctx.r10.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// std r10,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r10.u64);
	// subf r10,r11,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r11.s64;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// bge cr6,0x825eb188
	if (!ctx.cr6.lt) goto loc_825EB188;
loc_825EB0F0:
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// addi r10,r10,-4
	ctx.r10.s64 = ctx.r10.s64 + -4;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825eb11c
	if (ctx.cr6.lt) goto loc_825EB11C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d52d8
	ctx.lr = 0x825EB10C;
	sub_825D52D8(ctx, base);
	// cmplwi cr6,r3,1
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 1, ctx.xer);
	// beq cr6,0x825eb0f0
	if (ctx.cr6.eq) goto loc_825EB0F0;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825eb1cc
	goto loc_825EB1CC;
loc_825EB11C:
	// lbz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r4,r11,6
	ctx.r4.s64 = ctx.r11.s64 + 6;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// rldicr r9,r9,8,63
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFFFF;
	// lbz r5,2(r11)
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// lbz r6,3(r11)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// lbz r8,5(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 5);
	// lbz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 4);
	// rldicr r11,r9,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// lwz r10,8(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// ld r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// add r11,r11,r5
	ctx.r11.u64 = ctx.r11.u64 + ctx.r5.u64;
	// stw r4,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r4.u32);
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// rldicr r11,r11,8,55
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 8) & 0xFFFFFFFFFFFFFF00;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// neg r8,r10
	ctx.r8.s64 = -ctx.r10.s64;
	// addi r10,r10,48
	ctx.r10.s64 = ctx.r10.s64 + 48;
	// extsw r8,r8
	ctx.r8.s64 = ctx.r8.s32;
	// stw r10,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r10.u32);
	// sld r11,r11,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// std r11,0(r31)
	PPC_STORE_U64(ctx.r31.u32 + 0, ctx.r11.u64);
loc_825EB188:
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// b 0x825eb1cc
	goto loc_825EB1CC;
loc_825EB190:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d5468
	ctx.lr = 0x825EB198;
	sub_825D5468(ctx, base);
loc_825EB198:
	// ld r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r31.u32 + 0);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// rldicl r11,r11,1,63
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// add r30,r11,r30
	ctx.r30.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x825d5468
	ctx.lr = 0x825EB1B4;
	sub_825D5468(ctx, base);
	// add r11,r30,r24
	ctx.r11.u64 = ctx.r30.u64 + ctx.r24.u64;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// lhzx r11,r11,r29
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + ctx.r29.u32);
	// extsh r30,r11
	ctx.r30.s64 = ctx.r11.s16;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// blt cr6,0x825eb198
	if (ctx.cr6.lt) goto loc_825EB198;
loc_825EB1CC:
	// rlwinm r11,r25,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// li r3,0
	ctx.r3.s64 = 0;
	// add r11,r25,r11
	ctx.r11.u64 = ctx.r25.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// rlwimi r10,r30,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r30.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r22
	PPC_STORE_U32(ctx.r11.u32 + ctx.r22.u32, ctx.r10.u32);
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba44
	// ERROR 8239BA44
	return;
loc_825EB1F0:
	// rlwinm r11,r25,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r25,r11
	ctx.r11.u64 = ctx.r25.u64 + ctx.r11.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r22
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r22.u32);
	// oris r10,r10,32768
	ctx.r10.u64 = ctx.r10.u64 | 2147483648;
	// stwx r10,r11,r22
	PPC_STORE_U32(ctx.r11.u32 + ctx.r22.u32, ctx.r10.u32);
loc_825EB208:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba44
	// ERROR 8239BA44
	return;
}

__attribute__((alias("__imp__sub_825EB214"))) PPC_WEAK_FUNC(sub_825EB214);
PPC_FUNC_IMPL(__imp__sub_825EB214) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EB218"))) PPC_WEAK_FUNC(sub_825EB218);
PPC_FUNC_IMPL(__imp__sub_825EB218) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825EB220;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// lwz r11,21700(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21700);
	// lwz r29,268(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825eb248
	if (!ctx.cr6.eq) goto loc_825EB248;
	// bl 0x825d75b8
	ctx.lr = 0x825EB240;
	sub_825D75B8(ctx, base);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825EB248:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r25,r8,0
	ctx.r25.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825eb274
	if (!ctx.cr0.lt) goto loc_825EB274;
	// bl 0x825d5398
	ctx.lr = 0x825EB274;
	sub_825D5398(ctx, base);
loc_825EB274:
	// lwz r30,84(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// li r28,3
	ctx.r28.s64 = 3;
	// li r27,0
	ctx.r27.s64 = 0;
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplwi cr6,r11,3
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 3, ctx.xer);
	// bge cr6,0x825eb2e8
	if (!ctx.cr6.lt) goto loc_825EB2E8;
loc_825EB290:
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825eb2e8
	if (ctx.cr6.eq) goto loc_825EB2E8;
	// subfic r8,r11,64
	ctx.xer.ca = ctx.r11.u32 <= 64;
	ctx.r8.s64 = 64 - ctx.r11.s64;
	// ld r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// subf. r9,r11,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// clrldi r8,r8,32
	ctx.r8.u64 = ctx.r8.u64 & 0xFFFFFFFF;
	// subf r28,r11,r28
	ctx.r28.s64 = ctx.r28.s64 - ctx.r11.s64;
	// clrldi r11,r11,32
	ctx.r11.u64 = ctx.r11.u64 & 0xFFFFFFFF;
	// stw r9,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r9.u32);
	// srd r8,r10,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r10.u64 >> (ctx.r8.u8 & 0x7F));
	// sld r11,r10,r11
	ctx.r11.u64 = ctx.r11.u8 & 0x40 ? 0 : (ctx.r10.u64 << (ctx.r11.u8 & 0x7F));
	// rotlwi r10,r8,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r11,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r11.u64);
	// slw r11,r10,r28
	ctx.r11.u64 = ctx.r28.u8 & 0x20 ? 0 : (ctx.r10.u32 << (ctx.r28.u8 & 0x3F));
	// add r27,r11,r27
	ctx.r27.u64 = ctx.r11.u64 + ctx.r27.u64;
	// bge 0x825eb2d8
	if (!ctx.cr0.lt) goto loc_825EB2D8;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EB2D8;
	sub_825D5398(ctx, base);
loc_825EB2D8:
	// lwz r9,8(r30)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// addi r11,r9,16
	ctx.r11.s64 = ctx.r9.s64 + 16;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// bgt cr6,0x825eb290
	if (ctx.cr6.gt) goto loc_825EB290;
loc_825EB2E8:
	// subfic r9,r28,64
	ctx.xer.ca = ctx.r28.u32 <= 64;
	ctx.r9.s64 = 64 - ctx.r28.s64;
	// ld r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r30.u32 + 0);
	// clrldi r8,r28,32
	ctx.r8.u64 = ctx.r28.u64 & 0xFFFFFFFF;
	// lwz r10,8(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 8);
	// clrldi r9,r9,32
	ctx.r9.u64 = ctx.r9.u64 & 0xFFFFFFFF;
	// subf. r10,r28,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r28.s64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r10,8(r30)
	PPC_STORE_U32(ctx.r30.u32 + 8, ctx.r10.u32);
	// sld r8,r11,r8
	ctx.r8.u64 = ctx.r8.u8 & 0x40 ? 0 : (ctx.r11.u64 << (ctx.r8.u8 & 0x7F));
	// srd r9,r11,r9
	ctx.r9.u64 = ctx.r9.u8 & 0x40 ? 0 : (ctx.r11.u64 >> (ctx.r9.u8 & 0x7F));
	// rotlwi r11,r9,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// add r28,r11,r27
	ctx.r28.u64 = ctx.r11.u64 + ctx.r27.u64;
	// std r8,0(r30)
	PPC_STORE_U64(ctx.r30.u32 + 0, ctx.r8.u64);
	// bge 0x825eb324
	if (!ctx.cr0.lt) goto loc_825EB324;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825d5398
	ctx.lr = 0x825EB324;
	sub_825D5398(ctx, base);
loc_825EB324:
	// cmplwi cr6,r28,7
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 7, ctx.xer);
	// bgt cr6,0x825eb6e8
	if (ctx.cr6.gt) goto loc_825EB6E8;
	// lis r12,-32161
	ctx.r12.s64 = -2107703296;
	// addi r12,r12,-19644
	ctx.r12.s64 = ctx.r12.s64 + -19644;
	// rlwinm r0,r28,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr 
	switch (ctx.r28.u64) {
	case 0:
		goto loc_825EB364;
	case 1:
		goto loc_825EB36C;
	case 2:
		goto loc_825EB384;
	case 3:
		goto loc_825EB3D8;
	case 4:
		goto loc_825EB3F4;
	case 5:
		goto loc_825EB410;
	case 6:
		goto loc_825EB524;
	case 7:
		goto loc_825EB638;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-19612(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -19612);
	// lwz r18,-19604(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -19604);
	// lwz r18,-19580(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -19580);
	// lwz r18,-19496(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -19496);
	// lwz r18,-19468(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -19468);
	// lwz r18,-19440(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -19440);
	// lwz r18,-19164(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -19164);
	// lwz r18,-18888(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -18888);
loc_825EB364:
	// li r26,0
	ctx.r26.s64 = 0;
	// b 0x825eb6ec
	goto loc_825EB6EC;
loc_825EB36C:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r5,84(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// lwz r4,144(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// li r26,1
	ctx.r26.s64 = 1;
	// bl 0x825d6c20
	ctx.lr = 0x825EB380;
	sub_825D6C20(ctx, base);
	// b 0x825eb6ec
	goto loc_825EB6EC;
loc_825EB384:
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// lwz r5,84(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// lwz r4,144(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// li r26,2
	ctx.r26.s64 = 2;
	// bl 0x825d6c20
	ctx.lr = 0x825EB398;
	sub_825D6C20(ctx, base);
loc_825EB398:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r11,268(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825eb728
	if (!ctx.cr6.gt) goto loc_825EB728;
loc_825EB3AC:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825eb6d4
	if (!ctx.cr6.gt) goto loc_825EB6D4;
loc_825EB3BC:
	// add. r10,r7,r6
	ctx.r10.u64 = ctx.r7.u64 + ctx.r6.u64;
	ctx.cr0.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq 0x825eb6a8
	if (ctx.cr0.eq) goto loc_825EB6A8;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// bne cr6,0x825eb654
	if (!ctx.cr6.eq) goto loc_825EB654;
	// lwz r10,-20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + -20);
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// b 0x825eb6ac
	goto loc_825EB6AC;
loc_825EB3D8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r26,3
	ctx.r26.s64 = 3;
	// bl 0x825d70e8
	ctx.lr = 0x825EB3E4;
	sub_825D70E8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825eb6ec
	if (ctx.cr6.eq) goto loc_825EB6EC;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825EB3F4:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r26,4
	ctx.r26.s64 = 4;
	// bl 0x825d70e8
	ctx.lr = 0x825EB400;
	sub_825D70E8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825eb398
	if (ctx.cr6.eq) goto loc_825EB398;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825EB410:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r26,5
	ctx.r26.s64 = 5;
	// li r27,0
	ctx.r27.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825eb6ec
	if (!ctx.cr6.gt) goto loc_825EB6EC;
loc_825EB424:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825eb450
	if (!ctx.cr0.lt) goto loc_825EB450;
	// bl 0x825d5398
	ctx.lr = 0x825EB450;
	sub_825D5398(ctx, base);
loc_825EB450:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825eb4cc
	if (ctx.cr6.eq) goto loc_825EB4CC;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// li r30,0
	ctx.r30.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825eb510
	if (!ctx.cr6.gt) goto loc_825EB510;
loc_825EB468:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r28,r8,0
	ctx.r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825eb494
	if (!ctx.cr0.lt) goto loc_825EB494;
	// bl 0x825d5398
	ctx.lr = 0x825EB494;
	sub_825D5398(ctx, base);
loc_825EB494:
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mullw r11,r11,r27
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r27.s32);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r29
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r29.u32);
	// rlwimi r10,r28,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r28.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r29
	PPC_STORE_U32(ctx.r11.u32 + ctx.r29.u32, ctx.r10.u32);
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825eb468
	if (ctx.cr6.lt) goto loc_825EB468;
	// b 0x825eb510
	goto loc_825EB510;
loc_825EB4CC:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825eb510
	if (!ctx.cr6.gt) goto loc_825EB510;
loc_825EB4DC:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mullw r10,r10,r27
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r27.s32);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r29
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r29.u32);
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stwx r9,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + ctx.r29.u32, ctx.r9.u32);
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825eb4dc
	if (ctx.cr6.lt) goto loc_825EB4DC;
loc_825EB510:
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// cmpw cr6,r27,r11
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825eb424
	if (ctx.cr6.lt) goto loc_825EB424;
	// b 0x825eb6ec
	goto loc_825EB6EC;
loc_825EB524:
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// li r26,6
	ctx.r26.s64 = 6;
	// li r27,0
	ctx.r27.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825eb6ec
	if (!ctx.cr6.gt) goto loc_825EB6EC;
loc_825EB538:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r30,r8,0
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825eb564
	if (!ctx.cr0.lt) goto loc_825EB564;
	// bl 0x825d5398
	ctx.lr = 0x825EB564;
	sub_825D5398(ctx, base);
loc_825EB564:
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// beq cr6,0x825eb5e0
	if (ctx.cr6.eq) goto loc_825EB5E0;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r30,0
	ctx.r30.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825eb624
	if (!ctx.cr6.gt) goto loc_825EB624;
loc_825EB57C:
	// lwz r3,84(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 84);
	// ld r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U64(ctx.r3.u32 + 0);
	// lwz r10,8(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 8);
	// rldicr r9,r11,1,62
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0xFFFFFFFFFFFFFFFE;
	// rldicl r8,r11,1,63
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u64, 1) & 0x1;
	// addic. r11,r10,-1
	ctx.xer.ca = ctx.r10.u32 > 0;
	ctx.r11.s64 = ctx.r10.s64 + -1;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// rotlwi r28,r8,0
	ctx.r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// std r9,0(r3)
	PPC_STORE_U64(ctx.r3.u32 + 0, ctx.r9.u64);
	// stw r11,8(r3)
	PPC_STORE_U32(ctx.r3.u32 + 8, ctx.r11.u32);
	// bge 0x825eb5a8
	if (!ctx.cr0.lt) goto loc_825EB5A8;
	// bl 0x825d5398
	ctx.lr = 0x825EB5A8;
	sub_825D5398(ctx, base);
loc_825EB5A8:
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mullw r11,r11,r30
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r30.s32);
	// add r11,r11,r27
	ctx.r11.u64 = ctx.r11.u64 + ctx.r27.u64;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r10,r11,r29
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r29.u32);
	// rlwimi r10,r28,31,0,0
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r28.u32, 31) & 0x80000000) | (ctx.r10.u64 & 0xFFFFFFFF7FFFFFFF);
	// stwx r10,r11,r29
	PPC_STORE_U32(ctx.r11.u32 + ctx.r29.u32, ctx.r10.u32);
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825eb57c
	if (ctx.cr6.lt) goto loc_825EB57C;
	// b 0x825eb624
	goto loc_825EB624;
loc_825EB5E0:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825eb624
	if (!ctx.cr6.gt) goto loc_825EB624;
loc_825EB5F0:
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// add r10,r10,r27
	ctx.r10.u64 = ctx.r10.u64 + ctx.r27.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r9,r10,r29
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r29.u32);
	// clrlwi r9,r9,1
	ctx.r9.u64 = ctx.r9.u32 & 0x7FFFFFFF;
	// stwx r9,r10,r29
	PPC_STORE_U32(ctx.r10.u32 + ctx.r29.u32, ctx.r9.u32);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825eb5f0
	if (ctx.cr6.lt) goto loc_825EB5F0;
loc_825EB624:
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// addi r27,r27,1
	ctx.r27.s64 = ctx.r27.s64 + 1;
	// cmpw cr6,r27,r11
	ctx.cr6.compare<int32_t>(ctx.r27.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825eb538
	if (ctx.cr6.lt) goto loc_825EB538;
	// b 0x825eb6ec
	goto loc_825EB6EC;
loc_825EB638:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// li r26,7
	ctx.r26.s64 = 7;
	// bl 0x825ea3b8
	ctx.lr = 0x825EB644;
	sub_825EA3B8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825eb6ec
	if (ctx.cr6.eq) goto loc_825EB6EC;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825EB654:
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// bne cr6,0x825eb67c
	if (!ctx.cr6.eq) goto loc_825EB67C;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r10,r10,r11
	ctx.r10.s64 = ctx.r11.s64 - ctx.r10.s64;
	// lwz r10,0(r10)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// b 0x825eb6ac
	goto loc_825EB6AC;
loc_825EB67C:
	// lwz r9,136(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r10,-20(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + -20);
	// rlwinm r8,r9,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,1,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0x1;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// subf r9,r9,r11
	ctx.r9.s64 = ctx.r11.s64 - ctx.r9.s64;
	// lwz r9,0(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// rlwinm r9,r9,1,31,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0x1;
	// cmplw cr6,r10,r9
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r9.u32, ctx.xer);
	// beq cr6,0x825eb6ac
	if (ctx.cr6.eq) goto loc_825EB6AC;
loc_825EB6A8:
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
loc_825EB6AC:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// rlwinm r10,r10,31,0,0
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x80000000;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// xor r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 ^ ctx.r9.u64;
	// rlwimi r10,r9,0,1,31
	ctx.r10.u64 = (__builtin_rotateleft32(ctx.r9.u32, 0) & 0x7FFFFFFF) | (ctx.r10.u64 & 0xFFFFFFFF80000000);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// addi r11,r11,20
	ctx.r11.s64 = ctx.r11.s64 + 20;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// cmpw cr6,r7,r10
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825eb3bc
	if (ctx.cr6.lt) goto loc_825EB3BC;
loc_825EB6D4:
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmpw cr6,r6,r10
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825eb3ac
	if (ctx.cr6.lt) goto loc_825EB3AC;
	// b 0x825eb728
	goto loc_825EB728;
loc_825EB6E8:
	// lwz r26,80(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
loc_825EB6EC:
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// beq cr6,0x825eb728
	if (ctx.cr6.eq) goto loc_825EB728;
	// lwz r10,144(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825eb728
	if (!ctx.cr6.gt) goto loc_825EB728;
loc_825EB704:
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// not r9,r10
	ctx.r9.u64 = ~ctx.r10.u64;
	// rlwimi r9,r10,0,1,31
	ctx.r9.u64 = (__builtin_rotateleft32(ctx.r10.u32, 0) & 0x7FFFFFFF) | (ctx.r9.u64 & 0xFFFFFFFF80000000);
	// stw r9,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r9.u32);
	// addi r29,r29,20
	ctx.r29.s64 = ctx.r29.s64 + 20;
	// lwz r10,144(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 144);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825eb704
	if (ctx.cr6.lt) goto loc_825EB704;
loc_825EB728:
	// cmpwi cr6,r24,0
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 0, ctx.xer);
	// bne cr6,0x825eb740
	if (!ctx.cr6.eq) goto loc_825EB740;
	// stw r26,344(r31)
	PPC_STORE_U32(ctx.r31.u32 + 344, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825EB740:
	// cmpwi cr6,r24,5
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 5, ctx.xer);
	// bne cr6,0x825eb758
	if (!ctx.cr6.eq) goto loc_825EB758;
	// stw r26,20940(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20940, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825EB758:
	// cmpwi cr6,r24,4
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 4, ctx.xer);
	// bne cr6,0x825eb770
	if (!ctx.cr6.eq) goto loc_825EB770;
	// stw r26,20004(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20004, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825EB770:
	// cmpwi cr6,r24,3
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 3, ctx.xer);
	// bne cr6,0x825eb788
	if (!ctx.cr6.eq) goto loc_825EB788;
	// stw r26,14804(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14804, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825EB788:
	// cmpwi cr6,r24,2
	ctx.cr6.compare<int32_t>(ctx.r24.s32, 2, ctx.xer);
	// bne cr6,0x825eb7a0
	if (!ctx.cr6.eq) goto loc_825EB7A0;
	// stw r26,19988(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19988, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825EB7A0:
	// stw r26,348(r31)
	PPC_STORE_U32(ctx.r31.u32 + 348, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825EB7B0"))) PPC_WEAK_FUNC(sub_825EB7B0);
PPC_FUNC_IMPL(__imp__sub_825EB7B0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825EB7B8;
	sub_8239BA1C(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// li r29,1
	ctx.r29.s64 = 1;
	// li r11,-1
	ctx.r11.s64 = -1;
	// li r5,0
	ctx.r5.s64 = 0;
	// li r4,114
	ctx.r4.s64 = 114;
	// std r30,3576(r31)
	PPC_STORE_U64(ctx.r31.u32 + 3576, ctx.r30.u64);
	// li r3,14
	ctx.r3.s64 = 14;
	// std r30,3584(r31)
	PPC_STORE_U64(ctx.r31.u32 + 3584, ctx.r30.u64);
	// stw r11,3644(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3644, ctx.r11.u32);
	// li r11,1000
	ctx.r11.s64 = 1000;
	// std r30,3600(r31)
	PPC_STORE_U64(ctx.r31.u32 + 3600, ctx.r30.u64);
	// std r30,3608(r31)
	PPC_STORE_U64(ctx.r31.u32 + 3608, ctx.r30.u64);
	// stw r30,3632(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3632, ctx.r30.u32);
	// stw r30,452(r31)
	PPC_STORE_U32(ctx.r31.u32 + 452, ctx.r30.u32);
	// stw r30,396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 396, ctx.r30.u32);
	// stw r30,392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 392, ctx.r30.u32);
	// stw r30,400(r31)
	PPC_STORE_U32(ctx.r31.u32 + 400, ctx.r30.u32);
	// stw r30,3888(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3888, ctx.r30.u32);
	// stw r30,3892(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3892, ctx.r30.u32);
	// stw r30,3896(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3896, ctx.r30.u32);
	// stw r30,436(r31)
	PPC_STORE_U32(ctx.r31.u32 + 436, ctx.r30.u32);
	// stw r30,432(r31)
	PPC_STORE_U32(ctx.r31.u32 + 432, ctx.r30.u32);
	// stw r30,440(r31)
	PPC_STORE_U32(ctx.r31.u32 + 440, ctx.r30.u32);
	// stw r30,3884(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3884, ctx.r30.u32);
	// stw r30,3948(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3948, ctx.r30.u32);
	// stw r30,444(r31)
	PPC_STORE_U32(ctx.r31.u32 + 444, ctx.r30.u32);
	// stw r30,3900(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3900, ctx.r30.u32);
	// stw r30,15508(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15508, ctx.r30.u32);
	// stw r30,15512(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15512, ctx.r30.u32);
	// stw r30,3664(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3664, ctx.r30.u32);
	// stw r30,3668(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3668, ctx.r30.u32);
	// stw r30,3672(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3672, ctx.r30.u32);
	// stw r29,3676(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3676, ctx.r29.u32);
	// stw r30,3680(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3680, ctx.r30.u32);
	// stw r30,264(r31)
	PPC_STORE_U32(ctx.r31.u32 + 264, ctx.r30.u32);
	// stw r30,15768(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15768, ctx.r30.u32);
	// stw r30,1892(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1892, ctx.r30.u32);
	// stw r30,1896(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1896, ctx.r30.u32);
	// stw r30,1900(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1900, ctx.r30.u32);
	// stw r30,1904(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1904, ctx.r30.u32);
	// stw r30,1908(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1908, ctx.r30.u32);
	// stw r30,1912(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1912, ctx.r30.u32);
	// stw r30,268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 268, ctx.r30.u32);
	// stw r30,272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 272, ctx.r30.u32);
	// stw r30,15548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15548, ctx.r30.u32);
	// stw r30,15516(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15516, ctx.r30.u32);
	// std r30,2928(r31)
	PPC_STORE_U64(ctx.r31.u32 + 2928, ctx.r30.u64);
	// stw r30,2936(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2936, ctx.r30.u32);
	// stw r30,2940(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2940, ctx.r30.u32);
	// stw r30,2944(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2944, ctx.r30.u32);
	// stw r30,2948(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2948, ctx.r30.u32);
	// stw r30,1792(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1792, ctx.r30.u32);
	// stw r30,15520(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15520, ctx.r30.u32);
	// stw r30,15524(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15524, ctx.r30.u32);
	// stw r30,15528(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15528, ctx.r30.u32);
	// stw r11,15532(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15532, ctx.r11.u32);
	// stw r29,15536(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15536, ctx.r29.u32);
	// stw r30,3344(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3344, ctx.r30.u32);
	// stw r30,3348(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3348, ctx.r30.u32);
	// stw r30,21540(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21540, ctx.r30.u32);
	// bl 0x825f6c70
	ctx.lr = 0x825EB8B4;
	sub_825F6C70(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// stw r3,21540(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21540, ctx.r3.u32);
	// beq cr6,0x825eb8d0
	if (ctx.cr6.eq) goto loc_825EB8D0;
	// cmpwi cr6,r3,-1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, -1, ctx.xer);
	// beq cr6,0x825eb8d0
	if (ctx.cr6.eq) goto loc_825EB8D0;
	// stw r29,21540(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21540, ctx.r29.u32);
	// b 0x825eb8d4
	goto loc_825EB8D4;
loc_825EB8D0:
	// stw r30,21540(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21540, ctx.r30.u32);
loc_825EB8D4:
	// stw r30,15504(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15504, ctx.r30.u32);
	// stw r30,3352(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3352, ctx.r30.u32);
	// stw r30,21328(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21328, ctx.r30.u32);
	// stw r30,21332(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21332, ctx.r30.u32);
	// stw r30,21336(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21336, ctx.r30.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825EB8F0"))) PPC_WEAK_FUNC(sub_825EB8F0);
PPC_FUNC_IMPL(__imp__sub_825EB8F0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825EB8F8;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r11,r4,15
	ctx.r11.s64 = ctx.r4.s64 + 15;
	// addi r8,r5,15
	ctx.r8.s64 = ctx.r5.s64 + 15;
	// rlwinm r10,r11,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFF0;
	// mr r29,r6
	ctx.r29.u64 = ctx.r6.u64;
	// lwz r9,3924(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// rlwinm r11,r8,0,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFF0;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x825eb92c
	if (ctx.cr6.eq) goto loc_825EB92C;
	// mr r9,r11
	ctx.r9.u64 = ctx.r11.u64;
	// srawi r8,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 2;
	// b 0x825eb934
	goto loc_825EB934;
loc_825EB92C:
	// srawi r8,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 1;
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
loc_825EB934:
	// lwz r7,15472(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r7,7
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 7, ctx.xer);
	// bne cr6,0x825eb94c
	if (!ctx.cr6.eq) goto loc_825EB94C;
	// addi r11,r11,31
	ctx.r11.s64 = ctx.r11.s64 + 31;
	// rlwinm r11,r11,0,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFE0;
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
loc_825EB94C:
	// lwz r6,15300(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15300);
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// addi r11,r11,64
	ctx.r11.s64 = ctx.r11.s64 + 64;
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// addi r8,r8,32
	ctx.r8.s64 = ctx.r8.s64 + 32;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// mullw r30,r9,r8
	ctx.r30.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r8.s32);
	// mullw r26,r11,r10
	ctx.r26.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// bne cr6,0x825eba0c
	if (!ctx.cr6.eq) goto loc_825EBA0C;
	// lwz r9,21580(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21580);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// beq cr6,0x825eb984
	if (ctx.cr6.eq) goto loc_825EB984;
	// li r11,1
	ctx.r11.s64 = 1;
loc_825EB984:
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x825eb990
	if (ctx.cr6.eq) goto loc_825EB990;
	// addi r11,r29,-6
	ctx.r11.s64 = ctx.r29.s64 + -6;
loc_825EB990:
	// addi r7,r7,-7
	ctx.r7.s64 = ctx.r7.s64 + -7;
	// lwz r9,21356(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21356);
	// addi r6,r11,6
	ctx.r6.s64 = ctx.r11.s64 + 6;
	// lwz r8,21352(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21352);
	// cntlzw r11,r7
	ctx.r11.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r7,r11,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82626328
	ctx.lr = 0x825EB9C0;
	sub_82626328(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ebbfc
	if (!ctx.cr6.eq) goto loc_825EBBFC;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// beq cr6,0x825eb9e8
	if (ctx.cr6.eq) goto loc_825EB9E8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825d01a8
	ctx.lr = 0x825EB9DC;
	sub_825D01A8(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825EB9E8:
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// addi r4,r31,3700
	ctx.r4.s64 = ctx.r31.s64 + 3700;
	// bl 0x82626170
	ctx.lr = 0x825EB9F8;
	sub_82626170(ctx, base);
	// li r5,-1
	ctx.r5.s64 = -1;
	// addi r4,r31,3692
	ctx.r4.s64 = ctx.r31.s64 + 3692;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// bl 0x82626170
	ctx.lr = 0x825EBA08;
	sub_82626170(ctx, base);
	// b 0x825eba44
	goto loc_825EBA44;
loc_825EBA0C:
	// addi r11,r7,-7
	ctx.r11.s64 = ctx.r7.s64 + -7;
	// lwz r9,21356(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21356);
	// li r6,4
	ctx.r6.s64 = 4;
	// lwz r8,21352(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21352);
	// cntlzw r11,r11
	ctx.r11.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r7,r11,3,0,28
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82626328
	ctx.lr = 0x825EBA3C;
	sub_82626328(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ebbfc
	if (!ctx.cr6.eq) goto loc_825EBBFC;
loc_825EBA44:
	// addi r25,r31,3688
	ctx.r25.s64 = ctx.r31.s64 + 3688;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// li r5,-1
	ctx.r5.s64 = -1;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// bl 0x82626170
	ctx.lr = 0x825EBA58;
	sub_82626170(ctx, base);
	// addi r29,r31,3696
	ctx.r29.s64 = ctx.r31.s64 + 3696;
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82626170
	ctx.lr = 0x825EBA6C;
	sub_82626170(ctx, base);
	// addi r28,r31,3704
	ctx.r28.s64 = ctx.r31.s64 + 3704;
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x82626170
	ctx.lr = 0x825EBA80;
	sub_82626170(ctx, base);
	// addi r27,r31,3708
	ctx.r27.s64 = ctx.r31.s64 + 3708;
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x82626170
	ctx.lr = 0x825EBA94;
	sub_82626170(ctx, base);
	// lwz r11,0(r25)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ebbf8
	if (ctx.cr6.eq) goto loc_825EBBF8;
	// lwz r10,0(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825ebbf8
	if (ctx.cr6.eq) goto loc_825EBBF8;
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825ebbf8
	if (ctx.cr6.eq) goto loc_825EBBF8;
	// lwz r10,0(r27)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825ebbf8
	if (ctx.cr6.eq) goto loc_825EBBF8;
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// lwz r10,224(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 224);
	// li r4,0
	ctx.r4.s64 = 0;
	// rotlwi r3,r9,0
	ctx.r3.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// stw r9,3720(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3720, ctx.r9.u32);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r9,220(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// stw r8,3724(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3724, ctx.r8.u32);
	// lwz r8,8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r11,3724(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// add r7,r10,r11
	ctx.r7.u64 = ctx.r10.u64 + ctx.r11.u64;
	// rotlwi r11,r8,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r8.u32, 0);
	// stw r8,3728(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3728, ctx.r8.u32);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r7,3804(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3804, ctx.r7.u32);
	// stw r11,3808(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3808, ctx.r11.u32);
	// add r11,r3,r9
	ctx.r11.u64 = ctx.r3.u64 + ctx.r9.u64;
	// stw r11,3800(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3800, ctx.r11.u32);
	// bl 0x8239ca70
	ctx.lr = 0x825EBB14;
	sub_8239CA70(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// lwz r3,3724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3724);
	// li r4,128
	ctx.r4.s64 = 128;
	// bl 0x8239ca70
	ctx.lr = 0x825EBB24;
	sub_8239CA70(ctx, base);
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// li r4,128
	ctx.r4.s64 = 128;
	// lwz r3,3728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3728);
	// bl 0x8239ca70
	ctx.lr = 0x825EBB34;
	sub_8239CA70(ctx, base);
	// lwz r11,0(r29)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r29.u32 + 0);
	// lwz r7,220(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 220);
	// lwz r10,0(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// lwz r9,0(r27)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r27.u32 + 0);
	// lwz r8,3700(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3700);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// stw r6,3732(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3732, ctx.r6.u32);
	// rotlwi r6,r6,0
	ctx.r6.u64 = __builtin_rotateleft32(ctx.r6.u32, 0);
	// lwz r5,4(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// add r6,r6,r7
	ctx.r6.u64 = ctx.r6.u64 + ctx.r7.u64;
	// stw r5,3736(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3736, ctx.r5.u32);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r6,3756(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3756, ctx.r6.u32);
	// stw r11,3740(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3740, ctx.r11.u32);
	// lwz r11,0(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// stw r11,3776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3776, ctx.r11.u32);
	// lwz r11,4(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// stw r11,3780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3780, ctx.r11.u32);
	// lwz r11,8(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// stw r11,3784(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3784, ctx.r11.u32);
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// stw r11,3788(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3788, ctx.r11.u32);
	// lwz r11,4(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// stw r11,3792(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3792, ctx.r11.u32);
	// lwz r11,8(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// stw r11,3796(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3796, ctx.r11.u32);
	// beq cr6,0x825ebbbc
	if (ctx.cr6.eq) goto loc_825EBBBC;
	// lwz r11,0(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// stw r11,3744(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3744, ctx.r11.u32);
	// lwz r11,4(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 4);
	// stw r11,3748(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3748, ctx.r11.u32);
	// lwz r11,8(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 8);
	// stw r11,3752(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3752, ctx.r11.u32);
loc_825EBBBC:
	// lwz r11,3692(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3692);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ebbec
	if (ctx.cr6.eq) goto loc_825EBBEC;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r10,3760(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3760, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// lwz r9,4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// stw r9,3764(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3764, ctx.r9.u32);
	// lwz r11,8(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// stw r10,3772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3772, ctx.r10.u32);
	// stw r11,3768(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3768, ctx.r11.u32);
loc_825EBBEC:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825EBBF8:
	// li r3,2
	ctx.r3.s64 = 2;
loc_825EBBFC:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825EBC04"))) PPC_WEAK_FUNC(sub_825EBC04);
PPC_FUNC_IMPL(__imp__sub_825EBC04) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EBC08"))) PPC_WEAK_FUNC(sub_825EBC08);
PPC_FUNC_IMPL(__imp__sub_825EBC08) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba18
	ctx.lr = 0x825EBC10;
	sub_8239BA18(ctx, base);
	// lwz r11,3956(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3956);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ebda0
	if (ctx.cr6.eq) goto loc_825EBDA0;
	// lwz r11,2976(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 2976);
	// lwz r10,3356(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3356);
	// addi r11,r11,7
	ctx.r11.s64 = ctx.r11.s64 + 7;
	// lwz r6,136(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmplwi cr6,r10,2
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 2, ctx.xer);
	// rlwinm r9,r11,0,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFF8;
	// rlwinm r10,r6,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r11,r6,7,0,24
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 7) & 0xFFFFFF80;
	// stw r9,2980(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2980, ctx.r9.u32);
	// add r9,r10,r9
	ctx.r9.u64 = ctx.r10.u64 + ctx.r9.u64;
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r9,2984(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2984, ctx.r9.u32);
	// add r9,r8,r10
	ctx.r9.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r8,2988(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2988, ctx.r8.u32);
	// add r8,r9,r11
	ctx.r8.u64 = ctx.r9.u64 + ctx.r11.u64;
	// add r7,r8,r11
	ctx.r7.u64 = ctx.r8.u64 + ctx.r11.u64;
	// stw r9,2992(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2992, ctx.r9.u32);
	// add r9,r7,r11
	ctx.r9.u64 = ctx.r7.u64 + ctx.r11.u64;
	// stw r8,2996(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2996, ctx.r8.u32);
	// stw r7,3000(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3000, ctx.r7.u32);
	// stw r9,3004(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3004, ctx.r9.u32);
	// bne cr6,0x825ebcc4
	if (!ctx.cr6.eq) goto loc_825EBCC4;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r9,3008(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3008, ctx.r9.u32);
	// add r9,r8,r10
	ctx.r9.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r8,3012(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3012, ctx.r8.u32);
	// add r8,r9,r10
	ctx.r8.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// stw r9,3016(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3016, ctx.r9.u32);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r8,3020(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3020, ctx.r8.u32);
	// stw r10,3024(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3024, ctx.r10.u32);
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + ctx.r11.u64;
	// stw r9,3028(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3028, ctx.r9.u32);
	// add r9,r10,r11
	ctx.r9.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r10,3032(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3032, ctx.r10.u32);
	// add r10,r9,r11
	ctx.r10.u64 = ctx.r9.u64 + ctx.r11.u64;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r9,3036(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3036, ctx.r9.u32);
	// stw r10,3040(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3040, ctx.r10.u32);
	// stw r11,3044(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3044, ctx.r11.u32);
loc_825EBCC4:
	// lwz r10,15240(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15240);
	// lwz r11,15184(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15184);
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// lwz r9,3924(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3924);
	// addi r8,r11,31
	ctx.r8.s64 = ctx.r11.s64 + 31;
	// rlwinm r11,r10,0,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFE0;
	// rlwinm r10,r8,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFE0;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stw r11,15244(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15244, ctx.r11.u32);
	// stw r10,15188(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15188, ctx.r10.u32);
	// addi r10,r11,288
	ctx.r10.s64 = ctx.r11.s64 + 288;
	// addi r11,r10,288
	ctx.r11.s64 = ctx.r10.s64 + 288;
	// stw r10,15248(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15248, ctx.r10.u32);
	// addi r10,r11,96
	ctx.r10.s64 = ctx.r11.s64 + 96;
	// stw r11,15252(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15252, ctx.r11.u32);
	// addi r11,r10,96
	ctx.r11.s64 = ctx.r10.s64 + 96;
	// stw r10,15256(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15256, ctx.r10.u32);
	// addi r10,r11,96
	ctx.r10.s64 = ctx.r11.s64 + 96;
	// stw r11,15260(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15260, ctx.r11.u32);
	// lwz r11,144(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 144);
	// stw r10,15264(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15264, ctx.r10.u32);
	// rlwinm r10,r11,7,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 7) & 0xFFFFFF80;
	// beq cr6,0x825ebd38
	if (ctx.cr6.eq) goto loc_825EBD38;
	// lwz r9,460(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 460);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r11,r9
	ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r9,r9,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 4) & 0xFFFFFFF0;
	// b 0x825ebd44
	goto loc_825EBD44;
loc_825EBD38:
	// lwz r8,460(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 460);
	// rlwinm r9,r11,5,0,26
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
loc_825EBD44:
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r10,464(r3)
	PPC_STORE_U32(ctx.r3.u32 + 464, ctx.r10.u32);
	// lwz r10,140(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r7,15208(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15208);
	// addi r10,r10,2
	ctx.r10.s64 = ctx.r10.s64 + 2;
	// lwz r8,15292(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15292);
	// lwz r5,15268(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15268);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
	// mullw r6,r10,r6
	ctx.r6.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r6.s32);
	// stw r9,468(r3)
	PPC_STORE_U32(ctx.r3.u32 + 468, ctx.r9.u32);
	// lwz r9,15276(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15276);
	// lwz r10,15284(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 15284);
	// stw r7,15212(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15212, ctx.r7.u32);
	// rlwinm r6,r6,3,0,28
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 3) & 0xFFFFFFF8;
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r8,15296(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15296, ctx.r8.u32);
	// stw r9,15280(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15280, ctx.r9.u32);
	// stw r6,15272(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15272, ctx.r6.u32);
	// stw r11,15288(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15288, ctx.r11.u32);
loc_825EBDA0:
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// li r30,0
	ctx.r30.s64 = 0;
	// mr r8,r30
	ctx.r8.u64 = ctx.r30.u64;
	// mr r5,r30
	ctx.r5.u64 = ctx.r30.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// ble cr6,0x825ebe64
	if (!ctx.cr6.gt) goto loc_825EBE64;
loc_825EBDB8:
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// ble cr6,0x825ebe54
	if (!ctx.cr6.gt) goto loc_825EBE54;
	// cntlzw r10,r5
	ctx.r10.u64 = ctx.r5.u32 == 0 ? 32 : __builtin_clz(ctx.r5.u32);
	// rlwinm r4,r10,28,30,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0x2;
	// rlwinm r10,r8,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r8,r10
	ctx.r10.u64 = ctx.r8.u64 + ctx.r10.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
loc_825EBDDC:
	// lwz r6,136(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cntlzw r31,r11
	ctx.r31.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// lwz r7,140(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// lwz r9,268(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 268);
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// subf r6,r11,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r11.s64;
	// subf r7,r5,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r5.s64;
	// cntlzw r6,r6
	ctx.r6.u64 = ctx.r6.u32 == 0 ? 32 : __builtin_clz(ctx.r6.u32);
	// cntlzw r7,r7
	ctx.r7.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// rlwinm r6,r6,27,31,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 27) & 0x1;
	// rlwinm r7,r7,28,30,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 28) & 0x2;
	// rlwinm r31,r31,27,31,31
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 27) & 0x1;
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// or r6,r31,r4
	ctx.r6.u64 = ctx.r31.u64 | ctx.r4.u64;
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r6,r6,28
	ctx.r6.u64 = ctx.r6.u32 & 0xF;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// lwz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// rlwinm r7,r7,12,0,19
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 12) & 0xFFFFF000;
	// rlwinm r6,r6,0,20,15
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 0) & 0xFFFFFFFFFFFF0FFF;
	// or r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 | ctx.r6.u64;
	// stw r7,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r7.u32);
	// lwz r9,136(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmplw cr6,r11,r9
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r9.u32, ctx.xer);
	// blt cr6,0x825ebddc
	if (ctx.cr6.lt) goto loc_825EBDDC;
loc_825EBE54:
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// addi r5,r5,1
	ctx.r5.s64 = ctx.r5.s64 + 1;
	// cmplw cr6,r5,r11
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x825ebdb8
	if (ctx.cr6.lt) goto loc_825EBDB8;
loc_825EBE64:
	// lwz r11,272(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// stb r30,12(r11)
	PPC_STORE_U8(ctx.r11.u32 + 12, ctx.r30.u8);
	// lwz r11,272(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// stb r30,13(r11)
	PPC_STORE_U8(ctx.r11.u32 + 13, ctx.r30.u8);
	// lwz r11,272(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// stb r30,14(r11)
	PPC_STORE_U8(ctx.r11.u32 + 14, ctx.r30.u8);
	// lwz r11,272(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// stb r30,15(r11)
	PPC_STORE_U8(ctx.r11.u32 + 15, ctx.r30.u8);
	// lwz r11,272(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// stb r30,16(r11)
	PPC_STORE_U8(ctx.r11.u32 + 16, ctx.r30.u8);
	// lwz r11,272(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 272);
	// stb r30,17(r11)
	PPC_STORE_U8(ctx.r11.u32 + 17, ctx.r30.u8);
	// lwz r7,208(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// lwz r8,136(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,204(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// lwz r6,144(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 144);
	// addi r7,r7,-4
	ctx.r7.s64 = ctx.r7.s64 + -4;
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r11,1896(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1896);
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// addi r5,r9,-8
	ctx.r5.s64 = ctx.r9.s64 + -8;
	// lwz r9,1892(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1892);
	// stw r7,15180(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15180, ctx.r7.u32);
	// rlwinm r7,r8,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// stw r5,15176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15176, ctx.r5.u32);
	// rlwinm r8,r8,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
	// neg r6,r8
	ctx.r6.s64 = -ctx.r8.s64;
	// stw r8,1888(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1888, ctx.r8.u32);
	// ble cr6,0x825ebf10
	if (!ctx.cr6.gt) goto loc_825EBF10;
	// mr r8,r30
	ctx.r8.u64 = ctx.r30.u64;
loc_825EBEE8:
	// lwz r7,268(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 268);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// add r7,r7,r8
	ctx.r7.u64 = ctx.r7.u64 + ctx.r8.u64;
	// addi r8,r8,20
	ctx.r8.s64 = ctx.r8.s64 + 20;
	// lwz r5,0(r7)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r7.u32 + 0);
	// rlwinm r5,r5,0,4,2
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 0) & 0xFFFFFFFFEFFFFFFF;
	// stw r5,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r5.u32);
	// lwz r7,144(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 144);
	// cmplw cr6,r10,r7
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, ctx.r7.u32, ctx.xer);
	// blt cr6,0x825ebee8
	if (ctx.cr6.lt) goto loc_825EBEE8;
loc_825EBF10:
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
loc_825EBF14:
	// clrlwi r10,r29,31
	ctx.r10.u64 = ctx.r29.u32 & 0x1;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825ebf24
	if (ctx.cr6.eq) goto loc_825EBF24;
	// lwz r6,1888(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1888);
loc_825EBF24:
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// mr r31,r30
	ctx.r31.u64 = ctx.r30.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// ble cr6,0x825ec0f0
	if (!ctx.cr6.gt) goto loc_825EC0F0;
loc_825EBF34:
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r10,r9,-160
	ctx.r10.s64 = ctx.r9.s64 + -160;
loc_825EBF3C:
	// cmplwi cr6,r4,5
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 5, ctx.xer);
	// bgt cr6,0x825ec0c8
	if (ctx.cr6.gt) goto loc_825EC0C8;
	// lis r12,-32161
	ctx.r12.s64 = -2107703296;
	// addi r12,r12,-16548
	ctx.r12.s64 = ctx.r12.s64 + -16548;
	// rlwinm r0,r4,2,0,29
	ctx.r0.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// lwzx r0,r12,r0
	ctx.r0.u64 = PPC_LOAD_U32(ctx.r12.u32 + ctx.r0.u32);
	// mtctr r0
	ctx.ctr.u64 = ctx.r0.u64;
	// bctr 
	switch (ctx.r4.u64) {
	case 0:
		goto loc_825EBF74;
	case 1:
		goto loc_825EBFB4;
	case 2:
		goto loc_825EBFF8;
	case 3:
		goto loc_825EC024;
	case 4:
		goto loc_825EC04C;
	case 5:
		goto loc_825EC08C;
	default:
		__builtin_unreachable();
	}
	// lwz r18,-16524(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -16524);
	// lwz r18,-16460(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -16460);
	// lwz r18,-16392(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -16392);
	// lwz r18,-16348(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -16348);
	// lwz r18,-16308(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -16308);
	// lwz r18,-16244(r30)
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r30.u32 + -16244);
loc_825EBF74:
	// lwz r5,1900(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// subfic r8,r6,32
	ctx.xer.ca = ctx.r6.u32 <= 32;
	ctx.r8.s64 = 32 - ctx.r6.s64;
	// stw r10,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r10.u32);
	// addi r7,r6,48
	ctx.r7.s64 = ctx.r6.s64 + 48;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// stw r5,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r5.u32);
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// lwz r5,1900(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// stw r8,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r8.u32);
	// stw r5,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r5.u32);
	// lwz r8,1900(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// stw r7,20(r11)
	PPC_STORE_U32(ctx.r11.u32 + 20, ctx.r7.u32);
	// stw r8,16(r11)
	PPC_STORE_U32(ctx.r11.u32 + 16, ctx.r8.u32);
	// b 0x825ec0c8
	goto loc_825EC0C8;
loc_825EBFB4:
	// neg r7,r6
	ctx.r7.s64 = -ctx.r6.s64;
	// addi r8,r10,128
	ctx.r8.s64 = ctx.r10.s64 + 128;
	// addi r5,r7,32
	ctx.r5.s64 = ctx.r7.s64 + 32;
	// addi r7,r7,16
	ctx.r7.s64 = ctx.r7.s64 + 16;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r7,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r8,28(r11)
	PPC_STORE_U32(ctx.r11.u32 + 28, ctx.r8.u32);
	// stw r8,24(r11)
	PPC_STORE_U32(ctx.r11.u32 + 24, ctx.r8.u32);
	// add r8,r5,r9
	ctx.r8.u64 = ctx.r5.u64 + ctx.r9.u64;
	// lwz r5,1900(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// add r7,r7,r9
	ctx.r7.u64 = ctx.r7.u64 + ctx.r9.u64;
	// stw r8,36(r11)
	PPC_STORE_U32(ctx.r11.u32 + 36, ctx.r8.u32);
	// stw r5,32(r11)
	PPC_STORE_U32(ctx.r11.u32 + 32, ctx.r5.u32);
	// lwz r8,1900(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// stw r7,44(r11)
	PPC_STORE_U32(ctx.r11.u32 + 44, ctx.r7.u32);
	// stw r8,40(r11)
	PPC_STORE_U32(ctx.r11.u32 + 40, ctx.r8.u32);
	// b 0x825ec0c8
	goto loc_825EC0C8;
loc_825EBFF8:
	// lwz r5,1900(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// addi r8,r10,96
	ctx.r8.s64 = ctx.r10.s64 + 96;
	// stw r10,52(r11)
	PPC_STORE_U32(ctx.r11.u32 + 52, ctx.r10.u32);
	// addi r7,r10,-64
	ctx.r7.s64 = ctx.r10.s64 + -64;
	// stw r5,48(r11)
	PPC_STORE_U32(ctx.r11.u32 + 48, ctx.r5.u32);
	// stw r8,60(r11)
	PPC_STORE_U32(ctx.r11.u32 + 60, ctx.r8.u32);
	// stw r8,56(r11)
	PPC_STORE_U32(ctx.r11.u32 + 56, ctx.r8.u32);
	// lwz r8,1900(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1900);
	// stw r7,68(r11)
	PPC_STORE_U32(ctx.r11.u32 + 68, ctx.r7.u32);
	// stw r8,64(r11)
	PPC_STORE_U32(ctx.r11.u32 + 64, ctx.r8.u32);
	// b 0x825ec0c8
	goto loc_825EC0C8;
loc_825EC024:
	// addi r8,r10,128
	ctx.r8.s64 = ctx.r10.s64 + 128;
	// addi r7,r10,96
	ctx.r7.s64 = ctx.r10.s64 + 96;
	// addi r5,r10,64
	ctx.r5.s64 = ctx.r10.s64 + 64;
	// stw r8,76(r11)
	PPC_STORE_U32(ctx.r11.u32 + 76, ctx.r8.u32);
	// stw r8,72(r11)
	PPC_STORE_U32(ctx.r11.u32 + 72, ctx.r8.u32);
	// stw r7,84(r11)
	PPC_STORE_U32(ctx.r11.u32 + 84, ctx.r7.u32);
	// stw r7,80(r11)
	PPC_STORE_U32(ctx.r11.u32 + 80, ctx.r7.u32);
	// stw r5,92(r11)
	PPC_STORE_U32(ctx.r11.u32 + 92, ctx.r5.u32);
	// stw r5,88(r11)
	PPC_STORE_U32(ctx.r11.u32 + 88, ctx.r5.u32);
	// b 0x825ec0c8
	goto loc_825EC0C8;
loc_825EC04C:
	// lwz r28,1904(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r6,96
	ctx.r5.s64 = ctx.r6.s64 + 96;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r8,100(r11)
	PPC_STORE_U32(ctx.r11.u32 + 100, ctx.r8.u32);
	// stw r28,96(r11)
	PPC_STORE_U32(ctx.r11.u32 + 96, ctx.r28.u32);
	// subf r5,r5,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r5.s64;
	// lwz r8,1904(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// stw r7,108(r11)
	PPC_STORE_U32(ctx.r11.u32 + 108, ctx.r7.u32);
	// stw r8,104(r11)
	PPC_STORE_U32(ctx.r11.u32 + 104, ctx.r8.u32);
	// lwz r8,1904(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// stw r5,116(r11)
	PPC_STORE_U32(ctx.r11.u32 + 116, ctx.r5.u32);
	// stw r8,112(r11)
	PPC_STORE_U32(ctx.r11.u32 + 112, ctx.r8.u32);
	// b 0x825ec0c8
	goto loc_825EC0C8;
loc_825EC08C:
	// lwz r28,1904(r3)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// addi r8,r10,-32
	ctx.r8.s64 = ctx.r10.s64 + -32;
	// rlwinm r7,r6,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r5,r6,96
	ctx.r5.s64 = ctx.r6.s64 + 96;
	// subf r7,r7,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r7.s64;
	// rlwinm r5,r5,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r8,124(r11)
	PPC_STORE_U32(ctx.r11.u32 + 124, ctx.r8.u32);
	// stw r28,120(r11)
	PPC_STORE_U32(ctx.r11.u32 + 120, ctx.r28.u32);
	// subf r5,r5,r9
	ctx.r5.s64 = ctx.r9.s64 - ctx.r5.s64;
	// lwz r8,1904(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// stw r7,132(r11)
	PPC_STORE_U32(ctx.r11.u32 + 132, ctx.r7.u32);
	// stw r8,128(r11)
	PPC_STORE_U32(ctx.r11.u32 + 128, ctx.r8.u32);
	// lwz r8,1904(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1904);
	// stw r5,140(r11)
	PPC_STORE_U32(ctx.r11.u32 + 140, ctx.r5.u32);
	// stw r8,136(r11)
	PPC_STORE_U32(ctx.r11.u32 + 136, ctx.r8.u32);
loc_825EC0C8:
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r9,r9,32
	ctx.r9.s64 = ctx.r9.s64 + 32;
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// cmpwi cr6,r4,6
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 6, ctx.xer);
	// blt cr6,0x825ebf3c
	if (ctx.cr6.lt) goto loc_825EBF3C;
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// addi r11,r11,144
	ctx.r11.s64 = ctx.r11.s64 + 144;
	// cmplw cr6,r31,r10
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r10.u32, ctx.xer);
	// blt cr6,0x825ebf34
	if (ctx.cr6.lt) goto loc_825EBF34;
loc_825EC0F0:
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// cmplwi cr6,r29,2
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 2, ctx.xer);
	// blt cr6,0x825ebf14
	if (ctx.cr6.lt) goto loc_825EBF14;
	// lwz r11,204(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// lwz r10,14788(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 14788);
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// addi r11,r11,-8
	ctx.r11.s64 = ctx.r11.s64 + -8;
	// stw r11,236(r3)
	PPC_STORE_U32(ctx.r3.u32 + 236, ctx.r11.u32);
	// beq cr6,0x825ec128
	if (ctx.cr6.eq) goto loc_825EC128;
	// lwz r11,3760(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3760);
	// lwz r10,220(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r11,3772(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3772, ctx.r11.u32);
loc_825EC128:
	// lwz r9,3688(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3688);
	// lwz r10,140(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// lwz r11,136(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// lwz r8,1772(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1772);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// lwz r10,1780(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1780);
	// stw r9,3716(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3716, ctx.r9.u32);
	// rlwinm r9,r11,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r9,1776(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1776, ctx.r9.u32);
	// stw r11,1784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1784, ctx.r11.u32);
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
}

__attribute__((alias("__imp__sub_825EC160"))) PPC_WEAK_FUNC(sub_825EC160);
PPC_FUNC_IMPL(__imp__sub_825EC160) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba00
	ctx.lr = 0x825EC168;
	sub_8239BA00(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-4424
	ctx.r6.s64 = ctx.r11.s64 + -4424;
	// addi r4,r31,1988
	ctx.r4.s64 = ctx.r31.s64 + 1988;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// bl 0x8263f8c0
	ctx.lr = 0x825EC188;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-8832
	ctx.r6.s64 = ctx.r11.s64 + -8832;
	// addi r4,r31,2000
	ctx.r4.s64 = ctx.r31.s64 + 2000;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC1AC;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,7
	ctx.r7.s64 = 7;
	// addi r6,r11,2064
	ctx.r6.s64 = ctx.r11.s64 + 2064;
	// addi r4,r31,2116
	ctx.r4.s64 = ctx.r31.s64 + 2116;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC1D0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,1544
	ctx.r6.s64 = ctx.r11.s64 + 1544;
	// addi r4,r31,2128
	ctx.r4.s64 = ctx.r31.s64 + 2128;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC1F4;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-16
	ctx.r6.s64 = ctx.r11.s64 + -16;
	// addi r4,r31,2144
	ctx.r4.s64 = ctx.r31.s64 + 2144;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC218;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,504
	ctx.r6.s64 = ctx.r11.s64 + 504;
	// addi r4,r31,2156
	ctx.r4.s64 = ctx.r31.s64 + 2156;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC23C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,1024
	ctx.r6.s64 = ctx.r11.s64 + 1024;
	// addi r4,r31,2168
	ctx.r4.s64 = ctx.r31.s64 + 2168;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC260;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r26,r31,2280
	ctx.r26.s64 = ctx.r31.s64 + 2280;
	// addi r6,r11,4280
	ctx.r6.s64 = ctx.r11.s64 + 4280;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC288;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,2292
	ctx.r27.s64 = ctx.r31.s64 + 2292;
	// addi r6,r11,4544
	ctx.r6.s64 = ctx.r11.s64 + 4544;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC2B0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,2304
	ctx.r29.s64 = ctx.r31.s64 + 2304;
	// addi r6,r11,4808
	ctx.r6.s64 = ctx.r11.s64 + 4808;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC2D8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,2316
	ctx.r30.s64 = ctx.r31.s64 + 2316;
	// addi r6,r11,5072
	ctx.r6.s64 = ctx.r11.s64 + 5072;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC300;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r28,r31,2328
	ctx.r28.s64 = ctx.r31.s64 + 2328;
	// stw r26,2380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2380, ctx.r26.u32);
	// addi r6,r11,5336
	ctx.r6.s64 = ctx.r11.s64 + 5336;
	// stw r27,2384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2384, ctx.r27.u32);
	// li r7,138
	ctx.r7.s64 = 138;
	// stw r29,2388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2388, ctx.r29.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r30,2392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2392, ctx.r30.u32);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC338;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,2340
	ctx.r27.s64 = ctx.r31.s64 + 2340;
	// addi r6,r11,5632
	ctx.r6.s64 = ctx.r11.s64 + 5632;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC360;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,2352
	ctx.r29.s64 = ctx.r31.s64 + 2352;
	// addi r6,r11,5928
	ctx.r6.s64 = ctx.r11.s64 + 5928;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC388;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,2364
	ctx.r30.s64 = ctx.r31.s64 + 2364;
	// addi r6,r11,6224
	ctx.r6.s64 = ctx.r11.s64 + 6224;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC3B0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lwz r11,21596(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21596);
	// stw r28,2396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2396, ctx.r28.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r27,2400(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2400, ctx.r27.u32);
	// stw r29,2404(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2404, ctx.r29.u32);
	// stw r30,2408(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2408, ctx.r30.u32);
	// beq cr6,0x825ec484
	if (ctx.cr6.eq) goto loc_825EC484;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,21652
	ctx.r27.s64 = ctx.r31.s64 + 21652;
	// addi r6,r11,6520
	ctx.r6.s64 = ctx.r11.s64 + 6520;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC3F4;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r28,r31,21664
	ctx.r28.s64 = ctx.r31.s64 + 21664;
	// addi r6,r11,6824
	ctx.r6.s64 = ctx.r11.s64 + 6824;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC41C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,21676
	ctx.r29.s64 = ctx.r31.s64 + 21676;
	// addi r6,r11,7128
	ctx.r6.s64 = ctx.r11.s64 + 7128;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC444;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,21688
	ctx.r30.s64 = ctx.r31.s64 + 21688;
	// addi r6,r11,7432
	ctx.r6.s64 = ctx.r11.s64 + 7432;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC46C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// stw r27,2396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2396, ctx.r27.u32);
	// stw r28,2400(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2400, ctx.r28.u32);
	// stw r29,2404(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2404, ctx.r29.u32);
	// stw r30,2408(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2408, ctx.r30.u32);
loc_825EC484:
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,7736
	ctx.r6.s64 = ctx.r11.s64 + 7736;
	// addi r4,r31,21640
	ctx.r4.s64 = ctx.r31.s64 + 21640;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC4A0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,7928
	ctx.r6.s64 = ctx.r11.s64 + 7928;
	// addi r4,r31,2440
	ctx.r4.s64 = ctx.r31.s64 + 2440;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC4C4;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,7864
	ctx.r6.s64 = ctx.r11.s64 + 7864;
	// addi r4,r31,2452
	ctx.r4.s64 = ctx.r31.s64 + 2452;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC4E8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,7800
	ctx.r6.s64 = ctx.r11.s64 + 7800;
	// addi r4,r31,2464
	ctx.r4.s64 = ctx.r31.s64 + 2464;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC50C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,7992
	ctx.r6.s64 = ctx.r11.s64 + 7992;
	// addi r4,r31,2480
	ctx.r4.s64 = ctx.r31.s64 + 2480;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC530;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,8064
	ctx.r6.s64 = ctx.r11.s64 + 8064;
	// addi r4,r31,2492
	ctx.r4.s64 = ctx.r31.s64 + 2492;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC554;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,8136
	ctx.r6.s64 = ctx.r11.s64 + 8136;
	// addi r4,r31,2504
	ctx.r4.s64 = ctx.r31.s64 + 2504;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC578;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,8204
	ctx.r6.s64 = ctx.r11.s64 + 8204;
	// addi r4,r31,2520
	ctx.r4.s64 = ctx.r31.s64 + 2520;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC59C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,8240
	ctx.r6.s64 = ctx.r11.s64 + 8240;
	// addi r4,r31,2532
	ctx.r4.s64 = ctx.r31.s64 + 2532;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC5C0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,8276
	ctx.r6.s64 = ctx.r11.s64 + 8276;
	// addi r4,r31,2544
	ctx.r4.s64 = ctx.r31.s64 + 2544;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC5E4;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825ece88
	if (!ctx.cr6.eq) goto loc_825ECE88;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r26,r31,20084
	ctx.r26.s64 = ctx.r31.s64 + 20084;
	// addi r6,r11,9176
	ctx.r6.s64 = ctx.r11.s64 + 9176;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC618;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,20096
	ctx.r27.s64 = ctx.r31.s64 + 20096;
	// addi r6,r11,9240
	ctx.r6.s64 = ctx.r11.s64 + 9240;
	// li r7,7
	ctx.r7.s64 = 7;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC640;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,20108
	ctx.r29.s64 = ctx.r31.s64 + 20108;
	// addi r6,r11,9304
	ctx.r6.s64 = ctx.r11.s64 + 9304;
	// li r7,7
	ctx.r7.s64 = 7;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC668;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,20120
	ctx.r30.s64 = ctx.r31.s64 + 20120;
	// addi r6,r11,9368
	ctx.r6.s64 = ctx.r11.s64 + 9368;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC690;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r28,r31,20148
	ctx.r28.s64 = ctx.r31.s64 + 20148;
	// stw r26,20068(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20068, ctx.r26.u32);
	// addi r6,r11,9432
	ctx.r6.s64 = ctx.r11.s64 + 9432;
	// stw r27,20072(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20072, ctx.r27.u32);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r29,20076(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20076, ctx.r29.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r30,20080(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20080, ctx.r30.u32);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC6C8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,20160
	ctx.r27.s64 = ctx.r31.s64 + 20160;
	// addi r6,r11,9472
	ctx.r6.s64 = ctx.r11.s64 + 9472;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC6F0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,20172
	ctx.r29.s64 = ctx.r31.s64 + 20172;
	// addi r6,r11,9512
	ctx.r6.s64 = ctx.r11.s64 + 9512;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC718;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,20184
	ctx.r30.s64 = ctx.r31.s64 + 20184;
	// addi r6,r11,9552
	ctx.r6.s64 = ctx.r11.s64 + 9552;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC740;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r24,r31,20304
	ctx.r24.s64 = ctx.r31.s64 + 20304;
	// stw r28,20132(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20132, ctx.r28.u32);
	// addi r6,r11,9592
	ctx.r6.s64 = ctx.r11.s64 + 9592;
	// stw r27,20136(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20136, ctx.r27.u32);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r29,20140(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20140, ctx.r29.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r30,20144(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20144, ctx.r30.u32);
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC778;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r23,r31,20316
	ctx.r23.s64 = ctx.r31.s64 + 20316;
	// addi r6,r11,9848
	ctx.r6.s64 = ctx.r11.s64 + 9848;
	// li r7,7
	ctx.r7.s64 = 7;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC7A0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r25,r31,20328
	ctx.r25.s64 = ctx.r31.s64 + 20328;
	// addi r6,r11,10104
	ctx.r6.s64 = ctx.r11.s64 + 10104;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC7C8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r26,r31,20340
	ctx.r26.s64 = ctx.r31.s64 + 20340;
	// addi r6,r11,10360
	ctx.r6.s64 = ctx.r11.s64 + 10360;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC7F0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,20352
	ctx.r27.s64 = ctx.r31.s64 + 20352;
	// addi r6,r11,10616
	ctx.r6.s64 = ctx.r11.s64 + 10616;
	// li r7,7
	ctx.r7.s64 = 7;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC818;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r28,r31,20364
	ctx.r28.s64 = ctx.r31.s64 + 20364;
	// addi r6,r11,10872
	ctx.r6.s64 = ctx.r11.s64 + 10872;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC840;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,20376
	ctx.r29.s64 = ctx.r31.s64 + 20376;
	// addi r6,r11,11128
	ctx.r6.s64 = ctx.r11.s64 + 11128;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC868;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,20388
	ctx.r30.s64 = ctx.r31.s64 + 20388;
	// addi r6,r11,11384
	ctx.r6.s64 = ctx.r11.s64 + 11384;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC890;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r24,21016(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21016, ctx.r24.u32);
	// addi r6,r11,11640
	ctx.r6.s64 = ctx.r11.s64 + 11640;
	// stw r23,21020(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21020, ctx.r23.u32);
	// addi r4,r31,20400
	ctx.r4.s64 = ctx.r31.s64 + 20400;
	// stw r25,21024(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21024, ctx.r25.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r26,21028(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21028, ctx.r26.u32);
	// stw r27,21032(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21032, ctx.r27.u32);
	// stw r28,21036(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21036, ctx.r28.u32);
	// stw r29,21040(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21040, ctx.r29.u32);
	// stw r30,21044(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21044, ctx.r30.u32);
	// bl 0x8263f8c0
	ctx.lr = 0x825EC8D4;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,12152
	ctx.r6.s64 = ctx.r11.s64 + 12152;
	// addi r4,r31,20412
	ctx.r4.s64 = ctx.r31.s64 + 20412;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC8F8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,12664
	ctx.r6.s64 = ctx.r11.s64 + 12664;
	// addi r4,r31,20424
	ctx.r4.s64 = ctx.r31.s64 + 20424;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC91C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,13176
	ctx.r6.s64 = ctx.r11.s64 + 13176;
	// addi r4,r31,20436
	ctx.r4.s64 = ctx.r31.s64 + 20436;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC940;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,7
	ctx.r7.s64 = 7;
	// addi r6,r11,13688
	ctx.r6.s64 = ctx.r11.s64 + 13688;
	// addi r4,r31,20448
	ctx.r4.s64 = ctx.r31.s64 + 20448;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC964;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,14200
	ctx.r6.s64 = ctx.r11.s64 + 14200;
	// addi r4,r31,20460
	ctx.r4.s64 = ctx.r31.s64 + 20460;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC988;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,14712
	ctx.r6.s64 = ctx.r11.s64 + 14712;
	// addi r4,r31,20472
	ctx.r4.s64 = ctx.r31.s64 + 20472;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC9AC;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,15224
	ctx.r6.s64 = ctx.r11.s64 + 15224;
	// addi r4,r31,20484
	ctx.r4.s64 = ctx.r31.s64 + 20484;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC9D0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,15736
	ctx.r6.s64 = ctx.r11.s64 + 15736;
	// addi r4,r31,20496
	ctx.r4.s64 = ctx.r31.s64 + 20496;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825EC9F4;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,16032
	ctx.r6.s64 = ctx.r11.s64 + 16032;
	// addi r4,r31,20508
	ctx.r4.s64 = ctx.r31.s64 + 20508;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECA18;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,16328
	ctx.r6.s64 = ctx.r11.s64 + 16328;
	// addi r4,r31,20520
	ctx.r4.s64 = ctx.r31.s64 + 20520;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECA3C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,16624
	ctx.r6.s64 = ctx.r11.s64 + 16624;
	// addi r4,r31,20532
	ctx.r4.s64 = ctx.r31.s64 + 20532;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECA60;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r22,r31,20544
	ctx.r22.s64 = ctx.r31.s64 + 20544;
	// addi r6,r11,8380
	ctx.r6.s64 = ctx.r11.s64 + 8380;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECA88;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r23,r31,20556
	ctx.r23.s64 = ctx.r31.s64 + 20556;
	// addi r6,r11,8476
	ctx.r6.s64 = ctx.r11.s64 + 8476;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECAB0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r24,r31,20568
	ctx.r24.s64 = ctx.r31.s64 + 20568;
	// addi r6,r11,8572
	ctx.r6.s64 = ctx.r11.s64 + 8572;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECAD8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r26,r31,20580
	ctx.r26.s64 = ctx.r31.s64 + 20580;
	// addi r6,r11,8668
	ctx.r6.s64 = ctx.r11.s64 + 8668;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECB00;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,20592
	ctx.r27.s64 = ctx.r31.s64 + 20592;
	// addi r6,r11,8696
	ctx.r6.s64 = ctx.r11.s64 + 8696;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECB28;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r28,r31,20604
	ctx.r28.s64 = ctx.r31.s64 + 20604;
	// addi r6,r11,8724
	ctx.r6.s64 = ctx.r11.s64 + 8724;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECB50;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,20616
	ctx.r29.s64 = ctx.r31.s64 + 20616;
	// addi r6,r11,8752
	ctx.r6.s64 = ctx.r11.s64 + 8752;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECB78;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,20628
	ctx.r30.s64 = ctx.r31.s64 + 20628;
	// addi r6,r11,8780
	ctx.r6.s64 = ctx.r11.s64 + 8780;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECBA0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r25,r31,20640
	ctx.r25.s64 = ctx.r31.s64 + 20640;
	// stw r22,20232(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20232, ctx.r22.u32);
	// addi r6,r11,8808
	ctx.r6.s64 = ctx.r11.s64 + 8808;
	// stw r23,20236(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20236, ctx.r23.u32);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r24,20240(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20240, ctx.r24.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r26,20244(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20244, ctx.r26.u32);
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// stw r27,20248(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20248, ctx.r27.u32);
	// stw r28,20252(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20252, ctx.r28.u32);
	// stw r29,20256(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20256, ctx.r29.u32);
	// stw r30,20260(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20260, ctx.r30.u32);
	// bl 0x8263f8c0
	ctx.lr = 0x825ECBE8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r22,r31,20652
	ctx.r22.s64 = ctx.r31.s64 + 20652;
	// addi r6,r11,8844
	ctx.r6.s64 = ctx.r11.s64 + 8844;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECC10;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r23,r31,20664
	ctx.r23.s64 = ctx.r31.s64 + 20664;
	// addi r6,r11,8880
	ctx.r6.s64 = ctx.r11.s64 + 8880;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECC38;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r24,r31,20676
	ctx.r24.s64 = ctx.r31.s64 + 20676;
	// addi r6,r11,8916
	ctx.r6.s64 = ctx.r11.s64 + 8916;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r24
	ctx.r4.u64 = ctx.r24.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECC60;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r26,r31,20688
	ctx.r26.s64 = ctx.r31.s64 + 20688;
	// addi r6,r11,8952
	ctx.r6.s64 = ctx.r11.s64 + 8952;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECC88;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,20700
	ctx.r27.s64 = ctx.r31.s64 + 20700;
	// addi r6,r11,8988
	ctx.r6.s64 = ctx.r11.s64 + 8988;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECCB0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r28,r31,20712
	ctx.r28.s64 = ctx.r31.s64 + 20712;
	// addi r6,r11,9024
	ctx.r6.s64 = ctx.r11.s64 + 9024;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECCD8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,20724
	ctx.r29.s64 = ctx.r31.s64 + 20724;
	// addi r6,r11,9060
	ctx.r6.s64 = ctx.r11.s64 + 9060;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECD00;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,20736
	ctx.r30.s64 = ctx.r31.s64 + 20736;
	// stw r25,20200(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20200, ctx.r25.u32);
	// addi r6,r11,8312
	ctx.r6.s64 = ctx.r11.s64 + 8312;
	// stw r22,20204(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20204, ctx.r22.u32);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r23,20208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20208, ctx.r23.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r24,20212(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20212, ctx.r24.u32);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// stw r26,20216(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20216, ctx.r26.u32);
	// stw r27,20220(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20220, ctx.r27.u32);
	// stw r28,20224(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20224, ctx.r28.u32);
	// stw r29,20228(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20228, ctx.r29.u32);
	// bl 0x8263f8c0
	ctx.lr = 0x825ECD48;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r26,r31,20748
	ctx.r26.s64 = ctx.r31.s64 + 20748;
	// addi r6,r11,8408
	ctx.r6.s64 = ctx.r11.s64 + 8408;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECD70;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r28,r31,20760
	ctx.r28.s64 = ctx.r31.s64 + 20760;
	// addi r6,r11,8504
	ctx.r6.s64 = ctx.r11.s64 + 8504;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECD98;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,20772
	ctx.r29.s64 = ctx.r31.s64 + 20772;
	// addi r6,r11,8600
	ctx.r6.s64 = ctx.r11.s64 + 8600;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECDC0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,20784
	ctx.r27.s64 = ctx.r31.s64 + 20784;
	// stw r30,20268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20268, ctx.r30.u32);
	// addi r6,r11,9096
	ctx.r6.s64 = ctx.r11.s64 + 9096;
	// stw r26,20272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20272, ctx.r26.u32);
	// li r7,6
	ctx.r7.s64 = 6;
	// stw r28,20276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20276, ctx.r28.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r29,20280(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20280, ctx.r29.u32);
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECDF8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r28,r31,20796
	ctx.r28.s64 = ctx.r31.s64 + 20796;
	// addi r6,r11,9116
	ctx.r6.s64 = ctx.r11.s64 + 9116;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECE20;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,20808
	ctx.r29.s64 = ctx.r31.s64 + 20808;
	// addi r6,r11,9136
	ctx.r6.s64 = ctx.r11.s64 + 9136;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECE48;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,20820
	ctx.r30.s64 = ctx.r31.s64 + 20820;
	// addi r6,r11,9156
	ctx.r6.s64 = ctx.r11.s64 + 9156;
	// li r7,6
	ctx.r7.s64 = 6;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECE70;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// stw r27,20288(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20288, ctx.r27.u32);
	// stw r28,20292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20292, ctx.r28.u32);
	// stw r29,20296(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20296, ctx.r29.u32);
	// stw r30,20300(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20300, ctx.r30.u32);
loc_825ECE88:
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,2328
	ctx.r6.s64 = ctx.r11.s64 + 2328;
	// addi r4,r31,2040
	ctx.r4.s64 = ctx.r31.s64 + 2040;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECEA4;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,2816
	ctx.r6.s64 = ctx.r11.s64 + 2816;
	// addi r4,r31,2052
	ctx.r4.s64 = ctx.r31.s64 + 2052;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECEC8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,3304
	ctx.r6.s64 = ctx.r11.s64 + 3304;
	// addi r4,r31,2064
	ctx.r4.s64 = ctx.r31.s64 + 2064;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECEEC;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,7
	ctx.r7.s64 = 7;
	// addi r6,r11,3792
	ctx.r6.s64 = ctx.r11.s64 + 3792;
	// addi r4,r31,2076
	ctx.r4.s64 = ctx.r31.s64 + 2076;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECF10;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-13592
	ctx.r6.s64 = ctx.r11.s64 + -13592;
	// addi r4,r31,2180
	ctx.r4.s64 = ctx.r31.s64 + 2180;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECF34;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-12912
	ctx.r6.s64 = ctx.r11.s64 + -12912;
	// addi r4,r31,2192
	ctx.r4.s64 = ctx.r31.s64 + 2192;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECF58;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-12160
	ctx.r6.s64 = ctx.r11.s64 + -12160;
	// addi r4,r31,2204
	ctx.r4.s64 = ctx.r31.s64 + 2204;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECF7C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-11560
	ctx.r6.s64 = ctx.r11.s64 + -11560;
	// addi r4,r31,2216
	ctx.r4.s64 = ctx.r31.s64 + 2216;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECFA0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-11024
	ctx.r6.s64 = ctx.r11.s64 + -11024;
	// addi r4,r31,2228
	ctx.r4.s64 = ctx.r31.s64 + 2228;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECFC4;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-10608
	ctx.r6.s64 = ctx.r11.s64 + -10608;
	// addi r4,r31,2240
	ctx.r4.s64 = ctx.r31.s64 + 2240;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ECFE8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-10192
	ctx.r6.s64 = ctx.r11.s64 + -10192;
	// addi r4,r31,2428
	ctx.r4.s64 = ctx.r31.s64 + 2428;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ED00C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed038
	if (!ctx.cr6.eq) goto loc_825ED038;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-9488
	ctx.r6.s64 = ctx.r11.s64 + -9488;
	// addi r4,r31,2252
	ctx.r4.s64 = ctx.r31.s64 + 2252;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825ED030;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825ed044
	if (ctx.cr6.eq) goto loc_825ED044;
loc_825ED038:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825ED044:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
}

__attribute__((alias("__imp__sub_825ED050"))) PPC_WEAK_FUNC(sub_825ED050);
PPC_FUNC_IMPL(__imp__sub_825ED050) {
	PPC_FUNC_PROLOGUE();
	// li r11,0
	ctx.r11.s64 = 0;
	// li r10,1
	ctx.r10.s64 = 1;
	// li r9,-1
	ctx.r9.s64 = -1;
	// stw r11,20956(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20956, ctx.r11.u32);
	// stw r11,3428(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3428, ctx.r11.u32);
	// stw r11,3440(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3440, ctx.r11.u32);
	// stw r11,3436(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3436, ctx.r11.u32);
	// stw r11,20840(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20840, ctx.r11.u32);
	// stw r11,21164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21164, ctx.r11.u32);
	// stw r10,20972(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20972, ctx.r10.u32);
	// stw r11,3448(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3448, ctx.r11.u32);
	// stw r9,20872(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20872, ctx.r9.u32);
	// stw r11,404(r3)
	PPC_STORE_U32(ctx.r3.u32 + 404, ctx.r11.u32);
	// stw r11,20960(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20960, ctx.r11.u32);
	// stw r11,20968(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20968, ctx.r11.u32);
	// stw r11,20964(r3)
	PPC_STORE_U32(ctx.r3.u32 + 20964, ctx.r11.u32);
	// stw r11,3964(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3964, ctx.r11.u32);
	// stw r11,21080(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21080, ctx.r11.u32);
	// stw r11,336(r3)
	PPC_STORE_U32(ctx.r3.u32 + 336, ctx.r11.u32);
	// stw r11,328(r3)
	PPC_STORE_U32(ctx.r3.u32 + 328, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825ED0A4"))) PPC_WEAK_FUNC(sub_825ED0A4);
PPC_FUNC_IMPL(__imp__sub_825ED0A4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825ED0A8"))) PPC_WEAK_FUNC(sub_825ED0A8);
PPC_FUNC_IMPL(__imp__sub_825ED0A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// li r11,1
	ctx.r11.s64 = 1;
	// li r10,-1
	ctx.r10.s64 = -1;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// stw r30,20956(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20956, ctx.r30.u32);
	// li r5,0
	ctx.r5.s64 = 0;
	// stw r30,3428(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3428, ctx.r30.u32);
	// li r4,0
	ctx.r4.s64 = 0;
	// stw r30,3440(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3440, ctx.r30.u32);
	// stw r30,3436(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3436, ctx.r30.u32);
	// stw r30,20840(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20840, ctx.r30.u32);
	// stw r30,21164(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21164, ctx.r30.u32);
	// stw r11,20972(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20972, ctx.r11.u32);
	// stw r30,3448(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3448, ctx.r30.u32);
	// stw r10,20872(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20872, ctx.r10.u32);
	// stw r30,404(r31)
	PPC_STORE_U32(ctx.r31.u32 + 404, ctx.r30.u32);
	// stw r30,20960(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20960, ctx.r30.u32);
	// stw r30,20968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20968, ctx.r30.u32);
	// stw r30,20964(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20964, ctx.r30.u32);
	// stw r30,3964(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3964, ctx.r30.u32);
	// stw r30,21080(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21080, ctx.r30.u32);
	// stw r30,336(r31)
	PPC_STORE_U32(ctx.r31.u32 + 336, ctx.r30.u32);
	// stw r30,328(r31)
	PPC_STORE_U32(ctx.r31.u32 + 328, ctx.r30.u32);
	// stw r30,3672(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3672, ctx.r30.u32);
	// stw r30,3668(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3668, ctx.r30.u32);
	// bl 0x825d8910
	ctx.lr = 0x825ED130;
	sub_825D8910(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ed17c
	if (!ctx.cr6.eq) goto loc_825ED17C;
	// lwz r10,21356(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21356);
	// lwz r11,21352(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21352);
	// lwz r9,21176(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21176);
	// mullw r8,r11,r10
	ctx.r8.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// stw r30,3676(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3676, ctx.r30.u32);
	// stw r30,21236(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21236, ctx.r30.u32);
	// cmpw cr6,r8,r9
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, ctx.xer);
	// bgt cr6,0x825ed178
	if (ctx.cr6.gt) goto loc_825ED178;
	// lwz r9,21192(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21192);
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// bgt cr6,0x825ed178
	if (ctx.cr6.gt) goto loc_825ED178;
	// lwz r11,21196(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21196);
	// cmpw cr6,r10,r11
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r11.s32, ctx.xer);
	// bgt cr6,0x825ed178
	if (ctx.cr6.gt) goto loc_825ED178;
	// stw r30,21180(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21180, ctx.r30.u32);
	// b 0x825ed17c
	goto loc_825ED17C;
loc_825ED178:
	// li r3,14
	ctx.r3.s64 = 14;
loc_825ED17C:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825ED194"))) PPC_WEAK_FUNC(sub_825ED194);
PPC_FUNC_IMPL(__imp__sub_825ED194) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825ED198"))) PPC_WEAK_FUNC(sub_825ED198);
PPC_FUNC_IMPL(__imp__sub_825ED198) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,19976(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19976);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ed210
	if (ctx.cr6.eq) goto loc_825ED210;
	// lwz r11,19980(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19980);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ed1fc
	if (ctx.cr6.eq) goto loc_825ED1FC;
	// addi r7,r3,20448
	ctx.r7.s64 = ctx.r3.s64 + 20448;
	// lwz r11,20996(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20996);
	// addi r6,r3,20460
	ctx.r6.s64 = ctx.r3.s64 + 20460;
	// addi r5,r3,20472
	ctx.r5.s64 = ctx.r3.s64 + 20472;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r7,2412(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2412, ctx.r7.u32);
	// stw r6,2416(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2416, ctx.r6.u32);
	// stw r5,2420(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2420, ctx.r5.u32);
	// beq cr6,0x825ed1e8
	if (ctx.cr6.eq) goto loc_825ED1E8;
	// addi r11,r3,20400
	ctx.r11.s64 = ctx.r3.s64 + 20400;
	// addi r10,r3,20412
	ctx.r10.s64 = ctx.r3.s64 + 20412;
	// addi r9,r3,20424
	ctx.r9.s64 = ctx.r3.s64 + 20424;
	// addi r8,r3,20436
	ctx.r8.s64 = ctx.r3.s64 + 20436;
	// b 0x825ed25c
	goto loc_825ED25C;
loc_825ED1E8:
	// addi r11,r3,20496
	ctx.r11.s64 = ctx.r3.s64 + 20496;
	// addi r10,r3,20508
	ctx.r10.s64 = ctx.r3.s64 + 20508;
	// addi r9,r3,20520
	ctx.r9.s64 = ctx.r3.s64 + 20520;
	// addi r8,r3,20532
	ctx.r8.s64 = ctx.r3.s64 + 20532;
	// b 0x825ed25c
	goto loc_825ED25C;
loc_825ED1FC:
	// addi r11,r3,20496
	ctx.r11.s64 = ctx.r3.s64 + 20496;
	// addi r10,r3,20508
	ctx.r10.s64 = ctx.r3.s64 + 20508;
	// addi r9,r3,20520
	ctx.r9.s64 = ctx.r3.s64 + 20520;
	// addi r8,r3,20532
	ctx.r8.s64 = ctx.r3.s64 + 20532;
	// b 0x825ed244
	goto loc_825ED244;
loc_825ED210:
	// lwz r11,21596(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 21596);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ed234
	if (ctx.cr6.eq) goto loc_825ED234;
	// addi r10,r3,21664
	ctx.r10.s64 = ctx.r3.s64 + 21664;
	// addi r11,r3,21652
	ctx.r11.s64 = ctx.r3.s64 + 21652;
	// addi r9,r3,21676
	ctx.r9.s64 = ctx.r3.s64 + 21676;
	// addi r8,r3,21688
	ctx.r8.s64 = ctx.r3.s64 + 21688;
	// stw r10,2400(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2400, ctx.r10.u32);
	// b 0x825ed268
	goto loc_825ED268;
loc_825ED234:
	// addi r11,r3,2328
	ctx.r11.s64 = ctx.r3.s64 + 2328;
	// addi r10,r3,2340
	ctx.r10.s64 = ctx.r3.s64 + 2340;
	// addi r9,r3,2352
	ctx.r9.s64 = ctx.r3.s64 + 2352;
	// addi r8,r3,2364
	ctx.r8.s64 = ctx.r3.s64 + 2364;
loc_825ED244:
	// addi r5,r3,20472
	ctx.r5.s64 = ctx.r3.s64 + 20472;
	// addi r6,r3,20460
	ctx.r6.s64 = ctx.r3.s64 + 20460;
	// addi r7,r3,20448
	ctx.r7.s64 = ctx.r3.s64 + 20448;
	// stw r5,2420(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2420, ctx.r5.u32);
	// stw r6,2416(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2416, ctx.r6.u32);
	// stw r7,2412(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2412, ctx.r7.u32);
loc_825ED25C:
	// stw r10,2400(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2400, ctx.r10.u32);
	// addi r10,r3,20484
	ctx.r10.s64 = ctx.r3.s64 + 20484;
	// stw r10,2424(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2424, ctx.r10.u32);
loc_825ED268:
	// stw r11,2396(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2396, ctx.r11.u32);
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// stw r9,2404(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2404, ctx.r9.u32);
	// stw r8,2408(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2408, ctx.r8.u32);
	// stw r11,2376(r3)
	PPC_STORE_U32(ctx.r3.u32 + 2376, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825ED280"))) PPC_WEAK_FUNC(sub_825ED280);
PPC_FUNC_IMPL(__imp__sub_825ED280) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825ED288;
	sub_8239BA1C(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r8,16
	ctx.r8.s64 = 16;
	// li r10,32
	ctx.r10.s64 = 32;
	// lwz r9,180(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// stw r8,19700(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19700, ctx.r8.u32);
	// lwz r7,192(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// lwz r5,200(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// lwz r8,188(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// lwz r11,20056(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20056);
	// stw r10,19696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19696, ctx.r10.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// stw r9,164(r31)
	PPC_STORE_U32(ctx.r31.u32 + 164, ctx.r9.u32);
	// stw r7,168(r31)
	PPC_STORE_U32(ctx.r31.u32 + 168, ctx.r7.u32);
	// stw r5,176(r31)
	PPC_STORE_U32(ctx.r31.u32 + 176, ctx.r5.u32);
	// stw r8,172(r31)
	PPC_STORE_U32(ctx.r31.u32 + 172, ctx.r8.u32);
	// beq cr6,0x825ed2fc
	if (ctx.cr6.eq) goto loc_825ED2FC;
	// lwz r10,156(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// lwz r11,160(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// addi r6,r11,1
	ctx.r6.s64 = ctx.r11.s64 + 1;
	// srawi r11,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r10.s32 >> 1;
	// srawi r10,r6,1
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r6.s32 >> 1;
	// rlwinm r6,r11,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r4,r10,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,168(r31)
	PPC_STORE_U32(ctx.r31.u32 + 168, ctx.r11.u32);
	// stw r10,176(r31)
	PPC_STORE_U32(ctx.r31.u32 + 176, ctx.r10.u32);
	// stw r6,164(r31)
	PPC_STORE_U32(ctx.r31.u32 + 164, ctx.r6.u32);
	// stw r4,172(r31)
	PPC_STORE_U32(ctx.r31.u32 + 172, ctx.r4.u32);
loc_825ED2FC:
	// lwz r10,164(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 164);
	// lwz r11,168(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 168);
	// addi r10,r10,32
	ctx.r10.s64 = ctx.r10.s64 + 32;
	// lwz r6,156(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 156);
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// cmpw cr6,r9,r6
	ctx.cr6.compare<int32_t>(ctx.r9.s32, ctx.r6.s32, ctx.xer);
	// stw r10,184(r31)
	PPC_STORE_U32(ctx.r31.u32 + 184, ctx.r10.u32);
	// stw r11,196(r31)
	PPC_STORE_U32(ctx.r31.u32 + 196, ctx.r11.u32);
	// bne cr6,0x825ed330
	if (!ctx.cr6.eq) goto loc_825ED330;
	// lwz r11,160(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 160);
	// li r30,1
	ctx.r30.s64 = 1;
	// cmpw cr6,r8,r11
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825ed334
	if (ctx.cr6.eq) goto loc_825ED334;
loc_825ED330:
	// li r30,0
	ctx.r30.s64 = 0;
loc_825ED334:
	// addi r11,r9,64
	ctx.r11.s64 = ctx.r9.s64 + 64;
	// lwz r29,21184(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21184);
	// srawi r9,r9,4
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 4;
	// stw r30,152(r31)
	PPC_STORE_U32(ctx.r31.u32 + 152, ctx.r30.u32);
	// addi r10,r7,32
	ctx.r10.s64 = ctx.r7.s64 + 32;
	// lwz r3,3732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3732);
	// srawi r7,r8,4
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xF) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 4;
	// lwz r4,3760(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3760);
	// cmpwi cr6,r29,1
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 1, ctx.xer);
	// mullw r29,r7,r9
	ctx.r29.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r9.s32);
	// stw r11,204(r31)
	PPC_STORE_U32(ctx.r31.u32 + 204, ctx.r11.u32);
	// stw r9,136(r31)
	PPC_STORE_U32(ctx.r31.u32 + 136, ctx.r9.u32);
	// stw r10,208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 208, ctx.r10.u32);
	// stw r7,140(r31)
	PPC_STORE_U32(ctx.r31.u32 + 140, ctx.r7.u32);
	// stw r29,144(r31)
	PPC_STORE_U32(ctx.r31.u32 + 144, ctx.r29.u32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// addi r6,r8,64
	ctx.r6.s64 = ctx.r8.s64 + 64;
	// addi r8,r11,1
	ctx.r8.s64 = ctx.r11.s64 + 1;
	// addi r30,r10,1
	ctx.r30.s64 = ctx.r10.s64 + 1;
	// rlwinm r8,r8,5,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 5) & 0xFFFFFFE0;
	// addi r5,r5,32
	ctx.r5.s64 = ctx.r5.s64 + 32;
	// stw r9,148(r31)
	PPC_STORE_U32(ctx.r31.u32 + 148, ctx.r9.u32);
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r30,r30,4,0,27
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r6,212(r31)
	PPC_STORE_U32(ctx.r31.u32 + 212, ctx.r6.u32);
	// rlwinm r29,r11,4,0,27
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r4,r4,r8
	ctx.r4.u64 = ctx.r4.u64 + ctx.r8.u64;
	// stw r8,220(r31)
	PPC_STORE_U32(ctx.r31.u32 + 220, ctx.r8.u32);
	// stw r5,216(r31)
	PPC_STORE_U32(ctx.r31.u32 + 216, ctx.r5.u32);
	// stw r9,232(r31)
	PPC_STORE_U32(ctx.r31.u32 + 232, ctx.r9.u32);
	// add r9,r3,r8
	ctx.r9.u64 = ctx.r3.u64 + ctx.r8.u64;
	// stw r30,224(r31)
	PPC_STORE_U32(ctx.r31.u32 + 224, ctx.r30.u32);
	// stw r29,228(r31)
	PPC_STORE_U32(ctx.r31.u32 + 228, ctx.r29.u32);
	// stw r4,3772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3772, ctx.r4.u32);
	// stw r9,3756(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3756, ctx.r9.u32);
	// bne cr6,0x825ed3dc
	if (!ctx.cr6.eq) goto loc_825ED3DC;
	// lwz r9,14772(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x825ed3dc
	if (!ctx.cr6.gt) goto loc_825ED3DC;
	// ld r9,3576(r31)
	ctx.r9.u64 = PPC_LOAD_U64(ctx.r31.u32 + 3576);
	// cmpdi cr6,r9,1
	ctx.cr6.compare<int64_t>(ctx.r9.s64, 1, ctx.xer);
	// bgt cr6,0x825ed3fc
	if (ctx.cr6.gt) goto loc_825ED3FC;
loc_825ED3DC:
	// rlwinm r9,r11,4,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r11,96(r31)
	PPC_STORE_U32(ctx.r31.u32 + 96, ctx.r11.u32);
	// rlwinm r8,r10,3,0,28
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r10,108(r31)
	PPC_STORE_U32(ctx.r31.u32 + 108, ctx.r10.u32);
	// stw r6,104(r31)
	PPC_STORE_U32(ctx.r31.u32 + 104, ctx.r6.u32);
	// stw r5,116(r31)
	PPC_STORE_U32(ctx.r31.u32 + 116, ctx.r5.u32);
	// stw r9,100(r31)
	PPC_STORE_U32(ctx.r31.u32 + 100, ctx.r9.u32);
	// stw r8,112(r31)
	PPC_STORE_U32(ctx.r31.u32 + 112, ctx.r8.u32);
loc_825ED3FC:
	// lwz r11,3356(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3356);
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// bne cr6,0x825ed424
	if (!ctx.cr6.eq) goto loc_825ED424;
	// cmplwi cr6,r7,4
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 4, ctx.xer);
	// bge cr6,0x825ed43c
	if (!ctx.cr6.lt) goto loc_825ED43C;
	// li r11,2
	ctx.r11.s64 = 2;
	// subfc r11,r11,r7
	ctx.xer.ca = ctx.r7.u32 >= ctx.r11.u32;
	ctx.r11.s64 = ctx.r7.s64 - ctx.r11.s64;
	// subfe r11,r11,r11
	temp.u8 = (~ctx.r11.u32 + ctx.r11.u32 < ~ctx.r11.u32) | (~ctx.r11.u32 + ctx.r11.u32 + ctx.xer.ca < ctx.xer.ca);
	ctx.r11.u64 = ~ctx.r11.u64 + ctx.r11.u64 + ctx.xer.ca;
	ctx.xer.ca = temp.u8;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// b 0x825ed438
	goto loc_825ED438;
loc_825ED424:
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// bne cr6,0x825ed43c
	if (!ctx.cr6.eq) goto loc_825ED43C;
	// cmplwi cr6,r7,1
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 1, ctx.xer);
	// bne cr6,0x825ed43c
	if (!ctx.cr6.eq) goto loc_825ED43C;
	// li r11,1
	ctx.r11.s64 = 1;
loc_825ED438:
	// stw r11,3356(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3356, ctx.r11.u32);
loc_825ED43C:
	// li r11,3
	ctx.r11.s64 = 3;
	// li r10,10
	ctx.r10.s64 = 10;
	// li r9,64
	ctx.r9.s64 = 64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,2264(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2264, ctx.r11.u32);
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r10,2268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2268, ctx.r10.u32);
	// stw r9,2272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2272, ctx.r9.u32);
	// stw r11,2276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2276, ctx.r11.u32);
	// bl 0x825f19d8
	ctx.lr = 0x825ED464;
	sub_825F19D8(ctx, base);
	// lwz r11,188(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// srawi r11,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 4;
	// addi r10,r11,1
	ctx.r10.s64 = ctx.r11.s64 + 1;
	// lwz r11,21276(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21276);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// stw r10,21276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21276, ctx.r10.u32);
	// bge cr6,0x825ed4c8
	if (!ctx.cr6.lt) goto loc_825ED4C8;
	// subf r10,r11,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r11.s64;
	// rlwinm r30,r11,2,0,29
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// lwz r11,21240(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// rlwinm r29,r10,2,0,29
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r11,r30
	ctx.r3.u64 = ctx.r11.u64 + ctx.r30.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// bl 0x8239ca70
	ctx.lr = 0x825ED4A0;
	sub_8239CA70(ctx, base);
	// lwz r11,21252(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21252);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r11,r30
	ctx.r3.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x8239ca70
	ctx.lr = 0x825ED4B4;
	sub_8239CA70(ctx, base);
	// lwz r11,21268(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21268);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r3,r11,r30
	ctx.r3.u64 = ctx.r11.u64 + ctx.r30.u64;
	// bl 0x8239ca70
	ctx.lr = 0x825ED4C8;
	sub_8239CA70(ctx, base);
loc_825ED4C8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825ED4D0"))) PPC_WEAK_FUNC(sub_825ED4D0);
PPC_FUNC_IMPL(__imp__sub_825ED4D0) {
	PPC_FUNC_PROLOGUE();
	// li r6,12
	ctx.r6.s64 = 12;
	// stw r4,404(r3)
	PPC_STORE_U32(ctx.r3.u32 + 404, ctx.r4.u32);
	// li r10,10
	ctx.r10.s64 = 10;
	// li r9,9
	ctx.r9.s64 = 9;
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r1,-32
	ctx.r8.s64 = ctx.r1.s64 + -32;
	// stw r6,-24(r1)
	PPC_STORE_U32(ctx.r1.u32 + -24, ctx.r6.u32);
	// li r6,13
	ctx.r6.s64 = 13;
	// stw r10,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r10.u32);
	// addi r7,r1,-16
	ctx.r7.s64 = ctx.r1.s64 + -16;
	// stw r10,-28(r1)
	PPC_STORE_U32(ctx.r1.u32 + -28, ctx.r10.u32);
	// li r10,11
	ctx.r10.s64 = 11;
	// stw r9,-32(r1)
	PPC_STORE_U32(ctx.r1.u32 + -32, ctx.r9.u32);
	// stw r9,-12(r1)
	PPC_STORE_U32(ctx.r1.u32 + -12, ctx.r9.u32);
	// li r9,1
	ctx.r9.s64 = 1;
	// stw r6,-20(r1)
	PPC_STORE_U32(ctx.r1.u32 + -20, ctx.r6.u32);
	// li r6,8
	ctx.r6.s64 = 8;
	// stw r10,-4(r1)
	PPC_STORE_U32(ctx.r1.u32 + -4, ctx.r10.u32);
	// lwzx r10,r11,r8
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r8.u32);
	// stw r6,-16(r1)
	PPC_STORE_U32(ctx.r1.u32 + -16, ctx.r6.u32);
	// addi r8,r10,-1
	ctx.r8.s64 = ctx.r10.s64 + -1;
	// lwzx r11,r11,r7
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r7.u32);
	// addi r7,r11,-1
	ctx.r7.s64 = ctx.r11.s64 + -1;
	// stw r10,408(r3)
	PPC_STORE_U32(ctx.r3.u32 + 408, ctx.r10.u32);
	// stw r11,412(r3)
	PPC_STORE_U32(ctx.r3.u32 + 412, ctx.r11.u32);
	// slw r11,r9,r8
	ctx.r11.u64 = ctx.r8.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r8.u8 & 0x3F));
	// rlwinm r8,r11,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// slw r10,r9,r7
	ctx.r10.u64 = ctx.r7.u8 & 0x20 ? 0 : (ctx.r9.u32 << (ctx.r7.u8 & 0x3F));
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// rlwinm r9,r10,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,416(r3)
	PPC_STORE_U32(ctx.r3.u32 + 416, ctx.r11.u32);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r10,420(r3)
	PPC_STORE_U32(ctx.r3.u32 + 420, ctx.r10.u32);
	// stw r8,424(r3)
	PPC_STORE_U32(ctx.r3.u32 + 424, ctx.r8.u32);
	// stw r9,428(r3)
	PPC_STORE_U32(ctx.r3.u32 + 428, ctx.r9.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825ED560"))) PPC_WEAK_FUNC(sub_825ED560);
PPC_FUNC_IMPL(__imp__sub_825ED560) {
	PPC_FUNC_PROLOGUE();
	// lis r6,-32158
	ctx.r6.s64 = -2107506688;
	// lwz r5,3924(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3924);
	// lis r7,-32158
	ctx.r7.s64 = -2107506688;
	// lis r8,-32158
	ctx.r8.s64 = -2107506688;
	// lis r9,-32158
	ctx.r9.s64 = -2107506688;
	// lis r10,-32158
	ctx.r10.s64 = -2107506688;
	// lis r11,-32158
	ctx.r11.s64 = -2107506688;
	// addi r6,r6,-22536
	ctx.r6.s64 = ctx.r6.s64 + -22536;
	// addi r7,r7,-20584
	ctx.r7.s64 = ctx.r7.s64 + -20584;
	// addi r8,r8,-21528
	ctx.r8.s64 = ctx.r8.s64 + -21528;
	// addi r9,r9,-19720
	ctx.r9.s64 = ctx.r9.s64 + -19720;
	// addi r10,r10,-19200
	ctx.r10.s64 = ctx.r10.s64 + -19200;
	// addi r11,r11,-17416
	ctx.r11.s64 = ctx.r11.s64 + -17416;
	// stw r6,3160(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3160, ctx.r6.u32);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// stw r7,3164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3164, ctx.r7.u32);
	// stw r8,3180(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3180, ctx.r8.u32);
	// stw r9,3168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3168, ctx.r9.u32);
	// stw r10,3172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3172, ctx.r10.u32);
	// stw r11,3176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3176, ctx.r11.u32);
	// beq cr6,0x825ed5d8
	if (ctx.cr6.eq) goto loc_825ED5D8;
	// lis r9,-32158
	ctx.r9.s64 = -2107506688;
	// lis r10,-32158
	ctx.r10.s64 = -2107506688;
	// lis r11,-32158
	ctx.r11.s64 = -2107506688;
	// addi r9,r9,-21528
	ctx.r9.s64 = ctx.r9.s64 + -21528;
	// addi r10,r10,-18616
	ctx.r10.s64 = ctx.r10.s64 + -18616;
	// addi r11,r11,-18048
	ctx.r11.s64 = ctx.r11.s64 + -18048;
	// stw r9,3164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3164, ctx.r9.u32);
	// stw r10,3168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3168, ctx.r10.u32);
	// stw r11,3172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3172, ctx.r11.u32);
loc_825ED5D8:
	// lwz r11,1788(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1788);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ed634
	if (ctx.cr6.eq) goto loc_825ED634;
	// lis r6,-32156
	ctx.r6.s64 = -2107375616;
	// lis r7,-32156
	ctx.r7.s64 = -2107375616;
	// lis r8,-32156
	ctx.r8.s64 = -2107375616;
	// lis r9,-32156
	ctx.r9.s64 = -2107375616;
	// lis r10,-32156
	ctx.r10.s64 = -2107375616;
	// lis r11,-32156
	ctx.r11.s64 = -2107375616;
	// li r5,1
	ctx.r5.s64 = 1;
	// addi r6,r6,-440
	ctx.r6.s64 = ctx.r6.s64 + -440;
	// addi r7,r7,-1408
	ctx.r7.s64 = ctx.r7.s64 + -1408;
	// addi r8,r8,744
	ctx.r8.s64 = ctx.r8.s64 + 744;
	// addi r9,r9,1712
	ctx.r9.s64 = ctx.r9.s64 + 1712;
	// addi r10,r10,2672
	ctx.r10.s64 = ctx.r10.s64 + 2672;
	// stw r5,1796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1796, ctx.r5.u32);
	// addi r11,r11,-1408
	ctx.r11.s64 = ctx.r11.s64 + -1408;
	// stw r6,3160(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3160, ctx.r6.u32);
	// stw r7,3164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3164, ctx.r7.u32);
	// stw r8,3168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3168, ctx.r8.u32);
	// stw r9,3172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3172, ctx.r9.u32);
	// stw r10,3176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3176, ctx.r10.u32);
	// stw r11,3180(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3180, ctx.r11.u32);
loc_825ED634:
	// lwz r11,1796(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1796);
	// li r10,8
	ctx.r10.s64 = 8;
	// li r9,3
	ctx.r9.s64 = 3;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r11,0
	ctx.r11.s64 = 0;
	// beq cr6,0x825ed660
	if (ctx.cr6.eq) goto loc_825ED660;
	// stw r11,1916(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1916, ctx.r11.u32);
	// stw r10,1920(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1920, ctx.r10.u32);
	// stw r11,1924(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1924, ctx.r11.u32);
	// stw r9,1928(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1928, ctx.r9.u32);
	// blr 
	return;
loc_825ED660:
	// stw r10,1916(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1916, ctx.r10.u32);
	// stw r11,1920(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1920, ctx.r11.u32);
	// stw r9,1924(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1924, ctx.r9.u32);
	// stw r11,1928(r3)
	PPC_STORE_U32(ctx.r3.u32 + 1928, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825ED674"))) PPC_WEAK_FUNC(sub_825ED674);
PPC_FUNC_IMPL(__imp__sub_825ED674) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825ED678"))) PPC_WEAK_FUNC(sub_825ED678);
PPC_FUNC_IMPL(__imp__sub_825ED678) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// bl 0x825ed560
	ctx.lr = 0x825ED694;
	sub_825ED560(ctx, base);
	// lwz r11,1828(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1828);
	// lwz r10,1836(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1836);
	// lwz r9,1840(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1840);
	// lwz r8,1864(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1864);
	// lwz r7,1788(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1788);
	// stw r11,1832(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1832, ctx.r11.u32);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// stw r10,1852(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1852, ctx.r10.u32);
	// stw r9,1856(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1856, ctx.r9.u32);
	// stw r8,1860(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1860, ctx.r8.u32);
	// beq cr6,0x825ed6e8
	if (ctx.cr6.eq) goto loc_825ED6E8;
	// lwz r11,1824(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1824);
	// li r9,1
	ctx.r9.s64 = 1;
	// lwz r10,1844(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1844);
	// lwz r8,1848(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1848);
	// lwz r7,1868(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1868);
	// stw r11,1832(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1832, ctx.r11.u32);
	// stw r9,1796(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1796, ctx.r9.u32);
	// stw r10,1852(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1852, ctx.r10.u32);
	// stw r8,1856(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1856, ctx.r8.u32);
	// stw r7,1860(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1860, ctx.r7.u32);
loc_825ED6E8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,20056(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20056);
	// bl 0x82605dc0
	ctx.lr = 0x825ED6F4;
	sub_82605DC0(ctx, base);
	// lwz r11,1796(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1796);
	// li r10,8
	ctx.r10.s64 = 8;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r11,0
	ctx.r11.s64 = 0;
	// li r9,3
	ctx.r9.s64 = 3;
	// beq cr6,0x825ed720
	if (ctx.cr6.eq) goto loc_825ED720;
	// stw r11,1916(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1916, ctx.r11.u32);
	// stw r10,1920(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1920, ctx.r10.u32);
	// stw r11,1924(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1924, ctx.r11.u32);
	// stw r9,1928(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1928, ctx.r9.u32);
	// b 0x825ed730
	goto loc_825ED730;
loc_825ED720:
	// stw r10,1916(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1916, ctx.r10.u32);
	// stw r11,1920(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1920, ctx.r11.u32);
	// stw r9,1924(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1924, ctx.r9.u32);
	// stw r11,1928(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1928, ctx.r11.u32);
loc_825ED730:
	// bl 0x82601418
	ctx.lr = 0x825ED734;
	sub_82601418(ctx, base);
	// lis r5,-32156
	ctx.r5.s64 = -2107375616;
	// lis r6,-32156
	ctx.r6.s64 = -2107375616;
	// lwz r4,3956(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3956);
	// lis r7,-32156
	ctx.r7.s64 = -2107375616;
	// stw r3,260(r31)
	PPC_STORE_U32(ctx.r31.u32 + 260, ctx.r3.u32);
	// lis r8,-32156
	ctx.r8.s64 = -2107375616;
	// lis r9,-32156
	ctx.r9.s64 = -2107375616;
	// lis r10,-32156
	ctx.r10.s64 = -2107375616;
	// lis r11,-32156
	ctx.r11.s64 = -2107375616;
	// addi r5,r5,13712
	ctx.r5.s64 = ctx.r5.s64 + 13712;
	// addi r6,r6,6184
	ctx.r6.s64 = ctx.r6.s64 + 6184;
	// addi r7,r7,7632
	ctx.r7.s64 = ctx.r7.s64 + 7632;
	// addi r8,r8,9928
	ctx.r8.s64 = ctx.r8.s64 + 9928;
	// addi r9,r9,11416
	ctx.r9.s64 = ctx.r9.s64 + 11416;
	// addi r10,r10,-32288
	ctx.r10.s64 = ctx.r10.s64 + -32288;
	// stw r5,3120(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3120, ctx.r5.u32);
	// addi r11,r11,-27224
	ctx.r11.s64 = ctx.r11.s64 + -27224;
	// stw r6,3088(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3088, ctx.r6.u32);
	// stw r7,3092(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3092, ctx.r7.u32);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// stw r8,3096(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3096, ctx.r8.u32);
	// stw r9,3100(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3100, ctx.r9.u32);
	// stw r10,3112(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3112, ctx.r10.u32);
	// stw r11,3116(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3116, ctx.r11.u32);
	// beq cr6,0x825ed7ac
	if (ctx.cr6.eq) goto loc_825ED7AC;
	// lis r10,-32160
	ctx.r10.s64 = -2107637760;
	// lis r11,-32160
	ctx.r11.s64 = -2107637760;
	// addi r10,r10,5304
	ctx.r10.s64 = ctx.r10.s64 + 5304;
	// addi r11,r11,5728
	ctx.r11.s64 = ctx.r11.s64 + 5728;
	// b 0x825ed7bc
	goto loc_825ED7BC;
loc_825ED7AC:
	// lis r10,-32160
	ctx.r10.s64 = -2107637760;
	// lis r11,-32160
	ctx.r11.s64 = -2107637760;
	// addi r10,r10,2616
	ctx.r10.s64 = ctx.r10.s64 + 2616;
	// addi r11,r11,3008
	ctx.r11.s64 = ctx.r11.s64 + 3008;
loc_825ED7BC:
	// stw r11,15868(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15868, ctx.r11.u32);
	// lis r11,-32161
	ctx.r11.s64 = -2107703296;
	// stw r10,15864(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15864, ctx.r10.u32);
	// lis r10,-32160
	ctx.r10.s64 = -2107637760;
	// addi r11,r11,27344
	ctx.r11.s64 = ctx.r11.s64 + 27344;
	// addi r10,r10,5008
	ctx.r10.s64 = ctx.r10.s64 + 5008;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,15860(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15860, ctx.r11.u32);
	// stw r10,2952(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2952, ctx.r10.u32);
	// bl 0x82641198
	ctx.lr = 0x825ED7E4;
	sub_82641198(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825ED7F8"))) PPC_WEAK_FUNC(sub_825ED7F8);
PPC_FUNC_IMPL(__imp__sub_825ED7F8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825ED800;
	sub_8239BA1C(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r30,1
	ctx.r30.s64 = 1;
	// li r29,0
	ctx.r29.s64 = 0;
	// lwz r5,15472(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// stw r30,15520(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15520, ctx.r30.u32);
	// cmpwi cr6,r5,5
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 5, ctx.xer);
	// beq cr6,0x825ed828
	if (ctx.cr6.eq) goto loc_825ED828;
	// cmpwi cr6,r5,6
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 6, ctx.xer);
	// blt cr6,0x825ed898
	if (ctx.cr6.lt) goto loc_825ED898;
loc_825ED828:
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// cmpwi cr6,r5,7
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 7, ctx.xer);
	// addi r10,r11,5616
	ctx.r10.s64 = ctx.r11.s64 + 5616;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// addi r9,r11,5652
	ctx.r9.s64 = ctx.r11.s64 + 5652;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// stw r10,1836(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1836, ctx.r10.u32);
	// addi r8,r11,5688
	ctx.r8.s64 = ctx.r11.s64 + 5688;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// stw r9,1840(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1840, ctx.r9.u32);
	// addi r7,r11,5724
	ctx.r7.s64 = ctx.r11.s64 + 5724;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// stw r8,1844(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1844, ctx.r8.u32);
	// addi r6,r11,5760
	ctx.r6.s64 = ctx.r11.s64 + 5760;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// stw r7,1848(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1848, ctx.r7.u32);
	// addi r11,r11,5780
	ctx.r11.s64 = ctx.r11.s64 + 5780;
	// stw r6,1864(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1864, ctx.r6.u32);
	// stw r11,1868(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1868, ctx.r11.u32);
	// bne cr6,0x825ed87c
	if (!ctx.cr6.eq) goto loc_825ED87C;
	// stw r11,1860(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1860, ctx.r11.u32);
loc_825ED87C:
	// cmpwi cr6,r5,5
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 5, ctx.xer);
	// bne cr6,0x825ed890
	if (!ctx.cr6.eq) goto loc_825ED890;
	// stw r30,432(r31)
	PPC_STORE_U32(ctx.r31.u32 + 432, ctx.r30.u32);
	// stw r30,440(r31)
	PPC_STORE_U32(ctx.r31.u32 + 440, ctx.r30.u32);
	// b 0x825ed898
	goto loc_825ED898;
loc_825ED890:
	// stw r29,432(r31)
	PPC_STORE_U32(ctx.r31.u32 + 432, ctx.r29.u32);
	// stw r29,440(r31)
	PPC_STORE_U32(ctx.r31.u32 + 440, ctx.r29.u32);
loc_825ED898:
	// cmpwi cr6,r5,4
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 4, ctx.xer);
	// blt cr6,0x825ed918
	if (ctx.cr6.lt) goto loc_825ED918;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r3,-32157
	ctx.r3.s64 = -2107441152;
	// addi r11,r11,4808
	ctx.r11.s64 = ctx.r11.s64 + 4808;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// lis r7,-32139
	ctx.r7.s64 = -2106261504;
	// lis r6,-32139
	ctx.r6.s64 = -2106261504;
	// stw r11,1800(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1800, ctx.r11.u32);
	// lis r5,-32139
	ctx.r5.s64 = -2106261504;
	// lis r4,-32139
	ctx.r4.s64 = -2106261504;
	// addi r11,r3,21840
	ctx.r11.s64 = ctx.r3.s64 + 21840;
	// addi r10,r10,4600
	ctx.r10.s64 = ctx.r10.s64 + 4600;
	// addi r9,r9,4880
	ctx.r9.s64 = ctx.r9.s64 + 4880;
	// addi r8,r8,4944
	ctx.r8.s64 = ctx.r8.s64 + 4944;
	// addi r7,r7,4472
	ctx.r7.s64 = ctx.r7.s64 + 4472;
	// addi r6,r6,4536
	ctx.r6.s64 = ctx.r6.s64 + 4536;
	// stw r11,3068(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3068, ctx.r11.u32);
	// addi r5,r5,4672
	ctx.r5.s64 = ctx.r5.s64 + 4672;
	// stw r10,1812(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1812, ctx.r10.u32);
	// addi r4,r4,4736
	ctx.r4.s64 = ctx.r4.s64 + 4736;
	// stw r9,1804(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1804, ctx.r9.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r8,1808(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1808, ctx.r8.u32);
	// stw r7,1816(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1816, ctx.r7.u32);
	// stw r6,1820(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1820, ctx.r6.u32);
	// stw r5,1824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1824, ctx.r5.u32);
	// stw r4,1828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1828, ctx.r4.u32);
	// bl 0x825dd3b0
	ctx.lr = 0x825ED914;
	sub_825DD3B0(ctx, base);
	// b 0x825ed964
	goto loc_825ED964;
loc_825ED918:
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// lis r7,-32162
	ctx.r7.s64 = -2107768832;
	// addi r11,r11,5864
	ctx.r11.s64 = ctx.r11.s64 + 5864;
	// addi r10,r10,5800
	ctx.r10.s64 = ctx.r10.s64 + 5800;
	// addi r9,r9,5936
	ctx.r9.s64 = ctx.r9.s64 + 5936;
	// addi r8,r8,6000
	ctx.r8.s64 = ctx.r8.s64 + 6000;
	// addi r7,r7,30424
	ctx.r7.s64 = ctx.r7.s64 + 30424;
	// stw r11,1800(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1800, ctx.r11.u32);
	// stw r10,1812(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1812, ctx.r10.u32);
	// stw r9,1804(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1804, ctx.r9.u32);
	// stw r8,1808(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1808, ctx.r8.u32);
	// stw r7,3068(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3068, ctx.r7.u32);
	// stw r8,1816(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1816, ctx.r8.u32);
	// stw r9,1820(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1820, ctx.r9.u32);
	// stw r10,1824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1824, ctx.r10.u32);
	// stw r11,1828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1828, ctx.r11.u32);
loc_825ED964:
	// lwz r8,15472(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r8,7
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 7, ctx.xer);
	// bne cr6,0x825ed9e4
	if (!ctx.cr6.eq) goto loc_825ED9E4;
	// lwz r11,1836(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1836);
	// lwz r10,1840(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1840);
	// lwz r9,1864(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1864);
	// lwz r7,1828(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1828);
	// lwz r6,1804(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1804);
	// lwz r5,1808(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1808);
	// lwz r4,1788(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1788);
	// stw r11,1852(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1852, ctx.r11.u32);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// stw r10,1856(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1856, ctx.r10.u32);
	// stw r9,1860(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1860, ctx.r9.u32);
	// stw r29,1796(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1796, ctx.r29.u32);
	// stw r7,1832(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1832, ctx.r7.u32);
	// stw r6,20048(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20048, ctx.r6.u32);
	// stw r5,20052(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20052, ctx.r5.u32);
	// beq cr6,0x825ed9e4
	if (ctx.cr6.eq) goto loc_825ED9E4;
	// lwz r11,1824(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1824);
	// lwz r10,1844(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1844);
	// lwz r9,1848(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1848);
	// lwz r7,1868(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1868);
	// lwz r6,1816(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1816);
	// lwz r5,1820(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1820);
	// stw r30,1796(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1796, ctx.r30.u32);
	// stw r11,1832(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1832, ctx.r11.u32);
	// stw r10,1852(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1852, ctx.r10.u32);
	// stw r9,1856(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1856, ctx.r9.u32);
	// stw r7,1860(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1860, ctx.r7.u32);
	// stw r6,20048(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20048, ctx.r6.u32);
	// stw r5,20052(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20052, ctx.r5.u32);
loc_825ED9E4:
	// lis r9,-32162
	ctx.r9.s64 = -2107768832;
	// lis r10,-32162
	ctx.r10.s64 = -2107768832;
	// lis r11,-32157
	ctx.r11.s64 = -2107441152;
	// addi r9,r9,-10456
	ctx.r9.s64 = ctx.r9.s64 + -10456;
	// addi r10,r10,-7704
	ctx.r10.s64 = ctx.r10.s64 + -7704;
	// addi r11,r11,16216
	ctx.r11.s64 = ctx.r11.s64 + 16216;
	// cmpwi cr6,r8,6
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 6, ctx.xer);
	// stw r9,15772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15772, ctx.r9.u32);
	// stw r10,15776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15776, ctx.r10.u32);
	// stw r11,3060(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3060, ctx.r11.u32);
	// blt cr6,0x825eda38
	if (ctx.cr6.lt) goto loc_825EDA38;
	// lis r9,-32158
	ctx.r9.s64 = -2107506688;
	// lis r10,-32159
	ctx.r10.s64 = -2107572224;
	// lis r11,-32159
	ctx.r11.s64 = -2107572224;
	// addi r9,r9,-25288
	ctx.r9.s64 = ctx.r9.s64 + -25288;
	// addi r10,r10,5008
	ctx.r10.s64 = ctx.r10.s64 + 5008;
	// addi r11,r11,-1992
	ctx.r11.s64 = ctx.r11.s64 + -1992;
	// stw r9,15776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15776, ctx.r9.u32);
	// stw r10,3056(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3056, ctx.r10.u32);
	// stw r11,3064(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3064, ctx.r11.u32);
	// b 0x825eda44
	goto loc_825EDA44;
loc_825EDA38:
	// lis r11,-32168
	ctx.r11.s64 = -2108162048;
	// addi r11,r11,30856
	ctx.r11.s64 = ctx.r11.s64 + 30856;
	// stw r11,3056(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3056, ctx.r11.u32);
loc_825EDA44:
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// beq cr6,0x825edad0
	if (ctx.cr6.eq) goto loc_825EDAD0;
	// cmpwi cr6,r8,3
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 3, ctx.xer);
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// bge cr6,0x825eda5c
	if (!ctx.cr6.lt) goto loc_825EDA5C;
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_825EDA5C:
	// stw r11,1932(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1932, ctx.r11.u32);
	// cmpwi cr6,r8,4
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 4, ctx.xer);
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// bge cr6,0x825eda70
	if (!ctx.cr6.lt) goto loc_825EDA70;
	// mr r11,r29
	ctx.r11.u64 = ctx.r29.u64;
loc_825EDA70:
	// lis r10,-32158
	ctx.r10.s64 = -2107506688;
	// lwz r9,3164(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3164);
	// stw r11,1936(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1936, ctx.r11.u32);
	// addi r10,r10,-20584
	ctx.r10.s64 = ctx.r10.s64 + -20584;
	// cmplw cr6,r9,r10
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, ctx.r10.u32, ctx.xer);
	// bne cr6,0x825eda94
	if (!ctx.cr6.eq) goto loc_825EDA94;
	// lis r11,-32157
	ctx.r11.s64 = -2107441152;
	// addi r11,r11,13072
	ctx.r11.s64 = ctx.r11.s64 + 13072;
	// b 0x825eda9c
	goto loc_825EDA9C;
loc_825EDA94:
	// lis r11,-32157
	ctx.r11.s64 = -2107441152;
	// addi r11,r11,28040
	ctx.r11.s64 = ctx.r11.s64 + 28040;
loc_825EDA9C:
	// stw r11,3156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3156, ctx.r11.u32);
	// cmpwi cr6,r8,3
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 3, ctx.xer);
	// blt cr6,0x825edabc
	if (ctx.cr6.lt) goto loc_825EDABC;
	// lis r10,-32158
	ctx.r10.s64 = -2107506688;
	// lis r11,-32158
	ctx.r11.s64 = -2107506688;
	// addi r10,r10,22776
	ctx.r10.s64 = ctx.r10.s64 + 22776;
	// addi r11,r11,11992
	ctx.r11.s64 = ctx.r11.s64 + 11992;
	// b 0x825edaf8
	goto loc_825EDAF8;
loc_825EDABC:
	// lis r10,-32162
	ctx.r10.s64 = -2107768832;
	// lis r11,-32162
	ctx.r11.s64 = -2107768832;
	// addi r10,r10,14736
	ctx.r10.s64 = ctx.r10.s64 + 14736;
	// addi r11,r11,15104
	ctx.r11.s64 = ctx.r11.s64 + 15104;
	// b 0x825edaf8
	goto loc_825EDAF8;
loc_825EDAD0:
	// lis r9,-32162
	ctx.r9.s64 = -2107768832;
	// lwz r8,140(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// lis r10,-32162
	ctx.r10.s64 = -2107768832;
	// stw r30,1936(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1936, ctx.r30.u32);
	// addi r9,r9,22768
	ctx.r9.s64 = ctx.r9.s64 + 22768;
	// lis r11,-32162
	ctx.r11.s64 = -2107768832;
	// addi r10,r10,15824
	ctx.r10.s64 = ctx.r10.s64 + 15824;
	// stw r8,15468(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15468, ctx.r8.u32);
	// addi r11,r11,16192
	ctx.r11.s64 = ctx.r11.s64 + 16192;
	// stw r9,3156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3156, ctx.r9.u32);
loc_825EDAF8:
	// stw r11,3076(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3076, ctx.r11.u32);
	// lis r11,-32160
	ctx.r11.s64 = -2107637760;
	// stw r10,3072(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3072, ctx.r10.u32);
	// addi r11,r11,31744
	ctx.r11.s64 = ctx.r11.s64 + 31744;
	// stw r11,20020(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20020, ctx.r11.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825EDB14"))) PPC_WEAK_FUNC(sub_825EDB14);
PPC_FUNC_IMPL(__imp__sub_825EDB14) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EDB18"))) PPC_WEAK_FUNC(sub_825EDB18);
PPC_FUNC_IMPL(__imp__sub_825EDB18) {
	PPC_FUNC_PROLOGUE();
	// lis r4,9356
	ctx.r4.s64 = 613154816;
	// ori r4,r4,32769
	ctx.r4.u64 = ctx.r4.u64 | 32769;
	// b 0x82121108
	sub_82121108(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825EDB24"))) PPC_WEAK_FUNC(sub_825EDB24);
PPC_FUNC_IMPL(__imp__sub_825EDB24) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EDB28"))) PPC_WEAK_FUNC(sub_825EDB28);
PPC_FUNC_IMPL(__imp__sub_825EDB28) {
	PPC_FUNC_PROLOGUE();
	// lis r4,9356
	ctx.r4.s64 = 613154816;
	// ori r4,r4,32769
	ctx.r4.u64 = ctx.r4.u64 | 32769;
	// b 0x82120e68
	sub_82120E68(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825EDB34"))) PPC_WEAK_FUNC(sub_825EDB34);
PPC_FUNC_IMPL(__imp__sub_825EDB34) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EDB38"))) PPC_WEAK_FUNC(sub_825EDB38);
PPC_FUNC_IMPL(__imp__sub_825EDB38) {
	PPC_FUNC_PROLOGUE();
	// lis r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r5,r5,32768
	ctx.r5.u64 = ctx.r5.u64 | 32768;
	// b 0x826927c0
	sub_826927C0(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825EDB48"))) PPC_WEAK_FUNC(sub_825EDB48);
PPC_FUNC_IMPL(__imp__sub_825EDB48) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba18
	ctx.lr = 0x825EDB50;
	sub_8239BA18(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// addi r29,r5,1
	ctx.r29.s64 = ctx.r5.s64 + 1;
	// lis r4,9356
	ctx.r4.s64 = 613154816;
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// ori r4,r4,32769
	ctx.r4.u64 = ctx.r4.u64 | 32769;
	// rlwinm r3,r29,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// bl 0x82121108
	ctx.lr = 0x825EDB74;
	sub_82121108(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r11,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r11.u32);
	// bne cr6,0x825edb90
	if (!ctx.cr6.eq) goto loc_825EDB90;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
loc_825EDB90:
	// addi r10,r29,-1
	ctx.r10.s64 = ctx.r29.s64 + -1;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825edbcc
	if (!ctx.cr6.gt) goto loc_825EDBCC;
loc_825EDB9C:
	// lbz r9,0(r31)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r31.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r8,0(r30)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r30.u32 + 0);
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// extsb r9,r9
	ctx.r9.s64 = ctx.r9.s8;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// or r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 | ctx.r8.u64;
	// sth r9,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r9.u16);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// bne cr6,0x825edb9c
	if (!ctx.cr6.eq) goto loc_825EDB9C;
loc_825EDBCC:
	// li r10,0
	ctx.r10.s64 = 0;
	// li r3,0
	ctx.r3.s64 = 0;
	// sth r10,0(r11)
	PPC_STORE_U16(ctx.r11.u32 + 0, ctx.r10.u16);
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
}

__attribute__((alias("__imp__sub_825EDBE0"))) PPC_WEAK_FUNC(sub_825EDBE0);
PPC_FUNC_IMPL(__imp__sub_825EDBE0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9f8
	ctx.lr = 0x825EDBE8;
	sub_8239B9F8(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r30,r11,-20240
	ctx.r30.s64 = ctx.r11.s64 + -20240;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r29,r11,-20408
	ctx.r29.s64 = ctx.r11.s64 + -20408;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lwz r3,2596(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2596);
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// stw r30,2588(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2588, ctx.r30.u32);
	// addi r11,r11,6164
	ctx.r11.s64 = ctx.r11.s64 + 6164;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// addi r21,r31,2560
	ctx.r21.s64 = ctx.r31.s64 + 2560;
	// stw r29,2592(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2592, ctx.r29.u32);
	// addi r10,r10,6192
	ctx.r10.s64 = ctx.r10.s64 + 6192;
	// addi r9,r9,6232
	ctx.r9.s64 = ctx.r9.s64 + 6232;
	// addi r8,r8,6256
	ctx.r8.s64 = ctx.r8.s64 + 6256;
	// stw r11,2572(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2572, ctx.r11.u32);
	// addi r7,r31,2180
	ctx.r7.s64 = ctx.r31.s64 + 2180;
	// li r6,168
	ctx.r6.s64 = 168;
	// li r5,98
	ctx.r5.s64 = 98;
	// stw r10,2576(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2576, ctx.r10.u32);
	// lis r11,9356
	ctx.r11.s64 = 613154816;
	// stw r9,2580(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2580, ctx.r9.u32);
	// stw r8,2584(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2584, ctx.r8.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r7,0(r21)
	PPC_STORE_U32(ctx.r21.u32 + 0, ctx.r7.u32);
	// ori r20,r11,32769
	ctx.r20.u64 = ctx.r11.u64 | 32769;
	// stw r6,2564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2564, ctx.r6.u32);
	// stw r5,2568(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2568, ctx.r5.u32);
	// beq cr6,0x825edc70
	if (ctx.cr6.eq) goto loc_825EDC70;
	// mr r4,r20
	ctx.r4.u64 = ctx.r20.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EDC70;
	sub_82120E68(ctx, base);
loc_825EDC70:
	// li r23,0
	ctx.r23.s64 = 0;
	// mr r6,r21
	ctx.r6.u64 = ctx.r21.u64;
	// li r5,168
	ctx.r5.s64 = 168;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stw r23,2596(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2596, ctx.r23.u32);
	// bl 0x825edb48
	ctx.lr = 0x825EDC8C;
	sub_825EDB48(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ee134
	if (!ctx.cr6.eq) goto loc_825EE134;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r3,2636(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2636);
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r30,r11,-19880
	ctx.r30.s64 = ctx.r11.s64 + -19880;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// addi r29,r11,-20072
	ctx.r29.s64 = ctx.r11.s64 + -20072;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// stw r30,2628(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2628, ctx.r30.u32);
	// addi r22,r31,2600
	ctx.r22.s64 = ctx.r31.s64 + 2600;
	// addi r11,r11,6064
	ctx.r11.s64 = ctx.r11.s64 + 6064;
	// addi r10,r10,6096
	ctx.r10.s64 = ctx.r10.s64 + 6096;
	// stw r29,2632(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2632, ctx.r29.u32);
	// addi r9,r9,6136
	ctx.r9.s64 = ctx.r9.s64 + 6136;
	// addi r8,r8,6156
	ctx.r8.s64 = ctx.r8.s64 + 6156;
	// addi r7,r31,2192
	ctx.r7.s64 = ctx.r31.s64 + 2192;
	// li r6,185
	ctx.r6.s64 = 185;
	// stw r11,2612(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2612, ctx.r11.u32);
	// li r5,118
	ctx.r5.s64 = 118;
	// stw r10,2616(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2616, ctx.r10.u32);
	// stw r9,2620(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2620, ctx.r9.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r8,2624(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2624, ctx.r8.u32);
	// stw r7,0(r22)
	PPC_STORE_U32(ctx.r22.u32 + 0, ctx.r7.u32);
	// stw r6,2604(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2604, ctx.r6.u32);
	// stw r5,2608(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2608, ctx.r5.u32);
	// beq cr6,0x825edd0c
	if (ctx.cr6.eq) goto loc_825EDD0C;
	// mr r4,r20
	ctx.r4.u64 = ctx.r20.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EDD0C;
	sub_82120E68(ctx, base);
loc_825EDD0C:
	// mr r6,r22
	ctx.r6.u64 = ctx.r22.u64;
	// stw r23,2636(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2636, ctx.r23.u32);
	// li r5,185
	ctx.r5.s64 = 185;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x825edb48
	ctx.lr = 0x825EDD24;
	sub_825EDB48(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ee134
	if (!ctx.cr6.eq) goto loc_825EE134;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r3,2676(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2676);
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r30,r11,-19536
	ctx.r30.s64 = ctx.r11.s64 + -19536;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// addi r29,r11,-19688
	ctx.r29.s64 = ctx.r11.s64 + -19688;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// stw r30,2668(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2668, ctx.r30.u32);
	// addi r24,r31,2640
	ctx.r24.s64 = ctx.r31.s64 + 2640;
	// addi r11,r11,6348
	ctx.r11.s64 = ctx.r11.s64 + 6348;
	// addi r10,r10,6380
	ctx.r10.s64 = ctx.r10.s64 + 6380;
	// stw r29,2672(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2672, ctx.r29.u32);
	// addi r9,r9,6424
	ctx.r9.s64 = ctx.r9.s64 + 6424;
	// addi r8,r8,6440
	ctx.r8.s64 = ctx.r8.s64 + 6440;
	// addi r7,r31,2204
	ctx.r7.s64 = ctx.r31.s64 + 2204;
	// li r6,148
	ctx.r6.s64 = 148;
	// stw r11,2652(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2652, ctx.r11.u32);
	// li r5,80
	ctx.r5.s64 = 80;
	// stw r10,2656(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2656, ctx.r10.u32);
	// stw r9,2660(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2660, ctx.r9.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r8,2664(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2664, ctx.r8.u32);
	// stw r7,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r7.u32);
	// stw r6,2644(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2644, ctx.r6.u32);
	// stw r5,2648(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2648, ctx.r5.u32);
	// beq cr6,0x825edda4
	if (ctx.cr6.eq) goto loc_825EDDA4;
	// mr r4,r20
	ctx.r4.u64 = ctx.r20.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EDDA4;
	sub_82120E68(ctx, base);
loc_825EDDA4:
	// mr r6,r24
	ctx.r6.u64 = ctx.r24.u64;
	// stw r23,2676(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2676, ctx.r23.u32);
	// li r5,148
	ctx.r5.s64 = 148;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x825edb48
	ctx.lr = 0x825EDDBC;
	sub_825EDB48(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ee134
	if (!ctx.cr6.eq) goto loc_825EE134;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r3,2716(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2716);
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r30,r11,-19248
	ctx.r30.s64 = ctx.r11.s64 + -19248;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// addi r29,r11,-19384
	ctx.r29.s64 = ctx.r11.s64 + -19384;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// stw r30,2708(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2708, ctx.r30.u32);
	// addi r25,r31,2680
	ctx.r25.s64 = ctx.r31.s64 + 2680;
	// addi r11,r11,6268
	ctx.r11.s64 = ctx.r11.s64 + 6268;
	// addi r10,r10,6292
	ctx.r10.s64 = ctx.r10.s64 + 6292;
	// stw r29,2712(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2712, ctx.r29.u32);
	// addi r9,r9,6320
	ctx.r9.s64 = ctx.r9.s64 + 6320;
	// addi r8,r8,6340
	ctx.r8.s64 = ctx.r8.s64 + 6340;
	// addi r7,r31,2216
	ctx.r7.s64 = ctx.r31.s64 + 2216;
	// li r6,132
	ctx.r6.s64 = 132;
	// stw r11,2692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2692, ctx.r11.u32);
	// li r5,84
	ctx.r5.s64 = 84;
	// stw r10,2696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2696, ctx.r10.u32);
	// stw r9,2700(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2700, ctx.r9.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r8,2704(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2704, ctx.r8.u32);
	// stw r7,0(r25)
	PPC_STORE_U32(ctx.r25.u32 + 0, ctx.r7.u32);
	// stw r6,2684(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2684, ctx.r6.u32);
	// stw r5,2688(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2688, ctx.r5.u32);
	// beq cr6,0x825ede3c
	if (ctx.cr6.eq) goto loc_825EDE3C;
	// mr r4,r20
	ctx.r4.u64 = ctx.r20.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EDE3C;
	sub_82120E68(ctx, base);
loc_825EDE3C:
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// stw r23,2716(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2716, ctx.r23.u32);
	// li r5,132
	ctx.r5.s64 = 132;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x825edb48
	ctx.lr = 0x825EDE54;
	sub_825EDB48(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ee134
	if (!ctx.cr6.eq) goto loc_825EE134;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r3,2756(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2756);
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r30,r11,-18424
	ctx.r30.s64 = ctx.r11.s64 + -18424;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// addi r29,r11,-18320
	ctx.r29.s64 = ctx.r11.s64 + -18320;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// stw r30,2748(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2748, ctx.r30.u32);
	// addi r26,r31,2720
	ctx.r26.s64 = ctx.r31.s64 + 2720;
	// addi r11,r11,6528
	ctx.r11.s64 = ctx.r11.s64 + 6528;
	// addi r10,r10,6556
	ctx.r10.s64 = ctx.r10.s64 + 6556;
	// stw r29,2752(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2752, ctx.r29.u32);
	// addi r9,r9,6600
	ctx.r9.s64 = ctx.r9.s64 + 6600;
	// addi r8,r8,6616
	ctx.r8.s64 = ctx.r8.s64 + 6616;
	// addi r7,r31,2228
	ctx.r7.s64 = ctx.r31.s64 + 2228;
	// li r28,102
	ctx.r28.s64 = 102;
	// stw r11,2732(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2732, ctx.r11.u32);
	// li r6,57
	ctx.r6.s64 = 57;
	// stw r10,2736(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2736, ctx.r10.u32);
	// stw r9,2740(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2740, ctx.r9.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r8,2744(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2744, ctx.r8.u32);
	// stw r7,0(r26)
	PPC_STORE_U32(ctx.r26.u32 + 0, ctx.r7.u32);
	// stw r28,2724(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2724, ctx.r28.u32);
	// stw r6,2728(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2728, ctx.r6.u32);
	// beq cr6,0x825eded4
	if (ctx.cr6.eq) goto loc_825EDED4;
	// mr r4,r20
	ctx.r4.u64 = ctx.r20.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EDED4;
	sub_82120E68(ctx, base);
loc_825EDED4:
	// mr r6,r26
	ctx.r6.u64 = ctx.r26.u64;
	// stw r23,2756(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2756, ctx.r23.u32);
	// li r5,102
	ctx.r5.s64 = 102;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x825edb48
	ctx.lr = 0x825EDEEC;
	sub_825EDB48(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ee134
	if (!ctx.cr6.eq) goto loc_825EE134;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r3,2796(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2796);
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// stw r28,2764(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2764, ctx.r28.u32);
	// addi r30,r11,-18216
	ctx.r30.s64 = ctx.r11.s64 + -18216;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// addi r29,r11,-18112
	ctx.r29.s64 = ctx.r11.s64 + -18112;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// stw r30,2788(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2788, ctx.r30.u32);
	// addi r27,r31,2760
	ctx.r27.s64 = ctx.r31.s64 + 2760;
	// addi r11,r11,6448
	ctx.r11.s64 = ctx.r11.s64 + 6448;
	// addi r10,r10,6464
	ctx.r10.s64 = ctx.r10.s64 + 6464;
	// stw r29,2792(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2792, ctx.r29.u32);
	// addi r9,r9,6488
	ctx.r9.s64 = ctx.r9.s64 + 6488;
	// addi r8,r8,6516
	ctx.r8.s64 = ctx.r8.s64 + 6516;
	// addi r7,r31,2240
	ctx.r7.s64 = ctx.r31.s64 + 2240;
	// li r6,66
	ctx.r6.s64 = 66;
	// stw r11,2772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2772, ctx.r11.u32);
	// stw r10,2776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2776, ctx.r10.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r9,2780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2780, ctx.r9.u32);
	// stw r8,2784(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2784, ctx.r8.u32);
	// stw r7,0(r27)
	PPC_STORE_U32(ctx.r27.u32 + 0, ctx.r7.u32);
	// stw r6,2768(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2768, ctx.r6.u32);
	// beq cr6,0x825edf68
	if (ctx.cr6.eq) goto loc_825EDF68;
	// mr r4,r20
	ctx.r4.u64 = ctx.r20.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EDF68;
	sub_82120E68(ctx, base);
loc_825EDF68:
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// stw r23,2796(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2796, ctx.r23.u32);
	// li r5,102
	ctx.r5.s64 = 102;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x825edb48
	ctx.lr = 0x825EDF80;
	sub_825EDB48(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ee134
	if (!ctx.cr6.eq) goto loc_825EE134;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r3,2876(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2876);
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r29,r11,-18936
	ctx.r29.s64 = ctx.r11.s64 + -18936;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// addi r28,r11,-19112
	ctx.r28.s64 = ctx.r11.s64 + -19112;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// stw r29,2868(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2868, ctx.r29.u32);
	// addi r30,r31,2840
	ctx.r30.s64 = ctx.r31.s64 + 2840;
	// addi r11,r11,6724
	ctx.r11.s64 = ctx.r11.s64 + 6724;
	// addi r10,r10,6752
	ctx.r10.s64 = ctx.r10.s64 + 6752;
	// stw r28,2872(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2872, ctx.r28.u32);
	// addi r9,r9,6784
	ctx.r9.s64 = ctx.r9.s64 + 6784;
	// addi r8,r8,6820
	ctx.r8.s64 = ctx.r8.s64 + 6820;
	// addi r7,r31,2428
	ctx.r7.s64 = ctx.r31.s64 + 2428;
	// li r6,174
	ctx.r6.s64 = 174;
	// stw r11,2852(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2852, ctx.r11.u32);
	// li r5,108
	ctx.r5.s64 = 108;
	// stw r10,2856(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2856, ctx.r10.u32);
	// stw r9,2860(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2860, ctx.r9.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r8,2864(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2864, ctx.r8.u32);
	// stw r7,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r7.u32);
	// stw r6,2844(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2844, ctx.r6.u32);
	// stw r5,2848(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2848, ctx.r5.u32);
	// beq cr6,0x825ee000
	if (ctx.cr6.eq) goto loc_825EE000;
	// mr r4,r20
	ctx.r4.u64 = ctx.r20.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE000;
	sub_82120E68(ctx, base);
loc_825EE000:
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// stw r23,2876(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2876, ctx.r23.u32);
	// li r5,174
	ctx.r5.s64 = 174;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x825edb48
	ctx.lr = 0x825EE018;
	sub_825EDB48(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ee134
	if (!ctx.cr6.eq) goto loc_825EE134;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r3,2836(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2836);
	// lis r10,-32139
	ctx.r10.s64 = -2106261504;
	// addi r29,r11,-18592
	ctx.r29.s64 = ctx.r11.s64 + -18592;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lis r9,-32139
	ctx.r9.s64 = -2106261504;
	// addi r28,r11,-18760
	ctx.r28.s64 = ctx.r11.s64 + -18760;
	// lis r11,-32139
	ctx.r11.s64 = -2106261504;
	// lis r8,-32139
	ctx.r8.s64 = -2106261504;
	// stw r29,2828(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2828, ctx.r29.u32);
	// addi r30,r31,2800
	ctx.r30.s64 = ctx.r31.s64 + 2800;
	// addi r11,r11,6620
	ctx.r11.s64 = ctx.r11.s64 + 6620;
	// addi r10,r10,6636
	ctx.r10.s64 = ctx.r10.s64 + 6636;
	// stw r28,2832(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2832, ctx.r28.u32);
	// addi r9,r9,6656
	ctx.r9.s64 = ctx.r9.s64 + 6656;
	// addi r8,r8,6716
	ctx.r8.s64 = ctx.r8.s64 + 6716;
	// addi r7,r31,2252
	ctx.r7.s64 = ctx.r31.s64 + 2252;
	// li r6,162
	ctx.r6.s64 = 162;
	// stw r11,2812(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2812, ctx.r11.u32);
	// li r5,125
	ctx.r5.s64 = 125;
	// stw r10,2816(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2816, ctx.r10.u32);
	// stw r9,2820(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2820, ctx.r9.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r8,2824(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2824, ctx.r8.u32);
	// stw r7,0(r30)
	PPC_STORE_U32(ctx.r30.u32 + 0, ctx.r7.u32);
	// stw r6,2804(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2804, ctx.r6.u32);
	// stw r5,2808(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2808, ctx.r5.u32);
	// beq cr6,0x825ee098
	if (ctx.cr6.eq) goto loc_825EE098;
	// mr r4,r20
	ctx.r4.u64 = ctx.r20.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE098;
	sub_82120E68(ctx, base);
loc_825EE098:
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// stw r23,2836(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2836, ctx.r23.u32);
	// li r5,162
	ctx.r5.s64 = 162;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x825edb48
	ctx.lr = 0x825EE0B0;
	sub_825EDB48(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ee134
	if (!ctx.cr6.eq) goto loc_825EE134;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// stw r24,2904(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2904, ctx.r24.u32);
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// stw r21,2908(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2908, ctx.r21.u32);
	// lis r9,-32244
	ctx.r9.s64 = -2113142784;
	// stw r26,2912(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2912, ctx.r26.u32);
	// lis r8,-32244
	ctx.r8.s64 = -2113142784;
	// stw r25,2916(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2916, ctx.r25.u32);
	// addi r3,r31,1988
	ctx.r3.s64 = ctx.r31.s64 + 1988;
	// stw r22,2920(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2920, ctx.r22.u32);
	// addi r11,r11,-15800
	ctx.r11.s64 = ctx.r11.s64 + -15800;
	// stw r27,2924(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2924, ctx.r27.u32);
	// addi r10,r10,-14696
	ctx.r10.s64 = ctx.r10.s64 + -14696;
	// addi r9,r9,-18008
	ctx.r9.s64 = ctx.r9.s64 + -18008;
	// addi r8,r8,-16904
	ctx.r8.s64 = ctx.r8.s64 + -16904;
	// addi r7,r31,2040
	ctx.r7.s64 = ctx.r31.s64 + 2040;
	// stw r3,2012(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2012, ctx.r3.u32);
	// addi r6,r31,2052
	ctx.r6.s64 = ctx.r31.s64 + 2052;
	// stw r11,2020(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2020, ctx.r11.u32);
	// addi r5,r31,2064
	ctx.r5.s64 = ctx.r31.s64 + 2064;
	// stw r10,2024(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2024, ctx.r10.u32);
	// addi r4,r31,2076
	ctx.r4.s64 = ctx.r31.s64 + 2076;
	// stw r9,2028(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2028, ctx.r9.u32);
	// addi r30,r31,2000
	ctx.r30.s64 = ctx.r31.s64 + 2000;
	// stw r8,2032(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2032, ctx.r8.u32);
	// stw r7,2100(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2100, ctx.r7.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r6,2104(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2104, ctx.r6.u32);
	// stw r5,2108(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2108, ctx.r5.u32);
	// stw r4,2112(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2112, ctx.r4.u32);
	// stw r30,2016(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2016, ctx.r30.u32);
loc_825EE134:
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
}

__attribute__((alias("__imp__sub_825EE13C"))) PPC_WEAK_FUNC(sub_825EE13C);
PPC_FUNC_IMPL(__imp__sub_825EE13C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EE140"))) PPC_WEAK_FUNC(sub_825EE140);
PPC_FUNC_IMPL(__imp__sub_825EE140) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825EE148;
	sub_8239BA1C(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825ee1b8
	if (ctx.cr6.eq) goto loc_825EE1B8;
	// lwz r3,12(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// lis r11,0
	ctx.r11.s64 = 0;
	// li r30,0
	ctx.r30.s64 = 0;
	// ori r29,r11,32768
	ctx.r29.u64 = ctx.r11.u64 | 32768;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee180
	if (ctx.cr6.eq) goto loc_825EE180;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x826927c0
	ctx.lr = 0x825EE17C;
	sub_826927C0(ctx, base);
	// stw r30,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r30.u32);
loc_825EE180:
	// lwz r3,16(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee19c
	if (ctx.cr6.eq) goto loc_825EE19C;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x826927c0
	ctx.lr = 0x825EE198;
	sub_826927C0(ctx, base);
	// stw r30,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r30.u32);
loc_825EE19C:
	// lwz r3,20(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee1b8
	if (ctx.cr6.eq) goto loc_825EE1B8;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x826927c0
	ctx.lr = 0x825EE1B4;
	sub_826927C0(ctx, base);
	// stw r30,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r30.u32);
loc_825EE1B8:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825EE1C0"))) PPC_WEAK_FUNC(sub_825EE1C0);
PPC_FUNC_IMPL(__imp__sub_825EE1C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba18
	ctx.lr = 0x825EE1C8;
	sub_8239BA18(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,15372(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15372);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee1e0
	if (ctx.cr6.eq) goto loc_825EE1E0;
	// bl 0x8262cda8
	ctx.lr = 0x825EE1E0;
	sub_8262CDA8(ctx, base);
loc_825EE1E0:
	// lwz r3,15384(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15384);
	// lis r11,9356
	ctx.r11.s64 = 613154816;
	// li r30,0
	ctx.r30.s64 = 0;
	// ori r29,r11,32769
	ctx.r29.u64 = ctx.r11.u64 | 32769;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee204
	if (ctx.cr6.eq) goto loc_825EE204;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE200;
	sub_82120E68(ctx, base);
	// stw r30,15384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15384, ctx.r30.u32);
loc_825EE204:
	// lwz r3,15392(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15392);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee21c
	if (ctx.cr6.eq) goto loc_825EE21C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE218;
	sub_82120E68(ctx, base);
	// stw r30,15392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15392, ctx.r30.u32);
loc_825EE21C:
	// lwz r3,80(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 80);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee234
	if (ctx.cr6.eq) goto loc_825EE234;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE230;
	sub_82120E68(ctx, base);
	// stw r30,80(r31)
	PPC_STORE_U32(ctx.r31.u32 + 80, ctx.r30.u32);
loc_825EE234:
	// lwz r3,1880(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1880);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee24c
	if (ctx.cr6.eq) goto loc_825EE24C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE248;
	sub_82120E68(ctx, base);
	// stw r30,1880(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1880, ctx.r30.u32);
loc_825EE24C:
	// lwz r3,21564(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21564);
	// stw r30,1884(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1884, ctx.r30.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee270
	if (ctx.cr6.eq) goto loc_825EE270;
	// lis r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r5,r5,32768
	ctx.r5.u64 = ctx.r5.u64 | 32768;
	// bl 0x826927c0
	ctx.lr = 0x825EE26C;
	sub_826927C0(ctx, base);
	// stw r30,21564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21564, ctx.r30.u32);
loc_825EE270:
	// lwz r3,21568(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21568);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee288
	if (ctx.cr6.eq) goto loc_825EE288;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE284;
	sub_82120E68(ctx, base);
	// stw r30,21568(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21568, ctx.r30.u32);
loc_825EE288:
	// lwz r3,21572(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21572);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee2a0
	if (ctx.cr6.eq) goto loc_825EE2A0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE29C;
	sub_82120E68(ctx, base);
	// stw r30,21572(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21572, ctx.r30.u32);
loc_825EE2A0:
	// addis r28,r31,2
	ctx.r28.s64 = ctx.r31.s64 + 131072;
	// addi r28,r28,-31848
	ctx.r28.s64 = ctx.r28.s64 + -31848;
	// lwz r3,0(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 0);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee2c0
	if (ctx.cr6.eq) goto loc_825EE2C0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE2BC;
	sub_82120E68(ctx, base);
	// stw r30,0(r28)
	PPC_STORE_U32(ctx.r28.u32 + 0, ctx.r30.u32);
loc_825EE2C0:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// blt cr6,0x825ee2fc
	if (ctx.cr6.lt) goto loc_825EE2FC;
	// lwz r3,352(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 352);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee2e4
	if (ctx.cr6.eq) goto loc_825EE2E4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE2E0;
	sub_82120E68(ctx, base);
	// stw r30,352(r31)
	PPC_STORE_U32(ctx.r31.u32 + 352, ctx.r30.u32);
loc_825EE2E4:
	// lwz r3,15216(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15216);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee2fc
	if (ctx.cr6.eq) goto loc_825EE2FC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE2F8;
	sub_82120E68(ctx, base);
	// stw r30,15216(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15216, ctx.r30.u32);
loc_825EE2FC:
	// lwz r3,21240(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee314
	if (ctx.cr6.eq) goto loc_825EE314;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE310;
	sub_82120E68(ctx, base);
	// stw r30,21240(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21240, ctx.r30.u32);
loc_825EE314:
	// lwz r3,21268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21268);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee32c
	if (ctx.cr6.eq) goto loc_825EE32C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE328;
	sub_82120E68(ctx, base);
	// stw r30,21268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21268, ctx.r30.u32);
loc_825EE32C:
	// lwz r3,21252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21252);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee344
	if (ctx.cr6.eq) goto loc_825EE344;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE340;
	sub_82120E68(ctx, base);
	// stw r30,21252(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21252, ctx.r30.u32);
loc_825EE344:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825ee398
	if (!ctx.cr6.eq) goto loc_825EE398;
	// lwz r3,19992(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19992);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee368
	if (ctx.cr6.eq) goto loc_825EE368;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE364;
	sub_82120E68(ctx, base);
	// stw r30,19992(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19992, ctx.r30.u32);
loc_825EE368:
	// lwz r3,19996(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19996);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee380
	if (ctx.cr6.eq) goto loc_825EE380;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE37C;
	sub_82120E68(ctx, base);
	// stw r30,19996(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19996, ctx.r30.u32);
loc_825EE380:
	// lwz r3,20000(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20000);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee398
	if (ctx.cr6.eq) goto loc_825EE398;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE394;
	sub_82120E68(ctx, base);
	// stw r30,20000(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20000, ctx.r30.u32);
loc_825EE398:
	// lwz r3,21636(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21636);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee3b0
	if (ctx.cr6.eq) goto loc_825EE3B0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE3AC;
	sub_82120E68(ctx, base);
	// stw r30,21636(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21636, ctx.r30.u32);
loc_825EE3B0:
	// addi r4,r31,21652
	ctx.r4.s64 = ctx.r31.s64 + 21652;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE3BC;
	sub_8263F668(ctx, base);
	// addi r4,r31,21664
	ctx.r4.s64 = ctx.r31.s64 + 21664;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE3C8;
	sub_8263F668(ctx, base);
	// addi r4,r31,21676
	ctx.r4.s64 = ctx.r31.s64 + 21676;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE3D4;
	sub_8263F668(ctx, base);
	// addi r4,r31,21688
	ctx.r4.s64 = ctx.r31.s64 + 21688;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE3E0;
	sub_8263F668(ctx, base);
	// addi r4,r31,21640
	ctx.r4.s64 = ctx.r31.s64 + 21640;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE3EC;
	sub_8263F668(ctx, base);
	// lwz r3,20952(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20952);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee400
	if (ctx.cr6.eq) goto loc_825EE400;
	// bl 0x8261ead8
	ctx.lr = 0x825EE3FC;
	sub_8261EAD8(ctx, base);
	// stw r30,20952(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20952, ctx.r30.u32);
loc_825EE400:
	// lwz r3,1872(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1872);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee418
	if (ctx.cr6.eq) goto loc_825EE418;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE414;
	sub_82120E68(ctx, base);
	// stw r30,1872(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1872, ctx.r30.u32);
loc_825EE418:
	// lis r11,1
	ctx.r11.s64 = 65536;
	// ori r11,r11,33704
	ctx.r11.u64 = ctx.r11.u64 | 33704;
	// lwzx r11,r31,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r11.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825ee524
	if (!ctx.cr6.eq) goto loc_825EE524;
	// lwz r11,12(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 12);
	// lwz r3,3688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ee450
	if (ctx.cr6.eq) goto loc_825EE450;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee46c
	if (ctx.cr6.eq) goto loc_825EE46C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE44C;
	sub_82120E68(ctx, base);
	// b 0x825ee46c
	goto loc_825EE46C;
loc_825EE450:
	// bl 0x825ee140
	ctx.lr = 0x825EE454;
	sub_825EE140(ctx, base);
	// lwz r3,3688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee46c
	if (ctx.cr6.eq) goto loc_825EE46C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE468;
	sub_82120E68(ctx, base);
	// stw r30,3688(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3688, ctx.r30.u32);
loc_825EE46C:
	// lwz r3,3696(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3696);
	// bl 0x825ee140
	ctx.lr = 0x825EE474;
	sub_825EE140(ctx, base);
	// lwz r3,3696(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3696);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee48c
	if (ctx.cr6.eq) goto loc_825EE48C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE488;
	sub_82120E68(ctx, base);
	// stw r30,3696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3696, ctx.r30.u32);
loc_825EE48C:
	// lwz r3,3692(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3692);
	// stw r30,3696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3696, ctx.r30.u32);
	// stw r30,3688(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3688, ctx.r30.u32);
	// bl 0x825ee140
	ctx.lr = 0x825EE49C;
	sub_825EE140(ctx, base);
	// lwz r3,3692(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3692);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee4b4
	if (ctx.cr6.eq) goto loc_825EE4B4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE4B0;
	sub_82120E68(ctx, base);
	// stw r30,3692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3692, ctx.r30.u32);
loc_825EE4B4:
	// lwz r3,3700(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3700);
	// bl 0x825ee140
	ctx.lr = 0x825EE4BC;
	sub_825EE140(ctx, base);
	// lwz r3,3700(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3700);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee4d4
	if (ctx.cr6.eq) goto loc_825EE4D4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE4D0;
	sub_82120E68(ctx, base);
	// stw r30,3700(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3700, ctx.r30.u32);
loc_825EE4D4:
	// lwz r3,3704(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// stw r30,3692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3692, ctx.r30.u32);
	// stw r30,3700(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3700, ctx.r30.u32);
	// bl 0x825ee140
	ctx.lr = 0x825EE4E4;
	sub_825EE140(ctx, base);
	// lwz r3,3704(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee4fc
	if (ctx.cr6.eq) goto loc_825EE4FC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE4F8;
	sub_82120E68(ctx, base);
	// stw r30,3704(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3704, ctx.r30.u32);
loc_825EE4FC:
	// lwz r3,3708(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3708);
	// bl 0x825ee140
	ctx.lr = 0x825EE504;
	sub_825EE140(ctx, base);
	// lwz r3,3708(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3708);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee51c
	if (ctx.cr6.eq) goto loc_825EE51C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE518;
	sub_82120E68(ctx, base);
	// stw r30,3708(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3708, ctx.r30.u32);
loc_825EE51C:
	// stw r30,3708(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3708, ctx.r30.u32);
	// stw r30,3704(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3704, ctx.r30.u32);
loc_825EE524:
	// lwz r3,264(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee53c
	if (ctx.cr6.eq) goto loc_825EE53C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE538;
	sub_82120E68(ctx, base);
	// stw r30,264(r31)
	PPC_STORE_U32(ctx.r31.u32 + 264, ctx.r30.u32);
loc_825EE53C:
	// lwz r3,272(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 272);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee554
	if (ctx.cr6.eq) goto loc_825EE554;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE550;
	sub_82120E68(ctx, base);
	// stw r30,272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 272, ctx.r30.u32);
loc_825EE554:
	// lwz r3,1900(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1900);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee56c
	if (ctx.cr6.eq) goto loc_825EE56C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE568;
	sub_82120E68(ctx, base);
	// stw r30,1900(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1900, ctx.r30.u32);
loc_825EE56C:
	// lwz r3,1904(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1904);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee584
	if (ctx.cr6.eq) goto loc_825EE584;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE580;
	sub_82120E68(ctx, base);
	// stw r30,1904(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1904, ctx.r30.u32);
loc_825EE584:
	// lwz r3,1908(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1908);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee59c
	if (ctx.cr6.eq) goto loc_825EE59C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE598;
	sub_82120E68(ctx, base);
	// stw r30,1908(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1908, ctx.r30.u32);
loc_825EE59C:
	// lwz r3,1912(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1912);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee5b4
	if (ctx.cr6.eq) goto loc_825EE5B4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE5B0;
	sub_82120E68(ctx, base);
	// stw r30,1912(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1912, ctx.r30.u32);
loc_825EE5B4:
	// lwz r3,3916(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3916);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee5cc
	if (ctx.cr6.eq) goto loc_825EE5CC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE5C8;
	sub_82120E68(ctx, base);
	// stw r30,3916(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3916, ctx.r30.u32);
loc_825EE5CC:
	// lwz r3,21576(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21576);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee5e4
	if (ctx.cr6.eq) goto loc_825EE5E4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE5E0;
	sub_82120E68(ctx, base);
	// stw r30,21576(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21576, ctx.r30.u32);
loc_825EE5E4:
	// lwz r11,23968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23968);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ee6d8
	if (ctx.cr6.eq) goto loc_825EE6D8;
	// lwz r11,16472(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16472);
	// cmplw cr6,r31,r11
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x825ee68c
	if (!ctx.cr6.eq) goto loc_825EE68C;
	// lwz r3,3048(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3048);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee614
	if (ctx.cr6.eq) goto loc_825EE614;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE610;
	sub_82120E68(ctx, base);
	// stw r30,3048(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3048, ctx.r30.u32);
loc_825EE614:
	// lwz r3,15208(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15208);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee62c
	if (ctx.cr6.eq) goto loc_825EE62C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE628;
	sub_82120E68(ctx, base);
	// stw r30,15208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15208, ctx.r30.u32);
loc_825EE62C:
	// lwz r3,15268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15268);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee644
	if (ctx.cr6.eq) goto loc_825EE644;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE640;
	sub_82120E68(ctx, base);
	// stw r30,15268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15268, ctx.r30.u32);
loc_825EE644:
	// lwz r3,15292(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15292);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee65c
	if (ctx.cr6.eq) goto loc_825EE65C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE658;
	sub_82120E68(ctx, base);
	// stw r30,15292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15292, ctx.r30.u32);
loc_825EE65C:
	// lwz r3,1772(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee674
	if (ctx.cr6.eq) goto loc_825EE674;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE670;
	sub_82120E68(ctx, base);
	// stw r30,1772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1772, ctx.r30.u32);
loc_825EE674:
	// lwz r3,1780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1780);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee6a4
	if (ctx.cr6.eq) goto loc_825EE6A4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE688;
	sub_82120E68(ctx, base);
	// b 0x825ee6a0
	goto loc_825EE6A0;
loc_825EE68C:
	// stw r30,3048(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3048, ctx.r30.u32);
	// stw r30,15268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15268, ctx.r30.u32);
	// stw r30,15292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15292, ctx.r30.u32);
	// stw r30,15208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15208, ctx.r30.u32);
	// stw r30,1772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1772, ctx.r30.u32);
loc_825EE6A0:
	// stw r30,1780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1780, ctx.r30.u32);
loc_825EE6A4:
	// lwz r3,15276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15276);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee6bc
	if (ctx.cr6.eq) goto loc_825EE6BC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE6B8;
	sub_82120E68(ctx, base);
	// stw r30,15276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15276, ctx.r30.u32);
loc_825EE6BC:
	// lwz r3,15284(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15284);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee798
	if (ctx.cr6.eq) goto loc_825EE798;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE6D0;
	sub_82120E68(ctx, base);
	// stw r30,15284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15284, ctx.r30.u32);
	// b 0x825ee798
	goto loc_825EE798;
loc_825EE6D8:
	// lwz r3,3048(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3048);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee6f0
	if (ctx.cr6.eq) goto loc_825EE6F0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE6EC;
	sub_82120E68(ctx, base);
	// stw r30,3048(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3048, ctx.r30.u32);
loc_825EE6F0:
	// lwz r3,15208(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15208);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee708
	if (ctx.cr6.eq) goto loc_825EE708;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE704;
	sub_82120E68(ctx, base);
	// stw r30,15208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15208, ctx.r30.u32);
loc_825EE708:
	// lwz r3,15268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15268);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee720
	if (ctx.cr6.eq) goto loc_825EE720;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE71C;
	sub_82120E68(ctx, base);
	// stw r30,15268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15268, ctx.r30.u32);
loc_825EE720:
	// lwz r3,15292(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15292);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee738
	if (ctx.cr6.eq) goto loc_825EE738;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE734;
	sub_82120E68(ctx, base);
	// stw r30,15292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15292, ctx.r30.u32);
loc_825EE738:
	// lwz r3,15276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15276);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee750
	if (ctx.cr6.eq) goto loc_825EE750;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE74C;
	sub_82120E68(ctx, base);
	// stw r30,15276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15276, ctx.r30.u32);
loc_825EE750:
	// lwz r3,15284(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15284);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee768
	if (ctx.cr6.eq) goto loc_825EE768;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE764;
	sub_82120E68(ctx, base);
	// stw r30,15284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15284, ctx.r30.u32);
loc_825EE768:
	// lwz r3,1772(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee780
	if (ctx.cr6.eq) goto loc_825EE780;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE77C;
	sub_82120E68(ctx, base);
	// stw r30,1772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1772, ctx.r30.u32);
loc_825EE780:
	// lwz r3,1780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1780);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee798
	if (ctx.cr6.eq) goto loc_825EE798;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE794;
	sub_82120E68(ctx, base);
	// stw r30,1780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1780, ctx.r30.u32);
loc_825EE798:
	// lwz r3,276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 276);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee7b0
	if (ctx.cr6.eq) goto loc_825EE7B0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE7AC;
	sub_82120E68(ctx, base);
	// stw r30,276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 276, ctx.r30.u32);
loc_825EE7B0:
	// lwz r3,268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee7c8
	if (ctx.cr6.eq) goto loc_825EE7C8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE7C4;
	sub_82120E68(ctx, base);
	// stw r30,268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 268, ctx.r30.u32);
loc_825EE7C8:
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825ee7ec
	if (!ctx.cr6.eq) goto loc_825EE7EC;
	// lwz r3,3052(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3052);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee7ec
	if (ctx.cr6.eq) goto loc_825EE7EC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE7E8;
	sub_82120E68(ctx, base);
	// stw r30,3052(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3052, ctx.r30.u32);
loc_825EE7EC:
	// lwz r11,14788(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14788);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ee858
	if (ctx.cr6.eq) goto loc_825EE858;
	// lwz r3,14808(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14808);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee810
	if (ctx.cr6.eq) goto loc_825EE810;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE80C;
	sub_82120E68(ctx, base);
	// stw r30,14808(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14808, ctx.r30.u32);
loc_825EE810:
	// lwz r3,14812(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14812);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee828
	if (ctx.cr6.eq) goto loc_825EE828;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE824;
	sub_82120E68(ctx, base);
	// stw r30,14812(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14812, ctx.r30.u32);
loc_825EE828:
	// lwz r3,14816(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14816);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee840
	if (ctx.cr6.eq) goto loc_825EE840;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE83C;
	sub_82120E68(ctx, base);
	// stw r30,14816(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14816, ctx.r30.u32);
loc_825EE840:
	// lwz r3,3712(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3712);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee858
	if (ctx.cr6.eq) goto loc_825EE858;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE854;
	sub_82120E68(ctx, base);
	// stw r30,3712(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3712, ctx.r30.u32);
loc_825EE858:
	// lwz r3,1892(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1892);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee870
	if (ctx.cr6.eq) goto loc_825EE870;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE86C;
	sub_82120E68(ctx, base);
	// stw r30,1892(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1892, ctx.r30.u32);
loc_825EE870:
	// lwz r3,1896(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1896);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ee888
	if (ctx.cr6.eq) goto loc_825EE888;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EE884;
	sub_82120E68(ctx, base);
	// stw r30,1896(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1896, ctx.r30.u32);
loc_825EE888:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// stw r30,15768(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15768, ctx.r30.u32);
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// blt cr6,0x825ee8bc
	if (ctx.cr6.lt) goto loc_825EE8BC;
	// lwz r3,1972(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1972);
	// bl 0x826453d8
	ctx.lr = 0x825EE8A0;
	sub_826453D8(ctx, base);
	// lwz r3,1964(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1964);
	// stw r30,1972(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1972, ctx.r30.u32);
	// bl 0x82643638
	ctx.lr = 0x825EE8AC;
	sub_82643638(ctx, base);
	// lwz r3,1968(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1968);
	// stw r30,1964(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1964, ctx.r30.u32);
	// bl 0x82643f80
	ctx.lr = 0x825EE8B8;
	sub_82643F80(ctx, base);
	// stw r30,1968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1968, ctx.r30.u32);
loc_825EE8BC:
	// addi r4,r31,1988
	ctx.r4.s64 = ctx.r31.s64 + 1988;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE8C8;
	sub_8263F668(ctx, base);
	// addi r4,r31,2000
	ctx.r4.s64 = ctx.r31.s64 + 2000;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE8D4;
	sub_8263F668(ctx, base);
	// addi r4,r31,2040
	ctx.r4.s64 = ctx.r31.s64 + 2040;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE8E0;
	sub_8263F668(ctx, base);
	// addi r4,r31,2052
	ctx.r4.s64 = ctx.r31.s64 + 2052;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE8EC;
	sub_8263F668(ctx, base);
	// addi r4,r31,2064
	ctx.r4.s64 = ctx.r31.s64 + 2064;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE8F8;
	sub_8263F668(ctx, base);
	// addi r4,r31,2076
	ctx.r4.s64 = ctx.r31.s64 + 2076;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE904;
	sub_8263F668(ctx, base);
	// addi r4,r31,2116
	ctx.r4.s64 = ctx.r31.s64 + 2116;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE910;
	sub_8263F668(ctx, base);
	// addi r4,r31,2128
	ctx.r4.s64 = ctx.r31.s64 + 2128;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE91C;
	sub_8263F668(ctx, base);
	// addi r4,r31,2144
	ctx.r4.s64 = ctx.r31.s64 + 2144;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE928;
	sub_8263F668(ctx, base);
	// addi r4,r31,2156
	ctx.r4.s64 = ctx.r31.s64 + 2156;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE934;
	sub_8263F668(ctx, base);
	// addi r4,r31,2168
	ctx.r4.s64 = ctx.r31.s64 + 2168;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE940;
	sub_8263F668(ctx, base);
	// addi r4,r31,2180
	ctx.r4.s64 = ctx.r31.s64 + 2180;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE94C;
	sub_8263F668(ctx, base);
	// addi r4,r31,2192
	ctx.r4.s64 = ctx.r31.s64 + 2192;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE958;
	sub_8263F668(ctx, base);
	// addi r4,r31,2204
	ctx.r4.s64 = ctx.r31.s64 + 2204;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE964;
	sub_8263F668(ctx, base);
	// addi r4,r31,2216
	ctx.r4.s64 = ctx.r31.s64 + 2216;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE970;
	sub_8263F668(ctx, base);
	// addi r4,r31,2228
	ctx.r4.s64 = ctx.r31.s64 + 2228;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE97C;
	sub_8263F668(ctx, base);
	// addi r4,r31,2240
	ctx.r4.s64 = ctx.r31.s64 + 2240;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE988;
	sub_8263F668(ctx, base);
	// addi r4,r31,2428
	ctx.r4.s64 = ctx.r31.s64 + 2428;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE994;
	sub_8263F668(ctx, base);
	// addi r4,r31,2252
	ctx.r4.s64 = ctx.r31.s64 + 2252;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE9A0;
	sub_8263F668(ctx, base);
	// addi r4,r31,2280
	ctx.r4.s64 = ctx.r31.s64 + 2280;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE9AC;
	sub_8263F668(ctx, base);
	// addi r4,r31,2292
	ctx.r4.s64 = ctx.r31.s64 + 2292;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE9B8;
	sub_8263F668(ctx, base);
	// addi r4,r31,2304
	ctx.r4.s64 = ctx.r31.s64 + 2304;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE9C4;
	sub_8263F668(ctx, base);
	// addi r4,r31,2316
	ctx.r4.s64 = ctx.r31.s64 + 2316;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE9D0;
	sub_8263F668(ctx, base);
	// addi r4,r31,2328
	ctx.r4.s64 = ctx.r31.s64 + 2328;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE9DC;
	sub_8263F668(ctx, base);
	// addi r4,r31,2340
	ctx.r4.s64 = ctx.r31.s64 + 2340;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE9E8;
	sub_8263F668(ctx, base);
	// addi r4,r31,2352
	ctx.r4.s64 = ctx.r31.s64 + 2352;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EE9F4;
	sub_8263F668(ctx, base);
	// addi r4,r31,2364
	ctx.r4.s64 = ctx.r31.s64 + 2364;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA00;
	sub_8263F668(ctx, base);
	// addi r4,r31,2440
	ctx.r4.s64 = ctx.r31.s64 + 2440;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA0C;
	sub_8263F668(ctx, base);
	// addi r4,r31,2452
	ctx.r4.s64 = ctx.r31.s64 + 2452;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA18;
	sub_8263F668(ctx, base);
	// addi r4,r31,2464
	ctx.r4.s64 = ctx.r31.s64 + 2464;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA24;
	sub_8263F668(ctx, base);
	// addi r4,r31,2480
	ctx.r4.s64 = ctx.r31.s64 + 2480;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA30;
	sub_8263F668(ctx, base);
	// addi r4,r31,2492
	ctx.r4.s64 = ctx.r31.s64 + 2492;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA3C;
	sub_8263F668(ctx, base);
	// addi r4,r31,2504
	ctx.r4.s64 = ctx.r31.s64 + 2504;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA48;
	sub_8263F668(ctx, base);
	// addi r4,r31,2520
	ctx.r4.s64 = ctx.r31.s64 + 2520;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA54;
	sub_8263F668(ctx, base);
	// addi r4,r31,2532
	ctx.r4.s64 = ctx.r31.s64 + 2532;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA60;
	sub_8263F668(ctx, base);
	// addi r4,r31,2544
	ctx.r4.s64 = ctx.r31.s64 + 2544;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA6C;
	sub_8263F668(ctx, base);
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825eed18
	if (!ctx.cr6.eq) goto loc_825EED18;
	// addi r4,r31,20084
	ctx.r4.s64 = ctx.r31.s64 + 20084;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA84;
	sub_8263F668(ctx, base);
	// addi r4,r31,20096
	ctx.r4.s64 = ctx.r31.s64 + 20096;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA90;
	sub_8263F668(ctx, base);
	// addi r4,r31,20108
	ctx.r4.s64 = ctx.r31.s64 + 20108;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEA9C;
	sub_8263F668(ctx, base);
	// addi r4,r31,20120
	ctx.r4.s64 = ctx.r31.s64 + 20120;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEAA8;
	sub_8263F668(ctx, base);
	// addi r4,r31,20148
	ctx.r4.s64 = ctx.r31.s64 + 20148;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEAB4;
	sub_8263F668(ctx, base);
	// addi r4,r31,20160
	ctx.r4.s64 = ctx.r31.s64 + 20160;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEAC0;
	sub_8263F668(ctx, base);
	// addi r4,r31,20172
	ctx.r4.s64 = ctx.r31.s64 + 20172;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEACC;
	sub_8263F668(ctx, base);
	// addi r4,r31,20184
	ctx.r4.s64 = ctx.r31.s64 + 20184;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEAD8;
	sub_8263F668(ctx, base);
	// addi r4,r31,20304
	ctx.r4.s64 = ctx.r31.s64 + 20304;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEAE4;
	sub_8263F668(ctx, base);
	// addi r4,r31,20316
	ctx.r4.s64 = ctx.r31.s64 + 20316;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEAF0;
	sub_8263F668(ctx, base);
	// addi r4,r31,20328
	ctx.r4.s64 = ctx.r31.s64 + 20328;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEAFC;
	sub_8263F668(ctx, base);
	// addi r4,r31,20340
	ctx.r4.s64 = ctx.r31.s64 + 20340;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB08;
	sub_8263F668(ctx, base);
	// addi r4,r31,20352
	ctx.r4.s64 = ctx.r31.s64 + 20352;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB14;
	sub_8263F668(ctx, base);
	// addi r4,r31,20364
	ctx.r4.s64 = ctx.r31.s64 + 20364;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB20;
	sub_8263F668(ctx, base);
	// addi r4,r31,20376
	ctx.r4.s64 = ctx.r31.s64 + 20376;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB2C;
	sub_8263F668(ctx, base);
	// addi r4,r31,20388
	ctx.r4.s64 = ctx.r31.s64 + 20388;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB38;
	sub_8263F668(ctx, base);
	// addi r4,r31,20400
	ctx.r4.s64 = ctx.r31.s64 + 20400;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB44;
	sub_8263F668(ctx, base);
	// addi r4,r31,20412
	ctx.r4.s64 = ctx.r31.s64 + 20412;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB50;
	sub_8263F668(ctx, base);
	// addi r4,r31,20424
	ctx.r4.s64 = ctx.r31.s64 + 20424;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB5C;
	sub_8263F668(ctx, base);
	// addi r4,r31,20436
	ctx.r4.s64 = ctx.r31.s64 + 20436;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB68;
	sub_8263F668(ctx, base);
	// addi r4,r31,20448
	ctx.r4.s64 = ctx.r31.s64 + 20448;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB74;
	sub_8263F668(ctx, base);
	// addi r4,r31,20460
	ctx.r4.s64 = ctx.r31.s64 + 20460;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB80;
	sub_8263F668(ctx, base);
	// addi r4,r31,20472
	ctx.r4.s64 = ctx.r31.s64 + 20472;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB8C;
	sub_8263F668(ctx, base);
	// addi r4,r31,20484
	ctx.r4.s64 = ctx.r31.s64 + 20484;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEB98;
	sub_8263F668(ctx, base);
	// addi r4,r31,20496
	ctx.r4.s64 = ctx.r31.s64 + 20496;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEBA4;
	sub_8263F668(ctx, base);
	// addi r4,r31,20508
	ctx.r4.s64 = ctx.r31.s64 + 20508;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEBB0;
	sub_8263F668(ctx, base);
	// addi r4,r31,20520
	ctx.r4.s64 = ctx.r31.s64 + 20520;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEBBC;
	sub_8263F668(ctx, base);
	// addi r4,r31,20532
	ctx.r4.s64 = ctx.r31.s64 + 20532;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEBC8;
	sub_8263F668(ctx, base);
	// addi r4,r31,20544
	ctx.r4.s64 = ctx.r31.s64 + 20544;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEBD4;
	sub_8263F668(ctx, base);
	// addi r4,r31,20556
	ctx.r4.s64 = ctx.r31.s64 + 20556;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEBE0;
	sub_8263F668(ctx, base);
	// addi r4,r31,20568
	ctx.r4.s64 = ctx.r31.s64 + 20568;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEBEC;
	sub_8263F668(ctx, base);
	// addi r4,r31,20580
	ctx.r4.s64 = ctx.r31.s64 + 20580;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEBF8;
	sub_8263F668(ctx, base);
	// addi r4,r31,20592
	ctx.r4.s64 = ctx.r31.s64 + 20592;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC04;
	sub_8263F668(ctx, base);
	// addi r4,r31,20604
	ctx.r4.s64 = ctx.r31.s64 + 20604;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC10;
	sub_8263F668(ctx, base);
	// addi r4,r31,20616
	ctx.r4.s64 = ctx.r31.s64 + 20616;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC1C;
	sub_8263F668(ctx, base);
	// addi r4,r31,20628
	ctx.r4.s64 = ctx.r31.s64 + 20628;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC28;
	sub_8263F668(ctx, base);
	// addi r4,r31,20640
	ctx.r4.s64 = ctx.r31.s64 + 20640;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC34;
	sub_8263F668(ctx, base);
	// addi r4,r31,20652
	ctx.r4.s64 = ctx.r31.s64 + 20652;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC40;
	sub_8263F668(ctx, base);
	// addi r4,r31,20664
	ctx.r4.s64 = ctx.r31.s64 + 20664;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC4C;
	sub_8263F668(ctx, base);
	// addi r4,r31,20676
	ctx.r4.s64 = ctx.r31.s64 + 20676;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC58;
	sub_8263F668(ctx, base);
	// addi r4,r31,20688
	ctx.r4.s64 = ctx.r31.s64 + 20688;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC64;
	sub_8263F668(ctx, base);
	// addi r4,r31,20700
	ctx.r4.s64 = ctx.r31.s64 + 20700;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC70;
	sub_8263F668(ctx, base);
	// addi r4,r31,20712
	ctx.r4.s64 = ctx.r31.s64 + 20712;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC7C;
	sub_8263F668(ctx, base);
	// addi r4,r31,20724
	ctx.r4.s64 = ctx.r31.s64 + 20724;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC88;
	sub_8263F668(ctx, base);
	// addi r4,r31,20736
	ctx.r4.s64 = ctx.r31.s64 + 20736;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EEC94;
	sub_8263F668(ctx, base);
	// addi r4,r31,20748
	ctx.r4.s64 = ctx.r31.s64 + 20748;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EECA0;
	sub_8263F668(ctx, base);
	// addi r4,r31,20760
	ctx.r4.s64 = ctx.r31.s64 + 20760;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EECAC;
	sub_8263F668(ctx, base);
	// addi r4,r31,20772
	ctx.r4.s64 = ctx.r31.s64 + 20772;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EECB8;
	sub_8263F668(ctx, base);
	// addi r4,r31,20784
	ctx.r4.s64 = ctx.r31.s64 + 20784;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EECC4;
	sub_8263F668(ctx, base);
	// addi r4,r31,20796
	ctx.r4.s64 = ctx.r31.s64 + 20796;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EECD0;
	sub_8263F668(ctx, base);
	// addi r4,r31,20808
	ctx.r4.s64 = ctx.r31.s64 + 20808;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EECDC;
	sub_8263F668(ctx, base);
	// addi r4,r31,20820
	ctx.r4.s64 = ctx.r31.s64 + 20820;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f668
	ctx.lr = 0x825EECE8;
	sub_8263F668(ctx, base);
	// lwz r3,21316(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21316);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eed00
	if (ctx.cr6.eq) goto loc_825EED00;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EECFC;
	sub_82120E68(ctx, base);
	// stw r30,21316(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21316, ctx.r30.u32);
loc_825EED00:
	// lwz r3,21320(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21320);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eed18
	if (ctx.cr6.eq) goto loc_825EED18;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EED14;
	sub_82120E68(ctx, base);
	// stw r30,21320(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21320, ctx.r30.u32);
loc_825EED18:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82632438
	ctx.lr = 0x825EED20;
	sub_82632438(ctx, base);
	// addi r28,r31,2800
	ctx.r28.s64 = ctx.r31.s64 + 2800;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 36);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eed3c
	if (ctx.cr6.eq) goto loc_825EED3C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EED38;
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r30.u32);
loc_825EED3C:
	// addi r28,r31,2840
	ctx.r28.s64 = ctx.r31.s64 + 2840;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 36);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eed58
	if (ctx.cr6.eq) goto loc_825EED58;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EED54;
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r30.u32);
loc_825EED58:
	// addi r28,r31,2760
	ctx.r28.s64 = ctx.r31.s64 + 2760;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 36);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eed74
	if (ctx.cr6.eq) goto loc_825EED74;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EED70;
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r30.u32);
loc_825EED74:
	// addi r28,r31,2720
	ctx.r28.s64 = ctx.r31.s64 + 2720;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 36);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eed90
	if (ctx.cr6.eq) goto loc_825EED90;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EED8C;
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r30.u32);
loc_825EED90:
	// addi r28,r31,2680
	ctx.r28.s64 = ctx.r31.s64 + 2680;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 36);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eedac
	if (ctx.cr6.eq) goto loc_825EEDAC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEDA8;
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r30.u32);
loc_825EEDAC:
	// addi r28,r31,2640
	ctx.r28.s64 = ctx.r31.s64 + 2640;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 36);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eedc8
	if (ctx.cr6.eq) goto loc_825EEDC8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEDC4;
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r30.u32);
loc_825EEDC8:
	// addi r28,r31,2600
	ctx.r28.s64 = ctx.r31.s64 + 2600;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 36);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eede4
	if (ctx.cr6.eq) goto loc_825EEDE4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEDE0;
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r30.u32);
loc_825EEDE4:
	// addi r28,r31,2560
	ctx.r28.s64 = ctx.r31.s64 + 2560;
	// lwz r3,36(r28)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r28.u32 + 36);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eee00
	if (ctx.cr6.eq) goto loc_825EEE00;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEDFC;
	sub_82120E68(ctx, base);
	// stw r30,36(r28)
	PPC_STORE_U32(ctx.r28.u32 + 36, ctx.r30.u32);
loc_825EEE00:
	// lwz r3,352(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 352);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eee18
	if (ctx.cr6.eq) goto loc_825EEE18;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEE14;
	sub_82120E68(ctx, base);
	// stw r30,352(r31)
	PPC_STORE_U32(ctx.r31.u32 + 352, ctx.r30.u32);
loc_825EEE18:
	// lwz r3,356(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 356);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eee30
	if (ctx.cr6.eq) goto loc_825EEE30;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEE2C;
	sub_82120E68(ctx, base);
	// stw r30,356(r31)
	PPC_STORE_U32(ctx.r31.u32 + 356, ctx.r30.u32);
loc_825EEE30:
	// lwz r3,360(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 360);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eee48
	if (ctx.cr6.eq) goto loc_825EEE48;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEE44;
	sub_82120E68(ctx, base);
	// stw r30,360(r31)
	PPC_STORE_U32(ctx.r31.u32 + 360, ctx.r30.u32);
loc_825EEE48:
	// lwz r3,364(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 364);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eee60
	if (ctx.cr6.eq) goto loc_825EEE60;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEE5C;
	sub_82120E68(ctx, base);
	// stw r30,364(r31)
	PPC_STORE_U32(ctx.r31.u32 + 364, ctx.r30.u32);
loc_825EEE60:
	// lwz r3,368(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 368);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eee78
	if (ctx.cr6.eq) goto loc_825EEE78;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEE74;
	sub_82120E68(ctx, base);
	// stw r30,368(r31)
	PPC_STORE_U32(ctx.r31.u32 + 368, ctx.r30.u32);
loc_825EEE78:
	// lwz r3,372(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 372);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eee90
	if (ctx.cr6.eq) goto loc_825EEE90;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEE8C;
	sub_82120E68(ctx, base);
	// stw r30,372(r31)
	PPC_STORE_U32(ctx.r31.u32 + 372, ctx.r30.u32);
loc_825EEE90:
	// lwz r3,384(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 384);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eeea8
	if (ctx.cr6.eq) goto loc_825EEEA8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEEA4;
	sub_82120E68(ctx, base);
	// stw r30,384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 384, ctx.r30.u32);
loc_825EEEA8:
	// lwz r3,388(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 388);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eeec0
	if (ctx.cr6.eq) goto loc_825EEEC0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEEBC;
	sub_82120E68(ctx, base);
	// stw r30,388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 388, ctx.r30.u32);
loc_825EEEC0:
	// lwz r3,15216(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15216);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eeed8
	if (ctx.cr6.eq) goto loc_825EEED8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEED4;
	sub_82120E68(ctx, base);
	// stw r30,15216(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15216, ctx.r30.u32);
loc_825EEED8:
	// lwz r11,3956(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3956);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825eef68
	if (ctx.cr6.eq) goto loc_825EEF68;
	// lwz r3,460(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 460);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eeefc
	if (ctx.cr6.eq) goto loc_825EEEFC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEEF8;
	sub_82120E68(ctx, base);
	// stw r30,460(r31)
	PPC_STORE_U32(ctx.r31.u32 + 460, ctx.r30.u32);
loc_825EEEFC:
	// lwz r3,15184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15184);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eef14
	if (ctx.cr6.eq) goto loc_825EEF14;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEF10;
	sub_82120E68(ctx, base);
	// stw r30,15184(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15184, ctx.r30.u32);
loc_825EEF14:
	// lwz r3,2976(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2976);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eef2c
	if (ctx.cr6.eq) goto loc_825EEF2C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEF28;
	sub_82120E68(ctx, base);
	// stw r30,2976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2976, ctx.r30.u32);
loc_825EEF2C:
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825eef50
	if (!ctx.cr6.eq) goto loc_825EEF50;
	// lwz r3,3052(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3052);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eef50
	if (ctx.cr6.eq) goto loc_825EEF50;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEF4C;
	sub_82120E68(ctx, base);
	// stw r30,3052(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3052, ctx.r30.u32);
loc_825EEF50:
	// lwz r3,15240(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15240);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eef68
	if (ctx.cr6.eq) goto loc_825EEF68;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEF64;
	sub_82120E68(ctx, base);
	// stw r30,15240(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15240, ctx.r30.u32);
loc_825EEF68:
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eef90
	if (ctx.cr6.eq) goto loc_825EEF90;
	// bl 0x82626028
	ctx.lr = 0x825EEF78;
	sub_82626028(ctx, base);
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eef90
	if (ctx.cr6.eq) goto loc_825EEF90;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEF8C;
	sub_82120E68(ctx, base);
	// stw r30,15204(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15204, ctx.r30.u32);
loc_825EEF90:
	// lwz r3,23252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23252);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eefa8
	if (ctx.cr6.eq) goto loc_825EEFA8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEFA4;
	sub_82120E68(ctx, base);
	// stw r30,23252(r31)
	PPC_STORE_U32(ctx.r31.u32 + 23252, ctx.r30.u32);
loc_825EEFA8:
	// lwz r3,21420(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21420);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825eefc0
	if (ctx.cr6.eq) goto loc_825EEFC0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EEFBC;
	sub_82120E68(ctx, base);
	// stw r30,21420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21420, ctx.r30.u32);
loc_825EEFC0:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
}

__attribute__((alias("__imp__sub_825EEFCC"))) PPC_WEAK_FUNC(sub_825EEFCC);
PPC_FUNC_IMPL(__imp__sub_825EEFCC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EEFD0"))) PPC_WEAK_FUNC(sub_825EEFD0);
PPC_FUNC_IMPL(__imp__sub_825EEFD0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825EEFD8;
	sub_8239BA1C(ctx, base);
	// stwu r1,-128(r1)
	ea = -128 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// lwz r3,21564(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21564);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef004
	if (ctx.cr6.eq) goto loc_825EF004;
	// lis r5,0
	ctx.r5.s64 = 0;
	// li r4,0
	ctx.r4.s64 = 0;
	// ori r5,r5,32768
	ctx.r5.u64 = ctx.r5.u64 | 32768;
	// bl 0x826927c0
	ctx.lr = 0x825EF000;
	sub_826927C0(ctx, base);
	// stw r30,21564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21564, ctx.r30.u32);
loc_825EF004:
	// lwz r3,21568(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21568);
	// lis r11,9356
	ctx.r11.s64 = 613154816;
	// ori r29,r11,32769
	ctx.r29.u64 = ctx.r11.u64 | 32769;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef024
	if (ctx.cr6.eq) goto loc_825EF024;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF020;
	sub_82120E68(ctx, base);
	// stw r30,21568(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21568, ctx.r30.u32);
loc_825EF024:
	// lwz r3,21572(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21572);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef03c
	if (ctx.cr6.eq) goto loc_825EF03C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF038;
	sub_82120E68(ctx, base);
	// stw r30,21572(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21572, ctx.r30.u32);
loc_825EF03C:
	// lwz r3,21576(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21576);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef054
	if (ctx.cr6.eq) goto loc_825EF054;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF050;
	sub_82120E68(ctx, base);
	// stw r30,21576(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21576, ctx.r30.u32);
loc_825EF054:
	// lis r11,1
	ctx.r11.s64 = 65536;
	// ori r11,r11,33704
	ctx.r11.u64 = ctx.r11.u64 | 33704;
	// lwzx r11,r31,r11
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r11.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825ef164
	if (!ctx.cr6.eq) goto loc_825EF164;
	// lwz r3,3688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// bl 0x825ee140
	ctx.lr = 0x825EF070;
	sub_825EE140(ctx, base);
	// lwz r3,3688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef088
	if (ctx.cr6.eq) goto loc_825EF088;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF084;
	sub_82120E68(ctx, base);
	// stw r30,3688(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3688, ctx.r30.u32);
loc_825EF088:
	// lwz r3,3696(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3696);
	// bl 0x825ee140
	ctx.lr = 0x825EF090;
	sub_825EE140(ctx, base);
	// lwz r3,3696(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3696);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef0a8
	if (ctx.cr6.eq) goto loc_825EF0A8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF0A4;
	sub_82120E68(ctx, base);
	// stw r30,3696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3696, ctx.r30.u32);
loc_825EF0A8:
	// lwz r3,3692(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3692);
	// bl 0x825ee140
	ctx.lr = 0x825EF0B0;
	sub_825EE140(ctx, base);
	// lwz r3,3692(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3692);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef0c8
	if (ctx.cr6.eq) goto loc_825EF0C8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF0C4;
	sub_82120E68(ctx, base);
	// stw r30,3692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3692, ctx.r30.u32);
loc_825EF0C8:
	// lwz r3,3700(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3700);
	// bl 0x825ee140
	ctx.lr = 0x825EF0D0;
	sub_825EE140(ctx, base);
	// lwz r3,3700(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3700);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef0e8
	if (ctx.cr6.eq) goto loc_825EF0E8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF0E4;
	sub_82120E68(ctx, base);
	// stw r30,3700(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3700, ctx.r30.u32);
loc_825EF0E8:
	// lwz r3,3704(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// bl 0x825ee140
	ctx.lr = 0x825EF0F0;
	sub_825EE140(ctx, base);
	// lwz r3,3704(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3704);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef108
	if (ctx.cr6.eq) goto loc_825EF108;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF104;
	sub_82120E68(ctx, base);
	// stw r30,3704(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3704, ctx.r30.u32);
loc_825EF108:
	// lwz r3,3708(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3708);
	// bl 0x825ee140
	ctx.lr = 0x825EF110;
	sub_825EE140(ctx, base);
	// lwz r3,3708(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3708);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef128
	if (ctx.cr6.eq) goto loc_825EF128;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF124;
	sub_82120E68(ctx, base);
	// stw r30,3708(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3708, ctx.r30.u32);
loc_825EF128:
	// lwz r11,21580(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21580);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ef164
	if (ctx.cr6.eq) goto loc_825EF164;
	// li r5,-1
	ctx.r5.s64 = -1;
	// lwz r3,15204(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15204);
	// addi r4,r1,80
	ctx.r4.s64 = ctx.r1.s64 + 80;
	// stw r30,80(r1)
	PPC_STORE_U32(ctx.r1.u32 + 80, ctx.r30.u32);
	// bl 0x82626170
	ctx.lr = 0x825EF148;
	sub_82626170(ctx, base);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// bl 0x825ee140
	ctx.lr = 0x825EF150;
	sub_825EE140(ctx, base);
	// lwz r3,80(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 80);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef164
	if (ctx.cr6.eq) goto loc_825EF164;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF164;
	sub_82120E68(ctx, base);
loc_825EF164:
	// lwz r3,21240(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21240);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef17c
	if (ctx.cr6.eq) goto loc_825EF17C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF178;
	sub_82120E68(ctx, base);
	// stw r30,21240(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21240, ctx.r30.u32);
loc_825EF17C:
	// lwz r3,21268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21268);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef194
	if (ctx.cr6.eq) goto loc_825EF194;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF190;
	sub_82120E68(ctx, base);
	// stw r30,21268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21268, ctx.r30.u32);
loc_825EF194:
	// lwz r3,21252(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21252);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef1ac
	if (ctx.cr6.eq) goto loc_825EF1AC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF1A8;
	sub_82120E68(ctx, base);
	// stw r30,21252(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21252, ctx.r30.u32);
loc_825EF1AC:
	// lwz r3,3916(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3916);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef1c4
	if (ctx.cr6.eq) goto loc_825EF1C4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF1C0;
	sub_82120E68(ctx, base);
	// stw r30,3916(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3916, ctx.r30.u32);
loc_825EF1C4:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825ef218
	if (!ctx.cr6.eq) goto loc_825EF218;
	// lwz r3,19992(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19992);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef1e8
	if (ctx.cr6.eq) goto loc_825EF1E8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF1E4;
	sub_82120E68(ctx, base);
	// stw r30,19992(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19992, ctx.r30.u32);
loc_825EF1E8:
	// lwz r3,19996(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 19996);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef200
	if (ctx.cr6.eq) goto loc_825EF200;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF1FC;
	sub_82120E68(ctx, base);
	// stw r30,19996(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19996, ctx.r30.u32);
loc_825EF200:
	// lwz r3,20000(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20000);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef218
	if (ctx.cr6.eq) goto loc_825EF218;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF214;
	sub_82120E68(ctx, base);
	// stw r30,20000(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20000, ctx.r30.u32);
loc_825EF218:
	// lwz r11,3956(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3956);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ef48c
	if (ctx.cr6.eq) goto loc_825EF48C;
	// lwz r3,460(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 460);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef23c
	if (ctx.cr6.eq) goto loc_825EF23C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF238;
	sub_82120E68(ctx, base);
	// stw r30,460(r31)
	PPC_STORE_U32(ctx.r31.u32 + 460, ctx.r30.u32);
loc_825EF23C:
	// lwz r3,15184(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15184);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef254
	if (ctx.cr6.eq) goto loc_825EF254;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF250;
	sub_82120E68(ctx, base);
	// stw r30,15184(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15184, ctx.r30.u32);
loc_825EF254:
	// lwz r3,2976(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 2976);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef26c
	if (ctx.cr6.eq) goto loc_825EF26C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF268;
	sub_82120E68(ctx, base);
	// stw r30,2976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2976, ctx.r30.u32);
loc_825EF26C:
	// lwz r11,23968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23968);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ef360
	if (ctx.cr6.eq) goto loc_825EF360;
	// lwz r11,16472(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16472);
	// cmplw cr6,r31,r11
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r11.u32, ctx.xer);
	// bne cr6,0x825ef314
	if (!ctx.cr6.eq) goto loc_825EF314;
	// lwz r3,3048(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3048);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef29c
	if (ctx.cr6.eq) goto loc_825EF29C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF298;
	sub_82120E68(ctx, base);
	// stw r30,3048(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3048, ctx.r30.u32);
loc_825EF29C:
	// lwz r3,15268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15268);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef2b4
	if (ctx.cr6.eq) goto loc_825EF2B4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF2B0;
	sub_82120E68(ctx, base);
	// stw r30,15268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15268, ctx.r30.u32);
loc_825EF2B4:
	// lwz r3,15292(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15292);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef2cc
	if (ctx.cr6.eq) goto loc_825EF2CC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF2C8;
	sub_82120E68(ctx, base);
	// stw r30,15292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15292, ctx.r30.u32);
loc_825EF2CC:
	// lwz r3,15208(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15208);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef2e4
	if (ctx.cr6.eq) goto loc_825EF2E4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF2E0;
	sub_82120E68(ctx, base);
	// stw r30,15208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15208, ctx.r30.u32);
loc_825EF2E4:
	// lwz r3,1772(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef2fc
	if (ctx.cr6.eq) goto loc_825EF2FC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF2F8;
	sub_82120E68(ctx, base);
	// stw r30,1772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1772, ctx.r30.u32);
loc_825EF2FC:
	// lwz r3,1780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1780);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef32c
	if (ctx.cr6.eq) goto loc_825EF32C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF310;
	sub_82120E68(ctx, base);
	// b 0x825ef328
	goto loc_825EF328;
loc_825EF314:
	// stw r30,3048(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3048, ctx.r30.u32);
	// stw r30,15268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15268, ctx.r30.u32);
	// stw r30,15292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15292, ctx.r30.u32);
	// stw r30,15208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15208, ctx.r30.u32);
	// stw r30,1772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1772, ctx.r30.u32);
loc_825EF328:
	// stw r30,1780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1780, ctx.r30.u32);
loc_825EF32C:
	// lwz r3,15276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15276);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef344
	if (ctx.cr6.eq) goto loc_825EF344;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF340;
	sub_82120E68(ctx, base);
	// stw r30,15276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15276, ctx.r30.u32);
loc_825EF344:
	// lwz r3,15284(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15284);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef420
	if (ctx.cr6.eq) goto loc_825EF420;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF358;
	sub_82120E68(ctx, base);
	// stw r30,15284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15284, ctx.r30.u32);
	// b 0x825ef420
	goto loc_825EF420;
loc_825EF360:
	// lwz r3,15208(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15208);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef378
	if (ctx.cr6.eq) goto loc_825EF378;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF374;
	sub_82120E68(ctx, base);
	// stw r30,15208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15208, ctx.r30.u32);
loc_825EF378:
	// lwz r3,3048(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3048);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef390
	if (ctx.cr6.eq) goto loc_825EF390;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF38C;
	sub_82120E68(ctx, base);
	// stw r30,3048(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3048, ctx.r30.u32);
loc_825EF390:
	// lwz r3,15268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15268);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef3a8
	if (ctx.cr6.eq) goto loc_825EF3A8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF3A4;
	sub_82120E68(ctx, base);
	// stw r30,15268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15268, ctx.r30.u32);
loc_825EF3A8:
	// lwz r3,15292(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15292);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef3c0
	if (ctx.cr6.eq) goto loc_825EF3C0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF3BC;
	sub_82120E68(ctx, base);
	// stw r30,15292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15292, ctx.r30.u32);
loc_825EF3C0:
	// lwz r3,15276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15276);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef3d8
	if (ctx.cr6.eq) goto loc_825EF3D8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF3D4;
	sub_82120E68(ctx, base);
	// stw r30,15276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15276, ctx.r30.u32);
loc_825EF3D8:
	// lwz r3,15284(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15284);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef3f0
	if (ctx.cr6.eq) goto loc_825EF3F0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF3EC;
	sub_82120E68(ctx, base);
	// stw r30,15284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15284, ctx.r30.u32);
loc_825EF3F0:
	// lwz r3,1772(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef408
	if (ctx.cr6.eq) goto loc_825EF408;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF404;
	sub_82120E68(ctx, base);
	// stw r30,1772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1772, ctx.r30.u32);
loc_825EF408:
	// lwz r3,1780(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1780);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef420
	if (ctx.cr6.eq) goto loc_825EF420;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF41C;
	sub_82120E68(ctx, base);
	// stw r30,1780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1780, ctx.r30.u32);
loc_825EF420:
	// lwz r3,276(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 276);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef438
	if (ctx.cr6.eq) goto loc_825EF438;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF434;
	sub_82120E68(ctx, base);
	// stw r30,276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 276, ctx.r30.u32);
loc_825EF438:
	// lwz r3,268(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 268);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef450
	if (ctx.cr6.eq) goto loc_825EF450;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF44C;
	sub_82120E68(ctx, base);
	// stw r30,268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 268, ctx.r30.u32);
loc_825EF450:
	// lwz r11,16(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825ef474
	if (!ctx.cr6.eq) goto loc_825EF474;
	// lwz r3,3052(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3052);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef474
	if (ctx.cr6.eq) goto loc_825EF474;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF470;
	sub_82120E68(ctx, base);
	// stw r30,3052(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3052, ctx.r30.u32);
loc_825EF474:
	// lwz r3,15240(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15240);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef48c
	if (ctx.cr6.eq) goto loc_825EF48C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF488;
	sub_82120E68(ctx, base);
	// stw r30,15240(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15240, ctx.r30.u32);
loc_825EF48C:
	// lwz r11,14788(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14788);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825ef4e0
	if (ctx.cr6.eq) goto loc_825EF4E0;
	// lwz r3,14808(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14808);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef4b0
	if (ctx.cr6.eq) goto loc_825EF4B0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF4AC;
	sub_82120E68(ctx, base);
	// stw r30,14808(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14808, ctx.r30.u32);
loc_825EF4B0:
	// lwz r3,14812(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14812);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef4c8
	if (ctx.cr6.eq) goto loc_825EF4C8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF4C4;
	sub_82120E68(ctx, base);
	// stw r30,14812(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14812, ctx.r30.u32);
loc_825EF4C8:
	// lwz r3,14816(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14816);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef4e0
	if (ctx.cr6.eq) goto loc_825EF4E0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF4DC;
	sub_82120E68(ctx, base);
	// stw r30,14816(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14816, ctx.r30.u32);
loc_825EF4E0:
	// lwz r3,264(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 264);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef4f8
	if (ctx.cr6.eq) goto loc_825EF4F8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF4F4;
	sub_82120E68(ctx, base);
	// stw r30,264(r31)
	PPC_STORE_U32(ctx.r31.u32 + 264, ctx.r30.u32);
loc_825EF4F8:
	// lwz r3,272(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 272);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef510
	if (ctx.cr6.eq) goto loc_825EF510;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF50C;
	sub_82120E68(ctx, base);
	// stw r30,272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 272, ctx.r30.u32);
loc_825EF510:
	// lwz r3,1892(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1892);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef528
	if (ctx.cr6.eq) goto loc_825EF528;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF524;
	sub_82120E68(ctx, base);
	// stw r30,1892(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1892, ctx.r30.u32);
loc_825EF528:
	// lwz r3,1896(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1896);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef540
	if (ctx.cr6.eq) goto loc_825EF540;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF53C;
	sub_82120E68(ctx, base);
	// stw r30,1896(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1896, ctx.r30.u32);
loc_825EF540:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// blt cr6,0x825ef554
	if (ctx.cr6.lt) goto loc_825EF554;
	// lwz r3,1968(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1968);
	// bl 0x82643f80
	ctx.lr = 0x825EF554;
	sub_82643F80(ctx, base);
loc_825EF554:
	// lwz r3,15656(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15656);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef56c
	if (ctx.cr6.eq) goto loc_825EF56C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF568;
	sub_82120E68(ctx, base);
	// stw r30,15656(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15656, ctx.r30.u32);
loc_825EF56C:
	// lwz r3,15664(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15664);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef584
	if (ctx.cr6.eq) goto loc_825EF584;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF580;
	sub_82120E68(ctx, base);
	// stw r30,15664(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15664, ctx.r30.u32);
loc_825EF584:
	// lwz r3,15660(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15660);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef59c
	if (ctx.cr6.eq) goto loc_825EF59C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF598;
	sub_82120E68(ctx, base);
	// stw r30,15660(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15660, ctx.r30.u32);
loc_825EF59C:
	// lwz r3,15668(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15668);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef5b4
	if (ctx.cr6.eq) goto loc_825EF5B4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF5B0;
	sub_82120E68(ctx, base);
	// stw r30,15668(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15668, ctx.r30.u32);
loc_825EF5B4:
	// lwz r11,23968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23968);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825ef5fc
	if (ctx.cr6.eq) goto loc_825EF5FC;
	// lwz r11,16472(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16472);
	// cmplw cr6,r31,r11
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825ef5fc
	if (ctx.cr6.eq) goto loc_825EF5FC;
	// stw r30,15672(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15672, ctx.r30.u32);
	// stw r30,15680(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15680, ctx.r30.u32);
	// stw r30,15688(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15688, ctx.r30.u32);
	// stw r30,15696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15696, ctx.r30.u32);
	// stw r30,15704(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15704, ctx.r30.u32);
	// stw r30,15712(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15712, ctx.r30.u32);
	// stw r30,15720(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15720, ctx.r30.u32);
	// stw r30,15728(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15728, ctx.r30.u32);
	// stw r30,15736(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15736, ctx.r30.u32);
	// stw r30,15744(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15744, ctx.r30.u32);
	// stw r30,15752(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15752, ctx.r30.u32);
	// b 0x825ef718
	goto loc_825EF718;
loc_825EF5FC:
	// lwz r3,15672(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15672);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef614
	if (ctx.cr6.eq) goto loc_825EF614;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF610;
	sub_82120E68(ctx, base);
	// stw r30,15672(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15672, ctx.r30.u32);
loc_825EF614:
	// lwz r3,15680(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15680);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef62c
	if (ctx.cr6.eq) goto loc_825EF62C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF628;
	sub_82120E68(ctx, base);
	// stw r30,15680(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15680, ctx.r30.u32);
loc_825EF62C:
	// lwz r3,15688(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15688);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef644
	if (ctx.cr6.eq) goto loc_825EF644;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF640;
	sub_82120E68(ctx, base);
	// stw r30,15688(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15688, ctx.r30.u32);
loc_825EF644:
	// lwz r3,15696(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15696);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef65c
	if (ctx.cr6.eq) goto loc_825EF65C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF658;
	sub_82120E68(ctx, base);
	// stw r30,15696(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15696, ctx.r30.u32);
loc_825EF65C:
	// lwz r3,15704(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15704);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef674
	if (ctx.cr6.eq) goto loc_825EF674;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF670;
	sub_82120E68(ctx, base);
	// stw r30,15704(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15704, ctx.r30.u32);
loc_825EF674:
	// lwz r3,15712(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15712);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef68c
	if (ctx.cr6.eq) goto loc_825EF68C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF688;
	sub_82120E68(ctx, base);
	// stw r30,15712(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15712, ctx.r30.u32);
loc_825EF68C:
	// lwz r3,15720(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15720);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef6a4
	if (ctx.cr6.eq) goto loc_825EF6A4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF6A0;
	sub_82120E68(ctx, base);
	// stw r30,15720(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15720, ctx.r30.u32);
loc_825EF6A4:
	// lwz r3,15728(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15728);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef6bc
	if (ctx.cr6.eq) goto loc_825EF6BC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF6B8;
	sub_82120E68(ctx, base);
	// stw r30,15728(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15728, ctx.r30.u32);
loc_825EF6BC:
	// lwz r3,15736(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15736);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef6d4
	if (ctx.cr6.eq) goto loc_825EF6D4;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF6D0;
	sub_82120E68(ctx, base);
	// stw r30,15736(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15736, ctx.r30.u32);
loc_825EF6D4:
	// lwz r3,15744(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15744);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef6ec
	if (ctx.cr6.eq) goto loc_825EF6EC;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF6E8;
	sub_82120E68(ctx, base);
	// stw r30,15744(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15744, ctx.r30.u32);
loc_825EF6EC:
	// lwz r3,15752(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15752);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef704
	if (ctx.cr6.eq) goto loc_825EF704;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF700;
	sub_82120E68(ctx, base);
	// stw r30,15752(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15752, ctx.r30.u32);
loc_825EF704:
	// lwz r3,15760(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15760);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef71c
	if (ctx.cr6.eq) goto loc_825EF71C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF718;
	sub_82120E68(ctx, base);
loc_825EF718:
	// stw r30,15760(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15760, ctx.r30.u32);
loc_825EF71C:
	// lwz r11,3356(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3356);
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// blt cr6,0x825ef848
	if (ctx.cr6.lt) goto loc_825EF848;
	// lwz r3,15676(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15676);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef740
	if (ctx.cr6.eq) goto loc_825EF740;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF73C;
	sub_82120E68(ctx, base);
	// stw r30,15676(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15676, ctx.r30.u32);
loc_825EF740:
	// lwz r3,15684(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15684);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef758
	if (ctx.cr6.eq) goto loc_825EF758;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF754;
	sub_82120E68(ctx, base);
	// stw r30,15684(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15684, ctx.r30.u32);
loc_825EF758:
	// lwz r3,15692(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15692);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef770
	if (ctx.cr6.eq) goto loc_825EF770;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF76C;
	sub_82120E68(ctx, base);
	// stw r30,15692(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15692, ctx.r30.u32);
loc_825EF770:
	// lwz r3,15700(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15700);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef788
	if (ctx.cr6.eq) goto loc_825EF788;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF784;
	sub_82120E68(ctx, base);
	// stw r30,15700(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15700, ctx.r30.u32);
loc_825EF788:
	// lwz r3,15708(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15708);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef7a0
	if (ctx.cr6.eq) goto loc_825EF7A0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF79C;
	sub_82120E68(ctx, base);
	// stw r30,15708(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15708, ctx.r30.u32);
loc_825EF7A0:
	// lwz r3,15716(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15716);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef7b8
	if (ctx.cr6.eq) goto loc_825EF7B8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF7B4;
	sub_82120E68(ctx, base);
	// stw r30,15716(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15716, ctx.r30.u32);
loc_825EF7B8:
	// lwz r3,15724(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15724);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef7d0
	if (ctx.cr6.eq) goto loc_825EF7D0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF7CC;
	sub_82120E68(ctx, base);
	// stw r30,15724(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15724, ctx.r30.u32);
loc_825EF7D0:
	// lwz r3,15732(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15732);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef7e8
	if (ctx.cr6.eq) goto loc_825EF7E8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF7E4;
	sub_82120E68(ctx, base);
	// stw r30,15732(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15732, ctx.r30.u32);
loc_825EF7E8:
	// lwz r3,15740(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15740);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef800
	if (ctx.cr6.eq) goto loc_825EF800;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF7FC;
	sub_82120E68(ctx, base);
	// stw r30,15740(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15740, ctx.r30.u32);
loc_825EF800:
	// lwz r3,15748(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15748);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef818
	if (ctx.cr6.eq) goto loc_825EF818;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF814;
	sub_82120E68(ctx, base);
	// stw r30,15748(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15748, ctx.r30.u32);
loc_825EF818:
	// lwz r3,15756(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15756);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef830
	if (ctx.cr6.eq) goto loc_825EF830;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF82C;
	sub_82120E68(ctx, base);
	// stw r30,15756(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15756, ctx.r30.u32);
loc_825EF830:
	// lwz r3,15764(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15764);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef848
	if (ctx.cr6.eq) goto loc_825EF848;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF844;
	sub_82120E68(ctx, base);
	// stw r30,15764(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15764, ctx.r30.u32);
loc_825EF848:
	// lwz r3,21420(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21420);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825ef860
	if (ctx.cr6.eq) goto loc_825EF860;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EF85C;
	sub_82120E68(ctx, base);
	// stw r30,21420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21420, ctx.r30.u32);
loc_825EF860:
	// addi r1,r1,128
	ctx.r1.s64 = ctx.r1.s64 + 128;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825EF868"))) PPC_WEAK_FUNC(sub_825EF868);
PPC_FUNC_IMPL(__imp__sub_825EF868) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825EF870;
	sub_8239BA1C(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// addi r11,r4,15
	ctx.r11.s64 = ctx.r4.s64 + 15;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// rlwinm r30,r11,0,0,27
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFF0;
	// addi r11,r5,15
	ctx.r11.s64 = ctx.r5.s64 + 15;
	// cmpw cr6,r4,r30
	ctx.cr6.compare<int32_t>(ctx.r4.s32, ctx.r30.s32, ctx.xer);
	// rlwinm r29,r11,0,0,27
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFF0;
	// lwz r11,21500(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21500);
	// lwz r10,21504(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21504);
	// lwz r9,21508(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21508);
	// lwz r8,21512(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21512);
	// stw r4,156(r31)
	PPC_STORE_U32(ctx.r31.u32 + 156, ctx.r4.u32);
	// stw r5,160(r31)
	PPC_STORE_U32(ctx.r31.u32 + 160, ctx.r5.u32);
	// stw r11,21484(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21484, ctx.r11.u32);
	// stw r10,21488(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21488, ctx.r10.u32);
	// stw r9,21492(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21492, ctx.r9.u32);
	// stw r8,21496(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21496, ctx.r8.u32);
	// bne cr6,0x825ef8c4
	if (!ctx.cr6.eq) goto loc_825EF8C4;
	// cmpw cr6,r5,r29
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r29.s32, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// beq cr6,0x825ef8c8
	if (ctx.cr6.eq) goto loc_825EF8C8;
loc_825EF8C4:
	// li r11,0
	ctx.r11.s64 = 0;
loc_825EF8C8:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,152(r31)
	PPC_STORE_U32(ctx.r31.u32 + 152, ctx.r11.u32);
	// bl 0x825cc908
	ctx.lr = 0x825EF8D4;
	sub_825CC908(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825ef8ec
	if (!ctx.cr6.eq) goto loc_825EF8EC;
	// lwz r11,180(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// stw r11,21380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21380, ctx.r11.u32);
	// stw r10,21384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21384, ctx.r10.u32);
loc_825EF8EC:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r30,180(r31)
	PPC_STORE_U32(ctx.r31.u32 + 180, ctx.r30.u32);
	// stw r29,188(r31)
	PPC_STORE_U32(ctx.r31.u32 + 188, ctx.r29.u32);
	// bl 0x825cc908
	ctx.lr = 0x825EF8FC;
	sub_825CC908(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825ef914
	if (ctx.cr6.eq) goto loc_825EF914;
	// lwz r11,180(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// stw r11,21380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21380, ctx.r11.u32);
	// stw r10,21384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21384, ctx.r10.u32);
loc_825EF914:
	// li r11,-63
	ctx.r11.s64 = -63;
	// lwz r10,3924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// li r9,63
	ctx.r9.s64 = 63;
	// stw r30,21500(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21500, ctx.r30.u32);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r29,21504(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21504, ctx.r29.u32);
	// stw r11,240(r31)
	PPC_STORE_U32(ctx.r31.u32 + 240, ctx.r11.u32);
	// stw r9,244(r31)
	PPC_STORE_U32(ctx.r31.u32 + 244, ctx.r9.u32);
	// beq cr6,0x825ef958
	if (ctx.cr6.eq) goto loc_825EF958;
	// lwz r10,180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// lwz r11,188(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// srawi r10,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 2;
	// stw r11,200(r31)
	PPC_STORE_U32(ctx.r31.u32 + 200, ctx.r11.u32);
	// stw r10,192(r31)
	PPC_STORE_U32(ctx.r31.u32 + 192, ctx.r10.u32);
	// stw r10,21508(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21508, ctx.r10.u32);
	// stw r11,21512(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21512, ctx.r11.u32);
	// b 0x825ef978
	goto loc_825EF978;
loc_825EF958:
	// lwz r11,180(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// stw r11,192(r31)
	PPC_STORE_U32(ctx.r31.u32 + 192, ctx.r11.u32);
	// stw r11,21508(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21508, ctx.r11.u32);
	// stw r10,200(r31)
	PPC_STORE_U32(ctx.r31.u32 + 200, ctx.r10.u32);
	// stw r10,21512(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21512, ctx.r10.u32);
loc_825EF978:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bge cr6,0x825ef99c
	if (!ctx.cr6.lt) goto loc_825EF99C;
	// lwz r11,21508(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21508);
	// lwz r10,21512(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21512);
	// stw r30,21484(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21484, ctx.r30.u32);
	// stw r29,21488(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21488, ctx.r29.u32);
	// stw r11,21492(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21492, ctx.r11.u32);
	// stw r10,21496(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21496, ctx.r10.u32);
loc_825EF99C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ed280
	ctx.lr = 0x825EF9A4;
	sub_825ED280(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82632700
	ctx.lr = 0x825EF9AC;
	sub_82632700(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825EF9B8"))) PPC_WEAK_FUNC(sub_825EF9B8);
PPC_FUNC_IMPL(__imp__sub_825EF9B8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825EF9C0;
	sub_8239BA1C(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,9356
	ctx.r11.s64 = 613154816;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// ori r29,r11,32769
	ctx.r29.u64 = ctx.r11.u64 | 32769;
	// li r3,16
	ctx.r3.s64 = 16;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82121108
	ctx.lr = 0x825EF9DC;
	sub_82121108(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r11,15204(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15204, ctx.r11.u32);
	// beq cr6,0x825efbf0
	if (ctx.cr6.eq) goto loc_825EFBF0;
	// li r30,0
	ctx.r30.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// li r3,32
	ctx.r3.s64 = 32;
	// stw r30,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r30.u32);
	// stw r30,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r30.u32);
	// stw r30,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r30.u32);
	// stw r30,12(r11)
	PPC_STORE_U32(ctx.r11.u32 + 12, ctx.r30.u32);
	// bl 0x82121108
	ctx.lr = 0x825EFA0C;
	sub_82121108(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// li r3,32
	ctx.r3.s64 = 32;
	// stw r11,1900(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1900, ctx.r11.u32);
	// bl 0x82121108
	ctx.lr = 0x825EFA20;
	sub_82121108(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// li r3,32
	ctx.r3.s64 = 32;
	// stw r11,1904(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1904, ctx.r11.u32);
	// bl 0x82121108
	ctx.lr = 0x825EFA34;
	sub_82121108(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// li r3,32
	ctx.r3.s64 = 32;
	// stw r11,1908(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1908, ctx.r11.u32);
	// bl 0x82121108
	ctx.lr = 0x825EFA48;
	sub_82121108(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lwz r3,1900(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1900);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r11,1912(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1912, ctx.r11.u32);
	// beq cr6,0x825efbf0
	if (ctx.cr6.eq) goto loc_825EFBF0;
	// lwz r10,1904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1904);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825efbf0
	if (ctx.cr6.eq) goto loc_825EFBF0;
	// lwz r10,1908(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1908);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825efbf0
	if (ctx.cr6.eq) goto loc_825EFBF0;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825efbf0
	if (ctx.cr6.eq) goto loc_825EFBF0;
	// li r5,32
	ctx.r5.s64 = 32;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239ca70
	ctx.lr = 0x825EFA88;
	sub_8239CA70(ctx, base);
	// li r5,32
	ctx.r5.s64 = 32;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,1904(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1904);
	// bl 0x8239ca70
	ctx.lr = 0x825EFA98;
	sub_8239CA70(ctx, base);
	// li r5,32
	ctx.r5.s64 = 32;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,1908(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1908);
	// bl 0x8239ca70
	ctx.lr = 0x825EFAA8;
	sub_8239CA70(ctx, base);
	// li r5,32
	ctx.r5.s64 = 32;
	// li r4,0
	ctx.r4.s64 = 0;
	// lwz r3,1912(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1912);
	// bl 0x8239ca70
	ctx.lr = 0x825EFAB8;
	sub_8239CA70(ctx, base);
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// lwz r10,1900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1900);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825efaf4
	if (!ctx.cr6.eq) goto loc_825EFAF4;
	// li r11,1024
	ctx.r11.s64 = 1024;
	// li r9,1
	ctx.r9.s64 = 1;
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r11.u16);
	// lwz r10,1900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1900);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r11.u16);
	// lwz r10,1904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1904);
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r11.u16);
	// lwz r10,1904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1904);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r11.u16);
	// stw r9,3900(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3900, ctx.r9.u32);
	// b 0x825efb18
	goto loc_825EFB18;
loc_825EFAF4:
	// li r11,128
	ctx.r11.s64 = 128;
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r11.u16);
	// lwz r10,1900(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1900);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r11.u16);
	// lwz r10,1904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1904);
	// sth r11,16(r10)
	PPC_STORE_U16(ctx.r10.u32 + 16, ctx.r11.u16);
	// lwz r10,1904(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1904);
	// sth r11,0(r10)
	PPC_STORE_U16(ctx.r10.u32 + 0, ctx.r11.u16);
	// stw r30,3900(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3900, ctx.r30.u32);
loc_825EFB18:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// blt cr6,0x825efb4c
	if (ctx.cr6.lt) goto loc_825EFB4C;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,3340(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// bl 0x826452b8
	ctx.lr = 0x825EFB30;
	sub_826452B8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,1972(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1972, ctx.r3.u32);
	// beq cr6,0x825efbf0
	if (ctx.cr6.eq) goto loc_825EFBF0;
	// bl 0x82644210
	ctx.lr = 0x825EFB40;
	sub_82644210(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,1964(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1964, ctx.r3.u32);
	// beq cr6,0x825efbf0
	if (ctx.cr6.eq) goto loc_825EFBF0;
loc_825EFB4C:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// blt cr6,0x825efba0
	if (ctx.cr6.lt) goto loc_825EFBA0;
	// lwz r11,352(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 352);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825efb7c
	if (!ctx.cr6.eq) goto loc_825EFB7C;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x82121108
	ctx.lr = 0x825EFB70;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,352(r31)
	PPC_STORE_U32(ctx.r31.u32 + 352, ctx.r3.u32);
	// beq cr6,0x825efbf0
	if (ctx.cr6.eq) goto loc_825EFBF0;
loc_825EFB7C:
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// li r3,832
	ctx.r3.s64 = 832;
	// bl 0x82121108
	ctx.lr = 0x825EFB88;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15216(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15216, ctx.r3.u32);
	// beq cr6,0x825efbf0
	if (ctx.cr6.eq) goto loc_825EFBF0;
	// addi r11,r3,31
	ctx.r11.s64 = ctx.r3.s64 + 31;
	// rlwinm r11,r11,0,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFE0;
	// stw r11,15220(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15220, ctx.r11.u32);
loc_825EFBA0:
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// li r3,1568
	ctx.r3.s64 = 1568;
	// bl 0x82121108
	ctx.lr = 0x825EFBAC;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,1880(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1880, ctx.r3.u32);
	// beq cr6,0x825efbf0
	if (ctx.cr6.eq) goto loc_825EFBF0;
	// addi r11,r3,31
	ctx.r11.s64 = ctx.r3.s64 + 31;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// rlwinm r11,r11,0,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFE0;
	// li r3,832
	ctx.r3.s64 = 832;
	// stw r11,1884(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1884, ctx.r11.u32);
	// bl 0x82121108
	ctx.lr = 0x825EFBD0;
	sub_82121108(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r10,r11,60
	ctx.r10.s64 = ctx.r11.s64 + 60;
	// rlwinm r10,r10,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// stw r11,1872(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1872, ctx.r11.u32);
	// stw r10,1876(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1876, ctx.r10.u32);
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
loc_825EFBF0:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825EFBFC"))) PPC_WEAK_FUNC(sub_825EFBFC);
PPC_FUNC_IMPL(__imp__sub_825EFBFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825EFC00"))) PPC_WEAK_FUNC(sub_825EFC00);
PPC_FUNC_IMPL(__imp__sub_825EFC00) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba00
	ctx.lr = 0x825EFC08;
	sub_8239BA00(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// mr r25,r5
	ctx.r25.u64 = ctx.r5.u64;
	// addi r11,r28,15
	ctx.r11.s64 = ctx.r28.s64 + 15;
	// addi r10,r25,15
	ctx.r10.s64 = ctx.r25.s64 + 15;
	// rlwinm r11,r11,0,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFF0;
	// rlwinm r10,r10,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// srawi r24,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r24.s64 = ctx.r11.s32 >> 4;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// srawi r22,r10,4
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r22.s64 = ctx.r10.s32 >> 4;
	// mullw r26,r22,r24
	ctx.r26.s64 = int64_t(ctx.r22.s32) * int64_t(ctx.r24.s32);
	// stw r11,180(r31)
	PPC_STORE_U32(ctx.r31.u32 + 180, ctx.r11.u32);
	// stw r10,188(r31)
	PPC_STORE_U32(ctx.r31.u32 + 188, ctx.r10.u32);
	// stw r24,136(r31)
	PPC_STORE_U32(ctx.r31.u32 + 136, ctx.r24.u32);
	// stw r22,140(r31)
	PPC_STORE_U32(ctx.r31.u32 + 140, ctx.r22.u32);
	// bl 0x826327a0
	ctx.lr = 0x825EFC48;
	sub_826327A0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f03b8
	if (!ctx.cr6.eq) goto loc_825F03B8;
	// lis r11,1
	ctx.r11.s64 = 65536;
	// lis r10,-32127
	ctx.r10.s64 = -2105475072;
	// ori r9,r11,33704
	ctx.r9.u64 = ctx.r11.u64 | 33704;
	// li r11,32
	ctx.r11.s64 = 32;
	// stw r11,-4512(r10)
	PPC_STORE_U32(ctx.r10.u32 + -4512, ctx.r11.u32);
	// li r11,16
	ctx.r11.s64 = 16;
	// lis r10,-32127
	ctx.r10.s64 = -2105475072;
	// stw r11,-14764(r10)
	PPC_STORE_U32(ctx.r10.u32 + -14764, ctx.r11.u32);
	// lwzx r11,r31,r9
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r9.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825efc98
	if (ctx.cr6.eq) goto loc_825EFC98;
	// lis r11,1
	ctx.r11.s64 = 65536;
	// lis r10,1
	ctx.r10.s64 = 65536;
	// ori r11,r11,33696
	ctx.r11.u64 = ctx.r11.u64 | 33696;
	// ori r10,r10,33700
	ctx.r10.u64 = ctx.r10.u64 | 33700;
	// stwx r28,r31,r11
	PPC_STORE_U32(ctx.r31.u32 + ctx.r11.u32, ctx.r28.u32);
	// stwx r25,r31,r10
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, ctx.r25.u32);
	// b 0x825efcb4
	goto loc_825EFCB4;
loc_825EFC98:
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825eb8f0
	ctx.lr = 0x825EFCAC;
	sub_825EB8F0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f03b8
	if (!ctx.cr6.eq) goto loc_825F03B8;
loc_825EFCB4:
	// lis r11,9356
	ctx.r11.s64 = 613154816;
	// addi r23,r22,2
	ctx.r23.s64 = ctx.r22.s64 + 2;
	// ori r30,r11,32769
	ctx.r30.u64 = ctx.r11.u64 | 32769;
	// rlwinm r29,r23,2,0,29
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r23.u32 | (ctx.r23.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82121108
	ctx.lr = 0x825EFCD0;
	sub_82121108(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r11,21240(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21240, ctx.r11.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// li r27,0
	ctx.r27.s64 = 0;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stw r27,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r27.u32);
	// bl 0x82121108
	ctx.lr = 0x825EFCF4;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,21268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21268, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239ca70
	ctx.lr = 0x825EFD0C;
	sub_8239CA70(ctx, base);
	// lwz r11,21268(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21268);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stw r11,21264(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21264, ctx.r11.u32);
	// bl 0x82121108
	ctx.lr = 0x825EFD20;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,21252(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21252, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239ca70
	ctx.lr = 0x825EFD38;
	sub_8239CA70(ctx, base);
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825efdc8
	if (!ctx.cr6.eq) goto loc_825EFDC8;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// rlwinm r3,r24,9,0,22
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r24.u32 | (ctx.r24.u64 << 32), 9) & 0xFFFFFE00;
	// bl 0x82121108
	ctx.lr = 0x825EFD50;
	sub_82121108(ctx, base);
	// rlwinm r29,r24,7,0,24
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r24.u32 | (ctx.r24.u64 << 32), 7) & 0xFFFFFF80;
	// stw r3,19992(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19992, ctx.r3.u32);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82121108
	ctx.lr = 0x825EFD64;
	sub_82121108(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stw r11,19996(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19996, ctx.r11.u32);
	// bl 0x82121108
	ctx.lr = 0x825EFD78;
	sub_82121108(ctx, base);
	// stw r3,20000(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20000, ctx.r3.u32);
	// lwz r3,21636(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21636);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825efd94
	if (ctx.cr6.eq) goto loc_825EFD94;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x82120e68
	ctx.lr = 0x825EFD90;
	sub_82120E68(ctx, base);
	// stw r27,21636(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21636, ctx.r27.u32);
loc_825EFD94:
	// addi r11,r22,1
	ctx.r11.s64 = ctx.r22.s64 + 1;
	// addi r10,r24,1
	ctx.r10.s64 = ctx.r24.s64 + 1;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mullw r3,r11,r10
	ctx.r3.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// bl 0x82121108
	ctx.lr = 0x825EFDB0;
	sub_82121108(ctx, base);
	// stw r3,21636(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21636, ctx.r3.u32);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x825efdc8
	if (!ctx.cr6.eq) goto loc_825EFDC8;
	// li r3,-3
	ctx.r3.s64 = -3;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825EFDC8:
	// lwz r11,3956(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3956);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825f00bc
	if (ctx.cr6.eq) goto loc_825F00BC;
	// lwz r11,3356(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3356);
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// li r11,1536
	ctx.r11.s64 = 1536;
	// beq cr6,0x825efde8
	if (ctx.cr6.eq) goto loc_825EFDE8;
	// li r11,640
	ctx.r11.s64 = 640;
loc_825EFDE8:
	// mullw r11,r11,r24
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r24.s32);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82121108
	ctx.lr = 0x825EFDFC;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,2976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2976, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// lwz r11,3924(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// stw r27,460(r31)
	PPC_STORE_U32(ctx.r31.u32 + 460, ctx.r27.u32);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825efe24
	if (ctx.cr6.eq) goto loc_825EFE24;
	// mulli r3,r26,224
	ctx.r3.s64 = ctx.r26.s64 * 224;
	// b 0x825efe30
	goto loc_825EFE30;
loc_825EFE24:
	// rlwinm r11,r26,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// rlwinm r3,r11,6,0,25
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 6) & 0xFFFFFFC0;
loc_825EFE30:
	// bl 0x82121108
	ctx.lr = 0x825EFE34;
	sub_82121108(ctx, base);
	// rotlwi r11,r3,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// stw r3,460(r31)
	PPC_STORE_U32(ctx.r31.u32 + 460, ctx.r3.u32);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// lwz r11,23968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23968);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825efe5c
	if (ctx.cr6.eq) goto loc_825EFE5C;
	// lwz r10,16472(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16472);
	// cmplw cr6,r31,r10
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r10.u32, ctx.xer);
	// bne cr6,0x825efe88
	if (!ctx.cr6.eq) goto loc_825EFE88;
loc_825EFE5C:
	// rlwinm r11,r26,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// rlwinm r3,r11,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82121108
	ctx.lr = 0x825EFE70;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,3048(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3048, ctx.r3.u32);
	// bne cr6,0x825efe94
	if (!ctx.cr6.eq) goto loc_825EFE94;
loc_825EFE7C:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825EFE88:
	// lwz r11,16472(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16472);
	// lwz r11,3048(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 3048);
	// stw r11,3048(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3048, ctx.r11.u32);
loc_825EFE94:
	// lwz r11,3356(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3356);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// rlwinm r10,r11,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// subf r11,r11,r10
	ctx.r11.s64 = ctx.r10.s64 - ctx.r11.s64;
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// rlwinm r3,r11,5,0,26
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// bl 0x82121108
	ctx.lr = 0x825EFEB0;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15240(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15240, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// rlwinm r10,r28,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r27,15184(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15184, ctx.r27.u32);
	// addi r11,r25,72
	ctx.r11.s64 = ctx.r25.s64 + 72;
	// add r10,r28,r10
	ctx.r10.u64 = ctx.r28.u64 + ctx.r10.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r3,r10,160
	ctx.r3.s64 = ctx.r10.s64 + 160;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// bge cr6,0x825efee4
	if (!ctx.cr6.lt) goto loc_825EFEE4;
	// mr r3,r11
	ctx.r3.u64 = ctx.r11.u64;
loc_825EFEE4:
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// bl 0x82121108
	ctx.lr = 0x825EFEEC;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15184(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15184, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// addi r11,r3,31
	ctx.r11.s64 = ctx.r3.s64 + 31;
	// lwz r10,3924(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// lwz r3,460(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 460);
	// li r4,0
	ctx.r4.s64 = 0;
	// rlwinm r11,r11,0,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFE0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r11,15188(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15188, ctx.r11.u32);
	// beq cr6,0x825eff20
	if (ctx.cr6.eq) goto loc_825EFF20;
	// mulli r5,r26,224
	ctx.r5.s64 = ctx.r26.s64 * 224;
	// b 0x825eff2c
	goto loc_825EFF2C;
loc_825EFF20:
	// rlwinm r11,r26,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// rlwinm r5,r11,6,0,25
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 6) & 0xFFFFFFC0;
loc_825EFF2C:
	// bl 0x8239ca70
	ctx.lr = 0x825EFF30;
	sub_8239CA70(ctx, base);
	// lwz r11,23968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23968);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825f0020
	if (ctx.cr6.eq) goto loc_825F0020;
	// lwz r10,16472(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16472);
	// cmplw cr6,r31,r10
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r10.u32, ctx.xer);
	// bne cr6,0x825effb8
	if (!ctx.cr6.eq) goto loc_825EFFB8;
	// rlwinm r28,r26,4,0,27
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82121108
	ctx.lr = 0x825EFF58;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15208, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mullw r11,r23,r24
	ctx.r11.s64 = int64_t(ctx.r23.s32) * int64_t(ctx.r24.s32);
	// rlwinm r29,r11,4,0,27
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82121108
	ctx.lr = 0x825EFF78;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15268, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239ca70
	ctx.lr = 0x825EFF90;
	sub_8239CA70(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82121108
	ctx.lr = 0x825EFF9C;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15292, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// rlwinm r11,r26,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// stw r11,15296(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15296, ctx.r11.u32);
	// b 0x825effe0
	goto loc_825EFFE0;
loc_825EFFB8:
	// lwz r11,16472(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16472);
	// rlwinm r10,r26,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r9,15208(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 15208);
	// stw r9,15208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15208, ctx.r9.u32);
	// lwz r9,15268(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 15268);
	// stw r9,15268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15268, ctx.r9.u32);
	// lwz r11,15292(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 15292);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
	// stw r11,15292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15292, ctx.r11.u32);
	// stw r10,15296(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15296, ctx.r10.u32);
loc_825EFFE0:
	// rlwinm r29,r26,4,0,27
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82121108
	ctx.lr = 0x825EFFF0;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15276, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82121108
	ctx.lr = 0x825F0008;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15284, ctx.r3.u32);
	// bne cr6,0x825f00bc
	if (!ctx.cr6.eq) goto loc_825F00BC;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825F0020:
	// rlwinm r29,r26,4,0,27
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82121108
	ctx.lr = 0x825F0030;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15208, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mullw r11,r23,r24
	ctx.r11.s64 = int64_t(ctx.r23.s32) * int64_t(ctx.r24.s32);
	// rlwinm r28,r11,4,0,27
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// bl 0x82121108
	ctx.lr = 0x825F0050;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15268, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239ca70
	ctx.lr = 0x825F0068;
	sub_8239CA70(ctx, base);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82121108
	ctx.lr = 0x825F0074;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15292, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// rlwinm r11,r26,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// stw r11,15296(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15296, ctx.r11.u32);
	// bl 0x82121108
	ctx.lr = 0x825F0098;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15276, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82121108
	ctx.lr = 0x825F00B0;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15284, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
loc_825F00BC:
	// add r11,r26,r24
	ctx.r11.u64 = ctx.r26.u64 + ctx.r24.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// rlwinm r10,r11,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82121108
	ctx.lr = 0x825F00D4;
	sub_82121108(ctx, base);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r11,3916(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3916, ctx.r11.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// stw r11,3920(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3920, ctx.r11.u32);
	// rlwinm r3,r26,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82121108
	ctx.lr = 0x825F00F4;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,21576(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21576, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// rlwinm r3,r24,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r24.u32 | (ctx.r24.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	ctx.lr = 0x825F010C;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,264(r31)
	PPC_STORE_U32(ctx.r31.u32 + 264, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// lwz r11,14788(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14788);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825f013c
	if (ctx.cr6.eq) goto loc_825F013C;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// rlwinm r3,r24,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r24.u32 | (ctx.r24.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82121108
	ctx.lr = 0x825F0130;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,14808(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14808, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
loc_825F013C:
	// addi r11,r22,1
	ctx.r11.s64 = ctx.r22.s64 + 1;
	// lis r5,8320
	ctx.r5.s64 = 545259520;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// li r6,4
	ctx.r6.s64 = 4;
	// mullw r29,r11,r24
	ctx.r29.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r24.s32);
	// rlwinm r11,r29,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 1) & 0xFFFFFFFE;
	// ori r5,r5,4096
	ctx.r5.u64 = ctx.r5.u64 | 4096;
	// add r11,r29,r11
	ctx.r11.u64 = ctx.r29.u64 + ctx.r11.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// rlwinm r27,r11,9,0,22
	ctx.r27.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 9) & 0xFFFFFE00;
	// addi r4,r27,256
	ctx.r4.s64 = ctx.r27.s64 + 256;
	// bl 0x82692770
	ctx.lr = 0x825F016C;
	sub_82692770(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,21564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21564, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// rlwinm r10,r29,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r3,127
	ctx.r11.s64 = ctx.r3.s64 + 127;
	// add r10,r29,r10
	ctx.r10.u64 = ctx.r29.u64 + ctx.r10.u64;
	// rlwinm r11,r11,0,0,24
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFF80;
	// rlwinm r28,r10,4,0,27
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 4) & 0xFFFFFFF0;
	// add r10,r27,r11
	ctx.r10.u64 = ctx.r27.u64 + ctx.r11.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r10,r10,128
	ctx.r10.s64 = ctx.r10.s64 + 128;
	// mr r3,r28
	ctx.r3.u64 = ctx.r28.u64;
	// stw r11,21556(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21556, ctx.r11.u32);
	// rlwinm r10,r10,0,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFF80;
	// stw r10,21560(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21560, ctx.r10.u32);
	// bl 0x82121108
	ctx.lr = 0x825F01AC;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,21568(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21568, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// addi r3,r28,4
	ctx.r3.s64 = ctx.r28.s64 + 4;
	// bl 0x82121108
	ctx.lr = 0x825F01C4;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,21572(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21572, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// rlwinm r11,r29,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// add r11,r29,r11
	ctx.r11.u64 = ctx.r29.u64 + ctx.r11.u64;
	// rlwinm r3,r11,3,0,28
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// bl 0x82121108
	ctx.lr = 0x825F01E4;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 268, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// lwz r11,14788(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14788);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825f022c
	if (ctx.cr6.eq) goto loc_825F022C;
	// rlwinm r11,r26,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// rlwinm r29,r11,2,0,29
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x82121108
	ctx.lr = 0x825F0214;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 276, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239ca70
	ctx.lr = 0x825F022C;
	sub_8239CA70(ctx, base);
loc_825F022C:
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// li r3,20
	ctx.r3.s64 = 20;
	// bl 0x82121108
	ctx.lr = 0x825F0238;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 272, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// rlwinm r11,r24,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r24.u32 | (ctx.r24.u64 << 32), 1) & 0xFFFFFFFE;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// add r10,r24,r11
	ctx.r10.u64 = ctx.r24.u64 + ctx.r11.u64;
	// rlwinm r11,r24,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r24.u32 | (ctx.r24.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r10,5,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 5) & 0xFFFFFFE0;
	// add r11,r24,r11
	ctx.r11.u64 = ctx.r24.u64 + ctx.r11.u64;
	// rlwinm r3,r11,8,0,23
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFFFFFF00;
	// stw r10,1888(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1888, ctx.r10.u32);
	// bl 0x82121108
	ctx.lr = 0x825F0268;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,1892(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1892, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// rlwinm r11,r24,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r24.u32 | (ctx.r24.u64 << 32), 3) & 0xFFFFFFF8;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// add r11,r24,r11
	ctx.r11.u64 = ctx.r24.u64 + ctx.r11.u64;
	// rlwinm r3,r11,5,0,26
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 5) & 0xFFFFFFE0;
	// bl 0x82121108
	ctx.lr = 0x825F0288;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,1896(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1896, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// blt cr6,0x825f02b8
	if (ctx.cr6.lt) goto loc_825F02B8;
	// li r4,2
	ctx.r4.s64 = 2;
	// rlwinm r3,r24,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r24.u32 | (ctx.r24.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82643ee8
	ctx.lr = 0x825F02AC;
	sub_82643EE8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,1968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1968, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
loc_825F02B8:
	// lwz r11,23968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 23968);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825f033c
	if (ctx.cr6.eq) goto loc_825F033C;
	// lwz r10,16472(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16472);
	// cmplw cr6,r31,r10
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, ctx.r10.u32, ctx.xer);
	// bne cr6,0x825f0318
	if (!ctx.cr6.eq) goto loc_825F0318;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// rlwinm r3,r26,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x82121108
	ctx.lr = 0x825F02DC;
	sub_82121108(ctx, base);
	// rlwinm r11,r26,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 3) & 0xFFFFFFF8;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,1772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1772, ctx.r3.u32);
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// stw r11,1776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1776, ctx.r11.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// rlwinm r3,r26,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	ctx.lr = 0x825F0300;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,1780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1780, ctx.r3.u32);
	// bne cr6,0x825f037c
	if (!ctx.cr6.eq) goto loc_825F037C;
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825F0318:
	// lwz r11,16472(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16472);
	// rlwinm r9,r26,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r10,1772(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1772);
	// add r9,r9,r10
	ctx.r9.u64 = ctx.r9.u64 + ctx.r10.u64;
	// stw r10,1772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1772, ctx.r10.u32);
	// stw r9,1776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1776, ctx.r9.u32);
	// lwz r11,1780(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 1780);
	// stw r11,1780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1780, ctx.r11.u32);
	// b 0x825f037c
	goto loc_825F037C;
loc_825F033C:
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// rlwinm r3,r26,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x82121108
	ctx.lr = 0x825F0348;
	sub_82121108(ctx, base);
	// rlwinm r11,r26,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 3) & 0xFFFFFFF8;
	// rotlwi r10,r3,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r3.u32, 0);
	// stw r3,1772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1772, ctx.r3.u32);
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stw r11,1776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1776, ctx.r11.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// rlwinm r3,r26,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	ctx.lr = 0x825F0370;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,1780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1780, ctx.r3.u32);
	// beq cr6,0x825efe7c
	if (ctx.cr6.eq) goto loc_825EFE7C;
loc_825F037C:
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// lwz r10,16(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 16);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// stw r11,3716(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3716, ctx.r11.u32);
	// bne cr6,0x825f03b4
	if (!ctx.cr6.eq) goto loc_825F03B4;
	// rlwinm r11,r26,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// add r11,r26,r11
	ctx.r11.u64 = ctx.r26.u64 + ctx.r11.u64;
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	ctx.lr = 0x825F03A4;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,3052(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3052, ctx.r3.u32);
	// li r3,2
	ctx.r3.s64 = 2;
	// beq cr6,0x825f03b8
	if (ctx.cr6.eq) goto loc_825F03B8;
loc_825F03B4:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825F03B8:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
}

__attribute__((alias("__imp__sub_825F03C0"))) PPC_WEAK_FUNC(sub_825F03C0);
PPC_FUNC_IMPL(__imp__sub_825F03C0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825F03C8;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// mr r27,r5
	ctx.r27.u64 = ctx.r5.u64;
	// li r5,636
	ctx.r5.s64 = 636;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// bl 0x8239ca70
	ctx.lr = 0x825F03E8;
	sub_8239CA70(ctx, base);
	// addi r11,r28,16
	ctx.r11.s64 = ctx.r28.s64 + 16;
	// lis r10,8320
	ctx.r10.s64 = 545259520;
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// ori r26,r10,4096
	ctx.r26.u64 = ctx.r10.u64 | 4096;
	// add r29,r11,r25
	ctx.r29.u64 = ctx.r11.u64 + ctx.r25.u64;
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// bl 0x82692770
	ctx.lr = 0x825F0410;
	sub_82692770(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// stw r30,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r30.u32);
	// bne cr6,0x825f042c
	if (!ctx.cr6.eq) goto loc_825F042C;
loc_825F0420:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
loc_825F042C:
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8239ca70
	ctx.lr = 0x825F043C;
	sub_8239CA70(ctx, base);
	// add r11,r27,r28
	ctx.r11.u64 = ctx.r27.u64 + ctx.r28.u64;
	// add r10,r30,r28
	ctx.r10.u64 = ctx.r30.u64 + ctx.r28.u64;
	// addi r29,r11,32
	ctx.r29.s64 = ctx.r11.s64 + 32;
	// addi r11,r10,31
	ctx.r11.s64 = ctx.r10.s64 + 31;
	// li r6,4
	ctx.r6.s64 = 4;
	// rlwinm r11,r11,0,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFE0;
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// bl 0x82692770
	ctx.lr = 0x825F0468;
	sub_82692770(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// stw r30,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r30.u32);
	// beq cr6,0x825f0420
	if (ctx.cr6.eq) goto loc_825F0420;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239ca70
	ctx.lr = 0x825F0484;
	sub_8239CA70(ctx, base);
	// srawi r27,r28,1
	ctx.xer.ca = (ctx.r28.s32 < 0) & ((ctx.r28.u32 & 0x1) != 0);
	ctx.r27.s64 = ctx.r28.s32 >> 1;
	// add r10,r27,r30
	ctx.r10.u64 = ctx.r27.u64 + ctx.r30.u64;
	// clrlwi r11,r10,27
	ctx.r11.u64 = ctx.r10.u32 & 0x1F;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825f049c
	if (ctx.cr6.eq) goto loc_825F049C;
	// subfic r11,r11,32
	ctx.xer.ca = ctx.r11.u32 <= 32;
	ctx.r11.s64 = 32 - ctx.r11.s64;
loc_825F049C:
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// li r6,4
	ctx.r6.s64 = 4;
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// bl 0x82692770
	ctx.lr = 0x825F04B8;
	sub_82692770(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// stw r30,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r30.u32);
	// beq cr6,0x825f0420
	if (ctx.cr6.eq) goto loc_825F0420;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// bl 0x8239ca70
	ctx.lr = 0x825F04D4;
	sub_8239CA70(ctx, base);
	// add r10,r27,r30
	ctx.r10.u64 = ctx.r27.u64 + ctx.r30.u64;
	// clrlwi r11,r10,27
	ctx.r11.u64 = ctx.r10.u32 & 0x1F;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825f04e8
	if (ctx.cr6.eq) goto loc_825F04E8;
	// subfic r11,r11,32
	ctx.xer.ca = ctx.r11.u32 <= 32;
	ctx.r11.s64 = 32 - ctx.r11.s64;
loc_825F04E8:
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r28,592(r31)
	PPC_STORE_U32(ctx.r31.u32 + 592, ctx.r28.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r25,588(r31)
	PPC_STORE_U32(ctx.r31.u32 + 588, ctx.r25.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825F0504"))) PPC_WEAK_FUNC(sub_825F0504);
PPC_FUNC_IMPL(__imp__sub_825F0504) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F0508"))) PPC_WEAK_FUNC(sub_825F0508);
PPC_FUNC_IMPL(__imp__sub_825F0508) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825F0510;
	sub_8239BA1C(ctx, base);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r10,1
	ctx.r10.s64 = 1;
	// li r11,0
	ctx.r11.s64 = 0;
	// mr r30,r4
	ctx.r30.u64 = ctx.r4.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// lwz r9,3392(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3392);
	// stw r10,14788(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14788, ctx.r10.u32);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stw r11,284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 284, ctx.r11.u32);
	// beq cr6,0x825f056c
	if (ctx.cr6.eq) goto loc_825F056C;
	// lwz r10,21360(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21360);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825f0554
	if (ctx.cr6.eq) goto loc_825F0554;
	// lwz r10,21364(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21364);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// beq cr6,0x825f056c
	if (ctx.cr6.eq) goto loc_825F056C;
loc_825F0554:
	// li r10,-3
	ctx.r10.s64 = -3;
	// stw r11,3380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3380, ctx.r11.u32);
	// stw r11,3396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3396, ctx.r11.u32);
	// stw r11,3384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3384, ctx.r11.u32);
	// stw r11,3400(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3400, ctx.r11.u32);
	// stw r10,3376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3376, ctx.r10.u32);
loc_825F056C:
	// stw r11,3360(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3360, ctx.r11.u32);
	// lwz r11,1872(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1872);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f059c
	if (!ctx.cr6.eq) goto loc_825F059C;
	// lis r4,9356
	ctx.r4.s64 = 613154816;
	// li r3,832
	ctx.r3.s64 = 832;
	// ori r4,r4,32769
	ctx.r4.u64 = ctx.r4.u64 | 32769;
	// bl 0x82121108
	ctx.lr = 0x825F058C;
	sub_82121108(ctx, base);
	// addi r11,r3,60
	ctx.r11.s64 = ctx.r3.s64 + 60;
	// stw r3,1872(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1872, ctx.r3.u32);
	// rlwinm r11,r11,0,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFF0;
	// stw r11,1876(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1876, ctx.r11.u32);
loc_825F059C:
	// lwz r11,21184(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21184);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825f05c8
	if (ctx.cr6.eq) goto loc_825F05C8;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ef868
	ctx.lr = 0x825F05B8;
	sub_825EF868(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f05cc
	if (!ctx.cr6.eq) goto loc_825F05CC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ebc08
	ctx.lr = 0x825F05C8;
	sub_825EBC08(ctx, base);
loc_825F05C8:
	// li r3,0
	ctx.r3.s64 = 0;
loc_825F05CC:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825F05D4"))) PPC_WEAK_FUNC(sub_825F05D4);
PPC_FUNC_IMPL(__imp__sub_825F05D4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F05D8"))) PPC_WEAK_FUNC(sub_825F05D8);
PPC_FUNC_IMPL(__imp__sub_825F05D8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9e0
	ctx.lr = 0x825F05E0;
	sub_8239B9E0(ctx, base);
	// stwu r1,-240(r1)
	ea = -240 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// li r29,1
	ctx.r29.s64 = 1;
	// li r11,1000
	ctx.r11.s64 = 1000;
	// addi r27,r31,1988
	ctx.r27.s64 = ctx.r31.s64 + 1988;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// stw r5,3656(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3656, ctx.r5.u32);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// stw r30,20864(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20864, ctx.r30.u32);
	// mr r25,r6
	ctx.r25.u64 = ctx.r6.u64;
	// stw r30,404(r31)
	PPC_STORE_U32(ctx.r31.u32 + 404, ctx.r30.u32);
	// mr r24,r7
	ctx.r24.u64 = ctx.r7.u64;
	// stw r30,21236(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21236, ctx.r30.u32);
	// mr r18,r8
	ctx.r18.u64 = ctx.r8.u64;
	// stw r30,21240(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21240, ctx.r30.u32);
	// stw r30,21252(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21252, ctx.r30.u32);
	// stw r30,21256(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21256, ctx.r30.u32);
	// stw r30,21260(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21260, ctx.r30.u32);
	// stw r30,21244(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21244, ctx.r30.u32);
	// stw r29,15468(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15468, ctx.r29.u32);
	// stw r29,448(r31)
	PPC_STORE_U32(ctx.r31.u32 + 448, ctx.r29.u32);
	// stw r29,1944(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1944, ctx.r29.u32);
	// stw r11,15532(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15532, ctx.r11.u32);
	// stw r30,15548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15548, ctx.r30.u32);
	// stw r29,15536(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15536, ctx.r29.u32);
	// stw r30,3916(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3916, ctx.r30.u32);
	// stw r30,3920(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3920, ctx.r30.u32);
	// stw r30,3932(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3932, ctx.r30.u32);
	// stw r30,2976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2976, ctx.r30.u32);
	// stw r30,2972(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2972, ctx.r30.u32);
	// stw r30,2968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2968, ctx.r30.u32);
	// stw r30,1792(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1792, ctx.r30.u32);
	// stw r30,14796(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14796, ctx.r30.u32);
	// stw r30,14800(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14800, ctx.r30.u32);
	// stw r30,14792(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14792, ctx.r30.u32);
	// stw r29,3368(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3368, ctx.r29.u32);
	// stw r30,3372(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3372, ctx.r30.u32);
	// stw r30,3364(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3364, ctx.r30.u32);
	// stw r30,21232(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21232, ctx.r30.u32);
	// stw r30,21292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21292, ctx.r30.u32);
	// stw r30,356(r31)
	PPC_STORE_U32(ctx.r31.u32 + 356, ctx.r30.u32);
	// stw r30,360(r31)
	PPC_STORE_U32(ctx.r31.u32 + 360, ctx.r30.u32);
	// stw r30,364(r31)
	PPC_STORE_U32(ctx.r31.u32 + 364, ctx.r30.u32);
	// stw r30,368(r31)
	PPC_STORE_U32(ctx.r31.u32 + 368, ctx.r30.u32);
	// stw r30,372(r31)
	PPC_STORE_U32(ctx.r31.u32 + 372, ctx.r30.u32);
	// stw r30,352(r31)
	PPC_STORE_U32(ctx.r31.u32 + 352, ctx.r30.u32);
	// stw r30,14772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14772, ctx.r30.u32);
	// stw r30,21296(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21296, ctx.r30.u32);
	// stw r30,21300(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21300, ctx.r30.u32);
	// stw r30,21308(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21308, ctx.r30.u32);
	// stw r30,21304(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21304, ctx.r30.u32);
	// stw r30,14808(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14808, ctx.r30.u32);
	// stw r30,14812(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14812, ctx.r30.u32);
	// stw r30,14816(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14816, ctx.r30.u32);
	// stw r30,276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 276, ctx.r30.u32);
	// stw r30,3712(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3712, ctx.r30.u32);
	// stw r30,3700(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3700, ctx.r30.u32);
	// stw r30,3412(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3412, ctx.r30.u32);
	// stw r30,3052(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3052, ctx.r30.u32);
	// stw r30,3048(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3048, ctx.r30.u32);
	// stw r30,15240(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15240, ctx.r30.u32);
	// stw r30,15244(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15244, ctx.r30.u32);
	// stw r30,15248(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15248, ctx.r30.u32);
	// stw r30,15252(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15252, ctx.r30.u32);
	// stw r30,15256(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15256, ctx.r30.u32);
	// stw r30,15260(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15260, ctx.r30.u32);
	// stw r30,15264(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15264, ctx.r30.u32);
	// stw r30,15552(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15552, ctx.r30.u32);
	// stw r30,3420(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3420, ctx.r30.u32);
	// stw r30,15268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15268, ctx.r30.u32);
	// stw r30,15292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15292, ctx.r30.u32);
	// stw r30,15276(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15276, ctx.r30.u32);
	// stw r30,15284(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15284, ctx.r30.u32);
	// stw r30,3436(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3436, ctx.r30.u32);
	// stw r30,3440(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3440, ctx.r30.u32);
	// stw r30,3428(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3428, ctx.r30.u32);
	// stw r30,252(r31)
	PPC_STORE_U32(ctx.r31.u32 + 252, ctx.r30.u32);
	// stw r30,3444(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3444, ctx.r30.u32);
	// stw r30,3448(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3448, ctx.r30.u32);
	// stw r30,3716(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3716, ctx.r30.u32);
	// bl 0x82626248
	ctx.lr = 0x825F0728;
	sub_82626248(ctx, base);
	// addi r26,r31,2000
	ctx.r26.s64 = ctx.r31.s64 + 2000;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x82626248
	ctx.lr = 0x825F0734;
	sub_82626248(ctx, base);
	// addi r17,r31,2040
	ctx.r17.s64 = ctx.r31.s64 + 2040;
	// mr r3,r17
	ctx.r3.u64 = ctx.r17.u64;
	// bl 0x82626248
	ctx.lr = 0x825F0740;
	sub_82626248(ctx, base);
	// addi r16,r31,2052
	ctx.r16.s64 = ctx.r31.s64 + 2052;
	// mr r3,r16
	ctx.r3.u64 = ctx.r16.u64;
	// bl 0x82626248
	ctx.lr = 0x825F074C;
	sub_82626248(ctx, base);
	// addi r15,r31,2064
	ctx.r15.s64 = ctx.r31.s64 + 2064;
	// mr r3,r15
	ctx.r3.u64 = ctx.r15.u64;
	// bl 0x82626248
	ctx.lr = 0x825F0758;
	sub_82626248(ctx, base);
	// addi r14,r31,2076
	ctx.r14.s64 = ctx.r31.s64 + 2076;
	// mr r3,r14
	ctx.r3.u64 = ctx.r14.u64;
	// bl 0x82626248
	ctx.lr = 0x825F0764;
	sub_82626248(ctx, base);
	// addi r23,r31,2116
	ctx.r23.s64 = ctx.r31.s64 + 2116;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// bl 0x82626248
	ctx.lr = 0x825F0770;
	sub_82626248(ctx, base);
	// addi r22,r31,2128
	ctx.r22.s64 = ctx.r31.s64 + 2128;
	// mr r3,r22
	ctx.r3.u64 = ctx.r22.u64;
	// bl 0x82626248
	ctx.lr = 0x825F077C;
	sub_82626248(ctx, base);
	// addi r21,r31,2144
	ctx.r21.s64 = ctx.r31.s64 + 2144;
	// mr r3,r21
	ctx.r3.u64 = ctx.r21.u64;
	// bl 0x82626248
	ctx.lr = 0x825F0788;
	sub_82626248(ctx, base);
	// addi r20,r31,2156
	ctx.r20.s64 = ctx.r31.s64 + 2156;
	// mr r3,r20
	ctx.r3.u64 = ctx.r20.u64;
	// bl 0x82626248
	ctx.lr = 0x825F0794;
	sub_82626248(ctx, base);
	// addi r19,r31,2168
	ctx.r19.s64 = ctx.r31.s64 + 2168;
	// mr r3,r19
	ctx.r3.u64 = ctx.r19.u64;
	// bl 0x82626248
	ctx.lr = 0x825F07A0;
	sub_82626248(ctx, base);
	// addi r3,r31,2180
	ctx.r3.s64 = ctx.r31.s64 + 2180;
	// bl 0x82626248
	ctx.lr = 0x825F07A8;
	sub_82626248(ctx, base);
	// addi r3,r31,2192
	ctx.r3.s64 = ctx.r31.s64 + 2192;
	// bl 0x82626248
	ctx.lr = 0x825F07B0;
	sub_82626248(ctx, base);
	// addi r3,r31,2204
	ctx.r3.s64 = ctx.r31.s64 + 2204;
	// bl 0x82626248
	ctx.lr = 0x825F07B8;
	sub_82626248(ctx, base);
	// addi r3,r31,2216
	ctx.r3.s64 = ctx.r31.s64 + 2216;
	// bl 0x82626248
	ctx.lr = 0x825F07C0;
	sub_82626248(ctx, base);
	// addi r3,r31,2228
	ctx.r3.s64 = ctx.r31.s64 + 2228;
	// bl 0x82626248
	ctx.lr = 0x825F07C8;
	sub_82626248(ctx, base);
	// addi r3,r31,2240
	ctx.r3.s64 = ctx.r31.s64 + 2240;
	// bl 0x82626248
	ctx.lr = 0x825F07D0;
	sub_82626248(ctx, base);
	// addi r3,r31,2428
	ctx.r3.s64 = ctx.r31.s64 + 2428;
	// bl 0x82626248
	ctx.lr = 0x825F07D8;
	sub_82626248(ctx, base);
	// addi r3,r31,2252
	ctx.r3.s64 = ctx.r31.s64 + 2252;
	// bl 0x82626248
	ctx.lr = 0x825F07E0;
	sub_82626248(ctx, base);
	// addi r3,r31,2280
	ctx.r3.s64 = ctx.r31.s64 + 2280;
	// bl 0x82626248
	ctx.lr = 0x825F07E8;
	sub_82626248(ctx, base);
	// addi r3,r31,2292
	ctx.r3.s64 = ctx.r31.s64 + 2292;
	// bl 0x82626248
	ctx.lr = 0x825F07F0;
	sub_82626248(ctx, base);
	// addi r3,r31,2304
	ctx.r3.s64 = ctx.r31.s64 + 2304;
	// bl 0x82626248
	ctx.lr = 0x825F07F8;
	sub_82626248(ctx, base);
	// addi r3,r31,2316
	ctx.r3.s64 = ctx.r31.s64 + 2316;
	// bl 0x82626248
	ctx.lr = 0x825F0800;
	sub_82626248(ctx, base);
	// addi r3,r31,2328
	ctx.r3.s64 = ctx.r31.s64 + 2328;
	// bl 0x82626248
	ctx.lr = 0x825F0808;
	sub_82626248(ctx, base);
	// addi r3,r31,2340
	ctx.r3.s64 = ctx.r31.s64 + 2340;
	// bl 0x82626248
	ctx.lr = 0x825F0810;
	sub_82626248(ctx, base);
	// addi r3,r31,2352
	ctx.r3.s64 = ctx.r31.s64 + 2352;
	// bl 0x82626248
	ctx.lr = 0x825F0818;
	sub_82626248(ctx, base);
	// addi r3,r31,2364
	ctx.r3.s64 = ctx.r31.s64 + 2364;
	// bl 0x82626248
	ctx.lr = 0x825F0820;
	sub_82626248(ctx, base);
	// addi r3,r31,21652
	ctx.r3.s64 = ctx.r31.s64 + 21652;
	// stw r30,2140(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2140, ctx.r30.u32);
	// bl 0x82626248
	ctx.lr = 0x825F082C;
	sub_82626248(ctx, base);
	// addi r3,r31,21664
	ctx.r3.s64 = ctx.r31.s64 + 21664;
	// bl 0x82626248
	ctx.lr = 0x825F0834;
	sub_82626248(ctx, base);
	// addi r3,r31,21676
	ctx.r3.s64 = ctx.r31.s64 + 21676;
	// bl 0x82626248
	ctx.lr = 0x825F083C;
	sub_82626248(ctx, base);
	// addi r3,r31,21688
	ctx.r3.s64 = ctx.r31.s64 + 21688;
	// bl 0x82626248
	ctx.lr = 0x825F0844;
	sub_82626248(ctx, base);
	// addi r3,r31,21640
	ctx.r3.s64 = ctx.r31.s64 + 21640;
	// bl 0x82626248
	ctx.lr = 0x825F084C;
	sub_82626248(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825eb7b0
	ctx.lr = 0x825F0854;
	sub_825EB7B0(ctx, base);
	// stw r30,23252(r31)
	PPC_STORE_U32(ctx.r31.u32 + 23252, ctx.r30.u32);
	// stw r30,23248(r31)
	PPC_STORE_U32(ctx.r31.u32 + 23248, ctx.r30.u32);
	// stw r30,1788(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1788, ctx.r30.u32);
	// stw r30,14820(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14820, ctx.r30.u32);
	// stw r30,280(r31)
	PPC_STORE_U32(ctx.r31.u32 + 280, ctx.r30.u32);
	// stw r30,3980(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3980, ctx.r30.u32);
	// lis r11,1
	ctx.r11.s64 = 65536;
	// stw r30,472(r31)
	PPC_STORE_U32(ctx.r31.u32 + 472, ctx.r30.u32);
	// lis r10,1
	ctx.r10.s64 = 65536;
	// stw r30,15204(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15204, ctx.r30.u32);
	// ori r11,r11,33744
	ctx.r11.u64 = ctx.r11.u64 | 33744;
	// stw r30,3976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3976, ctx.r30.u32);
	// ori r10,r10,33748
	ctx.r10.u64 = ctx.r10.u64 | 33748;
	// stw r30,3408(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3408, ctx.r30.u32);
	// stw r30,19976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19976, ctx.r30.u32);
	// stw r30,3452(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3452, ctx.r30.u32);
	// stw r30,3956(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3956, ctx.r30.u32);
	// stwx r30,r31,r11
	PPC_STORE_U32(ctx.r31.u32 + ctx.r11.u32, ctx.r30.u32);
	// stwx r30,r31,r10
	PPC_STORE_U32(ctx.r31.u32 + ctx.r10.u32, ctx.r30.u32);
	// stw r30,1772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1772, ctx.r30.u32);
	// stw r30,1780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1780, ctx.r30.u32);
	// stw r30,376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 376, ctx.r30.u32);
	// stw r30,380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 380, ctx.r30.u32);
	// stw r30,384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 384, ctx.r30.u32);
	// stw r30,388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 388, ctx.r30.u32);
	// stw r30,15900(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15900, ctx.r30.u32);
	// stw r30,14756(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14756, ctx.r30.u32);
	// stw r30,15216(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15216, ctx.r30.u32);
	// stw r30,15220(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15220, ctx.r30.u32);
	// stw r30,20848(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20848, ctx.r30.u32);
	// stw r30,19992(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19992, ctx.r30.u32);
	// stw r30,19996(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19996, ctx.r30.u32);
	// stw r30,20000(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20000, ctx.r30.u32);
	// stw r30,20056(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20056, ctx.r30.u32);
	// stw r30,19980(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19980, ctx.r30.u32);
	// stw r30,19984(r31)
	PPC_STORE_U32(ctx.r31.u32 + 19984, ctx.r30.u32);
	// stw r30,21000(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21000, ctx.r30.u32);
	// stw r30,21004(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21004, ctx.r30.u32);
	// stw r29,20988(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20988, ctx.r29.u32);
	// stw r29,20996(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20996, ctx.r29.u32);
	// stw r29,20992(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20992, ctx.r29.u32);
	// stw r30,20832(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20832, ctx.r30.u32);
	// stw r29,20836(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20836, ctx.r29.u32);
	// stw r30,20840(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20840, ctx.r30.u32);
	// stw r30,21160(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21160, ctx.r30.u32);
	// stw r30,21164(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21164, ctx.r30.u32);
	// stw r30,3392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3392, ctx.r30.u32);
	// stw r30,21432(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21432, ctx.r30.u32);
	// stw r30,21468(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21468, ctx.r30.u32);
	// stw r30,21472(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21472, ctx.r30.u32);
	// stw r30,21476(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21476, ctx.r30.u32);
	// stw r29,20972(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20972, ctx.r29.u32);
	// stw r29,20980(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20980, ctx.r29.u32);
	// stw r29,20976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20976, ctx.r29.u32);
	// stw r30,21440(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21440, ctx.r30.u32);
	// stw r30,21444(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21444, ctx.r30.u32);
	// stw r30,21448(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21448, ctx.r30.u32);
	// stw r30,21452(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21452, ctx.r30.u32);
	// stw r30,21456(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21456, ctx.r30.u32);
	// stw r30,21460(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21460, ctx.r30.u32);
	// stw r30,21464(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21464, ctx.r30.u32);
	// stw r30,20956(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20956, ctx.r30.u32);
	// stw r30,20960(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20960, ctx.r30.u32);
	// stw r30,20964(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20964, ctx.r30.u32);
	// stw r30,20968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20968, ctx.r30.u32);
	// stw r30,21080(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21080, ctx.r30.u32);
	// stw r30,21084(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21084, ctx.r30.u32);
	// stw r30,21088(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21088, ctx.r30.u32);
	// stw r30,21092(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21092, ctx.r30.u32);
	// stw r30,21096(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21096, ctx.r30.u32);
	// stw r30,21100(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21100, ctx.r30.u32);
	// stw r30,21104(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21104, ctx.r30.u32);
	// stw r30,21108(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21108, ctx.r30.u32);
	// stw r30,21112(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21112, ctx.r30.u32);
	// stw r30,21116(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21116, ctx.r30.u32);
	// stw r30,21120(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21120, ctx.r30.u32);
	// stw r30,21124(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21124, ctx.r30.u32);
	// stw r30,21128(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21128, ctx.r30.u32);
	// stw r30,21132(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21132, ctx.r30.u32);
	// stw r30,21136(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21136, ctx.r30.u32);
	// stw r30,21140(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21140, ctx.r30.u32);
	// stw r30,328(r31)
	PPC_STORE_U32(ctx.r31.u32 + 328, ctx.r30.u32);
	// stw r30,15564(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15564, ctx.r30.u32);
	// stw r29,21200(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21200, ctx.r29.u32);
	// stw r29,21180(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21180, ctx.r29.u32);
	// stw r29,21184(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21184, ctx.r29.u32);
	// stw r30,21188(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21188, ctx.r30.u32);
	// stw r30,21168(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21168, ctx.r30.u32);
	// li r11,100
	ctx.r11.s64 = 100;
	// stw r30,21172(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21172, ctx.r30.u32);
	// lis r10,22349
	ctx.r10.s64 = 1464664064;
	// stw r30,21176(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21176, ctx.r30.u32);
	// stw r30,21196(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21196, ctx.r30.u32);
	// ori r10,r10,22066
	ctx.r10.u64 = ctx.r10.u64 | 22066;
	// stw r30,21192(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21192, ctx.r30.u32);
	// stw r30,15368(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15368, ctx.r30.u32);
	// cmplw cr6,r28,r10
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r10.u32, ctx.xer);
	// stw r11,23976(r31)
	PPC_STORE_U32(ctx.r31.u32 + 23976, ctx.r11.u32);
	// li r11,-1
	ctx.r11.s64 = -1;
	// stw r30,21228(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21228, ctx.r30.u32);
	// stw r30,21480(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21480, ctx.r30.u32);
	// stw r30,21360(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21360, ctx.r30.u32);
	// stw r30,21388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21388, ctx.r30.u32);
	// stw r30,21212(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21212, ctx.r30.u32);
	// stw r30,21216(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21216, ctx.r30.u32);
	// stw r30,21272(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21272, ctx.r30.u32);
	// stw r30,21268(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21268, ctx.r30.u32);
	// stw r30,21204(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21204, ctx.r30.u32);
	// stw r30,21208(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21208, ctx.r30.u32);
	// stw r30,21364(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21364, ctx.r30.u32);
	// stw r30,21368(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21368, ctx.r30.u32);
	// stw r30,21372(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21372, ctx.r30.u32);
	// stw r30,21376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21376, ctx.r30.u32);
	// stw r30,21528(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21528, ctx.r30.u32);
	// stw r30,21532(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21532, ctx.r30.u32);
	// stw r30,21536(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21536, ctx.r30.u32);
	// stw r29,21544(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21544, ctx.r29.u32);
	// stw r30,21548(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21548, ctx.r30.u32);
	// stw r30,1872(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1872, ctx.r30.u32);
	// stw r30,1876(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1876, ctx.r30.u32);
	// stw r30,21580(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21580, ctx.r30.u32);
	// stw r30,21584(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21584, ctx.r30.u32);
	// stw r29,21588(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21588, ctx.r29.u32);
	// stw r30,21592(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21592, ctx.r30.u32);
	// stw r30,21596(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21596, ctx.r30.u32);
	// stw r30,21632(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21632, ctx.r30.u32);
	// stw r30,21636(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21636, ctx.r30.u32);
	// stw r30,15192(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15192, ctx.r30.u32);
	// stw r29,21700(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21700, ctx.r29.u32);
	// stw r29,21704(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21704, ctx.r29.u32);
	// stw r11,15200(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15200, ctx.r11.u32);
	// beq cr6,0x825f0c20
	if (ctx.cr6.eq) goto loc_825F0C20;
	// lis r11,30573
	ctx.r11.s64 = 2003632128;
	// ori r11,r11,30258
	ctx.r11.u64 = ctx.r11.u64 | 30258;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0c20
	if (ctx.cr6.eq) goto loc_825F0C20;
	// lis r11,22349
	ctx.r11.s64 = 1464664064;
	// ori r11,r11,22067
	ctx.r11.u64 = ctx.r11.u64 | 22067;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0bf0
	if (ctx.cr6.eq) goto loc_825F0BF0;
	// lis r11,30573
	ctx.r11.s64 = 2003632128;
	// ori r11,r11,30259
	ctx.r11.u64 = ctx.r11.u64 | 30259;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0bf0
	if (ctx.cr6.eq) goto loc_825F0BF0;
	// lis r11,22349
	ctx.r11.s64 = 1464664064;
	// ori r11,r11,22081
	ctx.r11.u64 = ctx.r11.u64 | 22081;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0c18
	if (ctx.cr6.eq) goto loc_825F0C18;
	// lis r11,30573
	ctx.r11.s64 = 2003632128;
	// ori r11,r11,30305
	ctx.r11.u64 = ctx.r11.u64 | 30305;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0c18
	if (ctx.cr6.eq) goto loc_825F0C18;
	// lis r11,22349
	ctx.r11.s64 = 1464664064;
	// ori r11,r11,22065
	ctx.r11.u64 = ctx.r11.u64 | 22065;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0c10
	if (ctx.cr6.eq) goto loc_825F0C10;
	// lis r11,30573
	ctx.r11.s64 = 2003632128;
	// ori r11,r11,30257
	ctx.r11.u64 = ctx.r11.u64 | 30257;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0c10
	if (ctx.cr6.eq) goto loc_825F0C10;
	// lis r11,19792
	ctx.r11.s64 = 1297088512;
	// ori r11,r11,13363
	ctx.r11.u64 = ctx.r11.u64 | 13363;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0c08
	if (ctx.cr6.eq) goto loc_825F0C08;
	// lis r11,28016
	ctx.r11.s64 = 1836056576;
	// ori r11,r11,13363
	ctx.r11.u64 = ctx.r11.u64 | 13363;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0c08
	if (ctx.cr6.eq) goto loc_825F0C08;
	// lis r11,19792
	ctx.r11.s64 = 1297088512;
	// ori r11,r11,13362
	ctx.r11.u64 = ctx.r11.u64 | 13362;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0c00
	if (ctx.cr6.eq) goto loc_825F0C00;
	// lis r11,28016
	ctx.r11.s64 = 1836056576;
	// ori r11,r11,13362
	ctx.r11.u64 = ctx.r11.u64 | 13362;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0c00
	if (ctx.cr6.eq) goto loc_825F0C00;
	// lis r11,19792
	ctx.r11.s64 = 1297088512;
	// ori r11,r11,13395
	ctx.r11.u64 = ctx.r11.u64 | 13395;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0bf8
	if (ctx.cr6.eq) goto loc_825F0BF8;
	// lis r11,28016
	ctx.r11.s64 = 1836056576;
	// ori r11,r11,13427
	ctx.r11.u64 = ctx.r11.u64 | 13427;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0bf8
	if (ctx.cr6.eq) goto loc_825F0BF8;
	// lis r11,22349
	ctx.r11.s64 = 1464664064;
	// ori r11,r11,22096
	ctx.r11.u64 = ctx.r11.u64 | 22096;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0bf0
	if (ctx.cr6.eq) goto loc_825F0BF0;
	// lis r11,30573
	ctx.r11.s64 = 2003632128;
	// ori r11,r11,30320
	ctx.r11.u64 = ctx.r11.u64 | 30320;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0bf0
	if (ctx.cr6.eq) goto loc_825F0BF0;
	// lis r11,22358
	ctx.r11.s64 = 1465253888;
	// ori r11,r11,20530
	ctx.r11.u64 = ctx.r11.u64 | 20530;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0be0
	if (ctx.cr6.eq) goto loc_825F0BE0;
	// lis r11,30582
	ctx.r11.s64 = 2004221952;
	// ori r11,r11,28722
	ctx.r11.u64 = ctx.r11.u64 | 28722;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0be0
	if (ctx.cr6.eq) goto loc_825F0BE0;
	// lis r11,22349
	ctx.r11.s64 = 1464664064;
	// ori r11,r11,22098
	ctx.r11.u64 = ctx.r11.u64 | 22098;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0ba0
	if (ctx.cr6.eq) goto loc_825F0BA0;
	// lis r11,30573
	ctx.r11.s64 = 2003632128;
	// ori r11,r11,30322
	ctx.r11.u64 = ctx.r11.u64 | 30322;
	// cmplw cr6,r28,r11
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f0ba0
	if (ctx.cr6.eq) goto loc_825F0BA0;
	// li r3,6
	ctx.r3.s64 = 6;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825F0BA0:
	// li r11,7
	ctx.r11.s64 = 7;
	// stw r11,15472(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15472, ctx.r11.u32);
	// stw r29,21580(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21580, ctx.r29.u32);
	// lwz r11,21704(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21704);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825f0bcc
	if (!ctx.cr6.eq) goto loc_825F0BCC;
	// stw r29,21632(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21632, ctx.r29.u32);
	// stw r29,21596(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21596, ctx.r29.u32);
	// stw r29,21700(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21700, ctx.r29.u32);
	// stw r29,15192(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15192, ctx.r29.u32);
	// b 0x825f0c28
	goto loc_825F0C28;
loc_825F0BCC:
	// stw r30,21632(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21632, ctx.r30.u32);
	// stw r30,21596(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21596, ctx.r30.u32);
	// stw r30,21700(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21700, ctx.r30.u32);
	// stw r29,15192(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15192, ctx.r29.u32);
	// b 0x825f0c28
	goto loc_825F0C28;
loc_825F0BE0:
	// li r11,7
	ctx.r11.s64 = 7;
	// stw r11,15472(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15472, ctx.r11.u32);
	// stw r29,15368(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15368, ctx.r29.u32);
	// b 0x825f0c28
	goto loc_825F0C28;
loc_825F0BF0:
	// li r11,6
	ctx.r11.s64 = 6;
	// b 0x825f0c24
	goto loc_825F0C24;
loc_825F0BF8:
	// stw r30,15472(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15472, ctx.r30.u32);
	// b 0x825f0c28
	goto loc_825F0C28;
loc_825F0C00:
	// li r11,2
	ctx.r11.s64 = 2;
	// b 0x825f0c24
	goto loc_825F0C24;
loc_825F0C08:
	// li r11,3
	ctx.r11.s64 = 3;
	// b 0x825f0c24
	goto loc_825F0C24;
loc_825F0C10:
	// li r11,4
	ctx.r11.s64 = 4;
	// b 0x825f0c24
	goto loc_825F0C24;
loc_825F0C18:
	// li r11,7
	ctx.r11.s64 = 7;
	// b 0x825f0c24
	goto loc_825F0C24;
loc_825F0C20:
	// li r11,5
	ctx.r11.s64 = 5;
loc_825F0C24:
	// stw r11,15472(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15472, ctx.r11.u32);
loc_825F0C28:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// blt cr6,0x825f0c44
	if (ctx.cr6.lt) goto loc_825F0C44;
	// stw r29,14788(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14788, ctx.r29.u32);
	// stw r29,3956(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3956, ctx.r29.u32);
	// stw r30,3888(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3888, ctx.r30.u32);
	// stw r29,14756(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14756, ctx.r29.u32);
loc_825F0C44:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc908
	ctx.lr = 0x825F0C4C;
	sub_825CC908(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825f0c5c
	if (ctx.cr6.eq) goto loc_825F0C5C;
	// stw r30,14788(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14788, ctx.r30.u32);
	// b 0x825f0c60
	goto loc_825F0C60;
loc_825F0C5C:
	// stw r29,14788(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14788, ctx.r29.u32);
loc_825F0C60:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// bne cr6,0x825f0c88
	if (!ctx.cr6.eq) goto loc_825F0C88;
	// lwz r11,3924(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825f0c88
	if (!ctx.cr6.eq) goto loc_825F0C88;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc908
	ctx.lr = 0x825F0C80;
	sub_825CC908(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825f0c8c
	if (ctx.cr6.eq) goto loc_825F0C8C;
loc_825F0C88:
	// stw r30,15900(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15900, ctx.r30.u32);
loc_825F0C8C:
	// stw r29,3356(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3356, ctx.r29.u32);
	// lwz r11,14772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14772);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825f0ca0
	if (!ctx.cr6.gt) goto loc_825F0CA0;
	// stw r29,3356(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3356, ctx.r29.u32);
loc_825F0CA0:
	// lwz r11,3356(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3356);
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// ble cr6,0x825f0cb4
	if (!ctx.cr6.gt) goto loc_825F0CB4;
	// li r11,2
	ctx.r11.s64 = 2;
	// stw r11,3356(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3356, ctx.r11.u32);
loc_825F0CB4:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825f0cc8
	if (!ctx.cr6.eq) goto loc_825F0CC8;
	// stw r29,3356(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3356, ctx.r29.u32);
	// stw r29,20056(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20056, ctx.r29.u32);
loc_825F0CC8:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// lis r10,9356
	ctx.r10.s64 = 613154816;
	// ori r28,r10,32769
	ctx.r28.u64 = ctx.r10.u64 | 32769;
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// bne cr6,0x825f0d50
	if (!ctx.cr6.eq) goto loc_825F0D50;
	// bl 0x82601418
	ctx.lr = 0x825F0CE0;
	sub_82601418(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ed560
	ctx.lr = 0x825F0CEC;
	sub_825ED560(ctx, base);
	// bl 0x825ed678
	ctx.lr = 0x825F0CF0;
	sub_825ED678(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ed7f8
	ctx.lr = 0x825F0CF8;
	sub_825ED7F8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc908
	ctx.lr = 0x825F0D00;
	sub_825CC908(ctx, base);
	// cntlzw r10,r3
	ctx.r10.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// li r11,4096
	ctx.r11.s64 = 4096;
	// rlwinm r10,r10,27,31,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 27) & 0x1;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// li r3,4096
	ctx.r3.s64 = 4096;
	// stw r10,14772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14772, ctx.r10.u32);
	// stw r11,21324(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21324, ctx.r11.u32);
	// bl 0x82121108
	ctx.lr = 0x825F0D20;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,21316(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21316, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r3,21324(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21324);
	// bl 0x82121108
	ctx.lr = 0x825F0D38;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,21320(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21320, ctx.r3.u32);
	// bne cr6,0x825f0d80
	if (!ctx.cr6.eq) goto loc_825F0D80;
loc_825F0D44:
	// li r3,2
	ctx.r3.s64 = 2;
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
loc_825F0D50:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc908
	ctx.lr = 0x825F0D58;
	sub_825CC908(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825f0d64
	if (ctx.cr6.eq) goto loc_825F0D64;
	// stw r30,14772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 14772, ctx.r30.u32);
loc_825F0D64:
	// bl 0x82601418
	ctx.lr = 0x825F0D68;
	sub_82601418(ctx, base);
	// li r4,1
	ctx.r4.s64 = 1;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ed560
	ctx.lr = 0x825F0D74;
	sub_825ED560(ctx, base);
	// bl 0x825ed7f8
	ctx.lr = 0x825F0D78;
	sub_825ED7F8(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825ed678
	ctx.lr = 0x825F0D80;
	sub_825ED678(ctx, base);
loc_825F0D80:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825cc908
	ctx.lr = 0x825F0D88;
	sub_825CC908(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825f0da0
	if (ctx.cr6.eq) goto loc_825F0DA0;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x826319e0
	ctx.lr = 0x825F0D98;
	sub_826319E0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f1794
	if (!ctx.cr6.eq) goto loc_825F1794;
loc_825F0DA0:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r30,3360(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3360, ctx.r30.u32);
	// bl 0x82632600
	ctx.lr = 0x825F0DAC;
	sub_82632600(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f1794
	if (!ctx.cr6.eq) goto loc_825F1794;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r18,84(r31)
	PPC_STORE_U32(ctx.r31.u32 + 84, ctx.r18.u32);
	// bl 0x825ef9b8
	ctx.lr = 0x825F0DC0;
	sub_825EF9B8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f1794
	if (!ctx.cr6.eq) goto loc_825F1794;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825dd688
	ctx.lr = 0x825F0DD0;
	sub_825DD688(ctx, base);
	// mullw r11,r25,r24
	ctx.r11.s64 = int64_t(ctx.r25.s32) * int64_t(ctx.r24.s32);
	// stw r11,21176(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21176, ctx.r11.u32);
	// stw r25,21192(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21192, ctx.r25.u32);
	// stw r25,21352(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21352, ctx.r25.u32);
	// stw r24,21196(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21196, ctx.r24.u32);
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// stw r24,21356(r31)
	PPC_STORE_U32(ctx.r31.u32 + 21356, ctx.r24.u32);
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// stw r29,3676(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3676, ctx.r29.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825efc00
	ctx.lr = 0x825F0DFC;
	sub_825EFC00(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f1794
	if (!ctx.cr6.eq) goto loc_825F1794;
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f0508
	ctx.lr = 0x825F0E14;
	sub_825F0508(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f1794
	if (!ctx.cr6.eq) goto loc_825F1794;
	// addi r10,r31,1243
	ctx.r10.s64 = ctx.r31.s64 + 1243;
	// stw r30,3676(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3676, ctx.r30.u32);
	// addi r11,r31,603
	ctx.r11.s64 = ctx.r31.s64 + 603;
	// stw r30,3680(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3680, ctx.r30.u32);
	// rlwinm r10,r10,0,0,24
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFF80;
	// rlwinm r11,r11,0,0,24
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFF80;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r10,1764(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1764, ctx.r10.u32);
	// addi r10,r11,4
	ctx.r10.s64 = ctx.r11.s64 + 4;
	// stw r11,1768(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1768, ctx.r11.u32);
	// stw r11,1760(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1760, ctx.r11.u32);
	// stw r10,1756(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1756, ctx.r10.u32);
	// stw r30,248(r31)
	PPC_STORE_U32(ctx.r31.u32 + 248, ctx.r30.u32);
	// lwz r11,3100(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3100);
	// stw r11,3104(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3104, ctx.r11.u32);
	// stw r11,3084(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3084, ctx.r11.u32);
	// lwz r11,3096(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3096);
	// stw r11,3108(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3108, ctx.r11.u32);
	// stw r11,3080(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3080, ctx.r11.u32);
	// bl 0x825f3f88
	ctx.lr = 0x825F0E6C;
	sub_825F3F88(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x82614520
	ctx.lr = 0x825F0E74;
	sub_82614520(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263d468
	ctx.lr = 0x825F0E7C;
	sub_8263D468(ctx, base);
	// lwz r11,3688(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3688);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r11,3716(r31)
	PPC_STORE_U32(ctx.r31.u32 + 3716, ctx.r11.u32);
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 7, ctx.xer);
	// beq cr6,0x825f1544
	if (ctx.cr6.eq) goto loc_825F1544;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-4424
	ctx.r6.s64 = ctx.r11.s64 + -4424;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F0EAC;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-8832
	ctx.r6.s64 = ctx.r11.s64 + -8832;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F0ED0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,2064
	ctx.r6.s64 = ctx.r11.s64 + 2064;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F0EF4;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,1544
	ctx.r6.s64 = ctx.r11.s64 + 1544;
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F0F18;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,-16
	ctx.r6.s64 = ctx.r11.s64 + -16;
	// mr r4,r21
	ctx.r4.u64 = ctx.r21.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F0F3C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,504
	ctx.r6.s64 = ctx.r11.s64 + 504;
	// mr r4,r20
	ctx.r4.u64 = ctx.r20.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F0F60;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,1024
	ctx.r6.s64 = ctx.r11.s64 + 1024;
	// mr r4,r19
	ctx.r4.u64 = ctx.r19.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F0F84;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,2328
	ctx.r6.s64 = ctx.r11.s64 + 2328;
	// mr r4,r17
	ctx.r4.u64 = ctx.r17.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F0FA8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,6
	ctx.r7.s64 = 6;
	// addi r6,r11,2816
	ctx.r6.s64 = ctx.r11.s64 + 2816;
	// mr r4,r16
	ctx.r4.u64 = ctx.r16.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F0FCC;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,9
	ctx.r7.s64 = 9;
	// addi r6,r11,3304
	ctx.r6.s64 = ctx.r11.s64 + 3304;
	// mr r4,r15
	ctx.r4.u64 = ctx.r15.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F0FF0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,3792
	ctx.r6.s64 = ctx.r11.s64 + 3792;
	// mr r4,r14
	ctx.r4.u64 = ctx.r14.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1014;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-13592
	ctx.r6.s64 = ctx.r11.s64 + -13592;
	// addi r4,r31,2180
	ctx.r4.s64 = ctx.r31.s64 + 2180;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1038;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-12912
	ctx.r6.s64 = ctx.r11.s64 + -12912;
	// addi r4,r31,2192
	ctx.r4.s64 = ctx.r31.s64 + 2192;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F105C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-12160
	ctx.r6.s64 = ctx.r11.s64 + -12160;
	// addi r4,r31,2204
	ctx.r4.s64 = ctx.r31.s64 + 2204;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1080;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-11560
	ctx.r6.s64 = ctx.r11.s64 + -11560;
	// addi r4,r31,2216
	ctx.r4.s64 = ctx.r31.s64 + 2216;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F10A4;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-11024
	ctx.r6.s64 = ctx.r11.s64 + -11024;
	// addi r4,r31,2228
	ctx.r4.s64 = ctx.r31.s64 + 2228;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F10C8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-10608
	ctx.r6.s64 = ctx.r11.s64 + -10608;
	// addi r4,r31,2240
	ctx.r4.s64 = ctx.r31.s64 + 2240;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F10EC;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-10192
	ctx.r6.s64 = ctx.r11.s64 + -10192;
	// addi r4,r31,2428
	ctx.r4.s64 = ctx.r31.s64 + 2428;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1110;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,138
	ctx.r7.s64 = 138;
	// addi r6,r11,-9488
	ctx.r6.s64 = ctx.r11.s64 + -9488;
	// addi r4,r31,2252
	ctx.r4.s64 = ctx.r31.s64 + 2252;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1134;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r26,r31,2280
	ctx.r26.s64 = ctx.r31.s64 + 2280;
	// addi r6,r11,4280
	ctx.r6.s64 = ctx.r11.s64 + 4280;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F115C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,2292
	ctx.r27.s64 = ctx.r31.s64 + 2292;
	// addi r6,r11,4544
	ctx.r6.s64 = ctx.r11.s64 + 4544;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1184;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,2304
	ctx.r29.s64 = ctx.r31.s64 + 2304;
	// addi r6,r11,4808
	ctx.r6.s64 = ctx.r11.s64 + 4808;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F11AC;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,2316
	ctx.r30.s64 = ctx.r31.s64 + 2316;
	// addi r6,r11,5072
	ctx.r6.s64 = ctx.r11.s64 + 5072;
	// li r7,136
	ctx.r7.s64 = 136;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F11D4;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// stw r26,2380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2380, ctx.r26.u32);
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// stw r27,2384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2384, ctx.r27.u32);
	// li r7,138
	ctx.r7.s64 = 138;
	// stw r29,2388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2388, ctx.r29.u32);
	// addi r29,r31,2328
	ctx.r29.s64 = ctx.r31.s64 + 2328;
	// addi r6,r11,5336
	ctx.r6.s64 = ctx.r11.s64 + 5336;
	// stw r30,2392(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2392, ctx.r30.u32);
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F120C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r26,r31,2340
	ctx.r26.s64 = ctx.r31.s64 + 2340;
	// addi r6,r11,5632
	ctx.r6.s64 = ctx.r11.s64 + 5632;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1234;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,2352
	ctx.r27.s64 = ctx.r31.s64 + 2352;
	// addi r6,r11,5928
	ctx.r6.s64 = ctx.r11.s64 + 5928;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F125C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,2364
	ctx.r30.s64 = ctx.r31.s64 + 2364;
	// addi r6,r11,6224
	ctx.r6.s64 = ctx.r11.s64 + 6224;
	// li r7,138
	ctx.r7.s64 = 138;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1284;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// stw r29,2396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2396, ctx.r29.u32);
	// stw r26,2400(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2400, ctx.r26.u32);
	// stw r27,2404(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2404, ctx.r27.u32);
	// stw r30,2408(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2408, ctx.r30.u32);
	// lwz r11,21596(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 21596);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825f1358
	if (ctx.cr6.eq) goto loc_825F1358;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r26,r31,21652
	ctx.r26.s64 = ctx.r31.s64 + 21652;
	// addi r6,r11,6520
	ctx.r6.s64 = ctx.r11.s64 + 6520;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F12C8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r27,r31,21664
	ctx.r27.s64 = ctx.r31.s64 + 21664;
	// addi r6,r11,6824
	ctx.r6.s64 = ctx.r11.s64 + 6824;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F12F0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r29,r31,21676
	ctx.r29.s64 = ctx.r31.s64 + 21676;
	// addi r6,r11,7128
	ctx.r6.s64 = ctx.r11.s64 + 7128;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1318;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// addi r30,r31,21688
	ctx.r30.s64 = ctx.r31.s64 + 21688;
	// addi r6,r11,7432
	ctx.r6.s64 = ctx.r11.s64 + 7432;
	// li r7,8
	ctx.r7.s64 = 8;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1340;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// stw r26,2396(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2396, ctx.r26.u32);
	// stw r27,2400(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2400, ctx.r27.u32);
	// stw r29,2404(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2404, ctx.r29.u32);
	// stw r30,2408(r31)
	PPC_STORE_U32(ctx.r31.u32 + 2408, ctx.r30.u32);
loc_825F1358:
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,8
	ctx.r7.s64 = 8;
	// addi r6,r11,7736
	ctx.r6.s64 = ctx.r11.s64 + 7736;
	// addi r4,r31,21640
	ctx.r4.s64 = ctx.r31.s64 + 21640;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1374;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,7928
	ctx.r6.s64 = ctx.r11.s64 + 7928;
	// addi r4,r31,2440
	ctx.r4.s64 = ctx.r31.s64 + 2440;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1398;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,7864
	ctx.r6.s64 = ctx.r11.s64 + 7864;
	// addi r4,r31,2452
	ctx.r4.s64 = ctx.r31.s64 + 2452;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F13BC;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,7800
	ctx.r6.s64 = ctx.r11.s64 + 7800;
	// addi r4,r31,2464
	ctx.r4.s64 = ctx.r31.s64 + 2464;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F13E0;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,7992
	ctx.r6.s64 = ctx.r11.s64 + 7992;
	// addi r4,r31,2480
	ctx.r4.s64 = ctx.r31.s64 + 2480;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1404;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,8064
	ctx.r6.s64 = ctx.r11.s64 + 8064;
	// addi r4,r31,2492
	ctx.r4.s64 = ctx.r31.s64 + 2492;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1428;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,136
	ctx.r7.s64 = 136;
	// addi r6,r11,8136
	ctx.r6.s64 = ctx.r11.s64 + 8136;
	// addi r4,r31,2504
	ctx.r4.s64 = ctx.r31.s64 + 2504;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F144C;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,8204
	ctx.r6.s64 = ctx.r11.s64 + 8204;
	// addi r4,r31,2520
	ctx.r4.s64 = ctx.r31.s64 + 2520;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1470;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,8240
	ctx.r6.s64 = ctx.r11.s64 + 8240;
	// addi r4,r31,2532
	ctx.r4.s64 = ctx.r31.s64 + 2532;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F1494;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// lwz r5,3340(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// li r7,134
	ctx.r7.s64 = 134;
	// addi r6,r11,8276
	ctx.r6.s64 = ctx.r11.s64 + 8276;
	// addi r4,r31,2544
	ctx.r4.s64 = ctx.r31.s64 + 2544;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8263f8c0
	ctx.lr = 0x825F14B8;
	sub_8263F8C0(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,5
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 5, ctx.xer);
	// blt cr6,0x825f155c
	if (ctx.cr6.lt) goto loc_825F155C;
	// lwz r11,1972(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1972);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f14e8
	if (!ctx.cr6.eq) goto loc_825F14E8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r4,3340(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3340);
	// bl 0x826452b8
	ctx.lr = 0x825F14E4;
	sub_826452B8(ctx, base);
	// stw r3,1972(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1972, ctx.r3.u32);
loc_825F14E8:
	// lwz r11,1972(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1972);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
	// lwz r3,1964(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1964);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// bne cr6,0x825f1510
	if (!ctx.cr6.eq) goto loc_825F1510;
	// bl 0x82644210
	ctx.lr = 0x825F1504;
	sub_82644210(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,1964(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1964, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
loc_825F1510:
	// lwz r11,3180(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3180);
	// stw r11,52(r3)
	PPC_STORE_U32(ctx.r3.u32 + 52, ctx.r11.u32);
	// lwz r11,1968(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1968);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f155c
	if (!ctx.cr6.eq) goto loc_825F155C;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// li r4,2
	ctx.r4.s64 = 2;
	// rlwinm r3,r11,1,0,30
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// bl 0x82643ee8
	ctx.lr = 0x825F1534;
	sub_82643EE8(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,1968(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1968, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
	// b 0x825f155c
	goto loc_825F155C;
loc_825F1544:
	// lwz r11,1964(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1964);
	// lwz r10,3180(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 3180);
	// stw r10,52(r11)
	PPC_STORE_U32(ctx.r11.u32 + 52, ctx.r10.u32);
	// bl 0x825ec160
	ctx.lr = 0x825F1554;
	sub_825EC160(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f0d44
	if (!ctx.cr6.eq) goto loc_825F0D44;
loc_825F155C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825edbe0
	ctx.lr = 0x825F1564;
	sub_825EDBE0(ctx, base);
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// mr r27,r3
	ctx.r27.u64 = ctx.r3.u64;
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// blt cr6,0x825f1760
	if (ctx.cr6.lt) goto loc_825F1760;
	// lwz r11,352(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 352);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f1598
	if (!ctx.cr6.eq) goto loc_825F1598;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// li r3,16
	ctx.r3.s64 = 16;
	// bl 0x82121108
	ctx.lr = 0x825F158C;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,352(r31)
	PPC_STORE_U32(ctx.r31.u32 + 352, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
loc_825F1598:
	// lis r11,-32158
	ctx.r11.s64 = -2107506688;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// addi r11,r11,-25288
	ctx.r11.s64 = ctx.r11.s64 + -25288;
	// li r3,64
	ctx.r3.s64 = 64;
	// stw r11,15776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15776, ctx.r11.u32);
	// bl 0x82121108
	ctx.lr = 0x825F15B0;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,356(r31)
	PPC_STORE_U32(ctx.r31.u32 + 356, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// li r3,72
	ctx.r3.s64 = 72;
	// bl 0x82121108
	ctx.lr = 0x825F15C8;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,360(r31)
	PPC_STORE_U32(ctx.r31.u32 + 360, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// li r3,288
	ctx.r3.s64 = 288;
	// bl 0x82121108
	ctx.lr = 0x825F15E0;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,364(r31)
	PPC_STORE_U32(ctx.r31.u32 + 364, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// li r3,576
	ctx.r3.s64 = 576;
	// bl 0x82121108
	ctx.lr = 0x825F15F8;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,368(r31)
	PPC_STORE_U32(ctx.r31.u32 + 368, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// li r3,1008
	ctx.r3.s64 = 1008;
	// bl 0x82121108
	ctx.lr = 0x825F1610;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,372(r31)
	PPC_STORE_U32(ctx.r31.u32 + 372, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
	// lwz r11,1772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f1668
	if (!ctx.cr6.eq) goto loc_825F1668;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mullw r11,r10,r11
	ctx.r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// rlwinm r3,r11,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x82121108
	ctx.lr = 0x825F1640;
	sub_82121108(ctx, base);
	// stw r3,1772(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1772, ctx.r3.u32);
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,3,0,28
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 3) & 0xFFFFFFF8;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// stw r11,1776(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1776, ctx.r11.u32);
	// lwz r11,1772(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1772);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
loc_825F1668:
	// lwz r11,376(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 376);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f16ac
	if (!ctx.cr6.eq) goto loc_825F16AC;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r3,r11,4,0,27
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// bl 0x82121108
	ctx.lr = 0x825F1690;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,384(r31)
	PPC_STORE_U32(ctx.r31.u32 + 384, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// stw r11,376(r31)
	PPC_STORE_U32(ctx.r31.u32 + 376, ctx.r11.u32);
loc_825F16AC:
	// lwz r11,380(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 380);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f16e4
	if (!ctx.cr6.eq) goto loc_825F16E4;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	ctx.lr = 0x825F16D4;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,388(r31)
	PPC_STORE_U32(ctx.r31.u32 + 388, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
	// stw r3,380(r31)
	PPC_STORE_U32(ctx.r31.u32 + 380, ctx.r3.u32);
loc_825F16E4:
	// lwz r11,15216(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15216);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f1714
	if (!ctx.cr6.eq) goto loc_825F1714;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// li r3,832
	ctx.r3.s64 = 832;
	// bl 0x82121108
	ctx.lr = 0x825F16FC;
	sub_82121108(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,15216(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15216, ctx.r3.u32);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
	// addi r11,r3,31
	ctx.r11.s64 = ctx.r3.s64 + 31;
	// rlwinm r11,r11,0,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFE0;
	// stw r11,15220(r31)
	PPC_STORE_U32(ctx.r31.u32 + 15220, ctx.r11.u32);
loc_825F1714:
	// lwz r11,1780(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1780);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f1760
	if (!ctx.cr6.eq) goto loc_825F1760;
	// lwz r11,140(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// lwz r10,136(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// mullw r11,r10,r11
	ctx.r11.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// rlwinm r3,r11,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// bl 0x82121108
	ctx.lr = 0x825F1738;
	sub_82121108(ctx, base);
	// stw r3,1780(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1780, ctx.r3.u32);
	// lwz r11,136(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 136);
	// lwz r10,140(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 140);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// stw r11,1784(r31)
	PPC_STORE_U32(ctx.r31.u32 + 1784, ctx.r11.u32);
	// lwz r11,1780(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 1780);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825f0d44
	if (ctx.cr6.eq) goto loc_825F0D44;
loc_825F1760:
	// lwz r11,15472(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15472);
	// cmpwi cr6,r11,6
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 6, ctx.xer);
	// bne cr6,0x825f1790
	if (!ctx.cr6.eq) goto loc_825F1790;
	// addi r29,r31,15920
	ctx.r29.s64 = ctx.r31.s64 + 15920;
	// li r30,2
	ctx.r30.s64 = 2;
loc_825F1774:
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825fe838
	ctx.lr = 0x825F1780;
	sub_825FE838(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r29,r29,1888
	ctx.r29.s64 = ctx.r29.s64 + 1888;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f1774
	if (!ctx.cr6.eq) goto loc_825F1774;
loc_825F1790:
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
loc_825F1794:
	// addi r1,r1,240
	ctx.r1.s64 = ctx.r1.s64 + 240;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825F179C"))) PPC_WEAK_FUNC(sub_825F179C);
PPC_FUNC_IMPL(__imp__sub_825F179C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F17A0"))) PPC_WEAK_FUNC(sub_825F17A0);
PPC_FUNC_IMPL(__imp__sub_825F17A0) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba14
	ctx.lr = 0x825F17A8;
	sub_8239BA14(ctx, base);
	// lwz r11,156(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// lwz r8,160(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// addi r10,r11,15
	ctx.r10.s64 = ctx.r11.s64 + 15;
	// addi r9,r8,15
	ctx.r9.s64 = ctx.r8.s64 + 15;
	// rlwinm r10,r10,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// rlwinm r9,r9,0,0,27
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 0) & 0xFFFFFFF0;
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// bne cr6,0x825f17d4
	if (!ctx.cr6.eq) goto loc_825F17D4;
	// cmpw cr6,r8,r9
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r9.s32, ctx.xer);
	// li r8,1
	ctx.r8.s64 = 1;
	// beq cr6,0x825f17d8
	if (ctx.cr6.eq) goto loc_825F17D8;
loc_825F17D4:
	// li r8,0
	ctx.r8.s64 = 0;
loc_825F17D8:
	// srawi r7,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r10.s32 >> 1;
	// stw r10,21500(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21500, ctx.r10.u32);
	// lwz r10,204(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// srawi r6,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r9.s32 >> 1;
	// stw r9,21504(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21504, ctx.r9.u32);
	// stw r8,152(r3)
	PPC_STORE_U32(ctx.r3.u32 + 152, ctx.r8.u32);
	// rlwinm r9,r10,3,0,28
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r8,136(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// stw r7,21508(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21508, ctx.r7.u32);
	// addi r9,r9,-8
	ctx.r9.s64 = ctx.r9.s64 + -8;
	// lwz r7,208(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// rlwinm r10,r8,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r11,1968(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 1968);
	// rlwinm r8,r7,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r6,21512(r3)
	PPC_STORE_U32(ctx.r3.u32 + 21512, ctx.r6.u32);
	// cmpwi cr6,r10,2
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 2, ctx.xer);
	// addi r8,r8,-4
	ctx.r8.s64 = ctx.r8.s64 + -4;
	// stw r9,15176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15176, ctx.r9.u32);
	// stw r9,236(r3)
	PPC_STORE_U32(ctx.r3.u32 + 236, ctx.r9.u32);
	// li r9,2
	ctx.r9.s64 = 2;
	// stw r8,15180(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15180, ctx.r8.u32);
	// stw r10,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r10.u32);
	// li r10,2
	ctx.r10.s64 = 2;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// bgt cr6,0x825f1840
	if (ctx.cr6.gt) goto loc_825F1840;
	// li r10,1
	ctx.r10.s64 = 1;
loc_825F1840:
	// stw r10,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r10.u32);
	// lwz r11,3356(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3356);
	// lwz r31,136(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// lwz r10,188(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// twllei r11,0
	// lwz r9,200(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 200);
	// divwu r30,r31,r11
	ctx.r30.u32 = ctx.r31.u32 / ctx.r11.u32;
	// lwz r29,140(r3)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// divwu r10,r10,r11
	ctx.r10.u32 = ctx.r10.u32 / ctx.r11.u32;
	// lwz r7,220(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// divwu r9,r9,r11
	ctx.r9.u32 = ctx.r9.u32 / ctx.r11.u32;
	// lwz r6,224(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// divwu r8,r29,r11
	ctx.r8.u32 = ctx.r29.u32 / ctx.r11.u32;
	// twllei r11,0
	// twllei r11,0
	// stw r30,3816(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3816, ctx.r30.u32);
	// cmplwi cr6,r11,2
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 2, ctx.xer);
	// stw r10,3824(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3824, ctx.r10.u32);
	// stw r7,3836(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3836, ctx.r7.u32);
	// twllei r11,0
	// stw r6,3840(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3840, ctx.r6.u32);
	// stw r9,3832(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3832, ctx.r9.u32);
	// stw r8,3812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3812, ctx.r8.u32);
	// blt cr6,0x825f1918
	if (ctx.cr6.lt) goto loc_825F1918;
	// lwz r5,204(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// rlwinm r28,r10,1,0,30
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r4,208(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// cmplwi cr6,r11,4
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 4, ctx.xer);
	// stw r10,3856(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3856, ctx.r10.u32);
	// mullw r10,r10,r5
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r5.s32);
	// stw r9,3864(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3864, ctx.r9.u32);
	// stw r8,3844(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3844, ctx.r8.u32);
	// stw r28,3860(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3860, ctx.r28.u32);
	// mullw r11,r9,r4
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r4.s32);
	// rlwinm r27,r9,1,0,30
	ctx.r27.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r11,r11,r6
	ctx.r11.u64 = ctx.r11.u64 + ctx.r6.u64;
	// stw r27,3868(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3868, ctx.r27.u32);
	// stw r10,3872(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3872, ctx.r10.u32);
	// stw r11,3876(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3876, ctx.r11.u32);
	// bne cr6,0x825f18f8
	if (!ctx.cr6.eq) goto loc_825F18F8;
	// rlwinm r11,r8,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r30,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r11,3848(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3848, ctx.r11.u32);
	// stw r10,3852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3852, ctx.r10.u32);
	// b 0x825f1900
	goto loc_825F1900;
loc_825F18F8:
	// stw r29,3848(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3848, ctx.r29.u32);
	// stw r31,3852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3852, ctx.r31.u32);
loc_825F1900:
	// mullw r11,r8,r5
	ctx.r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// mullw r10,r8,r4
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r4.s32);
	// rlwinm r11,r11,4,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 4) & 0xFFFFFFF0;
	// rlwinm r10,r10,3,0,28
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 3) & 0xFFFFFFF8;
	// stw r11,15168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15168, ctx.r11.u32);
	// stw r10,15172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15172, ctx.r10.u32);
loc_825F1918:
	// lwz r4,268(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 268);
	// li r9,0
	ctx.r9.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// beq cr6,0x825f19d4
	if (ctx.cr6.eq) goto loc_825F19D4;
loc_825F192C:
	// lwz r10,136(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// ble cr6,0x825f19c4
	if (!ctx.cr6.gt) goto loc_825F19C4;
	// cntlzw r10,r6
	ctx.r10.u64 = ctx.r6.u32 == 0 ? 32 : __builtin_clz(ctx.r6.u32);
	// rlwinm r5,r10,28,30,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 28) & 0x2;
	// rlwinm r10,r9,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
loc_825F1954:
	// lwz r7,136(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cntlzw r31,r11
	ctx.r31.u64 = ctx.r11.u32 == 0 ? 32 : __builtin_clz(ctx.r11.u32);
	// lwz r8,140(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r7,r7,-1
	ctx.r7.s64 = ctx.r7.s64 + -1;
	// lwz r30,0(r10)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// subf r7,r11,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r11.s64;
	// subf r8,r6,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r6.s64;
	// cntlzw r7,r7
	ctx.r7.u64 = ctx.r7.u32 == 0 ? 32 : __builtin_clz(ctx.r7.u32);
	// cntlzw r8,r8
	ctx.r8.u64 = ctx.r8.u32 == 0 ? 32 : __builtin_clz(ctx.r8.u32);
	// rlwinm r7,r7,27,31,31
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 27) & 0x1;
	// rlwinm r8,r8,28,30,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 28) & 0x2;
	// rlwinm r31,r31,27,31,31
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 27) & 0x1;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// or r7,r31,r5
	ctx.r7.u64 = ctx.r31.u64 | ctx.r5.u64;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// clrlwi r7,r7,28
	ctx.r7.u64 = ctx.r7.u32 & 0xF;
	// rlwinm r31,r30,0,20,15
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 0) & 0xFFFFFFFFFFFF0FFF;
	// or r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 | ctx.r7.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// rlwinm r8,r8,12,0,19
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 12) & 0xFFFFF000;
	// or r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 | ctx.r31.u64;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// lwz r8,136(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// cmplw cr6,r11,r8
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r8.u32, ctx.xer);
	// blt cr6,0x825f1954
	if (ctx.cr6.lt) goto loc_825F1954;
loc_825F19C4:
	// lwz r11,140(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplw cr6,r6,r11
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, ctx.r11.u32, ctx.xer);
	// blt cr6,0x825f192c
	if (ctx.cr6.lt) goto loc_825F192C;
loc_825F19D4:
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
}

__attribute__((alias("__imp__sub_825F19D8"))) PPC_WEAK_FUNC(sub_825F19D8);
PPC_FUNC_IMPL(__imp__sub_825F19D8) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9f4
	ctx.lr = 0x825F19E0;
	sub_8239B9F4(ctx, base);
	// lwz r11,3924(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3924);
	// lwz r6,180(r3)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r3.u32 + 180);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r10,188(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// srawi r11,r6,1
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r6.s32 >> 1;
	// lwz r7,156(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// lwz r5,160(r3)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// addi r11,r11,15
	ctx.r11.s64 = ctx.r11.s64 + 15;
	// lwz r9,19696(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19696);
	// rlwinm r11,r11,0,0,27
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFF0;
	// srawi r8,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r11.s32 >> 1;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// add r26,r9,r11
	ctx.r26.u64 = ctx.r9.u64 + ctx.r11.u64;
	// addi r10,r10,15
	ctx.r10.s64 = ctx.r10.s64 + 15;
	// rlwinm r10,r10,0,0,27
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFF0;
	// srawi r29,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r29.s64 = ctx.r10.s32 >> 1;
	// srawi r30,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r30.s64 = ctx.r7.s32 >> 1;
	// srawi r21,r5,1
	ctx.xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r21.s64 = ctx.r5.s32 >> 1;
	// srawi r27,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r27.s64 = ctx.r11.s32 >> 4;
	// srawi r22,r10,4
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xF) != 0);
	ctx.r22.s64 = ctx.r10.s32 >> 4;
	// addi r20,r27,-1
	ctx.r20.s64 = ctx.r27.s64 + -1;
	// beq cr6,0x825f1a40
	if (ctx.cr6.eq) goto loc_825F1A40;
	// srawi r8,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r8.s64 = ctx.r11.s32 >> 2;
	// srawi r29,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r29.s64 = ctx.r10.s32 >> 2;
loc_825F1A40:
	// lwz r9,19700(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19700);
	// cmpw cr6,r11,r30
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r30.s32, ctx.xer);
	// lwz r7,19696(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 19696);
	// stw r6,14832(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14832, ctx.r6.u32);
	// rlwinm r6,r9,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// lwz r4,192(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 192);
	// rlwinm r5,r7,1,0,30
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// add r28,r9,r8
	ctx.r28.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r31,r5,r11
	ctx.r31.u64 = ctx.r5.u64 + ctx.r11.u64;
	// add r5,r5,r10
	ctx.r5.u64 = ctx.r5.u64 + ctx.r10.u64;
	// stw r4,14836(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14836, ctx.r4.u32);
	// lwz r4,188(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// stw r4,14840(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14840, ctx.r4.u32);
	// lwz r4,200(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 200);
	// stw r4,14844(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14844, ctx.r4.u32);
	// lwz r4,156(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// stw r4,14848(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14848, ctx.r4.u32);
	// lwz r4,160(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// stw r4,14852(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14852, ctx.r4.u32);
	// lwz r4,184(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 184);
	// stw r4,14856(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14856, ctx.r4.u32);
	// lwz r4,196(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 196);
	// stw r4,14860(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14860, ctx.r4.u32);
	// lwz r4,152(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 152);
	// stw r4,14864(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14864, ctx.r4.u32);
	// add r4,r6,r8
	ctx.r4.u64 = ctx.r6.u64 + ctx.r8.u64;
	// lwz r25,136(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// add r6,r6,r29
	ctx.r6.u64 = ctx.r6.u64 + ctx.r29.u64;
	// addi r24,r4,1
	ctx.r24.s64 = ctx.r4.s64 + 1;
	// mullw r9,r24,r9
	ctx.r9.s64 = int64_t(ctx.r24.s32) * int64_t(ctx.r9.s32);
	// stw r25,14868(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14868, ctx.r25.u32);
	// lwz r25,140(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// stw r25,14872(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14872, ctx.r25.u32);
	// lwz r25,144(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 144);
	// stw r25,14876(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14876, ctx.r25.u32);
	// lwz r25,148(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 148);
	// stw r25,14880(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14880, ctx.r25.u32);
	// lwz r25,204(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// stw r25,14884(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14884, ctx.r25.u32);
	// lwz r25,208(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// stw r25,14888(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14888, ctx.r25.u32);
	// lwz r25,212(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 212);
	// stw r25,14892(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14892, ctx.r25.u32);
	// lwz r25,216(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 216);
	// stw r25,14896(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14896, ctx.r25.u32);
	// lwz r25,220(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// stw r25,14900(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14900, ctx.r25.u32);
	// addi r25,r31,1
	ctx.r25.s64 = ctx.r31.s64 + 1;
	// lwz r23,224(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// mullw r7,r25,r7
	ctx.r7.s64 = int64_t(ctx.r25.s32) * int64_t(ctx.r7.s32);
	// stw r23,14904(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14904, ctx.r23.u32);
	// lwz r24,228(r3)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r3.u32 + 228);
	// stw r24,14908(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14908, ctx.r24.u32);
	// rlwinm r24,r4,3,0,28
	ctx.r24.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 3) & 0xFFFFFFF8;
	// lwz r25,232(r3)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r3.u32 + 232);
	// stw r25,14912(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14912, ctx.r25.u32);
	// rlwinm r25,r31,4,0,27
	ctx.r25.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 4) & 0xFFFFFFF0;
	// stw r11,14916(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14916, ctx.r11.u32);
	// stw r8,14920(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14920, ctx.r8.u32);
	// lwz r23,188(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// stw r23,14924(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14924, ctx.r23.u32);
	// lwz r23,200(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 200);
	// stw r30,14932(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14932, ctx.r30.u32);
	// stw r23,14928(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14928, ctx.r23.u32);
	// lwz r23,160(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// stw r26,14940(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14940, ctx.r26.u32);
	// stw r28,14944(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14944, ctx.r28.u32);
	// stw r23,14936(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14936, ctx.r23.u32);
	// bne cr6,0x825f1b68
	if (!ctx.cr6.eq) goto loc_825F1B68;
	// lwz r23,188(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// lwz r19,160(r3)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r3.u32 + 160);
	// cmpw cr6,r23,r19
	ctx.cr6.compare<int32_t>(ctx.r23.s32, ctx.r19.s32, ctx.xer);
	// li r23,1
	ctx.r23.s64 = 1;
	// beq cr6,0x825f1b6c
	if (ctx.cr6.eq) goto loc_825F1B6C;
loc_825F1B68:
	// li r23,0
	ctx.r23.s64 = 0;
loc_825F1B6C:
	// stw r23,14948(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14948, ctx.r23.u32);
	// stw r27,14952(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14952, ctx.r27.u32);
	// lwz r23,140(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// stw r23,14956(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14956, ctx.r23.u32);
	// lwz r23,140(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 140);
	// stw r20,14964(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14964, ctx.r20.u32);
	// mullw r23,r23,r27
	ctx.r23.s64 = int64_t(ctx.r23.s32) * int64_t(ctx.r27.s32);
	// stw r31,14968(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14968, ctx.r31.u32);
	// stw r4,14972(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14972, ctx.r4.u32);
	// stw r23,14960(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14960, ctx.r23.u32);
	// lwz r23,212(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 212);
	// stw r23,14976(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14976, ctx.r23.u32);
	// lwz r23,216(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 216);
	// stw r7,14984(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14984, ctx.r7.u32);
	// stw r9,14988(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14988, ctx.r9.u32);
	// stw r25,14992(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14992, ctx.r25.u32);
	// stw r24,14996(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14996, ctx.r24.u32);
	// stw r23,14980(r3)
	PPC_STORE_U32(ctx.r3.u32 + 14980, ctx.r23.u32);
	// lwz r23,180(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 180);
	// stw r23,15000(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15000, ctx.r23.u32);
	// lwz r23,192(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 192);
	// stw r10,15008(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15008, ctx.r10.u32);
	// stw r29,15012(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15012, ctx.r29.u32);
	// stw r23,15004(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15004, ctx.r23.u32);
	// lwz r23,156(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// stw r21,15020(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15020, ctx.r21.u32);
	// stw r23,15016(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15016, ctx.r23.u32);
	// lwz r23,184(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 184);
	// stw r23,15024(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15024, ctx.r23.u32);
	// lwz r23,196(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 196);
	// stw r23,15028(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15028, ctx.r23.u32);
	// lwz r23,180(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 180);
	// lwz r19,156(r3)
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// cmpw cr6,r23,r19
	ctx.cr6.compare<int32_t>(ctx.r23.s32, ctx.r19.s32, ctx.xer);
	// bne cr6,0x825f1c04
	if (!ctx.cr6.eq) goto loc_825F1C04;
	// cmpw cr6,r10,r21
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r21.s32, ctx.xer);
	// li r23,1
	ctx.r23.s64 = 1;
	// beq cr6,0x825f1c08
	if (ctx.cr6.eq) goto loc_825F1C08;
loc_825F1C04:
	// li r23,0
	ctx.r23.s64 = 0;
loc_825F1C08:
	// stw r23,15032(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15032, ctx.r23.u32);
	// cmpw cr6,r11,r30
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r30.s32, ctx.xer);
	// lwz r23,136(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// stw r22,15040(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15040, ctx.r22.u32);
	// stw r23,15036(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15036, ctx.r23.u32);
	// lwz r23,136(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 136);
	// mullw r23,r23,r22
	ctx.r23.s64 = int64_t(ctx.r23.s32) * int64_t(ctx.r22.s32);
	// stw r23,15044(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15044, ctx.r23.u32);
	// lwz r23,148(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 148);
	// stw r23,15048(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15048, ctx.r23.u32);
	// lwz r23,204(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 204);
	// stw r23,15052(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15052, ctx.r23.u32);
	// lwz r23,208(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 208);
	// stw r5,15060(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15060, ctx.r5.u32);
	// stw r6,15064(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15064, ctx.r6.u32);
	// stw r23,15056(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15056, ctx.r23.u32);
	// lwz r23,220(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 220);
	// stw r23,15068(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15068, ctx.r23.u32);
	// lwz r23,224(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 224);
	// stw r23,15072(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15072, ctx.r23.u32);
	// lwz r23,228(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 228);
	// stw r23,15076(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15076, ctx.r23.u32);
	// lwz r23,232(r3)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r3.u32 + 232);
	// stw r23,15080(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15080, ctx.r23.u32);
	// stw r11,15084(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15084, ctx.r11.u32);
	// stw r8,15088(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15088, ctx.r8.u32);
	// stw r10,15092(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15092, ctx.r10.u32);
	// stw r29,15096(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15096, ctx.r29.u32);
	// stw r30,15100(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15100, ctx.r30.u32);
	// stw r21,15104(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15104, ctx.r21.u32);
	// stw r26,15108(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15108, ctx.r26.u32);
	// stw r28,15112(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15112, ctx.r28.u32);
	// bne cr6,0x825f1c98
	if (!ctx.cr6.eq) goto loc_825F1C98;
	// cmpw cr6,r10,r21
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r21.s32, ctx.xer);
	// li r11,1
	ctx.r11.s64 = 1;
	// beq cr6,0x825f1c9c
	if (ctx.cr6.eq) goto loc_825F1C9C;
loc_825F1C98:
	// li r11,0
	ctx.r11.s64 = 0;
loc_825F1C9C:
	// mullw r10,r22,r27
	ctx.r10.s64 = int64_t(ctx.r22.s32) * int64_t(ctx.r27.s32);
	// stw r11,15116(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15116, ctx.r11.u32);
	// stw r27,15120(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15120, ctx.r27.u32);
	// stw r22,15124(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15124, ctx.r22.u32);
	// stw r20,15132(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15132, ctx.r20.u32);
	// stw r31,15136(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15136, ctx.r31.u32);
	// stw r10,15128(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15128, ctx.r10.u32);
	// stw r4,15140(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15140, ctx.r4.u32);
	// stw r5,15144(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15144, ctx.r5.u32);
	// stw r6,15148(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15148, ctx.r6.u32);
	// stw r7,15152(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15152, ctx.r7.u32);
	// stw r9,15156(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15156, ctx.r9.u32);
	// stw r25,15160(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15160, ctx.r25.u32);
	// stw r24,15164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15164, ctx.r24.u32);
	// b 0x8239ba44
	// ERROR 8239BA44
	return;
}

__attribute__((alias("__imp__sub_825F1CD8"))) PPC_WEAK_FUNC(sub_825F1CD8);
PPC_FUNC_IMPL(__imp__sub_825F1CD8) {
	PPC_FUNC_PROLOGUE();
	// mulli r11,r4,84
	ctx.r11.s64 = ctx.r4.s64 * 84;
	// lwz r10,20056(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 20056);
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lwz r10,14832(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14832);
	// stw r10,180(r3)
	PPC_STORE_U32(ctx.r3.u32 + 180, ctx.r10.u32);
	// lwz r10,14836(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14836);
	// lwz r9,180(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 180);
	// stw r10,192(r3)
	PPC_STORE_U32(ctx.r3.u32 + 192, ctx.r10.u32);
	// lwz r10,14840(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14840);
	// lwz r8,192(r3)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r3.u32 + 192);
	// stw r10,188(r3)
	PPC_STORE_U32(ctx.r3.u32 + 188, ctx.r10.u32);
	// lwz r10,14844(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14844);
	// lwz r7,188(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 188);
	// stw r9,164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 164, ctx.r9.u32);
	// stw r8,168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 168, ctx.r8.u32);
	// stw r10,200(r3)
	PPC_STORE_U32(ctx.r3.u32 + 200, ctx.r10.u32);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// stw r7,172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 172, ctx.r7.u32);
	// stw r10,176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 176, ctx.r10.u32);
	// lwz r10,14848(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14848);
	// stw r10,156(r3)
	PPC_STORE_U32(ctx.r3.u32 + 156, ctx.r10.u32);
	// lwz r10,14852(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14852);
	// stw r10,160(r3)
	PPC_STORE_U32(ctx.r3.u32 + 160, ctx.r10.u32);
	// beq cr6,0x825f1d6c
	if (ctx.cr6.eq) goto loc_825F1D6C;
	// lwz r9,156(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 156);
	// rotlwi r10,r10,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 0);
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addi r8,r10,1
	ctx.r8.s64 = ctx.r10.s64 + 1;
	// srawi r10,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r9.s32 >> 1;
	// srawi r9,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r8.s32 >> 1;
	// rlwinm r8,r10,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r9,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// stw r10,168(r3)
	PPC_STORE_U32(ctx.r3.u32 + 168, ctx.r10.u32);
	// stw r9,176(r3)
	PPC_STORE_U32(ctx.r3.u32 + 176, ctx.r9.u32);
	// stw r8,164(r3)
	PPC_STORE_U32(ctx.r3.u32 + 164, ctx.r8.u32);
	// stw r7,172(r3)
	PPC_STORE_U32(ctx.r3.u32 + 172, ctx.r7.u32);
loc_825F1D6C:
	// lwz r9,14856(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14856);
	// lwz r10,3732(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 3732);
	// stw r9,184(r3)
	PPC_STORE_U32(ctx.r3.u32 + 184, ctx.r9.u32);
	// lwz r9,14860(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14860);
	// stw r9,196(r3)
	PPC_STORE_U32(ctx.r3.u32 + 196, ctx.r9.u32);
	// lwz r9,14864(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14864);
	// stw r9,152(r3)
	PPC_STORE_U32(ctx.r3.u32 + 152, ctx.r9.u32);
	// lwz r9,14868(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14868);
	// stw r9,136(r3)
	PPC_STORE_U32(ctx.r3.u32 + 136, ctx.r9.u32);
	// lwz r9,14872(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14872);
	// stw r9,140(r3)
	PPC_STORE_U32(ctx.r3.u32 + 140, ctx.r9.u32);
	// lwz r9,14876(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14876);
	// stw r9,144(r3)
	PPC_STORE_U32(ctx.r3.u32 + 144, ctx.r9.u32);
	// lwz r9,14880(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14880);
	// stw r9,148(r3)
	PPC_STORE_U32(ctx.r3.u32 + 148, ctx.r9.u32);
	// lwz r9,14884(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14884);
	// stw r9,204(r3)
	PPC_STORE_U32(ctx.r3.u32 + 204, ctx.r9.u32);
	// lwz r9,14888(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14888);
	// stw r9,208(r3)
	PPC_STORE_U32(ctx.r3.u32 + 208, ctx.r9.u32);
	// lwz r9,14892(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14892);
	// stw r9,212(r3)
	PPC_STORE_U32(ctx.r3.u32 + 212, ctx.r9.u32);
	// lwz r9,14896(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14896);
	// stw r9,216(r3)
	PPC_STORE_U32(ctx.r3.u32 + 216, ctx.r9.u32);
	// lwz r9,14900(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14900);
	// stw r9,220(r3)
	PPC_STORE_U32(ctx.r3.u32 + 220, ctx.r9.u32);
	// rotlwi r9,r9,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r9.u32, 0);
	// lwz r8,14904(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14904);
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// stw r8,224(r3)
	PPC_STORE_U32(ctx.r3.u32 + 224, ctx.r8.u32);
	// lwz r9,14908(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14908);
	// stw r9,228(r3)
	PPC_STORE_U32(ctx.r3.u32 + 228, ctx.r9.u32);
	// lwz r11,14912(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14912);
	// stw r10,3756(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3756, ctx.r10.u32);
	// stw r11,232(r3)
	PPC_STORE_U32(ctx.r3.u32 + 232, ctx.r11.u32);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F1DF8"))) PPC_WEAK_FUNC(sub_825F1DF8);
PPC_FUNC_IMPL(__imp__sub_825F1DF8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9e0
	ctx.lr = 0x825F1E00;
	sub_8239B9E0(ctx, base);
	// stwu r1,-288(r1)
	ea = -288 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// mr r17,r9
	ctx.r17.u64 = ctx.r9.u64;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// mr r22,r8
	ctx.r22.u64 = ctx.r8.u64;
	// addi r10,r10,-22816
	ctx.r10.s64 = ctx.r10.s64 + -22816;
	// lwz r11,14828(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 14828);
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// lwz r9,14824(r28)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r28.u32 + 14824);
	// addi r8,r10,24
	ctx.r8.s64 = ctx.r10.s64 + 24;
	// mr r30,r5
	ctx.r30.u64 = ctx.r5.u64;
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// mulli r11,r11,84
	ctx.r11.s64 = ctx.r11.s64 * 84;
	// rlwinm r9,r9,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r28
	ctx.r11.u64 = ctx.r11.u64 + ctx.r28.u64;
	// mr r29,r6
	ctx.r29.u64 = ctx.r6.u64;
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// lwzx r19,r9,r8
	ctx.r19.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r8.u32);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// lwz r26,14832(r11)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14832);
	// mr r24,r27
	ctx.r24.u64 = ctx.r27.u64;
	// lwz r25,14840(r11)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14840);
	// mr r21,r22
	ctx.r21.u64 = ctx.r22.u64;
	// lwz r23,14884(r11)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14884);
	// mr r15,r17
	ctx.r15.u64 = ctx.r17.u64;
	// lwz r14,14888(r11)
	ctx.r14.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14888);
	// cmpwi cr6,r19,2
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 2, ctx.xer);
	// lwzx r18,r9,r10
	ctx.r18.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// bne cr6,0x825f1e84
	if (!ctx.cr6.eq) goto loc_825F1E84;
	// addi r11,r26,31
	ctx.r11.s64 = ctx.r26.s64 + 31;
	// rlwinm r26,r11,0,0,26
	ctx.r26.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFE0;
loc_825F1E84:
	// cmpwi cr6,r18,2
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 2, ctx.xer);
	// bne cr6,0x825f1e94
	if (!ctx.cr6.eq) goto loc_825F1E94;
	// addi r11,r25,31
	ctx.r11.s64 = ctx.r25.s64 + 31;
	// rlwinm r25,r11,0,0,26
	ctx.r25.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFE0;
loc_825F1E94:
	// lwz r11,3924(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 3924);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825f1eac
	if (ctx.cr6.eq) goto loc_825F1EAC;
	// mr r16,r25
	ctx.r16.u64 = ctx.r25.u64;
	// srawi r20,r26,2
	ctx.xer.ca = (ctx.r26.s32 < 0) & ((ctx.r26.u32 & 0x3) != 0);
	ctx.r20.s64 = ctx.r26.s32 >> 2;
	// b 0x825f1eb4
	goto loc_825F1EB4;
loc_825F1EAC:
	// srawi r20,r26,1
	ctx.xer.ca = (ctx.r26.s32 < 0) & ((ctx.r26.u32 & 0x1) != 0);
	ctx.r20.s64 = ctx.r26.s32 >> 1;
	// srawi r16,r25,1
	ctx.xer.ca = (ctx.r25.s32 < 0) & ((ctx.r25.u32 & 0x1) != 0);
	ctx.r16.s64 = ctx.r25.s32 >> 1;
loc_825F1EB4:
	// cmpwi cr6,r19,2
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 2, ctx.xer);
	// bne cr6,0x825f1f2c
	if (!ctx.cr6.eq) goto loc_825F1F2C;
	// lwz r10,15812(r28)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15812);
	// mr r9,r26
	ctx.r9.u64 = ctx.r26.u64;
	// lwz r11,15188(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15188);
	// mr r8,r17
	ctx.r8.u64 = ctx.r17.u64;
	// stw r14,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r14.u32);
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
	// stw r23,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r23.u32);
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// stw r16,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r16.u32);
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// stw r10,128(r1)
	PPC_STORE_U32(ctx.r1.u32 + 128, ctx.r10.u32);
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r11.u32);
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
	// stw r20,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r20.u32);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r11,128(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 128);
	// mtctr r11
	ctx.ctr.u64 = ctx.r11.u64;
	// bctrl 
	ctx.lr = 0x825F1F08;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
	// cmpwi cr6,r18,2
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 2, ctx.xer);
	// bne cr6,0x825f1f6c
	if (!ctx.cr6.eq) goto loc_825F1F6C;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// mr r4,r22
	ctx.r4.u64 = ctx.r22.u64;
	// mr r5,r17
	ctx.r5.u64 = ctx.r17.u64;
	// mr r24,r31
	ctx.r24.u64 = ctx.r31.u64;
	// mr r21,r30
	ctx.r21.u64 = ctx.r30.u64;
	// mr r15,r29
	ctx.r15.u64 = ctx.r29.u64;
	// b 0x825f1f34
	goto loc_825F1F34;
loc_825F1F2C:
	// cmpwi cr6,r18,2
	ctx.cr6.compare<int32_t>(ctx.r18.s32, 2, ctx.xer);
	// bne cr6,0x825f1f6c
	if (!ctx.cr6.eq) goto loc_825F1F6C;
loc_825F1F34:
	// lwz r11,15188(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15188);
	// mr r10,r25
	ctx.r10.u64 = ctx.r25.u64;
	// lwz r31,15816(r28)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r28.u32 + 15816);
	// mr r9,r26
	ctx.r9.u64 = ctx.r26.u64;
	// stw r14,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r14.u32);
	// mr r8,r15
	ctx.r8.u64 = ctx.r15.u64;
	// stw r23,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r23.u32);
	// mr r7,r21
	ctx.r7.u64 = ctx.r21.u64;
	// stw r16,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r16.u32);
	// mr r6,r24
	ctx.r6.u64 = ctx.r24.u64;
	// stw r11,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r11.u32);
	// stw r20,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r20.u32);
	// mtctr r31
	ctx.ctr.u64 = ctx.r31.u64;
	// bctrl 
	ctx.lr = 0x825F1F6C;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_825F1F6C:
	// mr r9,r24
	ctx.r9.u64 = ctx.r24.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r25,0
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 0, ctx.xer);
	// ble cr6,0x825f1fc0
	if (!ctx.cr6.gt) goto loc_825F1FC0;
	// mullw r7,r18,r23
	ctx.r7.s64 = int64_t(ctx.r18.s32) * int64_t(ctx.r23.s32);
loc_825F1F80:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// ble cr6,0x825f1fa8
	if (!ctx.cr6.gt) goto loc_825F1FA8;
	// mr r10,r27
	ctx.r10.u64 = ctx.r27.u64;
loc_825F1F90:
	// lbzx r6,r11,r9
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r9.u32);
	// add r11,r11,r19
	ctx.r11.u64 = ctx.r11.u64 + ctx.r19.u64;
	// cmpw cr6,r11,r26
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r26.s32, ctx.xer);
	// stb r6,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r6.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// blt cr6,0x825f1f90
	if (ctx.cr6.lt) goto loc_825F1F90;
loc_825F1FA8:
	// lwz r11,204(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 204);
	// add r8,r8,r18
	ctx.r8.u64 = ctx.r8.u64 + ctx.r18.u64;
	// add r9,r7,r9
	ctx.r9.u64 = ctx.r7.u64 + ctx.r9.u64;
	// add r27,r11,r27
	ctx.r27.u64 = ctx.r11.u64 + ctx.r27.u64;
	// cmpw cr6,r8,r25
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r25.s32, ctx.xer);
	// blt cr6,0x825f1f80
	if (ctx.cr6.lt) goto loc_825F1F80;
loc_825F1FC0:
	// mr r9,r21
	ctx.r9.u64 = ctx.r21.u64;
	// mr r7,r22
	ctx.r7.u64 = ctx.r22.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r16,0
	ctx.cr6.compare<int32_t>(ctx.r16.s32, 0, ctx.xer);
	// ble cr6,0x825f2018
	if (!ctx.cr6.gt) goto loc_825F2018;
	// mullw r6,r18,r14
	ctx.r6.s64 = int64_t(ctx.r18.s32) * int64_t(ctx.r14.s32);
loc_825F1FD8:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x825f2000
	if (!ctx.cr6.gt) goto loc_825F2000;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
loc_825F1FE8:
	// lbzx r5,r11,r9
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r9.u32);
	// add r11,r11,r19
	ctx.r11.u64 = ctx.r11.u64 + ctx.r19.u64;
	// cmpw cr6,r11,r20
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r20.s32, ctx.xer);
	// stb r5,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r5.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// blt cr6,0x825f1fe8
	if (ctx.cr6.lt) goto loc_825F1FE8;
loc_825F2000:
	// lwz r11,208(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 208);
	// add r8,r8,r18
	ctx.r8.u64 = ctx.r8.u64 + ctx.r18.u64;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r7,r11,r7
	ctx.r7.u64 = ctx.r11.u64 + ctx.r7.u64;
	// cmpw cr6,r8,r16
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r16.s32, ctx.xer);
	// blt cr6,0x825f1fd8
	if (ctx.cr6.lt) goto loc_825F1FD8;
loc_825F2018:
	// mr r9,r15
	ctx.r9.u64 = ctx.r15.u64;
	// mr r7,r17
	ctx.r7.u64 = ctx.r17.u64;
	// li r8,0
	ctx.r8.s64 = 0;
	// cmpwi cr6,r16,0
	ctx.cr6.compare<int32_t>(ctx.r16.s32, 0, ctx.xer);
	// ble cr6,0x825f2070
	if (!ctx.cr6.gt) goto loc_825F2070;
	// mullw r6,r18,r14
	ctx.r6.s64 = int64_t(ctx.r18.s32) * int64_t(ctx.r14.s32);
loc_825F2030:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x825f2058
	if (!ctx.cr6.gt) goto loc_825F2058;
	// mr r10,r7
	ctx.r10.u64 = ctx.r7.u64;
loc_825F2040:
	// lbzx r5,r11,r9
	ctx.r5.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r9.u32);
	// add r11,r11,r19
	ctx.r11.u64 = ctx.r11.u64 + ctx.r19.u64;
	// cmpw cr6,r11,r20
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r20.s32, ctx.xer);
	// stb r5,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r5.u8);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// blt cr6,0x825f2040
	if (ctx.cr6.lt) goto loc_825F2040;
loc_825F2058:
	// lwz r11,208(r28)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + 208);
	// add r8,r8,r18
	ctx.r8.u64 = ctx.r8.u64 + ctx.r18.u64;
	// add r9,r6,r9
	ctx.r9.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r7,r11,r7
	ctx.r7.u64 = ctx.r11.u64 + ctx.r7.u64;
	// cmpw cr6,r8,r16
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r16.s32, ctx.xer);
	// blt cr6,0x825f2030
	if (ctx.cr6.lt) goto loc_825F2030;
loc_825F2070:
	// addi r1,r1,288
	ctx.r1.s64 = ctx.r1.s64 + 288;
	// b 0x8239ba30
	sub_8239BA30(ctx, base);
	return;
}

__attribute__((alias("__imp__sub_825F2078"))) PPC_WEAK_FUNC(sub_825F2078);
PPC_FUNC_IMPL(__imp__sub_825F2078) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba10
	ctx.lr = 0x825F2080;
	sub_8239BA10(ctx, base);
	// addi r27,r6,-1
	ctx.r27.s64 = ctx.r6.s64 + -1;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// cmpwi cr6,r27,1
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 1, ctx.xer);
	// ble cr6,0x825f20e8
	if (!ctx.cr6.gt) goto loc_825F20E8;
	// addi r11,r27,-2
	ctx.r11.s64 = ctx.r27.s64 + -2;
	// rlwinm r31,r7,1,0,30
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r9,r11,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r8,r5,4
	ctx.r8.s64 = ctx.r5.s64 + 4;
	// add r11,r4,r7
	ctx.r11.u64 = ctx.r4.u64 + ctx.r7.u64;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// subf r28,r7,r31
	ctx.r28.s64 = ctx.r31.s64 - ctx.r7.s64;
loc_825F20AC:
	// lbz r29,0(r10)
	ctx.r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lbzx r30,r28,r11
	ctx.r30.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r11.u32);
	// add r10,r31,r10
	ctx.r10.u64 = ctx.r31.u64 + ctx.r10.u64;
	// lbz r26,0(r11)
	ctx.r26.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// add r30,r30,r29
	ctx.r30.u64 = ctx.r30.u64 + ctx.r29.u64;
	// rotlwi r29,r26,4
	ctx.r29.u64 = __builtin_rotateleft32(ctx.r26.u32, 4);
	// mulli r30,r30,-406
	ctx.r30.s64 = ctx.r30.s64 * -406;
	// srawi r30,r30,4
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0xF) != 0);
	ctx.r30.s64 = ctx.r30.s32 >> 4;
	// add r11,r11,r31
	ctx.r11.u64 = ctx.r11.u64 + ctx.r31.u64;
	// add r30,r30,r29
	ctx.r30.u64 = ctx.r30.u64 + ctx.r29.u64;
	// stw r30,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r30.u32);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// bne cr6,0x825f20ac
	if (!ctx.cr6.eq) goto loc_825F20AC;
loc_825F20E8:
	// rlwinm r11,r6,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// lbzx r8,r10,r7
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r7.u32);
	// addi r30,r5,4
	ctx.r30.s64 = ctx.r5.s64 + 4;
	// add r29,r11,r5
	ctx.r29.u64 = ctx.r11.u64 + ctx.r5.u64;
	// mulli r11,r9,-406
	ctx.r11.s64 = ctx.r9.s64 * -406;
	// srawi r10,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 3;
	// rotlwi r11,r8,4
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r8.u32, 4);
	// cmpwi cr6,r6,2
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 2, ctx.xer);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r11,-4(r29)
	PPC_STORE_U32(ctx.r29.u32 + -4, ctx.r11.u32);
	// lwz r11,0(r30)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// mulli r11,r11,-217
	ctx.r11.s64 = ctx.r11.s64 * -217;
	// srawi r11,r11,10
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3FF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 10;
	// rotlwi r10,r10,5
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r10.u32, 5);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r11,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r11.u32);
	// ble cr6,0x825f2180
	if (!ctx.cr6.gt) goto loc_825F2180;
	// addi r11,r6,-3
	ctx.r11.s64 = ctx.r6.s64 + -3;
	// rlwinm r31,r7,1,0,30
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r10,r11,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r11,r5,12
	ctx.r11.s64 = ctx.r5.s64 + 12;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825F2148:
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// add r4,r31,r4
	ctx.r4.u64 = ctx.r31.u64 + ctx.r4.u64;
	// lwz r9,-8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// lbz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// mulli r9,r9,-217
	ctx.r9.s64 = ctx.r9.s64 * -217;
	// srawi r9,r9,11
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7FF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 11;
	// rotlwi r8,r8,5
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r8.u32, 5);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r9.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f2148
	if (!ctx.cr6.eq) goto loc_825F2148;
loc_825F2180:
	// cmpwi cr6,r27,1
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 1, ctx.xer);
	// ble cr6,0x825f21c8
	if (!ctx.cr6.gt) goto loc_825F21C8;
	// addi r10,r27,-2
	ctx.r10.s64 = ctx.r27.s64 + -2;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825F2198:
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r4,4(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// add r9,r9,r4
	ctx.r9.u64 = ctx.r9.u64 + ctx.r4.u64;
	// mulli r9,r9,226
	ctx.r9.s64 = ctx.r9.s64 * 226;
	// srawi r9,r9,9
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1FF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 9;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f2198
	if (!ctx.cr6.eq) goto loc_825F2198;
loc_825F21C8:
	// addi r11,r6,-2
	ctx.r11.s64 = ctx.r6.s64 + -2;
	// lwz r10,-4(r29)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r29.u32 + -4);
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// lwzx r11,r11,r5
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + ctx.r5.u32);
	// mulli r11,r11,226
	ctx.r11.s64 = ctx.r11.s64 * 226;
	// srawi r11,r11,8
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xFF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 8;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r11,-4(r29)
	PPC_STORE_U32(ctx.r29.u32 + -4, ctx.r11.u32);
	// lwz r10,0(r30)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + 0);
	// ble cr6,0x825f226c
	if (!ctx.cr6.gt) goto loc_825F226C;
	// addi r11,r6,-1
	ctx.r11.s64 = ctx.r6.s64 + -1;
	// rlwinm r4,r7,1,0,30
	ctx.r4.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r11,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r11,r30
	ctx.r11.u64 = ctx.r30.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// li r3,0
	ctx.r3.s64 = 0;
loc_825F2210:
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// lwz r6,-4(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// add r10,r5,r10
	ctx.r10.u64 = ctx.r5.u64 + ctx.r10.u64;
	// mulli r10,r10,227
	ctx.r10.s64 = ctx.r10.s64 * 227;
	// srawi r10,r10,8
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// add r10,r10,r6
	ctx.r10.u64 = ctx.r10.u64 + ctx.r6.u64;
	// addi r10,r10,20
	ctx.r10.s64 = ctx.r10.s64 + 20;
	// mulli r10,r10,26
	ctx.r10.s64 = ctx.r10.s64 * 26;
	// srawi r10,r10,10
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3FF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 10;
	// cmplwi cr6,r10,255
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 255, ctx.xer);
	// ble cr6,0x825f224c
	if (!ctx.cr6.gt) goto loc_825F224C;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// blt cr6,0x825f224c
	if (ctx.cr6.lt) goto loc_825F224C;
	// li r10,255
	ctx.r10.s64 = 255;
loc_825F224C:
	// stb r10,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r10.u8);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// stbx r3,r9,r7
	PPC_STORE_U8(ctx.r9.u32 + ctx.r7.u32, ctx.r3.u8);
	// add r9,r4,r9
	ctx.r9.u64 = ctx.r4.u64 + ctx.r9.u64;
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f2210
	if (!ctx.cr6.eq) goto loc_825F2210;
loc_825F226C:
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
}

__attribute__((alias("__imp__sub_825F2270"))) PPC_WEAK_FUNC(sub_825F2270);
PPC_FUNC_IMPL(__imp__sub_825F2270) {
	PPC_FUNC_PROLOGUE();
	// addi r11,r4,-1
	ctx.r11.s64 = ctx.r4.s64 + -1;
	// cmpwi cr6,r11,1
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 1, ctx.xer);
	// ble cr6,0x825f22c0
	if (!ctx.cr6.gt) goto loc_825F22C0;
	// addi r10,r11,-2
	ctx.r10.s64 = ctx.r11.s64 + -2;
	// addi r11,r3,4
	ctx.r11.s64 = ctx.r3.s64 + 4;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825F228C:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r8,-4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r8,r8,7,0,24
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 7) & 0xFFFFFF80;
	// rlwinm r7,r7,7,0,24
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 7) & 0xFFFFFF80;
	// subf r9,r8,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r8.s64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// subf r9,r7,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r7.s64;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f228c
	if (!ctx.cr6.eq) goto loc_825F228C;
loc_825F22C0:
	// rlwinm r11,r4,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r4.u32 | (ctx.r4.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r10,r4,-2
	ctx.r10.s64 = ctx.r4.s64 + -2;
	// add r11,r11,r3
	ctx.r11.u64 = ctx.r11.u64 + ctx.r3.u64;
	// rlwinm r10,r10,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r4,2
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 2, ctx.xer);
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// lwzx r10,r10,r3
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r10.u32 + ctx.r3.u32);
	// rlwinm r9,r9,8,0,23
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// rlwinm r10,r10,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 8) & 0xFFFFFF00;
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// stw r10,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r10.u32);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r9,4(r3)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r3.u32 + 4);
	// rlwinm r10,r11,8,0,23
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 8) & 0xFFFFFF00;
	// srawi r11,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r9.s32 >> 1;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// stw r11,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r11.u32);
	// ble cr6,0x825f2348
	if (!ctx.cr6.gt) goto loc_825F2348;
	// addi r10,r4,-3
	ctx.r10.s64 = ctx.r4.s64 + -3;
	// addi r11,r3,8
	ctx.r11.s64 = ctx.r3.s64 + 8;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825F2318:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r7,4(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// rlwinm r8,r9,8,0,23
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 8) & 0xFFFFFF00;
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// add r9,r9,r7
	ctx.r9.u64 = ctx.r9.u64 + ctx.r7.u64;
	// srawi r9,r9,2
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 2;
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f2318
	if (!ctx.cr6.eq) goto loc_825F2318;
loc_825F2348:
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// blelr cr6
	if (!ctx.cr6.gt) return;
	// addi r10,r4,-1
	ctx.r10.s64 = ctx.r4.s64 + -1;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
loc_825F2364:
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r10,r10,128
	ctx.r10.s64 = ctx.r10.s64 + 128;
	// srawi r10,r10,8
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0xFF) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 8;
	// cmplwi cr6,r10,255
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 255, ctx.xer);
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// ble cr6,0x825f238c
	if (!ctx.cr6.gt) goto loc_825F238C;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// blt cr6,0x825f238c
	if (ctx.cr6.lt) goto loc_825F238C;
	// li r10,255
	ctx.r10.s64 = 255;
loc_825F238C:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stw r10,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r10.u32);
	// stw r8,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r8.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825f2364
	if (!ctx.cr6.eq) goto loc_825F2364;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F23A8"))) PPC_WEAK_FUNC(sub_825F23A8);
PPC_FUNC_IMPL(__imp__sub_825F23A8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9fc
	ctx.lr = 0x825F23B0;
	sub_8239B9FC(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r26,292(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// mr r22,r5
	ctx.r22.u64 = ctx.r5.u64;
	// mr r23,r7
	ctx.r23.u64 = ctx.r7.u64;
	// mr r21,r8
	ctx.r21.u64 = ctx.r8.u64;
	// mr r28,r9
	ctx.r28.u64 = ctx.r9.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825f240c
	if (!ctx.cr6.gt) goto loc_825F240C;
	// lwz r27,276(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// subf r29,r3,r6
	ctx.r29.s64 = ctx.r6.s64 - ctx.r3.s64;
	// mr r30,r10
	ctx.r30.u64 = ctx.r10.u64;
loc_825F23E4:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// add r3,r29,r31
	ctx.r3.u64 = ctx.r29.u64 + ctx.r31.u64;
	// bl 0x825f2078
	ctx.lr = 0x825F23FC;
	sub_825F2078(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r31,r31,r27
	ctx.r31.u64 = ctx.r31.u64 + ctx.r27.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f23e4
	if (!ctx.cr6.eq) goto loc_825F23E4;
loc_825F240C:
	// lwz r30,268(r1)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mr r31,r24
	ctx.r31.u64 = ctx.r24.u64;
	// lwz r27,284(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// lwz r25,260(r1)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// ble cr6,0x825f2454
	if (!ctx.cr6.gt) goto loc_825F2454;
	// subf r28,r24,r23
	ctx.r28.s64 = ctx.r23.s64 - ctx.r24.s64;
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
loc_825F242C:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// add r3,r28,r31
	ctx.r3.u64 = ctx.r28.u64 + ctx.r31.u64;
	// bl 0x825f2078
	ctx.lr = 0x825F2444;
	sub_825F2078(ctx, base);
	// addi r29,r29,-1
	ctx.r29.s64 = ctx.r29.s64 + -1;
	// add r31,r31,r27
	ctx.r31.u64 = ctx.r31.u64 + ctx.r27.u64;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825f242c
	if (!ctx.cr6.eq) goto loc_825F242C;
loc_825F2454:
	// mr r31,r22
	ctx.r31.u64 = ctx.r22.u64;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// ble cr6,0x825f248c
	if (!ctx.cr6.gt) goto loc_825F248C;
	// subf r29,r22,r21
	ctx.r29.s64 = ctx.r21.s64 - ctx.r22.s64;
loc_825F2464:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// add r3,r29,r31
	ctx.r3.u64 = ctx.r29.u64 + ctx.r31.u64;
	// bl 0x825f2078
	ctx.lr = 0x825F247C;
	sub_825F2078(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r31,r31,r27
	ctx.r31.u64 = ctx.r31.u64 + ctx.r27.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f2464
	if (!ctx.cr6.eq) goto loc_825F2464;
loc_825F248C:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba4c
	// ERROR 8239BA4C
	return;
}

__attribute__((alias("__imp__sub_825F2494"))) PPC_WEAK_FUNC(sub_825F2494);
PPC_FUNC_IMPL(__imp__sub_825F2494) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F2498"))) PPC_WEAK_FUNC(sub_825F2498);
PPC_FUNC_IMPL(__imp__sub_825F2498) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825F24A0;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// lwz r3,260(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// mr r25,r8
	ctx.r25.u64 = ctx.r8.u64;
	// mr r4,r9
	ctx.r4.u64 = ctx.r9.u64;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825f2538
	if (!ctx.cr6.gt) goto loc_825F2538;
	// lwz r29,244(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// subf r6,r31,r6
	ctx.r6.s64 = ctx.r6.s64 - ctx.r31.s64;
	// mr r30,r10
	ctx.r30.u64 = ctx.r10.u64;
loc_825F24D0:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f24f8
	if (!ctx.cr6.gt) goto loc_825F24F8;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_825F24E0:
	// lbzx r9,r11,r31
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r31.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x825f24e0
	if (ctx.cr6.lt) goto loc_825F24E0;
loc_825F24F8:
	// bl 0x825f2270
	ctx.lr = 0x825F24FC;
	sub_825F2270(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f2528
	if (!ctx.cr6.gt) goto loc_825F2528;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_825F250C:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r9,r6,r11
	ctx.r9.u64 = ctx.r6.u64 + ctx.r11.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// stbx r8,r9,r31
	PPC_STORE_U8(ctx.r9.u32 + ctx.r31.u32, ctx.r8.u8);
	// blt cr6,0x825f250c
	if (ctx.cr6.lt) goto loc_825F250C;
loc_825F2528:
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r31,r31,r29
	ctx.r31.u64 = ctx.r31.u64 + ctx.r29.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f24d0
	if (!ctx.cr6.eq) goto loc_825F24D0;
loc_825F2538:
	// lwz r29,236(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// mr r6,r28
	ctx.r6.u64 = ctx.r28.u64;
	// lwz r26,252(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,228(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// ble cr6,0x825f25c0
	if (!ctx.cr6.gt) goto loc_825F25C0;
	// subf r31,r28,r27
	ctx.r31.s64 = ctx.r27.s64 - ctx.r28.s64;
	// mr r30,r29
	ctx.r30.u64 = ctx.r29.u64;
loc_825F2558:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f2580
	if (!ctx.cr6.gt) goto loc_825F2580;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_825F2568:
	// lbzx r9,r11,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r6.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x825f2568
	if (ctx.cr6.lt) goto loc_825F2568;
loc_825F2580:
	// bl 0x825f2270
	ctx.lr = 0x825F2584;
	sub_825F2270(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f25b0
	if (!ctx.cr6.gt) goto loc_825F25B0;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_825F2594:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r9,r31,r11
	ctx.r9.u64 = ctx.r31.u64 + ctx.r11.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// stbx r8,r9,r6
	PPC_STORE_U8(ctx.r9.u32 + ctx.r6.u32, ctx.r8.u8);
	// blt cr6,0x825f2594
	if (ctx.cr6.lt) goto loc_825F2594;
loc_825F25B0:
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 + ctx.r26.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f2558
	if (!ctx.cr6.eq) goto loc_825F2558;
loc_825F25C0:
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// ble cr6,0x825f263c
	if (!ctx.cr6.gt) goto loc_825F263C;
	// subf r5,r5,r25
	ctx.r5.s64 = ctx.r25.s64 - ctx.r5.s64;
	// mr r31,r29
	ctx.r31.u64 = ctx.r29.u64;
loc_825F25D4:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f25fc
	if (!ctx.cr6.gt) goto loc_825F25FC;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_825F25E4:
	// lbzx r9,r11,r6
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r6.u32);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// blt cr6,0x825f25e4
	if (ctx.cr6.lt) goto loc_825F25E4;
loc_825F25FC:
	// bl 0x825f2270
	ctx.lr = 0x825F2600;
	sub_825F2270(ctx, base);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f262c
	if (!ctx.cr6.gt) goto loc_825F262C;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_825F2610:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// add r9,r5,r11
	ctx.r9.u64 = ctx.r5.u64 + ctx.r11.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmpw cr6,r11,r4
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r4.s32, ctx.xer);
	// stbx r8,r9,r6
	PPC_STORE_U8(ctx.r9.u32 + ctx.r6.u32, ctx.r8.u8);
	// blt cr6,0x825f2610
	if (ctx.cr6.lt) goto loc_825F2610;
loc_825F262C:
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// add r6,r6,r26
	ctx.r6.u64 = ctx.r6.u64 + ctx.r26.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825f25d4
	if (!ctx.cr6.eq) goto loc_825F25D4;
loc_825F263C:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825F2644"))) PPC_WEAK_FUNC(sub_825F2644);
PPC_FUNC_IMPL(__imp__sub_825F2644) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F2648"))) PPC_WEAK_FUNC(sub_825F2648);
PPC_FUNC_IMPL(__imp__sub_825F2648) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba04
	ctx.lr = 0x825F2650;
	sub_8239BA04(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r25,276(r1)
	ctx.r25.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// mr r23,r5
	ctx.r23.u64 = ctx.r5.u64;
	// mr r30,r7
	ctx.r30.u64 = ctx.r7.u64;
	// mr r31,r8
	ctx.r31.u64 = ctx.r8.u64;
	// mr r26,r10
	ctx.r26.u64 = ctx.r10.u64;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x825f26a8
	if (!ctx.cr6.gt) goto loc_825F26A8;
	// lwz r7,260(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r29,r6
	ctx.r29.u64 = ctx.r6.u64;
	// subf r27,r6,r3
	ctx.r27.s64 = ctx.r3.s64 - ctx.r6.s64;
	// mr r28,r9
	ctx.r28.u64 = ctx.r9.u64;
loc_825F2684:
	// mr r6,r26
	ctx.r6.u64 = ctx.r26.u64;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// add r4,r27,r29
	ctx.r4.u64 = ctx.r27.u64 + ctx.r29.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x825f2078
	ctx.lr = 0x825F2698;
	sub_825F2078(ctx, base);
	// addi r28,r28,-1
	ctx.r28.s64 = ctx.r28.s64 + -1;
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// bne cr6,0x825f2684
	if (!ctx.cr6.eq) goto loc_825F2684;
loc_825F26A8:
	// lwz r26,244(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r7,268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// lwz r27,252(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// ble cr6,0x825f271c
	if (!ctx.cr6.gt) goto loc_825F271C;
	// subf r28,r30,r24
	ctx.r28.s64 = ctx.r24.s64 - ctx.r30.s64;
	// mr r29,r26
	ctx.r29.u64 = ctx.r26.u64;
loc_825F26C4:
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// add r4,r28,r30
	ctx.r4.u64 = ctx.r28.u64 + ctx.r30.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825f2078
	ctx.lr = 0x825F26D8;
	sub_825F2078(ctx, base);
	// addi r29,r29,-1
	ctx.r29.s64 = ctx.r29.s64 + -1;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825f26c4
	if (!ctx.cr6.eq) goto loc_825F26C4;
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// ble cr6,0x825f271c
	if (!ctx.cr6.gt) goto loc_825F271C;
	// subf r29,r31,r23
	ctx.r29.s64 = ctx.r23.s64 - ctx.r31.s64;
	// mr r30,r26
	ctx.r30.u64 = ctx.r26.u64;
loc_825F26F8:
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// add r4,r29,r31
	ctx.r4.u64 = ctx.r29.u64 + ctx.r31.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f2078
	ctx.lr = 0x825F270C;
	sub_825F2078(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r31,r31,1
	ctx.r31.s64 = ctx.r31.s64 + 1;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f26f8
	if (!ctx.cr6.eq) goto loc_825F26F8;
loc_825F271C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba54
	// ERROR 8239BA54
	return;
}

__attribute__((alias("__imp__sub_825F2724"))) PPC_WEAK_FUNC(sub_825F2724);
PPC_FUNC_IMPL(__imp__sub_825F2724) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F2728"))) PPC_WEAK_FUNC(sub_825F2728);
PPC_FUNC_IMPL(__imp__sub_825F2728) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825F2730;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// lwz r3,260(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r26,r4
	ctx.r26.u64 = ctx.r4.u64;
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// mr r25,r8
	ctx.r25.u64 = ctx.r8.u64;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x825f27d4
	if (!ctx.cr6.gt) goto loc_825F27D4;
	// lwz r31,244(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// subf r30,r6,r11
	ctx.r30.s64 = ctx.r11.s64 - ctx.r6.s64;
	// mr r29,r9
	ctx.r29.u64 = ctx.r9.u64;
loc_825F2760:
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f2790
	if (!ctx.cr6.gt) goto loc_825F2790;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// add r10,r30,r6
	ctx.r10.u64 = ctx.r30.u64 + ctx.r6.u64;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
loc_825F2774:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825f2774
	if (!ctx.cr6.eq) goto loc_825F2774;
loc_825F2790:
	// bl 0x825f2270
	ctx.lr = 0x825F2794;
	sub_825F2270(ctx, base);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f27c4
	if (!ctx.cr6.gt) goto loc_825F27C4;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
loc_825F27A8:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + ctx.r31.u64;
	// bne cr6,0x825f27a8
	if (!ctx.cr6.eq) goto loc_825F27A8;
loc_825F27C4:
	// addi r29,r29,-1
	ctx.r29.s64 = ctx.r29.s64 + -1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825f2760
	if (!ctx.cr6.eq) goto loc_825F2760;
loc_825F27D4:
	// lwz r28,228(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// lwz r31,252(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// lwz r4,236(r1)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x825f28f0
	if (!ctx.cr6.gt) goto loc_825F28F0;
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// subf r30,r27,r26
	ctx.r30.s64 = ctx.r26.s64 - ctx.r27.s64;
	// mr r29,r28
	ctx.r29.u64 = ctx.r28.u64;
loc_825F27F4:
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f2824
	if (!ctx.cr6.gt) goto loc_825F2824;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// add r10,r30,r6
	ctx.r10.u64 = ctx.r30.u64 + ctx.r6.u64;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
loc_825F2808:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825f2808
	if (!ctx.cr6.eq) goto loc_825F2808;
loc_825F2824:
	// bl 0x825f2270
	ctx.lr = 0x825F2828;
	sub_825F2270(ctx, base);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f2858
	if (!ctx.cr6.gt) goto loc_825F2858;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
loc_825F283C:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + ctx.r31.u64;
	// bne cr6,0x825f283c
	if (!ctx.cr6.eq) goto loc_825F283C;
loc_825F2858:
	// addi r29,r29,-1
	ctx.r29.s64 = ctx.r29.s64 + -1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825f27f4
	if (!ctx.cr6.eq) goto loc_825F27F4;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x825f28f0
	if (!ctx.cr6.gt) goto loc_825F28F0;
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// subf r5,r25,r5
	ctx.r5.s64 = ctx.r5.s64 - ctx.r25.s64;
	// mr r30,r28
	ctx.r30.u64 = ctx.r28.u64;
loc_825F287C:
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f28ac
	if (!ctx.cr6.gt) goto loc_825F28AC;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// add r10,r5,r6
	ctx.r10.u64 = ctx.r5.u64 + ctx.r6.u64;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
loc_825F2890:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// add r10,r10,r31
	ctx.r10.u64 = ctx.r10.u64 + ctx.r31.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// bne cr6,0x825f2890
	if (!ctx.cr6.eq) goto loc_825F2890;
loc_825F28AC:
	// bl 0x825f2270
	ctx.lr = 0x825F28B0;
	sub_825F2270(ctx, base);
	// cmpwi cr6,r4,0
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 0, ctx.xer);
	// ble cr6,0x825f28e0
	if (!ctx.cr6.gt) goto loc_825F28E0;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// mr r9,r6
	ctx.r9.u64 = ctx.r6.u64;
	// mr r11,r4
	ctx.r11.u64 = ctx.r4.u64;
loc_825F28C4:
	// lwz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stb r8,0(r9)
	PPC_STORE_U8(ctx.r9.u32 + 0, ctx.r8.u8);
	// add r9,r9,r31
	ctx.r9.u64 = ctx.r9.u64 + ctx.r31.u64;
	// bne cr6,0x825f28c4
	if (!ctx.cr6.eq) goto loc_825F28C4;
loc_825F28E0:
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f287c
	if (!ctx.cr6.eq) goto loc_825F287C;
loc_825F28F0:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825F28F8"))) PPC_WEAK_FUNC(sub_825F28F8);
PPC_FUNC_IMPL(__imp__sub_825F28F8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825F2900;
	sub_8239BA08(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// lis r10,-32244
	ctx.r10.s64 = -2113142784;
	// mr r25,r9
	ctx.r25.u64 = ctx.r9.u64;
	// addi r10,r10,-22816
	ctx.r10.s64 = ctx.r10.s64 + -22816;
	// lwz r11,14828(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14828);
	// mr r26,r8
	ctx.r26.u64 = ctx.r8.u64;
	// lwz r7,14824(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 14824);
	// addi r30,r10,24
	ctx.r30.s64 = ctx.r10.s64 + 24;
	// mulli r9,r11,84
	ctx.r9.s64 = ctx.r11.s64 * 84;
	// lwz r29,188(r31)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// add r7,r7,r11
	ctx.r7.u64 = ctx.r7.u64 + ctx.r11.u64;
	// add r11,r9,r31
	ctx.r11.u64 = ctx.r9.u64 + ctx.r31.u64;
	// rlwinm r9,r7,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// mr r8,r27
	ctx.r8.u64 = ctx.r27.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// lwz r7,14884(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14884);
	// lwz r28,14888(r11)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r11.u32 + 14888);
	// lwzx r29,r9,r10
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r10.u32);
	// lwzx r30,r9,r30
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r9.u32 + ctx.r30.u32);
	// ble cr6,0x825f29ac
	if (!ctx.cr6.gt) goto loc_825F29AC;
	// lwz r9,180(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
loc_825F2960:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x825f298c
	if (!ctx.cr6.gt) goto loc_825F298C;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
loc_825F2970:
	// lbz r9,0(r10)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stbx r9,r11,r8
	PPC_STORE_U8(ctx.r11.u32 + ctx.r8.u32, ctx.r9.u8);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// lwz r9,180(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x825f2970
	if (ctx.cr6.lt) goto loc_825F2970;
loc_825F298C:
	// lwz r11,204(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// add r3,r3,r29
	ctx.r3.u64 = ctx.r3.u64 + ctx.r29.u64;
	// lwz r10,188(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// add r4,r4,r7
	ctx.r4.u64 = ctx.r4.u64 + ctx.r7.u64;
	// mullw r11,r11,r29
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r29.s32);
	// add r8,r11,r8
	ctx.r8.u64 = ctx.r11.u64 + ctx.r8.u64;
	// cmpw cr6,r3,r10
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825f2960
	if (ctx.cr6.lt) goto loc_825F2960;
loc_825F29AC:
	// lwz r11,200(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// mr r9,r26
	ctx.r9.u64 = ctx.r26.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825f2a10
	if (!ctx.cr6.gt) goto loc_825F2A10;
	// lwz r8,192(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
loc_825F29C4:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// ble cr6,0x825f29f0
	if (!ctx.cr6.gt) goto loc_825F29F0;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
loc_825F29D4:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stbx r8,r11,r9
	PPC_STORE_U8(ctx.r11.u32 + ctx.r9.u32, ctx.r8.u8);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// lwz r8,192(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x825f29d4
	if (ctx.cr6.lt) goto loc_825F29D4;
loc_825F29F0:
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + ctx.r29.u64;
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// add r5,r5,r28
	ctx.r5.u64 = ctx.r5.u64 + ctx.r28.u64;
	// mullw r11,r11,r29
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r29.s32);
	// add r9,r11,r9
	ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
	// cmpw cr6,r7,r10
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825f29c4
	if (ctx.cr6.lt) goto loc_825F29C4;
loc_825F2A10:
	// lwz r11,200(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// mr r9,r25
	ctx.r9.u64 = ctx.r25.u64;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// ble cr6,0x825f2a74
	if (!ctx.cr6.gt) goto loc_825F2A74;
	// lwz r8,192(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
loc_825F2A28:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// ble cr6,0x825f2a54
	if (!ctx.cr6.gt) goto loc_825F2A54;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
loc_825F2A38:
	// lbz r8,0(r10)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
	// stbx r8,r11,r9
	PPC_STORE_U8(ctx.r11.u32 + ctx.r9.u32, ctx.r8.u8);
	// add r11,r11,r30
	ctx.r11.u64 = ctx.r11.u64 + ctx.r30.u64;
	// lwz r8,192(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x825f2a38
	if (ctx.cr6.lt) goto loc_825F2A38;
loc_825F2A54:
	// lwz r11,208(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// add r7,r7,r29
	ctx.r7.u64 = ctx.r7.u64 + ctx.r29.u64;
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// add r6,r6,r28
	ctx.r6.u64 = ctx.r6.u64 + ctx.r28.u64;
	// mullw r11,r11,r29
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r29.s32);
	// add r9,r11,r9
	ctx.r9.u64 = ctx.r11.u64 + ctx.r9.u64;
	// cmpw cr6,r7,r10
	ctx.cr6.compare<int32_t>(ctx.r7.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825f2a28
	if (ctx.cr6.lt) goto loc_825F2A28;
loc_825F2A74:
	// cmpwi cr6,r30,2
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 2, ctx.xer);
	// bne cr6,0x825f2ac0
	if (!ctx.cr6.eq) goto loc_825F2AC0;
	// lwz r11,15188(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15188);
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// lwz r30,208(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// lwz r28,204(r31)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// lwz r10,200(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// lwz r9,192(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// lwz r8,188(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// lwz r7,180(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// lwz r24,15820(r31)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15820);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r11.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r30.u32);
	// stw r28,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r28.u32);
	// mtctr r24
	ctx.ctr.u64 = ctx.r24.u64;
	// bctrl 
	ctx.lr = 0x825F2AC0;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_825F2AC0:
	// cmpwi cr6,r29,2
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 2, ctx.xer);
	// bne cr6,0x825f2b04
	if (!ctx.cr6.eq) goto loc_825F2B04;
	// lwz r11,15188(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15188);
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// lwz r30,208(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 208);
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// lwz r10,204(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 204);
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// lwz r9,200(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 200);
	// lwz r8,192(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 192);
	// lwz r7,188(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 188);
	// lwz r6,180(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 180);
	// lwz r31,15824(r31)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r31.u32 + 15824);
	// stw r11,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r11.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r30.u32);
	// mtctr r31
	ctx.ctr.u64 = ctx.r31.u64;
	// bctrl 
	ctx.lr = 0x825F2B04;
	PPC_CALL_INDIRECT_FUNC(ctx.ctr.u32);
loc_825F2B04:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825F2B0C"))) PPC_WEAK_FUNC(sub_825F2B0C);
PPC_FUNC_IMPL(__imp__sub_825F2B0C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F2B10"))) PPC_WEAK_FUNC(sub_825F2B10);
PPC_FUNC_IMPL(__imp__sub_825F2B10) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba18
	ctx.lr = 0x825F2B18;
	sub_8239BA18(ctx, base);
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// addi r4,r6,4
	ctx.r4.s64 = ctx.r6.s64 + 4;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// ble cr6,0x825f2b60
	if (!ctx.cr6.gt) goto loc_825F2B60;
	// addi r11,r5,-1
	ctx.r11.s64 = ctx.r5.s64 + -1;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
loc_825F2B3C:
	// lbz r6,0(r9)
	ctx.r6.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// add r9,r8,r9
	ctx.r9.u64 = ctx.r8.u64 + ctx.r9.u64;
	// mulli r6,r6,315
	ctx.r6.s64 = ctx.r6.s64 * 315;
	// srawi r6,r6,4
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0xF) != 0);
	ctx.r6.s64 = ctx.r6.s32 >> 4;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r6,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r6.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x825f2b3c
	if (!ctx.cr6.eq) goto loc_825F2B3C;
loc_825F2B60:
	// addi r11,r5,-2
	ctx.r11.s64 = ctx.r5.s64 + -2;
	// rlwinm r29,r5,2,0,29
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r5.u32 | (ctx.r5.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r30,r11,2,0,29
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r31,r5,1
	ctx.r31.s64 = ctx.r5.s64 + 1;
	// cmpwi cr6,r31,1
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 1, ctx.xer);
	// lwzx r11,r30,r4
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r4.u32);
	// stwx r11,r29,r4
	PPC_STORE_U32(ctx.r29.u32 + ctx.r4.u32, ctx.r11.u32);
	// ble cr6,0x825f2bb8
	if (!ctx.cr6.gt) goto loc_825F2BB8;
	// addi r10,r31,-2
	ctx.r10.s64 = ctx.r31.s64 + -2;
	// addi r11,r4,8
	ctx.r11.s64 = ctx.r4.s64 + 8;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825F2B90:
	// lwz r9,-8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// mulli r9,r9,226
	ctx.r9.s64 = ctx.r9.s64 * 226;
	// srawi r9,r9,8
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// stw r9,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r9.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f2b90
	if (!ctx.cr6.eq) goto loc_825F2B90;
loc_825F2BB8:
	// addi r11,r4,4
	ctx.r11.s64 = ctx.r4.s64 + 4;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// lwz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// stw r10,-4(r4)
	PPC_STORE_U32(ctx.r4.u32 + -4, ctx.r10.u32);
	// ble cr6,0x825f2c0c
	if (!ctx.cr6.gt) goto loc_825F2C0C;
	// addi r9,r5,-1
	ctx.r9.s64 = ctx.r5.s64 + -1;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// rlwinm r9,r9,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
loc_825F2BDC:
	// lwz r8,-4(r10)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r10.u32 + -4);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lwz r6,4(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 4);
	// lwz r28,0(r10)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// add r8,r8,r6
	ctx.r8.u64 = ctx.r8.u64 + ctx.r6.u64;
	// mulli r8,r8,217
	ctx.r8.s64 = ctx.r8.s64 * 217;
	// srawi r8,r8,12
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0xFFF) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 12;
	// subf r8,r8,r28
	ctx.r8.s64 = ctx.r28.s64 - ctx.r8.s64;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x825f2bdc
	if (!ctx.cr6.eq) goto loc_825F2BDC;
loc_825F2C0C:
	// lwzx r10,r30,r4
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r30.u32 + ctx.r4.u32);
	// cmpwi cr6,r31,1
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 1, ctx.xer);
	// stwx r10,r29,r4
	PPC_STORE_U32(ctx.r29.u32 + ctx.r4.u32, ctx.r10.u32);
	// ble cr6,0x825f2c58
	if (!ctx.cr6.gt) goto loc_825F2C58;
	// addi r10,r31,-2
	ctx.r10.s64 = ctx.r31.s64 + -2;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825F2C28:
	// lwz r9,-4(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + -4);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// mulli r9,r9,406
	ctx.r9.s64 = ctx.r9.s64 * 406;
	// srawi r9,r9,8
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0xFF) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 8;
	// subf r9,r6,r9
	ctx.r9.s64 = ctx.r9.s64 - ctx.r6.s64;
	// stw r9,0(r11)
	PPC_STORE_U32(ctx.r11.u32 + 0, ctx.r9.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f2c28
	if (!ctx.cr6.eq) goto loc_825F2C28;
loc_825F2C58:
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// ble cr6,0x825f2ca4
	if (!ctx.cr6.gt) goto loc_825F2CA4;
	// mr r9,r4
	ctx.r9.u64 = ctx.r4.u64;
	// mr r10,r5
	ctx.r10.u64 = ctx.r5.u64;
loc_825F2C6C:
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// srawi r11,r11,4
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0xF) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 4;
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// ble cr6,0x825f2c8c
	if (!ctx.cr6.gt) goto loc_825F2C8C;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r11,255
	ctx.r11.s64 = 255;
	// bgt cr6,0x825f2c8c
	if (ctx.cr6.gt) goto loc_825F2C8C;
	// li r11,0
	ctx.r11.s64 = 0;
loc_825F2C8C:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stb r11,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r11.u8);
	// addi r9,r9,4
	ctx.r9.s64 = ctx.r9.s64 + 4;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825f2c6c
	if (!ctx.cr6.eq) goto loc_825F2C6C;
loc_825F2CA4:
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
}

__attribute__((alias("__imp__sub_825F2CA8"))) PPC_WEAK_FUNC(sub_825F2CA8);
PPC_FUNC_IMPL(__imp__sub_825F2CA8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba04
	ctx.lr = 0x825F2CB0;
	sub_8239BA04(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r8
	ctx.r29.u64 = ctx.r8.u64;
	// lwz r27,260(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r23,r5
	ctx.r23.u64 = ctx.r5.u64;
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// mr r30,r6
	ctx.r30.u64 = ctx.r6.u64;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// mr r25,r9
	ctx.r25.u64 = ctx.r9.u64;
	// mr r28,r10
	ctx.r28.u64 = ctx.r10.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// ble cr6,0x825f2d08
	if (!ctx.cr6.gt) goto loc_825F2D08;
	// lwz r11,244(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// mullw r26,r30,r11
	ctx.r26.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r11.s32);
loc_825F2CE8:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x825f2b10
	ctx.lr = 0x825F2CF8;
	sub_825F2B10(ctx, base);
	// add r31,r31,r30
	ctx.r31.u64 = ctx.r31.u64 + ctx.r30.u64;
	// add r3,r26,r3
	ctx.r3.u64 = ctx.r26.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r29
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r29.s32, ctx.xer);
	// blt cr6,0x825f2ce8
	if (ctx.cr6.lt) goto loc_825F2CE8;
loc_825F2D08:
	// lwz r26,252(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x825f2d44
	if (!ctx.cr6.gt) goto loc_825F2D44;
	// mullw r29,r30,r26
	ctx.r29.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r26.s32);
loc_825F2D20:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x825f2b10
	ctx.lr = 0x825F2D34;
	sub_825F2B10(ctx, base);
	// add r31,r31,r30
	ctx.r31.u64 = ctx.r31.u64 + ctx.r30.u64;
	// add r3,r29,r3
	ctx.r3.u64 = ctx.r29.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r28
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r28.s32, ctx.xer);
	// blt cr6,0x825f2d20
	if (ctx.cr6.lt) goto loc_825F2D20;
loc_825F2D44:
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x825f2d7c
	if (!ctx.cr6.gt) goto loc_825F2D7C;
	// mullw r29,r30,r26
	ctx.r29.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r26.s32);
loc_825F2D58:
	// li r7,1
	ctx.r7.s64 = 1;
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x825f2b10
	ctx.lr = 0x825F2D6C;
	sub_825F2B10(ctx, base);
	// add r31,r31,r30
	ctx.r31.u64 = ctx.r31.u64 + ctx.r30.u64;
	// add r3,r29,r3
	ctx.r3.u64 = ctx.r29.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r28
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r28.s32, ctx.xer);
	// blt cr6,0x825f2d58
	if (ctx.cr6.lt) goto loc_825F2D58;
loc_825F2D7C:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba54
	// ERROR 8239BA54
	return;
}

__attribute__((alias("__imp__sub_825F2D84"))) PPC_WEAK_FUNC(sub_825F2D84);
PPC_FUNC_IMPL(__imp__sub_825F2D84) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F2D88"))) PPC_WEAK_FUNC(sub_825F2D88);
PPC_FUNC_IMPL(__imp__sub_825F2D88) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba04
	ctx.lr = 0x825F2D90;
	sub_8239BA04(ctx, base);
	// lwz r23,100(r1)
	ctx.r23.u64 = PPC_LOAD_U32(ctx.r1.u32 + 100);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// li r28,0
	ctx.r28.s64 = 0;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// ble cr6,0x825f2e64
	if (!ctx.cr6.gt) goto loc_825F2E64;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r3,r7,-2
	ctx.r3.s64 = ctx.r7.s64 + -2;
	// add r27,r11,r23
	ctx.r27.u64 = ctx.r11.u64 + ctx.r23.u64;
	// lwz r11,84(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// rlwinm r25,r3,2,0,29
	ctx.r25.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r26,r7,-1
	ctx.r26.s64 = ctx.r7.s64 + -1;
	// mullw r24,r6,r11
	ctx.r24.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r11.s32);
loc_825F2DC0:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// ble cr6,0x825f2de8
	if (!ctx.cr6.gt) goto loc_825F2DE8;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
loc_825F2DD0:
	// lbzx r31,r11,r29
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r29.u32);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmpw cr6,r11,r7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r7.s32, ctx.xer);
	// stw r31,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r31.u32);
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// blt cr6,0x825f2dd0
	if (ctx.cr6.lt) goto loc_825F2DD0;
loc_825F2DE8:
	// cmpwi cr6,r26,1
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 1, ctx.xer);
	// ble cr6,0x825f2e24
	if (!ctx.cr6.gt) goto loc_825F2E24;
	// addi r3,r26,-2
	ctx.r3.s64 = ctx.r26.s64 + -2;
	// addi r11,r23,8
	ctx.r11.s64 = ctx.r23.s64 + 8;
	// rlwinm r3,r3,31,1,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r3.u32 | (ctx.r3.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
loc_825F2E00:
	// lwz r31,-8(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	// addi r3,r3,-1
	ctx.r3.s64 = ctx.r3.s64 + -1;
	// lwz r30,0(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// add r31,r31,r30
	ctx.r31.u64 = ctx.r31.u64 + ctx.r30.u64;
	// srawi r31,r31,1
	ctx.xer.ca = (ctx.r31.s32 < 0) & ((ctx.r31.u32 & 0x1) != 0);
	ctx.r31.s64 = ctx.r31.s32 >> 1;
	// stw r31,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r31.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f2e00
	if (!ctx.cr6.eq) goto loc_825F2E00;
loc_825F2E24:
	// lwzx r3,r25,r23
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r23.u32);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// stw r3,-4(r27)
	PPC_STORE_U32(ctx.r27.u32 + -4, ctx.r3.u32);
	// ble cr6,0x825f2e54
	if (!ctx.cr6.gt) goto loc_825F2E54;
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
loc_825F2E3C:
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// addi r3,r3,4
	ctx.r3.s64 = ctx.r3.s64 + 4;
	// stbx r31,r11,r29
	PPC_STORE_U8(ctx.r11.u32 + ctx.r29.u32, ctx.r31.u8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpw cr6,r11,r7
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r7.s32, ctx.xer);
	// blt cr6,0x825f2e3c
	if (ctx.cr6.lt) goto loc_825F2E3C;
loc_825F2E54:
	// add r28,r28,r6
	ctx.r28.u64 = ctx.r28.u64 + ctx.r6.u64;
	// add r29,r24,r29
	ctx.r29.u64 = ctx.r24.u64 + ctx.r29.u64;
	// cmpw cr6,r28,r8
	ctx.cr6.compare<int32_t>(ctx.r28.s32, ctx.r8.s32, ctx.xer);
	// blt cr6,0x825f2dc0
	if (ctx.cr6.lt) goto loc_825F2DC0;
loc_825F2E64:
	// lwz r26,92(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825f2f34
	if (!ctx.cr6.gt) goto loc_825F2F34;
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r8,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r11,r23
	ctx.r30.u64 = ctx.r11.u64 + ctx.r23.u64;
	// addi r29,r9,-1
	ctx.r29.s64 = ctx.r9.s64 + -1;
	// mullw r27,r6,r26
	ctx.r27.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r26.s32);
loc_825F2E90:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x825f2eb8
	if (!ctx.cr6.gt) goto loc_825F2EB8;
	// mr r8,r23
	ctx.r8.u64 = ctx.r23.u64;
loc_825F2EA0:
	// lbzx r7,r11,r3
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r3.u32);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// blt cr6,0x825f2ea0
	if (ctx.cr6.lt) goto loc_825F2EA0;
loc_825F2EB8:
	// cmpwi cr6,r29,1
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 1, ctx.xer);
	// ble cr6,0x825f2ef4
	if (!ctx.cr6.gt) goto loc_825F2EF4;
	// addi r8,r29,-2
	ctx.r8.s64 = ctx.r29.s64 + -2;
	// addi r11,r23,8
	ctx.r11.s64 = ctx.r23.s64 + 8;
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
loc_825F2ED0:
	// lwz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lwz r4,0(r11)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// add r7,r7,r4
	ctx.r7.u64 = ctx.r7.u64 + ctx.r4.u64;
	// srawi r7,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// stw r7,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r7.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f2ed0
	if (!ctx.cr6.eq) goto loc_825F2ED0;
loc_825F2EF4:
	// lwzx r8,r28,r23
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r23.u32);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stw r8,-4(r30)
	PPC_STORE_U32(ctx.r30.u32 + -4, ctx.r8.u32);
	// ble cr6,0x825f2f24
	if (!ctx.cr6.gt) goto loc_825F2F24;
	// mr r8,r23
	ctx.r8.u64 = ctx.r23.u64;
loc_825F2F0C:
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// stbx r7,r11,r3
	PPC_STORE_U8(ctx.r11.u32 + ctx.r3.u32, ctx.r7.u8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x825f2f0c
	if (ctx.cr6.lt) goto loc_825F2F0C;
loc_825F2F24:
	// add r31,r31,r6
	ctx.r31.u64 = ctx.r31.u64 + ctx.r6.u64;
	// add r3,r27,r3
	ctx.r3.u64 = ctx.r27.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r10
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825f2e90
	if (ctx.cr6.lt) goto loc_825F2E90;
loc_825F2F34:
	// mr r4,r5
	ctx.r4.u64 = ctx.r5.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825f3000
	if (!ctx.cr6.gt) goto loc_825F3000;
	// addi r8,r9,-2
	ctx.r8.s64 = ctx.r9.s64 + -2;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r29,r8,2,0,29
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r11,r23
	ctx.r31.u64 = ctx.r11.u64 + ctx.r23.u64;
	// addi r30,r9,-1
	ctx.r30.s64 = ctx.r9.s64 + -1;
	// mullw r28,r6,r26
	ctx.r28.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r26.s32);
loc_825F2F5C:
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x825f2f84
	if (!ctx.cr6.gt) goto loc_825F2F84;
	// mr r8,r23
	ctx.r8.u64 = ctx.r23.u64;
loc_825F2F6C:
	// lbzx r7,r11,r4
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r4.u32);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// stw r7,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r7.u32);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// blt cr6,0x825f2f6c
	if (ctx.cr6.lt) goto loc_825F2F6C;
loc_825F2F84:
	// cmpwi cr6,r30,1
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 1, ctx.xer);
	// ble cr6,0x825f2fc0
	if (!ctx.cr6.gt) goto loc_825F2FC0;
	// addi r8,r30,-2
	ctx.r8.s64 = ctx.r30.s64 + -2;
	// addi r11,r23,8
	ctx.r11.s64 = ctx.r23.s64 + 8;
	// rlwinm r8,r8,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
loc_825F2F9C:
	// lwz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lwz r5,0(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// srawi r7,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// stw r7,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r7.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f2f9c
	if (!ctx.cr6.eq) goto loc_825F2F9C;
loc_825F2FC0:
	// lwzx r8,r29,r23
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r29.u32 + ctx.r23.u32);
	// li r11,0
	ctx.r11.s64 = 0;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stw r8,-4(r31)
	PPC_STORE_U32(ctx.r31.u32 + -4, ctx.r8.u32);
	// ble cr6,0x825f2ff0
	if (!ctx.cr6.gt) goto loc_825F2FF0;
	// mr r8,r23
	ctx.r8.u64 = ctx.r23.u64;
loc_825F2FD8:
	// lwz r7,0(r8)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// addi r8,r8,4
	ctx.r8.s64 = ctx.r8.s64 + 4;
	// stbx r7,r11,r4
	PPC_STORE_U8(ctx.r11.u32 + ctx.r4.u32, ctx.r7.u8);
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
	// cmpw cr6,r11,r9
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r9.s32, ctx.xer);
	// blt cr6,0x825f2fd8
	if (ctx.cr6.lt) goto loc_825F2FD8;
loc_825F2FF0:
	// add r3,r3,r6
	ctx.r3.u64 = ctx.r3.u64 + ctx.r6.u64;
	// add r4,r28,r4
	ctx.r4.u64 = ctx.r28.u64 + ctx.r4.u64;
	// cmpw cr6,r3,r10
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r10.s32, ctx.xer);
	// blt cr6,0x825f2f5c
	if (ctx.cr6.lt) goto loc_825F2F5C;
loc_825F3000:
	// b 0x8239ba54
	// ERROR 8239BA54
	return;
}

__attribute__((alias("__imp__sub_825F3004"))) PPC_WEAK_FUNC(sub_825F3004);
PPC_FUNC_IMPL(__imp__sub_825F3004) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F3008"))) PPC_WEAK_FUNC(sub_825F3008);
PPC_FUNC_IMPL(__imp__sub_825F3008) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba10
	ctx.lr = 0x825F3010;
	sub_8239BA10(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r26,r5
	ctx.r26.u64 = ctx.r5.u64;
	// lwz r29,236(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// mr r27,r4
	ctx.r27.u64 = ctx.r4.u64;
	// mr r31,r8
	ctx.r31.u64 = ctx.r8.u64;
	// mr r28,r9
	ctx.r28.u64 = ctx.r9.u64;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// ble cr6,0x825f3058
	if (!ctx.cr6.gt) goto loc_825F3058;
	// mr r30,r6
	ctx.r30.u64 = ctx.r6.u64;
loc_825F303C:
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x825f2b10
	ctx.lr = 0x825F3048;
	sub_825F2B10(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f303c
	if (!ctx.cr6.eq) goto loc_825F303C;
loc_825F3058:
	// lwz r7,228(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// ble cr6,0x825f30b8
	if (!ctx.cr6.gt) goto loc_825F30B8;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// mr r30,r31
	ctx.r30.u64 = ctx.r31.u64;
loc_825F306C:
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x825f2b10
	ctx.lr = 0x825F307C;
	sub_825F2B10(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f306c
	if (!ctx.cr6.eq) goto loc_825F306C;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// ble cr6,0x825f30b8
	if (!ctx.cr6.gt) goto loc_825F30B8;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
loc_825F3098:
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x825f2b10
	ctx.lr = 0x825F30A8;
	sub_825F2B10(ctx, base);
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825f3098
	if (!ctx.cr6.eq) goto loc_825F3098;
loc_825F30B8:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
}

__attribute__((alias("__imp__sub_825F30C0"))) PPC_WEAK_FUNC(sub_825F30C0);
PPC_FUNC_IMPL(__imp__sub_825F30C0) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825F30C8;
	sub_8239BA08(ctx, base);
	// lwz r30,92(r1)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 92);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// ble cr6,0x825f31b0
	if (!ctx.cr6.gt) goto loc_825F31B0;
	// addi r31,r7,-2
	ctx.r31.s64 = ctx.r7.s64 + -2;
	// rlwinm r11,r7,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r25,r31,2,0,29
	ctx.r25.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r26,r11,r30
	ctx.r26.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addi r28,r7,-1
	ctx.r28.s64 = ctx.r7.s64 + -1;
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// mr r27,r6
	ctx.r27.u64 = ctx.r6.u64;
loc_825F30F0:
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// ble cr6,0x825f312c
	if (!ctx.cr6.gt) goto loc_825F312C;
	// addi r11,r7,-1
	ctx.r11.s64 = ctx.r7.s64 + -1;
	// rlwinm r31,r10,1,0,30
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
loc_825F3110:
	// lbz r24,0(r6)
	ctx.r24.u64 = PPC_LOAD_U8(ctx.r6.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// add r6,r31,r6
	ctx.r6.u64 = ctx.r31.u64 + ctx.r6.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r24,0(r3)
	PPC_STORE_U32(ctx.r3.u32 + 0, ctx.r24.u32);
	// addi r3,r3,8
	ctx.r3.s64 = ctx.r3.s64 + 8;
	// bne cr6,0x825f3110
	if (!ctx.cr6.eq) goto loc_825F3110;
loc_825F312C:
	// cmpwi cr6,r28,1
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 1, ctx.xer);
	// ble cr6,0x825f3168
	if (!ctx.cr6.gt) goto loc_825F3168;
	// addi r6,r28,-2
	ctx.r6.s64 = ctx.r28.s64 + -2;
	// addi r11,r30,8
	ctx.r11.s64 = ctx.r30.s64 + 8;
	// rlwinm r6,r6,31,1,31
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
loc_825F3144:
	// lwz r3,-8(r11)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// lwz r31,0(r11)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// add r3,r3,r31
	ctx.r3.u64 = ctx.r3.u64 + ctx.r31.u64;
	// srawi r3,r3,1
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// stw r3,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r3.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f3144
	if (!ctx.cr6.eq) goto loc_825F3144;
loc_825F3168:
	// lwzx r11,r25,r30
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r25.u32 + ctx.r30.u32);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// stw r11,-4(r26)
	PPC_STORE_U32(ctx.r26.u32 + -4, ctx.r11.u32);
	// ble cr6,0x825f31a0
	if (!ctx.cr6.gt) goto loc_825F31A0;
	// mr r6,r30
	ctx.r6.u64 = ctx.r30.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// mr r11,r7
	ctx.r11.u64 = ctx.r7.u64;
loc_825F3184:
	// lwz r31,0(r6)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r6.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// addi r6,r6,4
	ctx.r6.s64 = ctx.r6.s64 + 4;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stb r31,0(r3)
	PPC_STORE_U8(ctx.r3.u32 + 0, ctx.r31.u8);
	// add r3,r3,r10
	ctx.r3.u64 = ctx.r3.u64 + ctx.r10.u64;
	// bne cr6,0x825f3184
	if (!ctx.cr6.eq) goto loc_825F3184;
loc_825F31A0:
	// addi r27,r27,-1
	ctx.r27.s64 = ctx.r27.s64 + -1;
	// addi r29,r29,1
	ctx.r29.s64 = ctx.r29.s64 + 1;
	// cmplwi cr6,r27,0
	ctx.cr6.compare<uint32_t>(ctx.r27.u32, 0, ctx.xer);
	// bne cr6,0x825f30f0
	if (!ctx.cr6.eq) goto loc_825F30F0;
loc_825F31B0:
	// lwz r27,84(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 84);
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// ble cr6,0x825f3378
	if (!ctx.cr6.gt) goto loc_825F3378;
	// addi r10,r9,-2
	ctx.r10.s64 = ctx.r9.s64 + -2;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r28,r10,2,0,29
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r11,r30
	ctx.r29.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addi r3,r9,-1
	ctx.r3.s64 = ctx.r9.s64 + -1;
	// mr r31,r8
	ctx.r31.u64 = ctx.r8.u64;
loc_825F31D4:
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x825f3210
	if (!ctx.cr6.gt) goto loc_825F3210;
	// addi r11,r9,-1
	ctx.r11.s64 = ctx.r9.s64 + -1;
	// rlwinm r6,r27,1,0,30
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r7,r30
	ctx.r7.u64 = ctx.r30.u64;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
loc_825F31F4:
	// lbz r26,0(r10)
	ctx.r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// add r10,r6,r10
	ctx.r10.u64 = ctx.r6.u64 + ctx.r10.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r26,0(r7)
	PPC_STORE_U32(ctx.r7.u32 + 0, ctx.r26.u32);
	// addi r7,r7,8
	ctx.r7.s64 = ctx.r7.s64 + 8;
	// bne cr6,0x825f31f4
	if (!ctx.cr6.eq) goto loc_825F31F4;
loc_825F3210:
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// ble cr6,0x825f324c
	if (!ctx.cr6.gt) goto loc_825F324C;
	// addi r10,r3,-2
	ctx.r10.s64 = ctx.r3.s64 + -2;
	// addi r11,r30,8
	ctx.r11.s64 = ctx.r30.s64 + 8;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825F3228:
	// lwz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r6,0(r11)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// srawi r7,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// stw r7,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r7.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f3228
	if (!ctx.cr6.eq) goto loc_825F3228;
loc_825F324C:
	// lwzx r11,r28,r30
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r28.u32 + ctx.r30.u32);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stw r11,-4(r29)
	PPC_STORE_U32(ctx.r29.u32 + -4, ctx.r11.u32);
	// ble cr6,0x825f3284
	if (!ctx.cr6.gt) goto loc_825F3284;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// mr r7,r4
	ctx.r7.u64 = ctx.r4.u64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_825F3268:
	// lwz r6,0(r10)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stb r6,0(r7)
	PPC_STORE_U8(ctx.r7.u32 + 0, ctx.r6.u8);
	// add r7,r7,r27
	ctx.r7.u64 = ctx.r7.u64 + ctx.r27.u64;
	// bne cr6,0x825f3268
	if (!ctx.cr6.eq) goto loc_825F3268;
loc_825F3284:
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825f31d4
	if (!ctx.cr6.eq) goto loc_825F31D4;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// ble cr6,0x825f3378
	if (!ctx.cr6.gt) goto loc_825F3378;
	// addi r10,r9,-2
	ctx.r10.s64 = ctx.r9.s64 + -2;
	// rlwinm r11,r9,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mr r6,r5
	ctx.r6.u64 = ctx.r5.u64;
	// rlwinm r31,r10,2,0,29
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r3,r11,r30
	ctx.r3.u64 = ctx.r11.u64 + ctx.r30.u64;
	// addi r4,r9,-1
	ctx.r4.s64 = ctx.r9.s64 + -1;
	// mr r5,r8
	ctx.r5.u64 = ctx.r8.u64;
loc_825F32B8:
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x825f32f4
	if (!ctx.cr6.gt) goto loc_825F32F4;
	// addi r11,r9,-1
	ctx.r11.s64 = ctx.r9.s64 + -1;
	// rlwinm r7,r27,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r11,r11,31,1,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 31) & 0x7FFFFFFF;
	// mr r8,r30
	ctx.r8.u64 = ctx.r30.u64;
	// mr r10,r6
	ctx.r10.u64 = ctx.r6.u64;
	// addi r11,r11,1
	ctx.r11.s64 = ctx.r11.s64 + 1;
loc_825F32D8:
	// lbz r29,0(r10)
	ctx.r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stw r29,0(r8)
	PPC_STORE_U32(ctx.r8.u32 + 0, ctx.r29.u32);
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// bne cr6,0x825f32d8
	if (!ctx.cr6.eq) goto loc_825F32D8;
loc_825F32F4:
	// cmpwi cr6,r4,1
	ctx.cr6.compare<int32_t>(ctx.r4.s32, 1, ctx.xer);
	// ble cr6,0x825f3330
	if (!ctx.cr6.gt) goto loc_825F3330;
	// addi r10,r4,-2
	ctx.r10.s64 = ctx.r4.s64 + -2;
	// addi r11,r30,8
	ctx.r11.s64 = ctx.r30.s64 + 8;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825F330C:
	// lwz r8,-8(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + -8);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lwz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// srawi r8,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 1;
	// stw r8,-4(r11)
	PPC_STORE_U32(ctx.r11.u32 + -4, ctx.r8.u32);
	// addi r11,r11,8
	ctx.r11.s64 = ctx.r11.s64 + 8;
	// bne cr6,0x825f330c
	if (!ctx.cr6.eq) goto loc_825F330C;
loc_825F3330:
	// lwzx r11,r31,r30
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + ctx.r30.u32);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// stw r11,-4(r3)
	PPC_STORE_U32(ctx.r3.u32 + -4, ctx.r11.u32);
	// ble cr6,0x825f3368
	if (!ctx.cr6.gt) goto loc_825F3368;
	// mr r10,r30
	ctx.r10.u64 = ctx.r30.u64;
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// mr r11,r9
	ctx.r11.u64 = ctx.r9.u64;
loc_825F334C:
	// lwz r7,0(r10)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r10.u32 + 0);
	// addi r11,r11,-1
	ctx.r11.s64 = ctx.r11.s64 + -1;
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// stb r7,0(r8)
	PPC_STORE_U8(ctx.r8.u32 + 0, ctx.r7.u8);
	// add r8,r8,r27
	ctx.r8.u64 = ctx.r8.u64 + ctx.r27.u64;
	// bne cr6,0x825f334c
	if (!ctx.cr6.eq) goto loc_825F334C;
loc_825F3368:
	// addi r5,r5,-1
	ctx.r5.s64 = ctx.r5.s64 + -1;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// cmplwi cr6,r5,0
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, 0, ctx.xer);
	// bne cr6,0x825f32b8
	if (!ctx.cr6.eq) goto loc_825F32B8;
loc_825F3378:
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825F337C"))) PPC_WEAK_FUNC(sub_825F337C);
PPC_FUNC_IMPL(__imp__sub_825F337C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F3380"))) PPC_WEAK_FUNC(sub_825F3380);
PPC_FUNC_IMPL(__imp__sub_825F3380) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba04
	ctx.lr = 0x825F3388;
	sub_8239BA04(ctx, base);
	// rlwinm r30,r7,1,0,30
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r8,0(r4)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rlwinm r9,r7,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lbzx r10,r4,r7
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + ctx.r7.u32);
	// add r30,r7,r30
	ctx.r30.u64 = ctx.r7.u64 + ctx.r30.u64;
	// add r11,r9,r4
	ctx.r11.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r31,r8,r10
	ctx.r31.u64 = ctx.r8.u64 + ctx.r10.u64;
	// addi r23,r6,-2
	ctx.r23.s64 = ctx.r6.s64 + -2;
	// mulli r31,r31,14
	ctx.r31.s64 = ctx.r31.s64 * 14;
	// lbzx r30,r30,r4
	ctx.r30.u64 = PPC_LOAD_U8(ctx.r30.u32 + ctx.r4.u32);
	// add r10,r30,r10
	ctx.r10.u64 = ctx.r30.u64 + ctx.r10.u64;
	// cmpwi cr6,r23,2
	ctx.cr6.compare<int32_t>(ctx.r23.s32, 2, ctx.xer);
	// mulli r30,r10,11
	ctx.r30.s64 = ctx.r10.s64 * 11;
	// lbz r10,0(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// add r10,r31,r10
	ctx.r10.u64 = ctx.r31.u64 + ctx.r10.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// rlwinm r8,r10,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// subf r10,r30,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r30.s64;
	// addi r10,r10,63
	ctx.r10.s64 = ctx.r10.s64 + 63;
	// srawi r10,r10,7
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 7;
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// ble cr6,0x825f3474
	if (!ctx.cr6.gt) goto loc_825F3474;
	// addi r10,r23,-3
	ctx.r10.s64 = ctx.r23.s64 + -3;
	// rlwinm r30,r7,2,0,29
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r8,r10,31,1,31
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// rlwinm r31,r7,2,0,29
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r7,r30
	ctx.r30.u64 = ctx.r7.u64 + ctx.r30.u64;
	// addi r29,r5,8
	ctx.r29.s64 = ctx.r5.s64 + 8;
	// mr r10,r4
	ctx.r10.u64 = ctx.r4.u64;
	// add r31,r31,r4
	ctx.r31.u64 = ctx.r31.u64 + ctx.r4.u64;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r30,r30,r4
	ctx.r30.u64 = ctx.r30.u64 + ctx.r4.u64;
loc_825F340C:
	// lbz r27,0(r11)
	ctx.r27.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r8,r8,-1
	ctx.r8.s64 = ctx.r8.s64 + -1;
	// lbzx r28,r11,r7
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r7.u32);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// lbzx r24,r10,r7
	ctx.r24.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r7.u32);
	// cmplwi cr6,r8,0
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 0, ctx.xer);
	// add r28,r28,r27
	ctx.r28.u64 = ctx.r28.u64 + ctx.r27.u64;
	// lbz r27,0(r31)
	ctx.r27.u64 = PPC_LOAD_U8(ctx.r31.u32 + 0);
	// lbz r26,0(r30)
	ctx.r26.u64 = PPC_LOAD_U8(ctx.r30.u32 + 0);
	// add r31,r31,r9
	ctx.r31.u64 = ctx.r31.u64 + ctx.r9.u64;
	// mulli r28,r28,14
	ctx.r28.s64 = ctx.r28.s64 * 14;
	// lbz r25,0(r10)
	ctx.r25.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r28,r28,r24
	ctx.r28.u64 = ctx.r28.u64 + ctx.r24.u64;
	// add r26,r26,r25
	ctx.r26.u64 = ctx.r26.u64 + ctx.r25.u64;
	// add r28,r28,r27
	ctx.r28.u64 = ctx.r28.u64 + ctx.r27.u64;
	// mulli r26,r26,11
	ctx.r26.s64 = ctx.r26.s64 * 11;
	// rlwinm r27,r28,2,0,29
	ctx.r27.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r30,r30,r9
	ctx.r30.u64 = ctx.r30.u64 + ctx.r9.u64;
	// add r28,r28,r27
	ctx.r28.u64 = ctx.r28.u64 + ctx.r27.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r28,r26,r28
	ctx.r28.s64 = ctx.r28.s64 - ctx.r26.s64;
	// addi r28,r28,63
	ctx.r28.s64 = ctx.r28.s64 + 63;
	// srawi r28,r28,7
	ctx.xer.ca = (ctx.r28.s32 < 0) & ((ctx.r28.u32 & 0x7F) != 0);
	ctx.r28.s64 = ctx.r28.s32 >> 7;
	// stw r28,0(r29)
	PPC_STORE_U32(ctx.r29.u32 + 0, ctx.r28.u32);
	// addi r29,r29,8
	ctx.r29.s64 = ctx.r29.s64 + 8;
	// bne cr6,0x825f340c
	if (!ctx.cr6.eq) goto loc_825F340C;
loc_825F3474:
	// addi r10,r6,-1
	ctx.r10.s64 = ctx.r6.s64 + -1;
	// addi r8,r6,-3
	ctx.r8.s64 = ctx.r6.s64 + -3;
	// addi r31,r6,-4
	ctx.r31.s64 = ctx.r6.s64 + -4;
	// mullw r10,r10,r7
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r7.s32);
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// mullw r11,r23,r7
	ctx.r11.s64 = int64_t(ctx.r23.s32) * int64_t(ctx.r7.s32);
	// lbzx r11,r11,r4
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r4.u32);
	// mullw r8,r8,r7
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// lbzx r8,r8,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r8.u32 + ctx.r4.u32);
	// mullw r31,r31,r7
	ctx.r31.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r7.s32);
	// lbzx r4,r31,r4
	ctx.r4.u64 = PPC_LOAD_U8(ctx.r31.u32 + ctx.r4.u32);
	// add r4,r4,r11
	ctx.r4.u64 = ctx.r4.u64 + ctx.r11.u64;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// mulli r4,r4,11
	ctx.r4.s64 = ctx.r4.s64 * 11;
	// mulli r11,r11,14
	ctx.r11.s64 = ctx.r11.s64 * 14;
	// add r11,r8,r11
	ctx.r11.u64 = ctx.r8.u64 + ctx.r11.u64;
	// rlwinm r31,r23,2,0,29
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r23.u32 | (ctx.r23.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// subf r11,r4,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r4.s64;
	// addi r11,r11,63
	ctx.r11.s64 = ctx.r11.s64 + 63;
	// srawi r11,r11,7
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7F) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 7;
	// stwx r11,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r11.u32);
	// ble cr6,0x825f3528
	if (!ctx.cr6.gt) goto loc_825F3528;
	// addi r10,r6,-1
	ctx.r10.s64 = ctx.r6.s64 + -1;
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// li r6,0
	ctx.r6.s64 = 0;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825F34F0:
	// lwz r8,0(r5)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplwi cr6,r8,255
	ctx.cr6.compare<uint32_t>(ctx.r8.u32, 255, ctx.xer);
	// ble cr6,0x825f350c
	if (!ctx.cr6.gt) goto loc_825F350C;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// mr r8,r6
	ctx.r8.u64 = ctx.r6.u64;
	// blt cr6,0x825f350c
	if (ctx.cr6.lt) goto loc_825F350C;
	// li r8,255
	ctx.r8.s64 = 255;
loc_825F350C:
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// stb r8,0(r11)
	PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r8.u8);
	// stbx r6,r11,r7
	PPC_STORE_U8(ctx.r11.u32 + ctx.r7.u32, ctx.r6.u8);
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// bne cr6,0x825f34f0
	if (!ctx.cr6.eq) goto loc_825F34F0;
loc_825F3528:
	// b 0x8239ba54
	// ERROR 8239BA54
	return;
}

__attribute__((alias("__imp__sub_825F352C"))) PPC_WEAK_FUNC(sub_825F352C);
PPC_FUNC_IMPL(__imp__sub_825F352C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F3530"))) PPC_WEAK_FUNC(sub_825F3530);
PPC_FUNC_IMPL(__imp__sub_825F3530) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825F3538;
	sub_8239BA1C(ctx, base);
	// lbz r10,1(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 1);
	// addi r11,r4,2
	ctx.r11.s64 = ctx.r4.s64 + 2;
	// lbz r7,3(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 3);
	// addi r29,r6,-2
	ctx.r29.s64 = ctx.r6.s64 + -2;
	// lbz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// add r7,r7,r10
	ctx.r7.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// cmpwi cr6,r29,2
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 2, ctx.xer);
	// mulli r10,r10,14
	ctx.r10.s64 = ctx.r10.s64 * 14;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// mulli r8,r7,11
	ctx.r8.s64 = ctx.r7.s64 * 11;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// rlwinm r9,r10,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r8,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r8.s64;
	// addi r10,r10,64
	ctx.r10.s64 = ctx.r10.s64 + 64;
	// srawi r10,r10,7
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 7;
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// ble cr6,0x825f35f4
	if (!ctx.cr6.gt) goto loc_825F35F4;
	// addi r10,r29,-3
	ctx.r10.s64 = ctx.r29.s64 + -3;
	// addi r9,r5,8
	ctx.r9.s64 = ctx.r5.s64 + 8;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r10,1
	ctx.r10.s64 = ctx.r10.s64 + 1;
loc_825F3598:
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// lbz r8,1(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 1);
	// lbz r31,-2(r11)
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// lbz r7,3(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 3);
	// lbz r30,-1(r11)
	ctx.r30.u64 = PPC_LOAD_U8(ctx.r11.u32 + -1);
	// add r7,r7,r31
	ctx.r7.u64 = ctx.r7.u64 + ctx.r31.u64;
	// mulli r8,r8,14
	ctx.r8.s64 = ctx.r8.s64 * 14;
	// mulli r31,r7,11
	ctx.r31.s64 = ctx.r7.s64 * 11;
	// lbz r7,2(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + ctx.r30.u64;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// rlwinm r7,r8,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r31.s64;
	// addi r8,r8,64
	ctx.r8.s64 = ctx.r8.s64 + 64;
	// srawi r8,r8,7
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 7;
	// stw r8,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r8.u32);
	// addi r9,r9,8
	ctx.r9.s64 = ctx.r9.s64 + 8;
	// bne cr6,0x825f3598
	if (!ctx.cr6.eq) goto loc_825F3598;
loc_825F35F4:
	// add r11,r4,r6
	ctx.r11.u64 = ctx.r4.u64 + ctx.r6.u64;
	// rlwinm r7,r29,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// lbz r10,-2(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// lbz r9,-1(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + -1);
	// lbz r8,-3(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + -3);
	// lbz r11,-4(r11)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + -4);
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// mulli r4,r11,11
	ctx.r4.s64 = ctx.r11.s64 * 11;
	// mulli r11,r10,14
	ctx.r11.s64 = ctx.r10.s64 * 14;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// rlwinm r10,r11,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
	// subf r11,r4,r11
	ctx.r11.s64 = ctx.r11.s64 - ctx.r4.s64;
	// addi r11,r11,64
	ctx.r11.s64 = ctx.r11.s64 + 64;
	// srawi r11,r11,7
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7F) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 7;
	// stwx r11,r7,r5
	PPC_STORE_U32(ctx.r7.u32 + ctx.r5.u32, ctx.r11.u32);
	// ble cr6,0x825f3690
	if (!ctx.cr6.gt) goto loc_825F3690;
	// addi r10,r6,-1
	ctx.r10.s64 = ctx.r6.s64 + -1;
	// addi r11,r3,1
	ctx.r11.s64 = ctx.r3.s64 + 1;
	// rlwinm r10,r10,31,1,31
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// li r8,0
	ctx.r8.s64 = 0;
	// addi r9,r10,1
	ctx.r9.s64 = ctx.r10.s64 + 1;
loc_825F3658:
	// lwz r10,0(r5)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplwi cr6,r10,255
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 255, ctx.xer);
	// ble cr6,0x825f3674
	if (!ctx.cr6.gt) goto loc_825F3674;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// mr r10,r8
	ctx.r10.u64 = ctx.r8.u64;
	// blt cr6,0x825f3674
	if (ctx.cr6.lt) goto loc_825F3674;
	// li r10,255
	ctx.r10.s64 = 255;
loc_825F3674:
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// stb r10,-1(r11)
	PPC_STORE_U8(ctx.r11.u32 + -1, ctx.r10.u8);
	// stb r8,0(r11)
	PPC_STORE_U8(ctx.r11.u32 + 0, ctx.r8.u8);
	// addi r5,r5,8
	ctx.r5.s64 = ctx.r5.s64 + 8;
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// bne cr6,0x825f3658
	if (!ctx.cr6.eq) goto loc_825F3658;
loc_825F3690:
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825F3694"))) PPC_WEAK_FUNC(sub_825F3694);
PPC_FUNC_IMPL(__imp__sub_825F3694) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F3698"))) PPC_WEAK_FUNC(sub_825F3698);
PPC_FUNC_IMPL(__imp__sub_825F3698) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba00
	ctx.lr = 0x825F36A0;
	sub_8239BA00(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r11,r6
	ctx.r11.u64 = ctx.r6.u64;
	// lwz r27,292(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// mr r23,r5
	ctx.r23.u64 = ctx.r5.u64;
	// mr r24,r7
	ctx.r24.u64 = ctx.r7.u64;
	// mr r22,r8
	ctx.r22.u64 = ctx.r8.u64;
	// mr r6,r9
	ctx.r6.u64 = ctx.r9.u64;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// ble cr6,0x825f36f8
	if (!ctx.cr6.gt) goto loc_825F36F8;
	// lwz r28,276(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// subf r29,r3,r11
	ctx.r29.s64 = ctx.r11.s64 - ctx.r3.s64;
	// mr r30,r10
	ctx.r30.u64 = ctx.r10.u64;
loc_825F36D8:
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// add r3,r29,r31
	ctx.r3.u64 = ctx.r29.u64 + ctx.r31.u64;
	// bl 0x825f3530
	ctx.lr = 0x825F36E8;
	sub_825F3530(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r31,r31,r28
	ctx.r31.u64 = ctx.r31.u64 + ctx.r28.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f36d8
	if (!ctx.cr6.eq) goto loc_825F36D8;
loc_825F36F8:
	// lwz r30,268(r1)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// mr r31,r25
	ctx.r31.u64 = ctx.r25.u64;
	// lwz r26,284(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 284);
	// lwz r6,260(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// ble cr6,0x825f3738
	if (!ctx.cr6.gt) goto loc_825F3738;
	// subf r28,r25,r24
	ctx.r28.s64 = ctx.r24.s64 - ctx.r25.s64;
	// mr r29,r30
	ctx.r29.u64 = ctx.r30.u64;
loc_825F3718:
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// add r3,r28,r31
	ctx.r3.u64 = ctx.r28.u64 + ctx.r31.u64;
	// bl 0x825f3530
	ctx.lr = 0x825F3728;
	sub_825F3530(ctx, base);
	// addi r29,r29,-1
	ctx.r29.s64 = ctx.r29.s64 + -1;
	// add r31,r31,r26
	ctx.r31.u64 = ctx.r31.u64 + ctx.r26.u64;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825f3718
	if (!ctx.cr6.eq) goto loc_825F3718;
loc_825F3738:
	// mr r31,r23
	ctx.r31.u64 = ctx.r23.u64;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// ble cr6,0x825f3768
	if (!ctx.cr6.gt) goto loc_825F3768;
	// subf r29,r23,r22
	ctx.r29.s64 = ctx.r22.s64 - ctx.r23.s64;
loc_825F3748:
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// add r3,r29,r31
	ctx.r3.u64 = ctx.r29.u64 + ctx.r31.u64;
	// bl 0x825f3530
	ctx.lr = 0x825F3758;
	sub_825F3530(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r31,r31,r26
	ctx.r31.u64 = ctx.r31.u64 + ctx.r26.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f3748
	if (!ctx.cr6.eq) goto loc_825F3748;
loc_825F3768:
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
}

__attribute__((alias("__imp__sub_825F3770"))) PPC_WEAK_FUNC(sub_825F3770);
PPC_FUNC_IMPL(__imp__sub_825F3770) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba04
	ctx.lr = 0x825F3778;
	sub_8239BA04(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r28,276(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 276);
	// mr r11,r3
	ctx.r11.u64 = ctx.r3.u64;
	// mr r25,r4
	ctx.r25.u64 = ctx.r4.u64;
	// mr r23,r5
	ctx.r23.u64 = ctx.r5.u64;
	// mr r26,r7
	ctx.r26.u64 = ctx.r7.u64;
	// mr r24,r8
	ctx.r24.u64 = ctx.r8.u64;
	// mr r29,r10
	ctx.r29.u64 = ctx.r10.u64;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// ble cr6,0x825f37d0
	if (!ctx.cr6.gt) goto loc_825F37D0;
	// lwz r7,260(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r3,r6
	ctx.r3.u64 = ctx.r6.u64;
	// subf r30,r6,r11
	ctx.r30.s64 = ctx.r11.s64 - ctx.r6.s64;
	// mr r31,r9
	ctx.r31.u64 = ctx.r9.u64;
loc_825F37B0:
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// add r4,r30,r3
	ctx.r4.u64 = ctx.r30.u64 + ctx.r3.u64;
	// bl 0x825f3380
	ctx.lr = 0x825F37C0;
	sub_825F3380(ctx, base);
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825f37b0
	if (!ctx.cr6.eq) goto loc_825F37B0;
loc_825F37D0:
	// lwz r31,244(r1)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// lwz r7,268(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 268);
	// lwz r27,252(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// ble cr6,0x825f3840
	if (!ctx.cr6.gt) goto loc_825F3840;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// subf r29,r26,r25
	ctx.r29.s64 = ctx.r25.s64 - ctx.r26.s64;
	// mr r30,r31
	ctx.r30.u64 = ctx.r31.u64;
loc_825F37F0:
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// add r4,r29,r3
	ctx.r4.u64 = ctx.r29.u64 + ctx.r3.u64;
	// bl 0x825f3380
	ctx.lr = 0x825F3800;
	sub_825F3380(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f37f0
	if (!ctx.cr6.eq) goto loc_825F37F0;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// ble cr6,0x825f3840
	if (!ctx.cr6.gt) goto loc_825F3840;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// subf r30,r24,r23
	ctx.r30.s64 = ctx.r23.s64 - ctx.r24.s64;
loc_825F3820:
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// add r4,r30,r3
	ctx.r4.u64 = ctx.r30.u64 + ctx.r3.u64;
	// bl 0x825f3380
	ctx.lr = 0x825F3830;
	sub_825F3380(ctx, base);
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// addi r3,r3,1
	ctx.r3.s64 = ctx.r3.s64 + 1;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825f3820
	if (!ctx.cr6.eq) goto loc_825F3820;
loc_825F3840:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba54
	// ERROR 8239BA54
	return;
}

__attribute__((alias("__imp__sub_825F3848"))) PPC_WEAK_FUNC(sub_825F3848);
PPC_FUNC_IMPL(__imp__sub_825F3848) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba00
	ctx.lr = 0x825F3850;
	sub_8239BA00(ctx, base);
	// rlwinm r11,r7,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// lbz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rlwinm r10,r7,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r4
	ctx.r8.u64 = ctx.r11.u64 + ctx.r4.u64;
	// mulli r28,r9,34
	ctx.r28.s64 = ctx.r9.s64 * 34;
	// lbz r31,0(r8)
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// rotlwi r29,r31,1
	ctx.r29.u64 = __builtin_rotateleft32(ctx.r31.u32, 1);
	// add r9,r10,r4
	ctx.r9.u64 = ctx.r10.u64 + ctx.r4.u64;
	// add r31,r31,r29
	ctx.r31.u64 = ctx.r31.u64 + ctx.r29.u64;
	// rlwinm r10,r7,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r31,r31,r28
	ctx.r31.s64 = ctx.r28.s64 - ctx.r31.s64;
	// add r10,r7,r10
	ctx.r10.u64 = ctx.r7.u64 + ctx.r10.u64;
	// addi r25,r6,-4
	ctx.r25.s64 = ctx.r6.s64 + -4;
	// lbz r30,0(r9)
	ctx.r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r31,r30
	ctx.r31.u64 = ctx.r31.u64 + ctx.r30.u64;
	// add r10,r10,r4
	ctx.r10.u64 = ctx.r10.u64 + ctx.r4.u64;
	// addi r31,r31,16
	ctx.r31.s64 = ctx.r31.s64 + 16;
	// cmpwi cr6,r25,4
	ctx.cr6.compare<int32_t>(ctx.r25.s32, 4, ctx.xer);
	// srawi r31,r31,5
	ctx.xer.ca = (ctx.r31.s32 < 0) & ((ctx.r31.u32 & 0x1F) != 0);
	ctx.r31.s64 = ctx.r31.s32 >> 5;
	// stw r31,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r31.u32);
	// lbz r31,0(r8)
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r30,0(r4)
	ctx.r30.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rotlwi r29,r31,3
	ctx.r29.u64 = __builtin_rotateleft32(ctx.r31.u32, 3);
	// mulli r30,r30,25
	ctx.r30.s64 = ctx.r30.s64 * 25;
	// subf r31,r31,r29
	ctx.r31.s64 = ctx.r29.s64 - ctx.r31.s64;
	// add r31,r30,r31
	ctx.r31.u64 = ctx.r30.u64 + ctx.r31.u64;
	// addi r31,r31,16
	ctx.r31.s64 = ctx.r31.s64 + 16;
	// srawi r31,r31,5
	ctx.xer.ca = (ctx.r31.s32 < 0) & ((ctx.r31.u32 & 0x1F) != 0);
	ctx.r31.s64 = ctx.r31.s32 >> 5;
	// stw r31,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r31.u32);
	// lbz r30,0(r4)
	ctx.r30.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// lbz r28,0(r9)
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// rotlwi r30,r30,1
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r30.u32, 1);
	// lbz r31,0(r8)
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r29,0(r10)
	ctx.r29.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// subf r30,r28,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r28.s64;
	// rotlwi r27,r31,3
	ctx.r27.u64 = __builtin_rotateleft32(ctx.r31.u32, 3);
	// rlwinm r28,r30,1,0,30
	ctx.r28.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r27,r31,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r31.s64;
	// add r31,r30,r28
	ctx.r31.u64 = ctx.r30.u64 + ctx.r28.u64;
	// rlwinm r30,r27,2,0,29
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 2) & 0xFFFFFFFC;
	// add r31,r31,r30
	ctx.r31.u64 = ctx.r31.u64 + ctx.r30.u64;
	// add r31,r31,r29
	ctx.r31.u64 = ctx.r31.u64 + ctx.r29.u64;
	// addi r31,r31,16
	ctx.r31.s64 = ctx.r31.s64 + 16;
	// srawi r31,r31,5
	ctx.xer.ca = (ctx.r31.s32 < 0) & ((ctx.r31.u32 & 0x1F) != 0);
	ctx.r31.s64 = ctx.r31.s32 >> 5;
	// stw r31,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, ctx.r31.u32);
	// lbz r31,0(r8)
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// lbz r30,0(r9)
	ctx.r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// rotlwi r27,r31,3
	ctx.r27.u64 = __builtin_rotateleft32(ctx.r31.u32, 3);
	// lbz r28,0(r4)
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rotlwi r29,r30,1
	ctx.r29.u64 = __builtin_rotateleft32(ctx.r30.u32, 1);
	// subf r27,r31,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r31.s64;
	// add r31,r30,r29
	ctx.r31.u64 = ctx.r30.u64 + ctx.r29.u64;
	// rlwinm r30,r27,1,0,30
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 1) & 0xFFFFFFFE;
	// add r31,r31,r30
	ctx.r31.u64 = ctx.r31.u64 + ctx.r30.u64;
	// subf r31,r28,r31
	ctx.r31.s64 = ctx.r31.s64 - ctx.r28.s64;
	// addi r31,r31,8
	ctx.r31.s64 = ctx.r31.s64 + 8;
	// rlwinm r31,r31,1,0,30
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r31,r31,5
	ctx.xer.ca = (ctx.r31.s32 < 0) & ((ctx.r31.u32 & 0x1F) != 0);
	ctx.r31.s64 = ctx.r31.s32 >> 5;
	// stw r31,12(r5)
	PPC_STORE_U32(ctx.r5.u32 + 12, ctx.r31.u32);
	// ble cr6,0x825f3a04
	if (!ctx.cr6.gt) goto loc_825F3A04;
	// addi r31,r25,-5
	ctx.r31.s64 = ctx.r25.s64 + -5;
	// rlwinm r29,r7,3,0,28
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 3) & 0xFFFFFFF8;
	// rlwinm r30,r31,31,1,31
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r31,r5,16
	ctx.r31.s64 = ctx.r5.s64 + 16;
	// mr r28,r4
	ctx.r28.u64 = ctx.r4.u64;
	// add r29,r29,r4
	ctx.r29.u64 = ctx.r29.u64 + ctx.r4.u64;
	// addi r30,r30,1
	ctx.r30.s64 = ctx.r30.s64 + 1;
loc_825F3960:
	// lbz r26,0(r8)
	ctx.r26.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// lbz r23,0(r10)
	ctx.r23.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// rotlwi r26,r26,1
	ctx.r26.u64 = __builtin_rotateleft32(ctx.r26.u32, 1);
	// lbz r27,0(r9)
	ctx.r27.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r24,0(r29)
	ctx.r24.u64 = PPC_LOAD_U8(ctx.r29.u32 + 0);
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// subf r26,r23,r26
	ctx.r26.s64 = ctx.r26.s64 - ctx.r23.s64;
	// rotlwi r22,r27,3
	ctx.r22.u64 = __builtin_rotateleft32(ctx.r27.u32, 3);
	// rlwinm r23,r26,1,0,30
	ctx.r23.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r22,r27,r22
	ctx.r22.s64 = ctx.r22.s64 - ctx.r27.s64;
	// add r27,r26,r23
	ctx.r27.u64 = ctx.r26.u64 + ctx.r23.u64;
	// rlwinm r26,r22,2,0,29
	ctx.r26.u64 = __builtin_rotateleft64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// add r29,r29,r11
	ctx.r29.u64 = ctx.r29.u64 + ctx.r11.u64;
	// add r27,r27,r26
	ctx.r27.u64 = ctx.r27.u64 + ctx.r26.u64;
	// add r27,r27,r24
	ctx.r27.u64 = ctx.r27.u64 + ctx.r24.u64;
	// addi r27,r27,16
	ctx.r27.s64 = ctx.r27.s64 + 16;
	// srawi r27,r27,5
	ctx.xer.ca = (ctx.r27.s32 < 0) & ((ctx.r27.u32 & 0x1F) != 0);
	ctx.r27.s64 = ctx.r27.s32 >> 5;
	// stw r27,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r27.u32);
	// lbz r26,0(r10)
	ctx.r26.u64 = PPC_LOAD_U8(ctx.r10.u32 + 0);
	// add r10,r10,r11
	ctx.r10.u64 = ctx.r10.u64 + ctx.r11.u64;
	// lbz r23,0(r8)
	ctx.r23.u64 = PPC_LOAD_U8(ctx.r8.u32 + 0);
	// add r8,r8,r11
	ctx.r8.u64 = ctx.r8.u64 + ctx.r11.u64;
	// rotlwi r26,r26,1
	ctx.r26.u64 = __builtin_rotateleft32(ctx.r26.u32, 1);
	// lbz r27,0(r9)
	ctx.r27.u64 = PPC_LOAD_U8(ctx.r9.u32 + 0);
	// lbz r24,0(r28)
	ctx.r24.u64 = PPC_LOAD_U8(ctx.r28.u32 + 0);
	// add r9,r9,r11
	ctx.r9.u64 = ctx.r9.u64 + ctx.r11.u64;
	// subf r26,r23,r26
	ctx.r26.s64 = ctx.r26.s64 - ctx.r23.s64;
	// rotlwi r22,r27,3
	ctx.r22.u64 = __builtin_rotateleft32(ctx.r27.u32, 3);
	// rlwinm r23,r26,1,0,30
	ctx.r23.u64 = __builtin_rotateleft64(ctx.r26.u32 | (ctx.r26.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r22,r27,r22
	ctx.r22.s64 = ctx.r22.s64 - ctx.r27.s64;
	// add r27,r26,r23
	ctx.r27.u64 = ctx.r26.u64 + ctx.r23.u64;
	// rlwinm r26,r22,2,0,29
	ctx.r26.u64 = __builtin_rotateleft64(ctx.r22.u32 | (ctx.r22.u64 << 32), 2) & 0xFFFFFFFC;
	// add r28,r28,r11
	ctx.r28.u64 = ctx.r28.u64 + ctx.r11.u64;
	// add r27,r27,r26
	ctx.r27.u64 = ctx.r27.u64 + ctx.r26.u64;
	// add r27,r27,r24
	ctx.r27.u64 = ctx.r27.u64 + ctx.r24.u64;
	// addi r27,r27,16
	ctx.r27.s64 = ctx.r27.s64 + 16;
	// srawi r27,r27,5
	ctx.xer.ca = (ctx.r27.s32 < 0) & ((ctx.r27.u32 & 0x1F) != 0);
	ctx.r27.s64 = ctx.r27.s32 >> 5;
	// stw r27,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r27.u32);
	// addi r31,r31,8
	ctx.r31.s64 = ctx.r31.s64 + 8;
	// bne cr6,0x825f3960
	if (!ctx.cr6.eq) goto loc_825F3960;
loc_825F3A04:
	// addi r11,r6,-6
	ctx.r11.s64 = ctx.r6.s64 + -6;
	// mullw r10,r25,r7
	ctx.r10.s64 = int64_t(ctx.r25.s32) * int64_t(ctx.r7.s32);
	// mullw r9,r11,r7
	ctx.r9.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r7.s32);
	// lbzx r30,r9,r4
	ctx.r30.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// addi r11,r6,-8
	ctx.r11.s64 = ctx.r6.s64 + -8;
	// addi r31,r6,-3
	ctx.r31.s64 = ctx.r6.s64 + -3;
	// mullw r28,r11,r7
	ctx.r28.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r7.s32);
	// rlwinm r11,r6,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r27,r31,2,0,29
	ctx.r27.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r6,-2
	ctx.r8.s64 = ctx.r6.s64 + -2;
	// lbzx r31,r10,r4
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// add r26,r11,r5
	ctx.r26.u64 = ctx.r11.u64 + ctx.r5.u64;
	// rlwinm r29,r25,2,0,29
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r25.u32 | (ctx.r25.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r11,r8,r7
	ctx.r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r7.s32);
	// rlwinm r25,r8,2,0,29
	ctx.r25.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// rotlwi r8,r31,3
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r31.u32, 3);
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r31.s64;
	// rlwinm r31,r8,1,0,30
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// rotlwi r8,r30,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r30.u32, 1);
	// add r8,r30,r8
	ctx.r8.u64 = ctx.r30.u64 + ctx.r8.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// lbzx r31,r11,r4
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r4.u32);
	// subf r8,r31,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r31.s64;
	// addi r8,r8,8
	ctx.r8.s64 = ctx.r8.s64 + 8;
	// rlwinm r8,r8,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 1) & 0xFFFFFFFE;
	// srawi r8,r8,5
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 5;
	// stwx r8,r29,r5
	PPC_STORE_U32(ctx.r29.u32 + ctx.r5.u32, ctx.r8.u32);
	// lbzx r8,r11,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r4.u32);
	// lbzx r31,r9,r4
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// rotlwi r29,r8,1
	ctx.r29.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// lbzx r8,r10,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// lbzx r30,r28,r4
	ctx.r30.u64 = PPC_LOAD_U8(ctx.r28.u32 + ctx.r4.u32);
	// subf r31,r31,r29
	ctx.r31.s64 = ctx.r29.s64 - ctx.r31.s64;
	// rotlwi r28,r8,3
	ctx.r28.u64 = __builtin_rotateleft32(ctx.r8.u32, 3);
	// rlwinm r29,r31,1,0,30
	ctx.r29.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// subf r28,r8,r28
	ctx.r28.s64 = ctx.r28.s64 - ctx.r8.s64;
	// add r8,r31,r29
	ctx.r8.u64 = ctx.r31.u64 + ctx.r29.u64;
	// rlwinm r31,r28,2,0,29
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r28.u32 | (ctx.r28.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + ctx.r30.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// srawi r8,r8,5
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 5;
	// stwx r8,r27,r5
	PPC_STORE_U32(ctx.r27.u32 + ctx.r5.u32, ctx.r8.u32);
	// lbzx r8,r10,r4
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// lbzx r31,r11,r4
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r4.u32);
	// rotlwi r30,r8,3
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 3);
	// mulli r31,r31,25
	ctx.r31.s64 = ctx.r31.s64 * 25;
	// subf r8,r8,r30
	ctx.r8.s64 = ctx.r30.s64 - ctx.r8.s64;
	// add r8,r31,r8
	ctx.r8.u64 = ctx.r31.u64 + ctx.r8.u64;
	// addi r8,r8,16
	ctx.r8.s64 = ctx.r8.s64 + 16;
	// srawi r8,r8,5
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 5;
	// stwx r8,r25,r5
	PPC_STORE_U32(ctx.r25.u32 + ctx.r5.u32, ctx.r8.u32);
	// lbzx r11,r11,r4
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r11.u32 + ctx.r4.u32);
	// lbzx r10,r10,r4
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r10.u32 + ctx.r4.u32);
	// mulli r8,r11,34
	ctx.r8.s64 = ctx.r11.s64 * 34;
	// lbzx r9,r9,r4
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r9.u32 + ctx.r4.u32);
	// rotlwi r11,r10,1
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// subf r11,r11,r8
	ctx.r11.s64 = ctx.r8.s64 - ctx.r11.s64;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// addi r11,r11,16
	ctx.r11.s64 = ctx.r11.s64 + 16;
	// srawi r11,r11,5
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1F) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 5;
	// stw r11,-4(r26)
	PPC_STORE_U32(ctx.r26.u32 + -4, ctx.r11.u32);
	// ble cr6,0x825f3b40
	if (!ctx.cr6.gt) goto loc_825F3B40;
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
loc_825F3B0C:
	// lwz r11,0(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// ble cr6,0x825f3b28
	if (!ctx.cr6.gt) goto loc_825F3B28;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r11,0
	ctx.r11.s64 = 0;
	// blt cr6,0x825f3b28
	if (ctx.cr6.lt) goto loc_825F3B28;
	// li r11,255
	ctx.r11.s64 = 255;
loc_825F3B28:
	// addi r6,r6,-1
	ctx.r6.s64 = ctx.r6.s64 + -1;
	// stb r11,0(r10)
	PPC_STORE_U8(ctx.r10.u32 + 0, ctx.r11.u8);
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// bne cr6,0x825f3b0c
	if (!ctx.cr6.eq) goto loc_825F3B0C;
loc_825F3B40:
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
}

__attribute__((alias("__imp__sub_825F3B44"))) PPC_WEAK_FUNC(sub_825F3B44);
PPC_FUNC_IMPL(__imp__sub_825F3B44) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F3B48"))) PPC_WEAK_FUNC(sub_825F3B48);
PPC_FUNC_IMPL(__imp__sub_825F3B48) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba14
	ctx.lr = 0x825F3B50;
	sub_8239BA14(ctx, base);
	// lbz r10,2(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// addi r29,r6,-4
	ctx.r29.s64 = ctx.r6.s64 + -4;
	// lbz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// subfic r10,r10,5
	ctx.xer.ca = ctx.r10.u32 <= 5;
	ctx.r10.s64 = 5 - ctx.r10.s64;
	// lbz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// mulli r8,r11,34
	ctx.r8.s64 = ctx.r11.s64 * 34;
	// rlwinm r7,r10,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r11,r4,6
	ctx.r11.s64 = ctx.r4.s64 + 6;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// cmpwi cr6,r29,4
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 4, ctx.xer);
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// srawi r10,r10,5
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stw r10,0(r5)
	PPC_STORE_U32(ctx.r5.u32 + 0, ctx.r10.u32);
	// lbz r10,2(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// lbz r9,0(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rotlwi r8,r10,3
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// mulli r9,r9,25
	ctx.r9.s64 = ctx.r9.s64 * 25;
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// addi r10,r10,15
	ctx.r10.s64 = ctx.r10.s64 + 15;
	// srawi r10,r10,5
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stw r10,4(r5)
	PPC_STORE_U32(ctx.r5.u32 + 4, ctx.r10.u32);
	// lbz r10,0(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// lbz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// rotlwi r7,r10,1
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// lbz r10,2(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// subf r9,r9,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r9.s64;
	// rotlwi r7,r10,3
	ctx.r7.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// addi r9,r9,5
	ctx.r9.s64 = ctx.r9.s64 + 5;
	// subf r7,r10,r7
	ctx.r7.s64 = ctx.r7.s64 - ctx.r10.s64;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r7,r7,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// srawi r10,r10,5
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stw r10,8(r5)
	PPC_STORE_U32(ctx.r5.u32 + 8, ctx.r10.u32);
	// lbz r10,2(r4)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r4.u32 + 2);
	// lbz r9,4(r4)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r4.u32 + 4);
	// rotlwi r31,r10,3
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// lbz r7,0(r4)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r4.u32 + 0);
	// rotlwi r8,r9,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// subf r31,r10,r31
	ctx.r31.s64 = ctx.r31.s64 - ctx.r10.s64;
	// add r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r9,r31,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 1) & 0xFFFFFFFE;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// subf r10,r7,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r7.s64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r10,15
	ctx.r10.s64 = ctx.r10.s64 + 15;
	// srawi r10,r10,5
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stw r10,12(r5)
	PPC_STORE_U32(ctx.r5.u32 + 12, ctx.r10.u32);
	// ble cr6,0x825f3ccc
	if (!ctx.cr6.gt) goto loc_825F3CCC;
	// addi r10,r29,-5
	ctx.r10.s64 = ctx.r29.s64 + -5;
	// rlwinm r9,r10,31,1,31
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 31) & 0x7FFFFFFF;
	// addi r10,r5,16
	ctx.r10.s64 = ctx.r5.s64 + 16;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
loc_825F3C38:
	// lbz r31,-4(r11)
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r11.u32 + -4);
	// addi r9,r9,-1
	ctx.r9.s64 = ctx.r9.s64 + -1;
	// lbz r7,0(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// rotlwi r30,r31,1
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r31.u32, 1);
	// lbz r8,-2(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// lbz r31,2(r11)
	ctx.r31.u64 = PPC_LOAD_U8(ctx.r11.u32 + 2);
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// subf r7,r7,r30
	ctx.r7.s64 = ctx.r30.s64 - ctx.r7.s64;
	// rotlwi r30,r8,3
	ctx.r30.u64 = __builtin_rotateleft32(ctx.r8.u32, 3);
	// addi r7,r7,5
	ctx.r7.s64 = ctx.r7.s64 + 5;
	// subf r30,r8,r30
	ctx.r30.s64 = ctx.r30.s64 - ctx.r8.s64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r30,r30,2,0,29
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r30.u32 | (ctx.r30.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + ctx.r30.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// srawi r8,r8,5
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 5;
	// stw r8,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r8.u32);
	// lbz r8,0(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + 0);
	// lbz r7,-4(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + -4);
	// rotlwi r31,r8,1
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 1);
	// lbz r8,-2(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// lbz r30,-6(r11)
	ctx.r30.u64 = PPC_LOAD_U8(ctx.r11.u32 + -6);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// subf r7,r7,r31
	ctx.r7.s64 = ctx.r31.s64 - ctx.r7.s64;
	// rotlwi r31,r8,3
	ctx.r31.u64 = __builtin_rotateleft32(ctx.r8.u32, 3);
	// addi r7,r7,5
	ctx.r7.s64 = ctx.r7.s64 + 5;
	// subf r31,r8,r31
	ctx.r31.s64 = ctx.r31.s64 - ctx.r8.s64;
	// rlwinm r8,r7,1,0,30
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r7.u32 | (ctx.r7.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r31,r31,2,0,29
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// add r8,r8,r30
	ctx.r8.u64 = ctx.r8.u64 + ctx.r30.u64;
	// add r8,r8,r31
	ctx.r8.u64 = ctx.r8.u64 + ctx.r31.u64;
	// srawi r8,r8,5
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1F) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 5;
	// stw r8,4(r10)
	PPC_STORE_U32(ctx.r10.u32 + 4, ctx.r8.u32);
	// addi r10,r10,8
	ctx.r10.s64 = ctx.r10.s64 + 8;
	// bne cr6,0x825f3c38
	if (!ctx.cr6.eq) goto loc_825F3C38;
loc_825F3CCC:
	// addi r10,r6,-3
	ctx.r10.s64 = ctx.r6.s64 + -3;
	// add r11,r4,r6
	ctx.r11.u64 = ctx.r4.u64 + ctx.r6.u64;
	// rlwinm r31,r10,2,0,29
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r10,r6,2,0,29
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// rlwinm r7,r29,2,0,29
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r29.u32 | (ctx.r29.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r9,r6,-2
	ctx.r9.s64 = ctx.r6.s64 + -2;
	// add r29,r10,r5
	ctx.r29.u64 = ctx.r10.u64 + ctx.r5.u64;
	// lbz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -4);
	// rlwinm r30,r9,2,0,29
	ctx.r30.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// lbz r9,-6(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + -6);
	// rotlwi r27,r10,3
	ctx.r27.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// lbz r28,-2(r11)
	ctx.r28.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// rotlwi r8,r9,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r9.u32, 1);
	// subf r27,r10,r27
	ctx.r27.s64 = ctx.r27.s64 - ctx.r10.s64;
	// add r10,r9,r8
	ctx.r10.u64 = ctx.r9.u64 + ctx.r8.u64;
	// rlwinm r9,r27,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r27.u32 | (ctx.r27.u64 << 32), 1) & 0xFFFFFFFE;
	// li r4,0
	ctx.r4.s64 = 0;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// subf r10,r28,r10
	ctx.r10.s64 = ctx.r10.s64 - ctx.r28.s64;
	// rlwinm r10,r10,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addi r10,r10,15
	ctx.r10.s64 = ctx.r10.s64 + 15;
	// srawi r10,r10,5
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stwx r10,r7,r5
	PPC_STORE_U32(ctx.r7.u32 + ctx.r5.u32, ctx.r10.u32);
	// lbz r10,-2(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// lbz r9,-6(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + -6);
	// rotlwi r8,r10,1
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 1);
	// lbz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -4);
	// lbz r7,-8(r11)
	ctx.r7.u64 = PPC_LOAD_U8(ctx.r11.u32 + -8);
	// subf r9,r9,r8
	ctx.r9.s64 = ctx.r8.s64 - ctx.r9.s64;
	// rotlwi r8,r10,3
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// addi r9,r9,5
	ctx.r9.s64 = ctx.r9.s64 + 5;
	// subf r8,r10,r8
	ctx.r8.s64 = ctx.r8.s64 - ctx.r10.s64;
	// rlwinm r10,r9,1,0,30
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r8,r8,2,0,29
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// add r10,r9,r10
	ctx.r10.u64 = ctx.r9.u64 + ctx.r10.u64;
	// add r10,r10,r7
	ctx.r10.u64 = ctx.r10.u64 + ctx.r7.u64;
	// add r10,r10,r8
	ctx.r10.u64 = ctx.r10.u64 + ctx.r8.u64;
	// srawi r10,r10,5
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stwx r10,r31,r5
	PPC_STORE_U32(ctx.r31.u32 + ctx.r5.u32, ctx.r10.u32);
	// lbz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -4);
	// lbz r9,-2(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// rotlwi r8,r10,3
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r10.u32, 3);
	// mulli r9,r9,25
	ctx.r9.s64 = ctx.r9.s64 * 25;
	// subf r10,r10,r8
	ctx.r10.s64 = ctx.r8.s64 - ctx.r10.s64;
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// addi r10,r10,15
	ctx.r10.s64 = ctx.r10.s64 + 15;
	// srawi r10,r10,5
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1F) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 5;
	// stwx r10,r30,r5
	PPC_STORE_U32(ctx.r30.u32 + ctx.r5.u32, ctx.r10.u32);
	// lbz r10,-4(r11)
	ctx.r10.u64 = PPC_LOAD_U8(ctx.r11.u32 + -4);
	// lbz r8,-2(r11)
	ctx.r8.u64 = PPC_LOAD_U8(ctx.r11.u32 + -2);
	// subfic r10,r10,5
	ctx.xer.ca = ctx.r10.u32 <= 5;
	ctx.r10.s64 = 5 - ctx.r10.s64;
	// lbz r9,-6(r11)
	ctx.r9.u64 = PPC_LOAD_U8(ctx.r11.u32 + -6);
	// mulli r8,r8,34
	ctx.r8.s64 = ctx.r8.s64 * 34;
	// rlwinm r11,r10,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// add r11,r11,r8
	ctx.r11.u64 = ctx.r11.u64 + ctx.r8.u64;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// srawi r11,r11,5
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1F) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 5;
	// stw r11,-4(r29)
	PPC_STORE_U32(ctx.r29.u32 + -4, ctx.r11.u32);
	// ble cr6,0x825f3df0
	if (!ctx.cr6.gt) goto loc_825F3DF0;
loc_825F3DC0:
	// lwz r11,0(r5)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r5.u32 + 0);
	// cmplwi cr6,r11,255
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 255, ctx.xer);
	// ble cr6,0x825f3ddc
	if (!ctx.cr6.gt) goto loc_825F3DDC;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// li r11,0
	ctx.r11.s64 = 0;
	// blt cr6,0x825f3ddc
	if (ctx.cr6.lt) goto loc_825F3DDC;
	// li r11,255
	ctx.r11.s64 = 255;
loc_825F3DDC:
	// stbx r11,r4,r3
	PPC_STORE_U8(ctx.r4.u32 + ctx.r3.u32, ctx.r11.u8);
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// addi r5,r5,4
	ctx.r5.s64 = ctx.r5.s64 + 4;
	// cmpw cr6,r4,r6
	ctx.cr6.compare<int32_t>(ctx.r4.s32, ctx.r6.s32, ctx.xer);
	// blt cr6,0x825f3dc0
	if (ctx.cr6.lt) goto loc_825F3DC0;
loc_825F3DF0:
	// b 0x8239ba64
	// ERROR 8239BA64
	return;
}

__attribute__((alias("__imp__sub_825F3DF4"))) PPC_WEAK_FUNC(sub_825F3DF4);
PPC_FUNC_IMPL(__imp__sub_825F3DF4) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F3DF8"))) PPC_WEAK_FUNC(sub_825F3DF8);
PPC_FUNC_IMPL(__imp__sub_825F3DF8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba04
	ctx.lr = 0x825F3E00;
	sub_8239BA04(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r8
	ctx.r29.u64 = ctx.r8.u64;
	// lwz r27,260(r1)
	ctx.r27.u64 = PPC_LOAD_U32(ctx.r1.u32 + 260);
	// mr r30,r6
	ctx.r30.u64 = ctx.r6.u64;
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// mr r23,r5
	ctx.r23.u64 = ctx.r5.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// mr r25,r9
	ctx.r25.u64 = ctx.r9.u64;
	// mr r28,r10
	ctx.r28.u64 = ctx.r10.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// ble cr6,0x825f3e54
	if (!ctx.cr6.gt) goto loc_825F3E54;
	// lwz r11,244(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 244);
	// mullw r26,r30,r11
	ctx.r26.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r11.s32);
loc_825F3E38:
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x825f3b48
	ctx.lr = 0x825F3E44;
	sub_825F3B48(ctx, base);
	// add r31,r31,r30
	ctx.r31.u64 = ctx.r31.u64 + ctx.r30.u64;
	// add r3,r26,r3
	ctx.r3.u64 = ctx.r26.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r29
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r29.s32, ctx.xer);
	// blt cr6,0x825f3e38
	if (ctx.cr6.lt) goto loc_825F3E38;
loc_825F3E54:
	// lwz r26,252(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 252);
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x825f3e8c
	if (!ctx.cr6.gt) goto loc_825F3E8C;
	// mullw r29,r30,r26
	ctx.r29.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r26.s32);
loc_825F3E6C:
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x825f3b48
	ctx.lr = 0x825F3E7C;
	sub_825F3B48(ctx, base);
	// add r31,r31,r30
	ctx.r31.u64 = ctx.r31.u64 + ctx.r30.u64;
	// add r3,r29,r3
	ctx.r3.u64 = ctx.r29.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r28
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r28.s32, ctx.xer);
	// blt cr6,0x825f3e6c
	if (ctx.cr6.lt) goto loc_825F3E6C;
loc_825F3E8C:
	// mr r3,r23
	ctx.r3.u64 = ctx.r23.u64;
	// li r31,0
	ctx.r31.s64 = 0;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x825f3ec0
	if (!ctx.cr6.gt) goto loc_825F3EC0;
	// mullw r29,r30,r26
	ctx.r29.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r26.s32);
loc_825F3EA0:
	// mr r6,r25
	ctx.r6.u64 = ctx.r25.u64;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// bl 0x825f3b48
	ctx.lr = 0x825F3EB0;
	sub_825F3B48(ctx, base);
	// add r31,r31,r30
	ctx.r31.u64 = ctx.r31.u64 + ctx.r30.u64;
	// add r3,r29,r3
	ctx.r3.u64 = ctx.r29.u64 + ctx.r3.u64;
	// cmpw cr6,r31,r28
	ctx.cr6.compare<int32_t>(ctx.r31.s32, ctx.r28.s32, ctx.xer);
	// blt cr6,0x825f3ea0
	if (ctx.cr6.lt) goto loc_825F3EA0;
loc_825F3EC0:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba54
	// ERROR 8239BA54
	return;
}

__attribute__((alias("__imp__sub_825F3EC8"))) PPC_WEAK_FUNC(sub_825F3EC8);
PPC_FUNC_IMPL(__imp__sub_825F3EC8) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba0c
	ctx.lr = 0x825F3ED0;
	sub_8239BA0C(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r29,r7
	ctx.r29.u64 = ctx.r7.u64;
	// lwz r28,236(r1)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r1.u32 + 236);
	// mr r26,r4
	ctx.r26.u64 = ctx.r4.u64;
	// mr r25,r5
	ctx.r25.u64 = ctx.r5.u64;
	// mr r31,r8
	ctx.r31.u64 = ctx.r8.u64;
	// mr r27,r9
	ctx.r27.u64 = ctx.r9.u64;
	// mr r7,r10
	ctx.r7.u64 = ctx.r10.u64;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// ble cr6,0x825f3f20
	if (!ctx.cr6.gt) goto loc_825F3F20;
	// mr r4,r3
	ctx.r4.u64 = ctx.r3.u64;
	// mr r30,r6
	ctx.r30.u64 = ctx.r6.u64;
loc_825F3F00:
	// mr r6,r29
	ctx.r6.u64 = ctx.r29.u64;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x825f3848
	ctx.lr = 0x825F3F10;
	sub_825F3848(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f3f00
	if (!ctx.cr6.eq) goto loc_825F3F00;
loc_825F3F20:
	// lwz r7,228(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// ble cr6,0x825f3f80
	if (!ctx.cr6.gt) goto loc_825F3F80;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r30,r31
	ctx.r30.u64 = ctx.r31.u64;
loc_825F3F34:
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x825f3848
	ctx.lr = 0x825F3F44;
	sub_825F3848(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f3f34
	if (!ctx.cr6.eq) goto loc_825F3F34;
	// cmpwi cr6,r31,0
	ctx.cr6.compare<int32_t>(ctx.r31.s32, 0, ctx.xer);
	// ble cr6,0x825f3f80
	if (!ctx.cr6.gt) goto loc_825F3F80;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
loc_825F3F60:
	// mr r6,r27
	ctx.r6.u64 = ctx.r27.u64;
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r3,r4
	ctx.r3.u64 = ctx.r4.u64;
	// bl 0x825f3848
	ctx.lr = 0x825F3F70;
	sub_825F3848(ctx, base);
	// addi r31,r31,-1
	ctx.r31.s64 = ctx.r31.s64 + -1;
	// addi r4,r4,1
	ctx.r4.s64 = ctx.r4.s64 + 1;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825f3f60
	if (!ctx.cr6.eq) goto loc_825F3F60;
loc_825F3F80:
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba5c
	// ERROR 8239BA5C
	return;
}

__attribute__((alias("__imp__sub_825F3F88"))) PPC_WEAK_FUNC(sub_825F3F88);
PPC_FUNC_IMPL(__imp__sub_825F3F88) {
	PPC_FUNC_PROLOGUE();
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba18
	ctx.lr = 0x825F3F90;
	sub_8239BA18(ctx, base);
	// lis r8,-32161
	ctx.r8.s64 = -2107703296;
	// lis r9,-32161
	ctx.r9.s64 = -2107703296;
	// lis r10,-32161
	ctx.r10.s64 = -2107703296;
	// lis r11,-32161
	ctx.r11.s64 = -2107703296;
	// addi r31,r8,13976
	ctx.r31.s64 = ctx.r8.s64 + 13976;
	// addi r30,r9,14192
	ctx.r30.s64 = ctx.r9.s64 + 14192;
	// addi r29,r10,15864
	ctx.r29.s64 = ctx.r10.s64 + 15864;
	// addi r28,r11,16072
	ctx.r28.s64 = ctx.r11.s64 + 16072;
	// lis r8,-32161
	ctx.r8.s64 = -2107703296;
	// lis r9,-32161
	ctx.r9.s64 = -2107703296;
	// stw r31,3184(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3184, ctx.r31.u32);
	// lis r10,-32161
	ctx.r10.s64 = -2107703296;
	// stw r30,3188(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3188, ctx.r30.u32);
	// lis r11,-32161
	ctx.r11.s64 = -2107703296;
	// stw r29,3192(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3192, ctx.r29.u32);
	// addi r8,r8,11432
	ctx.r8.s64 = ctx.r8.s64 + 11432;
	// stw r28,3196(r3)
	PPC_STORE_U32(ctx.r3.u32 + 3196, ctx.r28.u32);
	// addi r9,r9,12296
	ctx.r9.s64 = ctx.r9.s64 + 12296;
	// addi r10,r10,11656
	ctx.r10.s64 = ctx.r10.s64 + 11656;
	// addi r11,r11,12480
	ctx.r11.s64 = ctx.r11.s64 + 12480;
	// lis r4,-32161
	ctx.r4.s64 = -2107703296;
	// lis r5,-32161
	ctx.r5.s64 = -2107703296;
	// stw r8,15796(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15796, ctx.r8.u32);
	// lis r6,-32161
	ctx.r6.s64 = -2107703296;
	// stw r9,15804(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15804, ctx.r9.u32);
	// lis r7,-32161
	ctx.r7.s64 = -2107703296;
	// stw r10,15800(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15800, ctx.r10.u32);
	// stw r11,15808(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15808, ctx.r11.u32);
	// addi r4,r4,9128
	ctx.r4.s64 = ctx.r4.s64 + 9128;
	// addi r5,r5,9800
	ctx.r5.s64 = ctx.r5.s64 + 9800;
	// addi r6,r6,9368
	ctx.r6.s64 = ctx.r6.s64 + 9368;
	// addi r7,r7,10024
	ctx.r7.s64 = ctx.r7.s64 + 10024;
	// rotlwi r11,r31,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r31.u32, 0);
	// rotlwi r10,r30,0
	ctx.r10.u64 = __builtin_rotateleft32(ctx.r30.u32, 0);
	// stw r4,15780(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15780, ctx.r4.u32);
	// rotlwi r9,r29,0
	ctx.r9.u64 = __builtin_rotateleft32(ctx.r29.u32, 0);
	// stw r5,15788(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15788, ctx.r5.u32);
	// rotlwi r8,r28,0
	ctx.r8.u64 = __builtin_rotateleft32(ctx.r28.u32, 0);
	// stw r6,15784(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15784, ctx.r6.u32);
	// stw r7,15792(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15792, ctx.r7.u32);
	// stw r11,15812(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15812, ctx.r11.u32);
	// stw r10,15816(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15816, ctx.r10.u32);
	// stw r9,15820(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15820, ctx.r9.u32);
	// stw r8,15824(r3)
	PPC_STORE_U32(ctx.r3.u32 + 15824, ctx.r8.u32);
	// b 0x8239ba68
	// ERROR 8239BA68
	return;
}

__attribute__((alias("__imp__sub_825F4044"))) PPC_WEAK_FUNC(sub_825F4044);
PPC_FUNC_IMPL(__imp__sub_825F4044) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F4048"))) PPC_WEAK_FUNC(sub_825F4048);
PPC_FUNC_IMPL(__imp__sub_825F4048) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r11,0
	ctx.r11.s64 = 0;
	// addi r3,r31,52
	ctx.r3.s64 = ctx.r31.s64 + 52;
	// stw r11,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r11.u32);
	// stw r11,304(r31)
	PPC_STORE_U32(ctx.r31.u32 + 304, ctx.r11.u32);
	// stw r11,308(r31)
	PPC_STORE_U32(ctx.r31.u32 + 308, ctx.r11.u32);
	// stw r11,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r11.u32);
	// stw r11,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r11.u32);
	// stw r11,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r11.u32);
	// stw r11,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r11.u32);
	// stw r11,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r11.u32);
	// stw r11,24(r31)
	PPC_STORE_U32(ctx.r31.u32 + 24, ctx.r11.u32);
	// stw r11,28(r31)
	PPC_STORE_U32(ctx.r31.u32 + 28, ctx.r11.u32);
	// stw r11,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r11.u32);
	// stw r11,36(r31)
	PPC_STORE_U32(ctx.r31.u32 + 36, ctx.r11.u32);
	// stw r11,40(r31)
	PPC_STORE_U32(ctx.r31.u32 + 40, ctx.r11.u32);
	// stw r11,44(r31)
	PPC_STORE_U32(ctx.r31.u32 + 44, ctx.r11.u32);
	// stw r11,48(r31)
	PPC_STORE_U32(ctx.r31.u32 + 48, ctx.r11.u32);
	// stw r11,300(r31)
	PPC_STORE_U32(ctx.r31.u32 + 300, ctx.r11.u32);
	// stw r11,292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 292, ctx.r11.u32);
	// stw r11,296(r31)
	PPC_STORE_U32(ctx.r31.u32 + 296, ctx.r11.u32);
	// bl 0x826455c0
	ctx.lr = 0x825F40B0;
	sub_826455C0(ctx, base);
	// addi r3,r31,156
	ctx.r3.s64 = ctx.r31.s64 + 156;
	// bl 0x826454f0
	ctx.lr = 0x825F40B8;
	sub_826454F0(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F40CC"))) PPC_WEAK_FUNC(sub_825F40CC);
PPC_FUNC_IMPL(__imp__sub_825F40CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F40D0"))) PPC_WEAK_FUNC(sub_825F40D0);
PPC_FUNC_IMPL(__imp__sub_825F40D0) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// addi r3,r31,52
	ctx.r3.s64 = ctx.r31.s64 + 52;
	// bl 0x826455e0
	ctx.lr = 0x825F40F0;
	sub_826455E0(ctx, base);
	// addi r3,r31,156
	ctx.r3.s64 = ctx.r31.s64 + 156;
	// bl 0x82645510
	ctx.lr = 0x825F40F8;
	sub_82645510(ctx, base);
	// lwz r3,304(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 304);
	// li r30,0
	ctx.r30.s64 = 0;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825f4110
	if (ctx.cr6.eq) goto loc_825F4110;
	// bl 0x825edb28
	ctx.lr = 0x825F410C;
	sub_825EDB28(ctx, base);
	// stw r30,304(r31)
	PPC_STORE_U32(ctx.r31.u32 + 304, ctx.r30.u32);
loc_825F4110:
	// lwz r3,308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 308);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825f4124
	if (ctx.cr6.eq) goto loc_825F4124;
	// bl 0x825edb28
	ctx.lr = 0x825F4120;
	sub_825EDB28(ctx, base);
	// stw r30,308(r31)
	PPC_STORE_U32(ctx.r31.u32 + 308, ctx.r30.u32);
loc_825F4124:
	// lwz r3,0(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825f4138
	if (ctx.cr6.eq) goto loc_825F4138;
	// bl 0x825edb28
	ctx.lr = 0x825F4134;
	sub_825EDB28(ctx, base);
	// stw r30,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r30.u32);
loc_825F4138:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F4150"))) PPC_WEAK_FUNC(sub_825F4150);
PPC_FUNC_IMPL(__imp__sub_825F4150) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba1c
	ctx.lr = 0x825F4158;
	sub_8239BA1C(ctx, base);
	// stwu r1,-176(r1)
	ea = -176 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r4
	ctx.r31.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// mr r6,r7
	ctx.r6.u64 = ctx.r7.u64;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x825f4464
	if (ctx.cr6.eq) goto loc_825F4464;
	// cmplwi cr6,r9,0
	ctx.cr6.compare<uint32_t>(ctx.r9.u32, 0, ctx.xer);
	// beq cr6,0x825f4464
	if (ctx.cr6.eq) goto loc_825F4464;
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825f4464
	if (ctx.cr6.eq) goto loc_825F4464;
	// lwz r11,296(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 296);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825f419c
	if (ctx.cr6.eq) goto loc_825F419C;
	// lwz r11,32(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// addze r10,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r10.s64 = temp.s64;
	// b 0x825f41a0
	goto loc_825F41A0;
loc_825F419C:
	// lwz r10,32(r3)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r3.u32 + 32);
loc_825F41A0:
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r7,22101
	ctx.r7.s64 = 1448411136;
	// ori r30,r7,22857
	ctx.r30.u64 = ctx.r7.u64 | 22857;
	// lwz r7,16(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// cmplw cr6,r7,r30
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x825f4248
	if (ctx.cr6.eq) goto loc_825F4248;
	// lis r30,12338
	ctx.r30.s64 = 808583168;
	// ori r30,r30,13385
	ctx.r30.u64 = ctx.r30.u64 | 13385;
	// cmplw cr6,r7,r30
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x825f4248
	if (ctx.cr6.eq) goto loc_825F4248;
	// lis r30,12593
	ctx.r30.s64 = 825294848;
	// ori r30,r30,13392
	ctx.r30.u64 = ctx.r30.u64 | 13392;
	// cmplw cr6,r7,r30
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x825f4248
	if (ctx.cr6.eq) goto loc_825F4248;
	// lis r30,12849
	ctx.r30.s64 = 842072064;
	// ori r30,r30,22105
	ctx.r30.u64 = ctx.r30.u64 | 22105;
	// cmplw cr6,r7,r30
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, ctx.r30.u32, ctx.xer);
	// beq cr6,0x825f4248
	if (ctx.cr6.eq) goto loc_825F4248;
	// lhz r7,14(r11)
	ctx.r7.u64 = PPC_LOAD_U16(ctx.r11.u32 + 14);
	// lwz r30,4(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// lwz r29,8(r11)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// mullw r11,r7,r30
	ctx.r11.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r30.s32);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// mullw r11,r11,r29
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r29.s32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lwz r7,28(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// lhz r11,14(r11)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + 14);
	// mullw r11,r11,r7
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r7.s32);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// b 0x825f4280
	goto loc_825F4280;
loc_825F4248:
	// lwz r7,8(r11)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r11,4(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// mullw r11,r7,r11
	ctx.r11.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// rlwinm r7,r11,1,0,30
	ctx.r7.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r11,r7
	ctx.r11.u64 = ctx.r11.u64 + ctx.r7.u64;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
loc_825F4280:
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r11.u32);
	// rotlwi r11,r11,0
	ctx.r11.u64 = __builtin_rotateleft32(ctx.r11.u32, 0);
	// cmpw cr6,r8,r11
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825f4458
	if (ctx.cr6.lt) goto loc_825F4458;
	// lwz r11,0(r4)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r4.u32 + 0);
	// cmpw cr6,r5,r11
	ctx.cr6.compare<int32_t>(ctx.r5.s32, ctx.r11.s32, ctx.xer);
	// blt cr6,0x825f4458
	if (ctx.cr6.lt) goto loc_825F4458;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825f4464
	if (ctx.cr6.eq) goto loc_825F4464;
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x825f4464
	if (ctx.cr6.eq) goto loc_825F4464;
	// lwz r11,292(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 292);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825f442c
	if (ctx.cr6.eq) goto loc_825F442C;
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r9,12850
	ctx.r9.s64 = 842137600;
	// extsw r30,r10
	ctx.r30.s64 = ctx.r10.s32;
	// ori r7,r9,13392
	ctx.r7.u64 = ctx.r9.u64 | 13392;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// lwz r5,16(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// cmplw cr6,r5,r7
	ctx.cr6.compare<uint32_t>(ctx.r5.u32, ctx.r7.u32, ctx.xer);
	// lwz r11,28(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// addi r3,r3,156
	ctx.r3.s64 = ctx.r3.s64 + 156;
	// bne cr6,0x825f438c
	if (!ctx.cr6.eq) goto loc_825F438C;
	// std r30,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r30.u64);
	// mr r29,r7
	ctx.r29.u64 = ctx.r7.u64;
	// lis r5,-32249
	ctx.r5.s64 = -2113470464;
	// extsw r30,r8
	ctx.r30.s64 = ctx.r8.s32;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// stw r29,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r29.u32);
	// extsw r29,r11
	ctx.r29.s64 = ctx.r11.s32;
	// lfd f4,-31520(r5)
	ctx.fpscr.disableFlushMode();
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r5.u32 + -31520);
	// extsw r5,r9
	ctx.r5.s64 = ctx.r9.s32;
	// std r30,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r30.u64);
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// fmr f2,f4
	ctx.f2.f64 = ctx.f4.f64;
	// std r5,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r5.u64);
	// mullw r10,r8,r9
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// rlwinm r9,r11,1,0,30
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// add r8,r11,r6
	ctx.r8.u64 = ctx.r11.u64 + ctx.r6.u64;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// add r5,r10,r31
	ctx.r5.u64 = ctx.r10.u64 + ctx.r31.u64;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// add r9,r11,r6
	ctx.r9.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rlwinm r11,r10,1,0,30
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// add r6,r11,r31
	ctx.r6.u64 = ctx.r11.u64 + ctx.r31.u64;
	// lfd f0,112(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// std r29,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r29.u64);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lfd f12,128(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// lfd f13,120(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fdiv f3,f13,f0
	ctx.f3.f64 = ctx.f13.f64 / ctx.f0.f64;
	// lfd f11,112(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// fdiv f1,f12,f11
	ctx.f1.f64 = ctx.f12.f64 / ctx.f11.f64;
	// bl 0x8264fce0
	ctx.lr = 0x825F4380;
	sub_8264FCE0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
loc_825F438C:
	// std r30,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r30.u64);
	// extsw r29,r11
	ctx.r29.s64 = ctx.r11.s32;
	// stw r5,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r5.u32);
	// lis r5,-32249
	ctx.r5.s64 = -2113470464;
	// extsw r30,r8
	ctx.r30.s64 = ctx.r8.s32;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// std r29,112(r1)
	PPC_STORE_U64(ctx.r1.u32 + 112, ctx.r29.u64);
	// lfd f4,-31520(r5)
	ctx.fpscr.disableFlushMode();
	ctx.f4.u64 = PPC_LOAD_U64(ctx.r5.u32 + -31520);
	// fmr f2,f4
	ctx.f2.f64 = ctx.f4.f64;
	// extsw r5,r9
	ctx.r5.s64 = ctx.r9.s32;
	// mullw r10,r8,r9
	ctx.r10.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r9.s32);
	// std r5,120(r1)
	PPC_STORE_U64(ctx.r1.u32 + 120, ctx.r5.u64);
	// rlwinm r9,r11,2,0,29
	ctx.r9.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// add r8,r11,r6
	ctx.r8.u64 = ctx.r11.u64 + ctx.r6.u64;
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// mr r7,r6
	ctx.r7.u64 = ctx.r6.u64;
	// add r5,r10,r31
	ctx.r5.u64 = ctx.r10.u64 + ctx.r31.u64;
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// add r9,r11,r6
	ctx.r9.u64 = ctx.r11.u64 + ctx.r6.u64;
	// rlwinm r11,r10,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 2) & 0xFFFFFFFC;
	// add r11,r10,r11
	ctx.r11.u64 = ctx.r10.u64 + ctx.r11.u64;
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// add r6,r11,r31
	ctx.r6.u64 = ctx.r11.u64 + ctx.r31.u64;
	// lfd f0,128(r1)
	ctx.f0.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// std r30,128(r1)
	PPC_STORE_U64(ctx.r1.u32 + 128, ctx.r30.u64);
	// fcfid f0,f0
	ctx.f0.f64 = double(ctx.f0.s64);
	// lfd f11,112(r1)
	ctx.f11.u64 = PPC_LOAD_U64(ctx.r1.u32 + 112);
	// fcfid f11,f11
	ctx.f11.f64 = double(ctx.f11.s64);
	// lfd f13,120(r1)
	ctx.f13.u64 = PPC_LOAD_U64(ctx.r1.u32 + 120);
	// fcfid f13,f13
	ctx.f13.f64 = double(ctx.f13.s64);
	// fdiv f3,f13,f0
	ctx.f3.f64 = ctx.f13.f64 / ctx.f0.f64;
	// lfd f12,128(r1)
	ctx.f12.u64 = PPC_LOAD_U64(ctx.r1.u32 + 128);
	// fcfid f12,f12
	ctx.f12.f64 = double(ctx.f12.s64);
	// fdiv f1,f12,f11
	ctx.f1.f64 = ctx.f12.f64 / ctx.f11.f64;
	// bl 0x8264fce0
	ctx.lr = 0x825F4420;
	sub_8264FCE0(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
loc_825F442C:
	// lwz r7,28(r3)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r3.u32 + 28);
	// mr r8,r10
	ctx.r8.u64 = ctx.r10.u64;
	// lwz r4,0(r3)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r5,r31
	ctx.r5.u64 = ctx.r31.u64;
	// addi r3,r3,52
	ctx.r3.s64 = ctx.r3.s64 + 52;
	// bl 0x8264b350
	ctx.lr = 0x825F4444;
	sub_8264B350(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825f4464
	if (ctx.cr6.eq) goto loc_825F4464;
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
loc_825F4458:
	// li r11,0
	ctx.r11.s64 = 0;
	// stw r11,0(r4)
	PPC_STORE_U32(ctx.r4.u32 + 0, ctx.r11.u32);
	// stw r11,0(r9)
	PPC_STORE_U32(ctx.r9.u32 + 0, ctx.r11.u32);
loc_825F4464:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,176
	ctx.r1.s64 = ctx.r1.s64 + 176;
	// b 0x8239ba6c
	// ERROR 8239BA6C
	return;
}

__attribute__((alias("__imp__sub_825F4470"))) PPC_WEAK_FUNC(sub_825F4470);
PPC_FUNC_IMPL(__imp__sub_825F4470) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825f44a4
	if (!ctx.cr6.eq) goto loc_825F44A4;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825F44A4:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f40d0
	ctx.lr = 0x825F44AC;
	sub_825F40D0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825edb28
	ctx.lr = 0x825F44B4;
	sub_825EDB28(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F44CC"))) PPC_WEAK_FUNC(sub_825F44CC);
PPC_FUNC_IMPL(__imp__sub_825F44CC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F44D0"))) PPC_WEAK_FUNC(sub_825F44D0);
PPC_FUNC_IMPL(__imp__sub_825F44D0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9f0
	ctx.lr = 0x825F44D8;
	sub_8239B9F0(ctx, base);
	// stwu r1,-208(r1)
	ea = -208 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lis r11,12850
	ctx.r11.s64 = 842137600;
	// mr r30,r8
	ctx.r30.u64 = ctx.r8.u64;
	// ori r8,r11,13392
	ctx.r8.u64 = ctx.r11.u64 | 13392;
	// lwz r11,332(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 332);
	// mr r31,r6
	ctx.r31.u64 = ctx.r6.u64;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bgt cr6,0x825f476c
	if (ctx.cr6.gt) goto loc_825F476C;
	// beq cr6,0x825f46f8
	if (ctx.cr6.eq) goto loc_825F46F8;
	// lis r8,12338
	ctx.r8.s64 = 808583168;
	// ori r8,r8,13385
	ctx.r8.u64 = ctx.r8.u64 | 13385;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bgt cr6,0x825f4568
	if (ctx.cr6.gt) goto loc_825F4568;
	// beq cr6,0x825f4588
	if (ctx.cr6.eq) goto loc_825F4588;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq cr6,0x825f4524
	if (ctx.cr6.eq) goto loc_825F4524;
	// cmpwi cr6,r11,3
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 3, ctx.xer);
	// bne cr6,0x825f4530
	if (!ctx.cr6.eq) goto loc_825F4530;
loc_825F4524:
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// ble cr6,0x825f47bc
	if (!ctx.cr6.gt) goto loc_825F47BC;
loc_825F452C:
	// li r3,-1
	ctx.r3.s64 = -1;
loc_825F4530:
	// lwz r11,340(r1)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r1.u32 + 340);
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// mullw r8,r31,r11
	ctx.r8.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r11.s32);
	// addi r8,r8,31
	ctx.r8.s64 = ctx.r8.s64 + 31;
	// rlwinm r8,r8,0,0,26
	ctx.r8.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 0) & 0xFFFFFFE0;
	// srawi r8,r8,3
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r8.s32 >> 3;
	// addze r8,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r8.s64 = temp.s64;
	// mullw r26,r8,r3
	ctx.r26.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r3.s32);
	// bne cr6,0x825f47c4
	if (!ctx.cr6.eq) goto loc_825F47C4;
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// srawi r10,r10,3
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// mullw r8,r26,r8
	ctx.r8.s64 = int64_t(ctx.r26.s32) * int64_t(ctx.r8.s32);
	// b 0x825f47f8
	goto loc_825F47F8;
loc_825F4568:
	// lis r8,12593
	ctx.r8.s64 = 825294848;
	// ori r8,r8,13392
	ctx.r8.u64 = ctx.r8.u64 | 13392;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// beq cr6,0x825f4698
	if (ctx.cr6.eq) goto loc_825F4698;
	// lis r8,12849
	ctx.r8.s64 = 842072064;
	// ori r8,r8,22105
	ctx.r8.u64 = ctx.r8.u64 | 22105;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bne cr6,0x825f4530
	if (!ctx.cr6.eq) goto loc_825F4530;
loc_825F4588:
	// lwz r20,316(r1)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// srawi r29,r31,1
	ctx.xer.ca = (ctx.r31.s32 < 0) & ((ctx.r31.u32 & 0x1) != 0);
	ctx.r29.s64 = ctx.r31.s32 >> 1;
	// lwz r8,324(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// mullw r11,r31,r7
	ctx.r11.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r7.s32);
	// lwz r6,292(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// lwz r7,308(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// lwz r3,300(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// srawi r28,r30,1
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x1) != 0);
	ctx.r28.s64 = ctx.r30.s32 >> 1;
	// srawi r21,r20,1
	ctx.xer.ca = (ctx.r20.s32 < 0) & ((ctx.r20.u32 & 0x1) != 0);
	ctx.r21.s64 = ctx.r20.s32 >> 1;
	// srawi r19,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r19.s64 = ctx.r8.s32 >> 1;
	// srawi r24,r6,1
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r24.s64 = ctx.r6.s32 >> 1;
	// mullw r27,r31,r6
	ctx.r27.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r6.s32);
	// mullw r6,r30,r7
	ctx.r6.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r7.s32);
	// add r27,r27,r5
	ctx.r27.u64 = ctx.r27.u64 + ctx.r5.u64;
	// add r6,r6,r4
	ctx.r6.u64 = ctx.r6.u64 + ctx.r4.u64;
	// srawi r25,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r25.s64 = ctx.r10.s32 >> 1;
	// add r27,r27,r10
	ctx.r27.u64 = ctx.r27.u64 + ctx.r10.u64;
	// add r26,r6,r3
	ctx.r26.u64 = ctx.r6.u64 + ctx.r3.u64;
	// mullw r10,r24,r29
	ctx.r10.s64 = int64_t(ctx.r24.s32) * int64_t(ctx.r29.s32);
	// srawi r6,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r11.s32 >> 2;
	// srawi r7,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + ctx.r25.u64;
	// add r25,r6,r11
	ctx.r25.u64 = ctx.r6.u64 + ctx.r11.u64;
	// srawi r3,r3,1
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 1;
	// mullw r6,r7,r28
	ctx.r6.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r28.s32);
loc_825F45EC:
	// add r7,r11,r10
	ctx.r7.u64 = ctx.r11.u64 + ctx.r10.u64;
	// mullw r11,r30,r9
	ctx.r11.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r9.s32);
	// add r9,r25,r10
	ctx.r9.u64 = ctx.r25.u64 + ctx.r10.u64;
	// add r23,r9,r5
	ctx.r23.u64 = ctx.r9.u64 + ctx.r5.u64;
	// srawi r9,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 2;
loc_825F4600:
	// add r10,r6,r3
	ctx.r10.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r25,r7,r5
	ctx.r25.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r7,r9,r11
	ctx.r7.u64 = ctx.r9.u64 + ctx.r11.u64;
	// add r9,r11,r10
	ctx.r9.u64 = ctx.r11.u64 + ctx.r10.u64;
	// add r11,r7,r10
	ctx.r11.u64 = ctx.r7.u64 + ctx.r10.u64;
	// add r24,r9,r4
	ctx.r24.u64 = ctx.r9.u64 + ctx.r4.u64;
	// add r22,r11,r4
	ctx.r22.u64 = ctx.r11.u64 + ctx.r4.u64;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// ble cr6,0x825f464c
	if (!ctx.cr6.gt) goto loc_825F464C;
	// mr r18,r8
	ctx.r18.u64 = ctx.r8.u64;
loc_825F4628:
	// mr r5,r20
	ctx.r5.u64 = ctx.r20.u64;
	// mr r4,r27
	ctx.r4.u64 = ctx.r27.u64;
	// mr r3,r26
	ctx.r3.u64 = ctx.r26.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F4638;
	sub_8239CB70(ctx, base);
	// addi r18,r18,-1
	ctx.r18.s64 = ctx.r18.s64 + -1;
	// add r27,r27,r31
	ctx.r27.u64 = ctx.r27.u64 + ctx.r31.u64;
	// add r26,r26,r30
	ctx.r26.u64 = ctx.r26.u64 + ctx.r30.u64;
	// cmplwi cr6,r18,0
	ctx.cr6.compare<uint32_t>(ctx.r18.u32, 0, ctx.xer);
	// bne cr6,0x825f4628
	if (!ctx.cr6.eq) goto loc_825F4628;
loc_825F464C:
	// cmpwi cr6,r19,0
	ctx.cr6.compare<int32_t>(ctx.r19.s32, 0, ctx.xer);
	// ble cr6,0x825f48c0
	if (!ctx.cr6.gt) goto loc_825F48C0;
loc_825F4654:
	// mr r5,r21
	ctx.r5.u64 = ctx.r21.u64;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// mr r3,r24
	ctx.r3.u64 = ctx.r24.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F4664;
	sub_8239CB70(ctx, base);
	// mr r5,r21
	ctx.r5.u64 = ctx.r21.u64;
	// mr r4,r23
	ctx.r4.u64 = ctx.r23.u64;
	// mr r3,r22
	ctx.r3.u64 = ctx.r22.u64;
	// add r25,r29,r25
	ctx.r25.u64 = ctx.r29.u64 + ctx.r25.u64;
	// add r24,r28,r24
	ctx.r24.u64 = ctx.r28.u64 + ctx.r24.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F467C;
	sub_8239CB70(ctx, base);
	// addi r19,r19,-1
	ctx.r19.s64 = ctx.r19.s64 + -1;
	// add r23,r29,r23
	ctx.r23.u64 = ctx.r29.u64 + ctx.r23.u64;
	// add r22,r28,r22
	ctx.r22.u64 = ctx.r28.u64 + ctx.r22.u64;
	// cmplwi cr6,r19,0
	ctx.cr6.compare<uint32_t>(ctx.r19.u32, 0, ctx.xer);
	// bne cr6,0x825f4654
	if (!ctx.cr6.eq) goto loc_825F4654;
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239ba40
	// ERROR 8239BA40
	return;
loc_825F4698:
	// lwz r6,292(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// srawi r29,r31,2
	ctx.xer.ca = (ctx.r31.s32 < 0) & ((ctx.r31.u32 & 0x3) != 0);
	ctx.r29.s64 = ctx.r31.s32 >> 2;
	// lwz r20,316(r1)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// srawi r28,r30,2
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x3) != 0);
	ctx.r28.s64 = ctx.r30.s32 >> 2;
	// mullw r27,r31,r6
	ctx.r27.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r6.s32);
	// lwz r3,308(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// add r27,r27,r5
	ctx.r27.u64 = ctx.r27.u64 + ctx.r5.u64;
	// srawi r21,r20,2
	ctx.xer.ca = (ctx.r20.s32 < 0) & ((ctx.r20.u32 & 0x3) != 0);
	ctx.r21.s64 = ctx.r20.s32 >> 2;
	// srawi r25,r10,2
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x3) != 0);
	ctx.r25.s64 = ctx.r10.s32 >> 2;
	// add r27,r27,r10
	ctx.r27.u64 = ctx.r27.u64 + ctx.r10.u64;
	// mullw r8,r30,r3
	ctx.r8.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r3.s32);
	// mullw r10,r29,r6
	ctx.r10.s64 = int64_t(ctx.r29.s32) * int64_t(ctx.r6.s32);
	// mullw r11,r31,r7
	ctx.r11.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r7.s32);
	// lwz r7,300(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// add r26,r8,r4
	ctx.r26.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + ctx.r25.u64;
	// lwz r8,324(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// srawi r25,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r25.s64 = ctx.r11.s32 >> 2;
	// mullw r6,r28,r3
	ctx.r6.s64 = int64_t(ctx.r28.s32) * int64_t(ctx.r3.s32);
	// mr r19,r8
	ctx.r19.u64 = ctx.r8.u64;
	// add r26,r26,r7
	ctx.r26.u64 = ctx.r26.u64 + ctx.r7.u64;
	// srawi r3,r7,2
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r7.s32 >> 2;
	// add r25,r25,r11
	ctx.r25.u64 = ctx.r25.u64 + ctx.r11.u64;
	// b 0x825f45ec
	goto loc_825F45EC;
loc_825F46F8:
	// lwz r6,292(r1)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// srawi r29,r31,1
	ctx.xer.ca = (ctx.r31.s32 < 0) & ((ctx.r31.u32 & 0x1) != 0);
	ctx.r29.s64 = ctx.r31.s32 >> 1;
	// lwz r20,316(r1)
	ctx.r20.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// srawi r28,r30,1
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x1) != 0);
	ctx.r28.s64 = ctx.r30.s32 >> 1;
	// mullw r27,r31,r6
	ctx.r27.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r6.s32);
	// lwz r3,308(r1)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// add r27,r27,r5
	ctx.r27.u64 = ctx.r27.u64 + ctx.r5.u64;
	// srawi r21,r20,1
	ctx.xer.ca = (ctx.r20.s32 < 0) & ((ctx.r20.u32 & 0x1) != 0);
	ctx.r21.s64 = ctx.r20.s32 >> 1;
	// srawi r25,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r25.s64 = ctx.r10.s32 >> 1;
	// add r27,r27,r10
	ctx.r27.u64 = ctx.r27.u64 + ctx.r10.u64;
	// mullw r10,r29,r6
	ctx.r10.s64 = int64_t(ctx.r29.s32) * int64_t(ctx.r6.s32);
	// mullw r11,r31,r7
	ctx.r11.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r7.s32);
	// lwz r7,300(r1)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mullw r8,r30,r3
	ctx.r8.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r3.s32);
	// add r10,r10,r25
	ctx.r10.u64 = ctx.r10.u64 + ctx.r25.u64;
	// srawi r25,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r25.s64 = ctx.r11.s32 >> 1;
	// add r26,r8,r4
	ctx.r26.u64 = ctx.r8.u64 + ctx.r4.u64;
	// mullw r6,r28,r3
	ctx.r6.s64 = int64_t(ctx.r28.s32) * int64_t(ctx.r3.s32);
	// lwz r8,324(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// srawi r3,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r7.s32 >> 1;
	// add r25,r25,r11
	ctx.r25.u64 = ctx.r25.u64 + ctx.r11.u64;
	// add r26,r26,r7
	ctx.r26.u64 = ctx.r26.u64 + ctx.r7.u64;
	// add r7,r11,r10
	ctx.r7.u64 = ctx.r11.u64 + ctx.r10.u64;
	// mullw r11,r30,r9
	ctx.r11.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r9.s32);
	// add r9,r25,r10
	ctx.r9.u64 = ctx.r25.u64 + ctx.r10.u64;
	// mr r19,r8
	ctx.r19.u64 = ctx.r8.u64;
	// add r23,r9,r5
	ctx.r23.u64 = ctx.r9.u64 + ctx.r5.u64;
	// srawi r9,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r9.s64 = ctx.r11.s32 >> 1;
	// b 0x825f4600
	goto loc_825F4600;
loc_825F476C:
	// lis r8,22101
	ctx.r8.s64 = 1448411136;
	// ori r8,r8,22857
	ctx.r8.u64 = ctx.r8.u64 | 22857;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bgt cr6,0x825f47a4
	if (ctx.cr6.gt) goto loc_825F47A4;
	// beq cr6,0x825f4588
	if (ctx.cr6.eq) goto loc_825F4588;
	// lis r8,12889
	ctx.r8.s64 = 844693504;
	// ori r8,r8,21849
	ctx.r8.u64 = ctx.r8.u64 | 21849;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// beq cr6,0x825f47b4
	if (ctx.cr6.eq) goto loc_825F47B4;
	// lis r8,21849
	ctx.r8.s64 = 1431896064;
	// ori r8,r8,22105
	ctx.r8.u64 = ctx.r8.u64 | 22105;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// beq cr6,0x825f47b4
	if (ctx.cr6.eq) goto loc_825F47B4;
	// b 0x825f4530
	goto loc_825F4530;
loc_825F47A4:
	// lis r8,22870
	ctx.r8.s64 = 1498808320;
	// ori r8,r8,22869
	ctx.r8.u64 = ctx.r8.u64 | 22869;
	// cmpw cr6,r11,r8
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r8.s32, ctx.xer);
	// bne cr6,0x825f4530
	if (!ctx.cr6.eq) goto loc_825F4530;
loc_825F47B4:
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// ble cr6,0x825f452c
	if (!ctx.cr6.gt) goto loc_825F452C;
loc_825F47BC:
	// li r3,1
	ctx.r3.s64 = 1;
	// b 0x825f4530
	goto loc_825F4530;
loc_825F47C4:
	// cmpwi cr6,r26,0
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 0, ctx.xer);
	// mr r6,r26
	ctx.r6.u64 = ctx.r26.u64;
	// bgt cr6,0x825f47d4
	if (ctx.cr6.gt) goto loc_825F47D4;
	// neg r6,r26
	ctx.r6.s64 = -ctx.r26.s64;
loc_825F47D4:
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// bgt cr6,0x825f47e0
	if (ctx.cr6.gt) goto loc_825F47E0;
	// neg r7,r7
	ctx.r7.s64 = -ctx.r7.s64;
loc_825F47E0:
	// lwz r8,292(r1)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r1.u32 + 292);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// subf r8,r8,r7
	ctx.r8.s64 = ctx.r7.s64 - ctx.r8.s64;
	// addi r7,r8,-1
	ctx.r7.s64 = ctx.r8.s64 + -1;
	// srawi r8,r10,3
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r10.s32 >> 3;
	// mullw r10,r7,r6
	ctx.r10.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r6.s32);
loc_825F47F8:
	// add r7,r10,r8
	ctx.r7.u64 = ctx.r10.u64 + ctx.r8.u64;
	// mullw r10,r30,r11
	ctx.r10.s64 = int64_t(ctx.r30.s32) * int64_t(ctx.r11.s32);
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// rlwinm r10,r10,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFE0;
	// srawi r10,r10,3
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// mullw r27,r10,r3
	ctx.r27.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r3.s32);
	// bne cr6,0x825f4834
	if (!ctx.cr6.eq) goto loc_825F4834;
	// lwz r10,300(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// lwz r9,308(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// srawi r10,r10,3
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// mullw r9,r27,r9
	ctx.r9.s64 = int64_t(ctx.r27.s32) * int64_t(ctx.r9.s32);
	// b 0x825f486c
	goto loc_825F486C;
loc_825F4834:
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// mr r8,r27
	ctx.r8.u64 = ctx.r27.u64;
	// bgt cr6,0x825f4844
	if (ctx.cr6.gt) goto loc_825F4844;
	// neg r8,r27
	ctx.r8.s64 = -ctx.r27.s64;
loc_825F4844:
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bgt cr6,0x825f4850
	if (ctx.cr6.gt) goto loc_825F4850;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
loc_825F4850:
	// lwz r10,308(r1)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r1.u32 + 308);
	// subf r10,r10,r9
	ctx.r10.s64 = ctx.r9.s64 - ctx.r10.s64;
	// lwz r9,300(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 300);
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// addi r10,r10,-1
	ctx.r10.s64 = ctx.r10.s64 + -1;
	// srawi r9,r9,3
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x7) != 0);
	ctx.r9.s64 = ctx.r9.s32 >> 3;
	// mullw r10,r10,r8
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r8.s32);
loc_825F486C:
	// add r10,r10,r9
	ctx.r10.u64 = ctx.r10.u64 + ctx.r9.u64;
	// lwz r9,316(r1)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r1.u32 + 316);
	// lwz r29,324(r1)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r1.u32 + 324);
	// add r31,r7,r5
	ctx.r31.u64 = ctx.r7.u64 + ctx.r5.u64;
	// mullw r11,r9,r11
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// addi r11,r11,31
	ctx.r11.s64 = ctx.r11.s64 + 31;
	// add r30,r10,r4
	ctx.r30.u64 = ctx.r10.u64 + ctx.r4.u64;
	// rlwinm r11,r11,0,0,26
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 0) & 0xFFFFFFE0;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// addze r28,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r28.s64 = temp.s64;
	// ble cr6,0x825f48c0
	if (!ctx.cr6.gt) goto loc_825F48C0;
loc_825F489C:
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F48AC;
	sub_8239CB70(ctx, base);
	// addi r29,r29,-1
	ctx.r29.s64 = ctx.r29.s64 + -1;
	// add r31,r26,r31
	ctx.r31.u64 = ctx.r26.u64 + ctx.r31.u64;
	// add r30,r27,r30
	ctx.r30.u64 = ctx.r27.u64 + ctx.r30.u64;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825f489c
	if (!ctx.cr6.eq) goto loc_825F489C;
loc_825F48C0:
	// addi r1,r1,208
	ctx.r1.s64 = ctx.r1.s64 + 208;
	// b 0x8239ba40
	// ERROR 8239BA40
	return;
}

__attribute__((alias("__imp__sub_825F48C8"))) PPC_WEAK_FUNC(sub_825F48C8);
PPC_FUNC_IMPL(__imp__sub_825F48C8) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// bne cr6,0x825f4994
	if (!ctx.cr6.eq) goto loc_825F4994;
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825f4984
	if (ctx.cr6.eq) goto loc_825F4984;
	// cmpwi cr6,r3,3
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 3, ctx.xer);
	// beq cr6,0x825f4984
	if (ctx.cr6.eq) goto loc_825F4984;
	// lis r11,12889
	ctx.r11.s64 = 844693504;
	// ori r11,r11,21849
	ctx.r11.u64 = ctx.r11.u64 | 21849;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f498c
	if (ctx.cr6.eq) goto loc_825F498C;
	// lis r11,22870
	ctx.r11.s64 = 1498808320;
	// ori r11,r11,22869
	ctx.r11.u64 = ctx.r11.u64 | 22869;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f498c
	if (ctx.cr6.eq) goto loc_825F498C;
	// lis r11,12850
	ctx.r11.s64 = 842137600;
	// ori r11,r11,13392
	ctx.r11.u64 = ctx.r11.u64 | 13392;
	// cmplw cr6,r3,r11
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f498c
	if (ctx.cr6.eq) goto loc_825F498C;
	// lis r11,22101
	ctx.r11.s64 = 1448411136;
	// ori r11,r11,22857
	ctx.r11.u64 = ctx.r11.u64 | 22857;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f496c
	if (ctx.cr6.eq) goto loc_825F496C;
	// lis r11,12338
	ctx.r11.s64 = 808583168;
	// ori r11,r11,13385
	ctx.r11.u64 = ctx.r11.u64 | 13385;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f496c
	if (ctx.cr6.eq) goto loc_825F496C;
	// lis r11,12849
	ctx.r11.s64 = 842072064;
	// ori r11,r11,22105
	ctx.r11.u64 = ctx.r11.u64 | 22105;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f496c
	if (ctx.cr6.eq) goto loc_825F496C;
	// lis r11,12593
	ctx.r11.s64 = 825294848;
	// ori r11,r11,13392
	ctx.r11.u64 = ctx.r11.u64 | 13392;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// bne cr6,0x825f4984
	if (!ctx.cr6.eq) goto loc_825F4984;
	// srawi r11,r4,2
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r4.s32 >> 2;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// subf. r11,r11,r4
	ctx.r11.s64 = ctx.r4.s64 - ctx.r11.s64;
	ctx.cr0.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beq 0x825f4984
	if (ctx.cr0.eq) goto loc_825F4984;
loc_825F4964:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_825F496C:
	// clrlwi r11,r4,31
	ctx.r11.u64 = ctx.r4.u32 & 0x1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825f4964
	if (!ctx.cr6.eq) goto loc_825F4964;
	// clrlwi r11,r5,31
	ctx.r11.u64 = ctx.r5.u32 & 0x1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825f4964
	if (!ctx.cr6.eq) goto loc_825F4964;
loc_825F4984:
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825F498C:
	// clrlwi r3,r4,31
	ctx.r3.u64 = ctx.r4.u32 & 0x1;
	// blr 
	return;
loc_825F4994:
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// beq cr6,0x825f4a70
	if (ctx.cr6.eq) goto loc_825F4A70;
	// cmpwi cr6,r3,3
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 3, ctx.xer);
	// beq cr6,0x825f4a70
	if (ctx.cr6.eq) goto loc_825F4A70;
	// lis r11,12889
	ctx.r11.s64 = 844693504;
	// ori r11,r11,21849
	ctx.r11.u64 = ctx.r11.u64 | 21849;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4a4c
	if (ctx.cr6.eq) goto loc_825F4A4C;
	// lis r11,22870
	ctx.r11.s64 = 1498808320;
	// ori r11,r11,22869
	ctx.r11.u64 = ctx.r11.u64 | 22869;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4a4c
	if (ctx.cr6.eq) goto loc_825F4A4C;
	// lis r11,22101
	ctx.r11.s64 = 1448411136;
	// ori r11,r11,22857
	ctx.r11.u64 = ctx.r11.u64 | 22857;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4a28
	if (ctx.cr6.eq) goto loc_825F4A28;
	// lis r11,12338
	ctx.r11.s64 = 808583168;
	// ori r11,r11,13385
	ctx.r11.u64 = ctx.r11.u64 | 13385;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4a28
	if (ctx.cr6.eq) goto loc_825F4A28;
	// lis r11,12849
	ctx.r11.s64 = 842072064;
	// ori r11,r11,22105
	ctx.r11.u64 = ctx.r11.u64 | 22105;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4a28
	if (ctx.cr6.eq) goto loc_825F4A28;
	// lis r11,12593
	ctx.r11.s64 = 825294848;
	// ori r11,r11,13392
	ctx.r11.u64 = ctx.r11.u64 | 13392;
	// cmpw cr6,r3,r11
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r11.s32, ctx.xer);
	// bne cr6,0x825f4984
	if (!ctx.cr6.eq) goto loc_825F4984;
	// clrlwi r11,r4,30
	ctx.r11.u64 = ctx.r4.u32 & 0x3;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825f4a68
	if (!ctx.cr6.eq) goto loc_825F4A68;
	// clrlwi r11,r5,31
	ctx.r11.u64 = ctx.r5.u32 & 0x1;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_825F4A28:
	// clrlwi r11,r4,31
	ctx.r11.u64 = ctx.r4.u32 & 0x1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825f4a44
	if (!ctx.cr6.eq) goto loc_825F4A44;
	// clrlwi r11,r5,30
	ctx.r11.u64 = ctx.r5.u32 & 0x3;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
loc_825F4A44:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_825F4A4C:
	// clrlwi r11,r4,31
	ctx.r11.u64 = ctx.r4.u32 & 0x1;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825f4a68
	if (!ctx.cr6.eq) goto loc_825F4A68;
	// clrlwi r11,r5,31
	ctx.r11.u64 = ctx.r5.u32 & 0x1;
	// li r3,0
	ctx.r3.s64 = 0;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// beqlr cr6
	if (ctx.cr6.eq) return;
loc_825F4A68:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
loc_825F4A70:
	// clrlwi r3,r5,31
	ctx.r3.u64 = ctx.r5.u32 & 0x1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F4A78"))) PPC_WEAK_FUNC(sub_825F4A78);
PPC_FUNC_IMPL(__imp__sub_825F4A78) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// bne cr6,0x825f4aac
	if (!ctx.cr6.eq) goto loc_825F4AAC;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825F4AAC:
	// lwz r11,296(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 296);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825f4ad4
	if (!ctx.cr6.eq) goto loc_825F4AD4;
	// bl 0x825f4470
	ctx.lr = 0x825F4AC0;
	sub_825F4470(ctx, base);
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
loc_825F4AD4:
	// bl 0x825f40d0
	ctx.lr = 0x825F4AD8;
	sub_825F40D0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825edb28
	ctx.lr = 0x825F4AE0;
	sub_825EDB28(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F4AF8"))) PPC_WEAK_FUNC(sub_825F4AF8);
PPC_FUNC_IMPL(__imp__sub_825F4AF8) {
	PPC_FUNC_PROLOGUE();
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f4b0c
	if (!ctx.cr6.eq) goto loc_825F4B0C;
	// li r3,0
	ctx.r3.s64 = 0;
	// blr 
	return;
loc_825F4B0C:
	// lis r10,22101
	ctx.r10.s64 = 1448411136;
	// lwz r11,16(r11)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// ori r10,r10,22857
	ctx.r10.u64 = ctx.r10.u64 | 22857;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x825f4b54
	if (ctx.cr6.eq) goto loc_825F4B54;
	// lis r10,12338
	ctx.r10.s64 = 808583168;
	// ori r10,r10,13385
	ctx.r10.u64 = ctx.r10.u64 | 13385;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x825f4b54
	if (ctx.cr6.eq) goto loc_825F4B54;
	// lis r10,12593
	ctx.r10.s64 = 825294848;
	// ori r10,r10,13392
	ctx.r10.u64 = ctx.r10.u64 | 13392;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// beq cr6,0x825f4b54
	if (ctx.cr6.eq) goto loc_825F4B54;
	// lis r10,12849
	ctx.r10.s64 = 842072064;
	// li r3,2
	ctx.r3.s64 = 2;
	// ori r10,r10,22105
	ctx.r10.u64 = ctx.r10.u64 | 22105;
	// cmplw cr6,r11,r10
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, ctx.r10.u32, ctx.xer);
	// bnelr cr6
	if (!ctx.cr6.eq) return;
loc_825F4B54:
	// li r3,1
	ctx.r3.s64 = 1;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F4B5C"))) PPC_WEAK_FUNC(sub_825F4B5C);
PPC_FUNC_IMPL(__imp__sub_825F4B5C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F4B60"))) PPC_WEAK_FUNC(sub_825F4B60);
PPC_FUNC_IMPL(__imp__sub_825F4B60) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	// lwz r11,0(r3)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// lis r10,22101
	ctx.r10.s64 = 1448411136;
	// ori r9,r10,22857
	ctx.r9.u64 = ctx.r10.u64 | 22857;
	// lwz r10,16(r11)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// lhz r11,14(r11)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r11.u32 + 14);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// beq cr6,0x825f4bac
	if (ctx.cr6.eq) goto loc_825F4BAC;
	// lis r9,12338
	ctx.r9.s64 = 808583168;
	// ori r9,r9,13385
	ctx.r9.u64 = ctx.r9.u64 | 13385;
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// beq cr6,0x825f4bac
	if (ctx.cr6.eq) goto loc_825F4BAC;
	// lis r9,12849
	ctx.r9.s64 = 842072064;
	// ori r9,r9,22105
	ctx.r9.u64 = ctx.r9.u64 | 22105;
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// beq cr6,0x825f4bac
	if (ctx.cr6.eq) goto loc_825F4BAC;
	// lis r9,12593
	ctx.r9.s64 = 825294848;
	// ori r9,r9,13392
	ctx.r9.u64 = ctx.r9.u64 | 13392;
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// bne cr6,0x825f4bcc
	if (!ctx.cr6.eq) goto loc_825F4BCC;
loc_825F4BAC:
	// mullw r11,r11,r4
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r4.s32);
	// mullw r11,r11,r5
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r5.s32);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// srawi r10,r11,31
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 31;
	// xor r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 ^ ctx.r10.u64;
	// subf r3,r10,r11
	ctx.r3.s64 = ctx.r11.s64 - ctx.r10.s64;
	// blr 
	return;
loc_825F4BCC:
	// mullw r11,r11,r4
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r4.s32);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// addi r11,r11,3
	ctx.r11.s64 = ctx.r11.s64 + 3;
	// srawi r11,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 2;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// mullw r11,r11,r5
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r5.s32);
	// rlwinm r11,r11,2,0,29
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r10,r11,31
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7FFFFFFF) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 31;
	// xor r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 ^ ctx.r10.u64;
	// subf r3,r10,r11
	ctx.r3.s64 = ctx.r11.s64 - ctx.r10.s64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F4BFC"))) PPC_WEAK_FUNC(sub_825F4BFC);
PPC_FUNC_IMPL(__imp__sub_825F4BFC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F4C00"))) PPC_WEAK_FUNC(sub_825F4C00);
PPC_FUNC_IMPL(__imp__sub_825F4C00) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// stwu r1,-96(r1)
	ea = -96 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r11,32(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 32);
	// lwz r9,296(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 296);
	// mr r7,r11
	ctx.r7.u64 = ctx.r11.u64;
	// stw r10,300(r8)
	PPC_STORE_U32(ctx.r8.u32 + 300, ctx.r10.u32);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// bne cr6,0x825f4c8c
	if (!ctx.cr6.eq) goto loc_825F4C8C;
	// lwz r10,12(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r9,36(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 36);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// bne cr6,0x825f4c4c
	if (!ctx.cr6.eq) goto loc_825F4C4C;
	// lwz r10,16(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// lwz r9,40(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 40);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// beq cr6,0x825f4c54
	if (ctx.cr6.eq) goto loc_825F4C54;
loc_825F4C4C:
	// li r10,1
	ctx.r10.s64 = 1;
	// stw r10,300(r8)
	PPC_STORE_U32(ctx.r8.u32 + 300, ctx.r10.u32);
loc_825F4C54:
	// lwz r10,28(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// lwz r9,44(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 44);
	// cmpw cr6,r10,r9
	ctx.cr6.compare<int32_t>(ctx.r10.s32, ctx.r9.s32, ctx.xer);
	// bne cr6,0x825f4c70
	if (!ctx.cr6.eq) goto loc_825F4C70;
	// lwz r10,48(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 48);
	// cmpw cr6,r11,r10
	ctx.cr6.compare<int32_t>(ctx.r11.s32, ctx.r10.s32, ctx.xer);
	// beq cr6,0x825f4c7c
	if (ctx.cr6.eq) goto loc_825F4C7C;
loc_825F4C70:
	// lwz r11,300(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 300);
	// addi r11,r11,2
	ctx.r11.s64 = ctx.r11.s64 + 2;
	// stw r11,300(r8)
	PPC_STORE_U32(ctx.r8.u32 + 300, ctx.r11.u32);
loc_825F4C7C:
	// lwz r11,0(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r10,16(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// stw r10,8(r11)
	PPC_STORE_U32(ctx.r11.u32 + 8, ctx.r10.u32);
	// b 0x825f4cb0
	goto loc_825F4CB0;
loc_825F4C8C:
	// lwz r11,16(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// li r10,3
	ctx.r10.s64 = 3;
	// lwz r9,0(r8)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// srawi r11,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 1;
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// srawi r7,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r7.s32 >> 1;
	// stw r10,300(r8)
	PPC_STORE_U32(ctx.r8.u32 + 300, ctx.r10.u32);
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// stw r11,8(r9)
	PPC_STORE_U32(ctx.r9.u32 + 8, ctx.r11.u32);
loc_825F4CB0:
	// lwz r11,0(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// mr r3,r8
	ctx.r3.u64 = ctx.r8.u64;
	// lwz r10,12(r8)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// stw r10,4(r11)
	PPC_STORE_U32(ctx.r11.u32 + 4, ctx.r10.u32);
	// lwz r6,0(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r5,8(r6)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r6.u32 + 8);
	// lwz r4,4(r6)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r6.u32 + 4);
	// bl 0x825f4b60
	ctx.lr = 0x825F4CD0;
	sub_825F4B60(ctx, base);
	// stw r3,20(r6)
	PPC_STORE_U32(ctx.r6.u32 + 20, ctx.r3.u32);
	// lwz r11,292(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 292);
	// lwz r6,28(r8)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r8.u32 + 28);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// lwz r11,0(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 0);
	// lwz r4,12(r8)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r8.u32 + 12);
	// lwz r5,8(r11)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// beq cr6,0x825f4d0c
	if (ctx.cr6.eq) goto loc_825F4D0C;
	// addi r3,r8,156
	ctx.r3.s64 = ctx.r8.s64 + 156;
	// bl 0x82645578
	ctx.lr = 0x825F4CF8;
	sub_82645578(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
loc_825F4D0C:
	// addi r3,r8,52
	ctx.r3.s64 = ctx.r8.s64 + 52;
	// bl 0x82645708
	ctx.lr = 0x825F4D14;
	sub_82645708(ctx, base);
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,96
	ctx.r1.s64 = ctx.r1.s64 + 96;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F4D28"))) PPC_WEAK_FUNC(sub_825F4D28);
PPC_FUNC_IMPL(__imp__sub_825F4D28) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba08
	ctx.lr = 0x825F4D30;
	sub_8239BA08(ctx, base);
	// stwu r1,-160(r1)
	ea = -160 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r24,r4
	ctx.r24.u64 = ctx.r4.u64;
	// mr r28,r3
	ctx.r28.u64 = ctx.r3.u64;
	// mr r26,r5
	ctx.r26.u64 = ctx.r5.u64;
	// mr r25,r6
	ctx.r25.u64 = ctx.r6.u64;
	// mr r27,r7
	ctx.r27.u64 = ctx.r7.u64;
	// cmplwi cr6,r24,0
	ctx.cr6.compare<uint32_t>(ctx.r24.u32, 0, ctx.xer);
	// bne cr6,0x825f4d5c
	if (!ctx.cr6.eq) goto loc_825F4D5C;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825F4D5C:
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// beq cr6,0x825f4da0
	if (ctx.cr6.eq) goto loc_825F4DA0;
	// lwz r30,16(r28)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r28.u32 + 16);
	// li r6,0
	ctx.r6.s64 = 0;
	// mr r5,r25
	ctx.r5.u64 = ctx.r25.u64;
	// lhz r29,14(r28)
	ctx.r29.u64 = PPC_LOAD_U16(ctx.r28.u32 + 14);
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825f48c8
	ctx.lr = 0x825F4D80;
	sub_825F48C8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f4da0
	if (!ctx.cr6.eq) goto loc_825F4DA0;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// beq cr6,0x825f4f1c
	if (ctx.cr6.eq) goto loc_825F4F1C;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825f4db4
	if (!ctx.cr6.eq) goto loc_825F4DB4;
	// cmplwi cr6,r29,32
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 32, ctx.xer);
	// beq cr6,0x825f4e24
	if (ctx.cr6.eq) goto loc_825F4E24;
loc_825F4DA0:
	// li r11,0
	ctx.r11.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825F4DB4:
	// lis r11,22870
	ctx.r11.s64 = 1498808320;
	// ori r11,r11,22869
	ctx.r11.u64 = ctx.r11.u64 | 22869;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4e24
	if (ctx.cr6.eq) goto loc_825F4E24;
	// lis r11,12889
	ctx.r11.s64 = 844693504;
	// ori r11,r11,21849
	ctx.r11.u64 = ctx.r11.u64 | 21849;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4e24
	if (ctx.cr6.eq) goto loc_825F4E24;
	// lis r11,22101
	ctx.r11.s64 = 1448411136;
	// ori r11,r11,22857
	ctx.r11.u64 = ctx.r11.u64 | 22857;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4e24
	if (ctx.cr6.eq) goto loc_825F4E24;
	// lis r11,12338
	ctx.r11.s64 = 808583168;
	// ori r11,r11,13385
	ctx.r11.u64 = ctx.r11.u64 | 13385;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4e24
	if (ctx.cr6.eq) goto loc_825F4E24;
	// lis r11,12849
	ctx.r11.s64 = 842072064;
	// ori r11,r11,22105
	ctx.r11.u64 = ctx.r11.u64 | 22105;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4e24
	if (ctx.cr6.eq) goto loc_825F4E24;
	// lis r11,12850
	ctx.r11.s64 = 842137600;
	// ori r11,r11,13392
	ctx.r11.u64 = ctx.r11.u64 | 13392;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f4e24
	if (ctx.cr6.eq) goto loc_825F4E24;
	// lis r11,12593
	ctx.r11.s64 = 825294848;
	// ori r11,r11,13392
	ctx.r11.u64 = ctx.r11.u64 | 13392;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// bne cr6,0x825f4e6c
	if (!ctx.cr6.eq) goto loc_825F4E6C;
loc_825F4E24:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,312
	ctx.r3.s64 = 312;
	// bl 0x825edb18
	ctx.lr = 0x825F4E30;
	sub_825EDB18(ctx, base);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825f4da0
	if (ctx.cr6.eq) goto loc_825F4DA0;
	// bl 0x825f4048
	ctx.lr = 0x825F4E40;
	sub_825F4048(ctx, base);
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,40
	ctx.r3.s64 = 40;
	// bl 0x825edb18
	ctx.lr = 0x825F4E4C;
	sub_825EDB18(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// bne cr6,0x825f4e80
	if (!ctx.cr6.eq) goto loc_825F4E80;
loc_825F4E5C:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f40d0
	ctx.lr = 0x825F4E64;
	sub_825F40D0(ctx, base);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825edb28
	ctx.lr = 0x825F4E6C;
	sub_825EDB28(ctx, base);
loc_825F4E6C:
	// li r11,0
	ctx.r11.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825F4E80:
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// li r9,10
	ctx.r9.s64 = 10;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_825F4E8C:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x825f4e8c
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_825F4E8C;
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r11,8(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x825f4eb4
	if (ctx.cr6.gt) goto loc_825F4EB4;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
loc_825F4EB4:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r11.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r11,20(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f4edc
	if (!ctx.cr6.eq) goto loc_825F4EDC;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	// bl 0x825f4b60
	ctx.lr = 0x825F4ED8;
	sub_825F4B60(ctx, base);
	// stw r3,20(r8)
	PPC_STORE_U32(ctx.r8.u32 + 20, ctx.r3.u32);
loc_825F4EDC:
	// mr r7,r25
	ctx.r7.u64 = ctx.r25.u64;
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	// mr r6,r26
	ctx.r6.u64 = ctx.r26.u64;
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	// addi r3,r31,156
	ctx.r3.s64 = ctx.r31.s64 + 156;
	// bl 0x82645578
	ctx.lr = 0x825F4EF4;
	sub_82645578(ctx, base);
	// li r11,-1
	ctx.r11.s64 = -1;
	// stw r26,28(r31)
	PPC_STORE_U32(ctx.r31.u32 + 28, ctx.r26.u32);
	// li r3,0
	ctx.r3.s64 = 0;
	// stw r25,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r25.u32);
	// stw r27,292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 292, ctx.r27.u32);
	// stw r11,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r11.u32);
	// stw r11,24(r31)
	PPC_STORE_U32(ctx.r31.u32 + 24, ctx.r11.u32);
	// stw r31,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r31.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825F4F1C:
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// beq cr6,0x825f4fdc
	if (ctx.cr6.eq) goto loc_825F4FDC;
	// cmpwi cr6,r30,3
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 3, ctx.xer);
	// bne cr6,0x825f4f58
	if (!ctx.cr6.eq) goto loc_825F4F58;
	// cmpwi cr6,r29,15
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 15, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// cmpwi cr6,r29,16
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 16, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// cmpwi cr6,r29,32
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 32, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// li r11,0
	ctx.r11.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825F4F58:
	// lis r11,12889
	ctx.r11.s64 = 844693504;
	// ori r11,r11,21849
	ctx.r11.u64 = ctx.r11.u64 | 21849;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// lis r11,22870
	ctx.r11.s64 = 1498808320;
	// ori r11,r11,22869
	ctx.r11.u64 = ctx.r11.u64 | 22869;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// lis r11,14677
	ctx.r11.s64 = 961871872;
	// ori r11,r11,22105
	ctx.r11.u64 = ctx.r11.u64 | 22105;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// lis r11,22101
	ctx.r11.s64 = 1448411136;
	// ori r11,r11,22857
	ctx.r11.u64 = ctx.r11.u64 | 22857;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// lis r11,12338
	ctx.r11.s64 = 808583168;
	// ori r11,r11,13385
	ctx.r11.u64 = ctx.r11.u64 | 13385;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// lis r11,12849
	ctx.r11.s64 = 842072064;
	// ori r11,r11,22105
	ctx.r11.u64 = ctx.r11.u64 | 22105;
	// cmpw cr6,r30,r11
	ctx.cr6.compare<int32_t>(ctx.r30.s32, ctx.r11.s32, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// lis r11,12850
	ctx.r11.s64 = 842137600;
	// ori r11,r11,13392
	ctx.r11.u64 = ctx.r11.u64 | 13392;
	// cmplw cr6,r30,r11
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, ctx.r11.u32, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// li r11,0
	ctx.r11.s64 = 0;
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r11.u32);
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
loc_825F4FDC:
	// cmpwi cr6,r29,8
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 8, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// cmpwi cr6,r29,16
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 16, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// cmpwi cr6,r29,24
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 24, ctx.xer);
	// beq cr6,0x825f4ffc
	if (ctx.cr6.eq) goto loc_825F4FFC;
	// cmpwi cr6,r29,32
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 32, ctx.xer);
	// bne cr6,0x825f4da0
	if (!ctx.cr6.eq) goto loc_825F4DA0;
loc_825F4FFC:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,312
	ctx.r3.s64 = 312;
	// bl 0x825edb18
	ctx.lr = 0x825F5008;
	sub_825EDB18(ctx, base);
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825f4da0
	if (ctx.cr6.eq) goto loc_825F4DA0;
	// bl 0x825f4048
	ctx.lr = 0x825F5018;
	sub_825F4048(ctx, base);
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// bne cr6,0x825f5050
	if (!ctx.cr6.eq) goto loc_825F5050;
	// cmpwi cr6,r29,8
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 8, ctx.xer);
	// bne cr6,0x825f5080
	if (!ctx.cr6.eq) goto loc_825F5080;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,1064
	ctx.r3.s64 = 1064;
	// bl 0x825edb18
	ctx.lr = 0x825F5034;
	sub_825EDB18(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r3.u32);
	// beq cr6,0x825f4e5c
	if (ctx.cr6.eq) goto loc_825F4E5C;
	// li r5,1064
	ctx.r5.s64 = 1064;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F504C;
	sub_8239CB70(ctx, base);
	// b 0x825f50bc
	goto loc_825F50BC;
loc_825F5050:
	// cmpwi cr6,r30,3
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 3, ctx.xer);
	// bne cr6,0x825f5080
	if (!ctx.cr6.eq) goto loc_825F5080;
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,52
	ctx.r3.s64 = 52;
	// bl 0x825edb18
	ctx.lr = 0x825F5064;
	sub_825EDB18(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r3.u32);
	// beq cr6,0x825f4e5c
	if (ctx.cr6.eq) goto loc_825F4E5C;
	// li r5,52
	ctx.r5.s64 = 52;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F507C;
	sub_8239CB70(ctx, base);
	// b 0x825f50bc
	goto loc_825F50BC;
loc_825F5080:
	// li r4,0
	ctx.r4.s64 = 0;
	// li r3,40
	ctx.r3.s64 = 40;
	// bl 0x825edb18
	ctx.lr = 0x825F508C;
	sub_825EDB18(ctx, base);
	// mr r10,r3
	ctx.r10.u64 = ctx.r3.u64;
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// stw r10,0(r31)
	PPC_STORE_U32(ctx.r31.u32 + 0, ctx.r10.u32);
	// beq cr6,0x825f4e5c
	if (ctx.cr6.eq) goto loc_825F4E5C;
	// mr r11,r28
	ctx.r11.u64 = ctx.r28.u64;
	// li r9,10
	ctx.r9.s64 = 10;
	// mtctr r9
	ctx.ctr.u64 = ctx.r9.u64;
loc_825F50A8:
	// lwz r9,0(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 0);
	// addi r11,r11,4
	ctx.r11.s64 = ctx.r11.s64 + 4;
	// stw r9,0(r10)
	PPC_STORE_U32(ctx.r10.u32 + 0, ctx.r9.u32);
	// addi r10,r10,4
	ctx.r10.s64 = ctx.r10.s64 + 4;
	// bdnz 0x825f50a8
	--ctx.ctr.u64;
	if (ctx.ctr.u32 != 0) goto loc_825F50A8;
loc_825F50BC:
	// lwz r10,0(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r11,8(r10)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r10.u32 + 8);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x825f50d0
	if (ctx.cr6.gt) goto loc_825F50D0;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
loc_825F50D0:
	// stw r11,8(r10)
	PPC_STORE_U32(ctx.r10.u32 + 8, ctx.r11.u32);
	// lwz r8,0(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r11,20(r8)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r8.u32 + 20);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// bne cr6,0x825f50f8
	if (!ctx.cr6.eq) goto loc_825F50F8;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	// bl 0x825f4b60
	ctx.lr = 0x825F50F4;
	sub_825F4B60(ctx, base);
	// stw r3,20(r8)
	PPC_STORE_U32(ctx.r8.u32 + 20, ctx.r3.u32);
loc_825F50F8:
	// mr r7,r25
	ctx.r7.u64 = ctx.r25.u64;
	// lwz r5,8(r28)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r28.u32 + 8);
	// mr r6,r26
	ctx.r6.u64 = ctx.r26.u64;
	// lwz r4,4(r28)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r28.u32 + 4);
	// addi r3,r31,52
	ctx.r3.s64 = ctx.r31.s64 + 52;
	// bl 0x82645648
	ctx.lr = 0x825F5110;
	sub_82645648(ctx, base);
	// li r11,-1
	ctx.r11.s64 = -1;
	// li r10,0
	ctx.r10.s64 = 0;
	// stw r26,28(r31)
	PPC_STORE_U32(ctx.r31.u32 + 28, ctx.r26.u32);
	// stw r25,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r25.u32);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// li r3,1
	ctx.r3.s64 = 1;
	// stw r11,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r11.u32);
	// stw r11,24(r31)
	PPC_STORE_U32(ctx.r31.u32 + 24, ctx.r11.u32);
	// stw r10,292(r31)
	PPC_STORE_U32(ctx.r31.u32 + 292, ctx.r10.u32);
	// stw r31,0(r24)
	PPC_STORE_U32(ctx.r24.u32 + 0, ctx.r31.u32);
	// beq cr6,0x825f5140
	if (ctx.cr6.eq) goto loc_825F5140;
	// li r3,0
	ctx.r3.s64 = 0;
loc_825F5140:
	// addi r1,r1,160
	ctx.r1.s64 = ctx.r1.s64 + 160;
	// b 0x8239ba58
	// ERROR 8239BA58
	return;
}

__attribute__((alias("__imp__sub_825F5148"))) PPC_WEAK_FUNC(sub_825F5148);
PPC_FUNC_IMPL(__imp__sub_825F5148) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba00
	ctx.lr = 0x825F5150;
	sub_8239BA00(ctx, base);
	// stwu r1,-224(r1)
	ea = -224 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// mr r29,r5
	ctx.r29.u64 = ctx.r5.u64;
	// mr r26,r6
	ctx.r26.u64 = ctx.r6.u64;
	// mr r25,r7
	ctx.r25.u64 = ctx.r7.u64;
	// mr r24,r8
	ctx.r24.u64 = ctx.r8.u64;
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// mr r23,r9
	ctx.r23.u64 = ctx.r9.u64;
	// lwz r10,300(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 300);
	// cmpwi cr6,r10,0
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 0, ctx.xer);
	// lwz r28,16(r11)
	ctx.r28.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// lhz r27,14(r11)
	ctx.r27.u64 = PPC_LOAD_U16(ctx.r11.u32 + 14);
	// bne cr6,0x825f5190
	if (!ctx.cr6.eq) goto loc_825F5190;
	// bl 0x825f4150
	ctx.lr = 0x825F5188;
	sub_825F4150(ctx, base);
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
loc_825F5190:
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// bne cr6,0x825f5228
	if (!ctx.cr6.eq) goto loc_825F5228;
	// lwz r6,304(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 304);
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x825f53a0
	if (ctx.cr6.eq) goto loc_825F53A0;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// mr r22,r9
	ctx.r22.u64 = ctx.r9.u64;
	// bgt cr6,0x825f51bc
	if (ctx.cr6.gt) goto loc_825F51BC;
	// neg r22,r9
	ctx.r22.s64 = -ctx.r9.s64;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
loc_825F51BC:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r30,0
	ctx.r30.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r7,40(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// lwz r6,36(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r27.u32);
	// stw r28,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r28.u32);
	// stw r22,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r22.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r30.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r30.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// bl 0x825f44d0
	ctx.lr = 0x825F5200;
	sub_825F44D0(ctx, base);
	// mr r9,r23
	ctx.r9.u64 = ctx.r23.u64;
	// lwz r4,304(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 304);
	// mr r8,r24
	ctx.r8.u64 = ctx.r24.u64;
	// mr r7,r25
	ctx.r7.u64 = ctx.r25.u64;
	// mr r6,r26
	ctx.r6.u64 = ctx.r26.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f4150
	ctx.lr = 0x825F5220;
	sub_825F4150(ctx, base);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// b 0x825f5390
	goto loc_825F5390;
loc_825F5228:
	// cmpwi cr6,r10,2
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 2, ctx.xer);
	// bne cr6,0x825f52a8
	if (!ctx.cr6.eq) goto loc_825F52A8;
	// lwz r7,308(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 308);
	// cmplwi cr6,r7,0
	ctx.cr6.compare<uint32_t>(ctx.r7.u32, 0, ctx.xer);
	// beq cr6,0x825f53a0
	if (ctx.cr6.eq) goto loc_825F53A0;
	// mr r9,r23
	ctx.r9.u64 = ctx.r23.u64;
	// mr r8,r24
	ctx.r8.u64 = ctx.r24.u64;
	// mr r6,r26
	ctx.r6.u64 = ctx.r26.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f4150
	ctx.lr = 0x825F5254;
	sub_825F4150(ctx, base);
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// li r11,0
	ctx.r11.s64 = 0;
	// lwz r30,24(r31)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lwz r24,20(r31)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// li r10,0
	ctx.r10.s64 = 0;
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r8,44(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// lwz r5,308(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 308);
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r27.u32);
	// stw r28,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r28.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r30.u32);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r24.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// stw r6,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r6.u32);
	// bl 0x825f44d0
	ctx.lr = 0x825F52A4;
	sub_825F44D0(ctx, base);
	// b 0x825f5390
	goto loc_825F5390;
loc_825F52A8:
	// cmpwi cr6,r10,3
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 3, ctx.xer);
	// bne cr6,0x825f53a0
	if (!ctx.cr6.eq) goto loc_825F53A0;
	// lwz r10,308(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 308);
	// cmplwi cr6,r10,0
	ctx.cr6.compare<uint32_t>(ctx.r10.u32, 0, ctx.xer);
	// beq cr6,0x825f53a0
	if (ctx.cr6.eq) goto loc_825F53A0;
	// lwz r6,304(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 304);
	// cmplwi cr6,r6,0
	ctx.cr6.compare<uint32_t>(ctx.r6.u32, 0, ctx.xer);
	// beq cr6,0x825f53a0
	if (ctx.cr6.eq) goto loc_825F53A0;
	// lwz r9,8(r11)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r11.u32 + 8);
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// mr r22,r9
	ctx.r22.u64 = ctx.r9.u64;
	// bgt cr6,0x825f52e0
	if (ctx.cr6.gt) goto loc_825F52E0;
	// neg r22,r9
	ctx.r22.s64 = -ctx.r9.s64;
	// neg r9,r9
	ctx.r9.s64 = -ctx.r9.s64;
loc_825F52E0:
	// lwz r8,4(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 4);
	// li r30,0
	ctx.r30.s64 = 0;
	// lwz r11,8(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 8);
	// mr r5,r4
	ctx.r5.u64 = ctx.r4.u64;
	// mr r4,r6
	ctx.r4.u64 = ctx.r6.u64;
	// lwz r10,4(r31)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r31.u32 + 4);
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r7,40(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// lwz r6,36(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r27.u32);
	// stw r28,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r28.u32);
	// stw r22,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r22.u32);
	// stw r30,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r30.u32);
	// stw r30,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r30.u32);
	// stw r11,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r11.u32);
	// stw r8,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r8.u32);
	// bl 0x825f44d0
	ctx.lr = 0x825F5324;
	sub_825F44D0(ctx, base);
	// mr r9,r23
	ctx.r9.u64 = ctx.r23.u64;
	// lwz r7,308(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 308);
	// mr r8,r24
	ctx.r8.u64 = ctx.r24.u64;
	// lwz r4,304(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 304);
	// mr r6,r26
	ctx.r6.u64 = ctx.r26.u64;
	// mr r5,r29
	ctx.r5.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x825f4150
	ctx.lr = 0x825F5344;
	sub_825F4150(ctx, base);
	// lwz r7,32(r31)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r31.u32 + 32);
	// lwz r6,28(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 28);
	// mr r29,r3
	ctx.r29.u64 = ctx.r3.u64;
	// lwz r11,24(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 24);
	// li r10,0
	ctx.r10.s64 = 0;
	// lwz r24,20(r31)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r31.u32 + 20);
	// mr r4,r25
	ctx.r4.u64 = ctx.r25.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r9,48(r31)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// lwz r8,44(r31)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// lwz r5,308(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 308);
	// stw r27,132(r1)
	PPC_STORE_U32(ctx.r1.u32 + 132, ctx.r27.u32);
	// stw r28,124(r1)
	PPC_STORE_U32(ctx.r1.u32 + 124, ctx.r28.u32);
	// stw r11,100(r1)
	PPC_STORE_U32(ctx.r1.u32 + 100, ctx.r11.u32);
	// stw r24,92(r1)
	PPC_STORE_U32(ctx.r1.u32 + 92, ctx.r24.u32);
	// stw r30,84(r1)
	PPC_STORE_U32(ctx.r1.u32 + 84, ctx.r30.u32);
	// stw r7,116(r1)
	PPC_STORE_U32(ctx.r1.u32 + 116, ctx.r7.u32);
	// stw r6,108(r1)
	PPC_STORE_U32(ctx.r1.u32 + 108, ctx.r6.u32);
	// bl 0x825f44d0
	ctx.lr = 0x825F5390;
	sub_825F44D0(ctx, base);
loc_825F5390:
	// lwz r11,0(r26)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r26.u32 + 0);
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bne cr6,0x825f53a4
	if (!ctx.cr6.eq) goto loc_825F53A4;
loc_825F53A0:
	// li r3,1
	ctx.r3.s64 = 1;
loc_825F53A4:
	// addi r1,r1,224
	ctx.r1.s64 = ctx.r1.s64 + 224;
	// b 0x8239ba50
	// ERROR 8239BA50
	return;
}

__attribute__((alias("__imp__sub_825F53AC"))) PPC_WEAK_FUNC(sub_825F53AC);
PPC_FUNC_IMPL(__imp__sub_825F53AC) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F53B0"))) PPC_WEAK_FUNC(sub_825F53B0);
PPC_FUNC_IMPL(__imp__sub_825F53B0) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9f8
	ctx.lr = 0x825F53B8;
	sub_8239B9F8(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x825f583c
	if (ctx.cr6.eq) goto loc_825F583C;
	// lwz r31,304(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 304);
	// cmplwi cr6,r31,0
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 0, ctx.xer);
	// beq cr6,0x825f583c
	if (ctx.cr6.eq) goto loc_825F583C;
	// bl 0x825f4af8
	ctx.lr = 0x825F53D8;
	sub_825F4AF8(ctx, base);
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// bne cr6,0x825f5698
	if (!ctx.cr6.eq) goto loc_825F5698;
	// lwz r8,0(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lis r11,22101
	ctx.r11.s64 = 1448411136;
	// li r10,0
	ctx.r10.s64 = 0;
	// ori r30,r11,22857
	ctx.r30.u64 = ctx.r11.u64 | 22857;
	// li r11,0
	ctx.r11.s64 = 0;
	// li r23,0
	ctx.r23.s64 = 0;
	// lwz r3,16(r8)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r8.u32 + 16);
	// li r24,0
	ctx.r24.s64 = 0;
	// li r21,0
	ctx.r21.s64 = 0;
	// li r29,0
	ctx.r29.s64 = 0;
	// li r22,0
	ctx.r22.s64 = 0;
	// li r20,0
	ctx.r20.s64 = 0;
	// li r8,0
	ctx.r8.s64 = 0;
	// li r6,0
	ctx.r6.s64 = 0;
	// li r7,0
	ctx.r7.s64 = 0;
	// cmpw cr6,r3,r30
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r30.s32, ctx.xer);
	// beq cr6,0x825f5510
	if (ctx.cr6.eq) goto loc_825F5510;
	// lis r30,12338
	ctx.r30.s64 = 808583168;
	// ori r30,r30,13385
	ctx.r30.u64 = ctx.r30.u64 | 13385;
	// cmpw cr6,r3,r30
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r30.s32, ctx.xer);
	// beq cr6,0x825f5510
	if (ctx.cr6.eq) goto loc_825F5510;
	// lis r30,12849
	ctx.r30.s64 = 842072064;
	// ori r30,r30,22105
	ctx.r30.u64 = ctx.r30.u64 | 22105;
	// cmpw cr6,r3,r30
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r30.s32, ctx.xer);
	// beq cr6,0x825f5510
	if (ctx.cr6.eq) goto loc_825F5510;
	// lis r30,12593
	ctx.r30.s64 = 825294848;
	// ori r30,r30,13392
	ctx.r30.u64 = ctx.r30.u64 | 13392;
	// cmpw cr6,r3,r30
	ctx.cr6.compare<int32_t>(ctx.r3.s32, ctx.r30.s32, ctx.xer);
	// bne cr6,0x825f55ec
	if (!ctx.cr6.eq) goto loc_825F55EC;
	// lwz r11,36(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// lwz r8,12(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// srawi r10,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 2;
	// lwz r7,16(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// mr r23,r8
	ctx.r23.u64 = ctx.r8.u64;
	// lwz r6,40(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r5,r8,2
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 2;
	// addze r24,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r24.s64 = temp.s64;
	// srawi r8,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 1;
	// mr r21,r24
	ctx.r21.u64 = ctx.r24.u64;
	// addze r29,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r29.s64 = temp.s64;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mr r22,r29
	ctx.r22.u64 = ctx.r29.u64;
	// mr r20,r29
	ctx.r20.u64 = ctx.r29.u64;
	// bne cr6,0x825f54d8
	if (!ctx.cr6.eq) goto loc_825F54D8;
	// lwz r7,8(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// mullw r9,r6,r11
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r11.s32);
	// rlwinm r3,r9,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r5,r7,r10
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// mullw r7,r7,r11
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
	// srawi r6,r8,2
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 2;
	// add r3,r9,r3
	ctx.r3.u64 = ctx.r9.u64 + ctx.r3.u64;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// add r8,r7,r8
	ctx.r8.u64 = ctx.r7.u64 + ctx.r8.u64;
	// srawi r7,r3,2
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r3.s32 >> 2;
	// addze r3,r7
	temp.s64 = ctx.r7.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r3.s64 = temp.s64;
	// add r7,r6,r9
	ctx.r7.u64 = ctx.r6.u64 + ctx.r9.u64;
	// add r9,r3,r6
	ctx.r9.u64 = ctx.r3.u64 + ctx.r6.u64;
	// add r6,r7,r5
	ctx.r6.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r7,r9,r5
	ctx.r7.u64 = ctx.r9.u64 + ctx.r5.u64;
	// b 0x825f55ec
	goto loc_825F55EC;
loc_825F54D8:
	// mullw r6,r6,r11
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r11.s32);
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// rlwinm r5,r6,2,0,29
	ctx.r5.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r7,r8,2
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 2;
	// add r5,r6,r5
	ctx.r5.u64 = ctx.r6.u64 + ctx.r5.u64;
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// srawi r5,r5,2
	ctx.xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 2;
	// addi r9,r9,1
	ctx.r9.s64 = ctx.r9.s64 + 1;
	// addze r3,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r3.s64 = temp.s64;
	// mullw r5,r9,r10
	ctx.r5.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// mullw r9,r9,r11
	ctx.r9.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// add r8,r9,r8
	ctx.r8.u64 = ctx.r9.u64 + ctx.r8.u64;
	// add r3,r3,r7
	ctx.r3.u64 = ctx.r3.u64 + ctx.r7.u64;
	// b 0x825f55e0
	goto loc_825F55E0;
loc_825F5510:
	// lwz r11,36(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// lwz r8,12(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// lwz r7,16(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// mr r23,r8
	ctx.r23.u64 = ctx.r8.u64;
	// lwz r6,40(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r5,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r8.s32 >> 1;
	// addze r24,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r24.s64 = temp.s64;
	// srawi r8,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r7.s32 >> 1;
	// mr r21,r24
	ctx.r21.u64 = ctx.r24.u64;
	// addze r29,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r29.s64 = temp.s64;
	// srawi r8,r29,1
	ctx.xer.ca = (ctx.r29.s32 < 0) & ((ctx.r29.u32 & 0x1) != 0);
	ctx.r8.s64 = ctx.r29.s32 >> 1;
	// addze r22,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r22.s64 = temp.s64;
	// lwz r8,8(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// mr r20,r22
	ctx.r20.u64 = ctx.r22.u64;
	// bne cr6,0x825f55a0
	if (!ctx.cr6.eq) goto loc_825F55A0;
	// lwz r7,4(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mullw r9,r6,r11
	ctx.r9.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r11.s32);
	// mullw r6,r8,r10
	ctx.r6.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// srawi r5,r6,1
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r6.s32 >> 1;
	// rlwinm r6,r9,2,0,29
	ctx.r6.u64 = __builtin_rotateleft64(ctx.r9.u32 | (ctx.r9.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r3,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r7.s32 >> 1;
	// add r30,r9,r6
	ctx.r30.u64 = ctx.r9.u64 + ctx.r6.u64;
	// addze r6,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r6.s64 = temp.s64;
	// srawi r3,r30,2
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r30.s32 >> 2;
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
	// addze r30,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r30.s64 = temp.s64;
	// add r8,r8,r7
	ctx.r8.u64 = ctx.r8.u64 + ctx.r7.u64;
	// add r7,r30,r6
	ctx.r7.u64 = ctx.r30.u64 + ctx.r6.u64;
	// add r3,r6,r5
	ctx.r3.u64 = ctx.r6.u64 + ctx.r5.u64;
	// add r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r6,r3,r9
	ctx.r6.u64 = ctx.r3.u64 + ctx.r9.u64;
	// b 0x825f55ec
	goto loc_825F55EC;
loc_825F55A0:
	// srawi r7,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r8.s32 >> 1;
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// mullw r6,r6,r11
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r11.s32);
	// addze r7,r7
	temp.s64 = ctx.r7.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r7.s64 = temp.s64;
	// srawi r5,r9,1
	ctx.xer.ca = (ctx.r9.s32 < 0) & ((ctx.r9.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r9.s32 >> 1;
	// addi r3,r7,1
	ctx.r3.s64 = ctx.r7.s64 + 1;
	// addze r7,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r7.s64 = temp.s64;
	// mullw r5,r3,r10
	ctx.r5.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r10.s32);
	// rlwinm r3,r6,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r6.u32 | (ctx.r6.u64 << 32), 2) & 0xFFFFFFFC;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r3,r6,r3
	ctx.r3.u64 = ctx.r6.u64 + ctx.r3.u64;
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
	// srawi r3,r3,2
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 2;
	// add r8,r8,r9
	ctx.r8.u64 = ctx.r8.u64 + ctx.r9.u64;
	// addze r9,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r9.s64 = temp.s64;
	// add r3,r9,r7
	ctx.r3.u64 = ctx.r9.u64 + ctx.r7.u64;
loc_825F55E0:
	// add r9,r7,r5
	ctx.r9.u64 = ctx.r7.u64 + ctx.r5.u64;
	// add r7,r3,r5
	ctx.r7.u64 = ctx.r3.u64 + ctx.r5.u64;
	// add r6,r9,r6
	ctx.r6.u64 = ctx.r9.u64 + ctx.r6.u64;
loc_825F55EC:
	// add r30,r8,r4
	ctx.r30.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r28,r6,r4
	ctx.r28.u64 = ctx.r6.u64 + ctx.r4.u64;
	// add r26,r7,r4
	ctx.r26.u64 = ctx.r7.u64 + ctx.r4.u64;
	// rlwinm r27,r11,1,0,30
	ctx.r27.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r10,1,0,30
	ctx.r25.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// ble cr6,0x825f562c
	if (!ctx.cr6.gt) goto loc_825F562C;
loc_825F5608:
	// mr r5,r23
	ctx.r5.u64 = ctx.r23.u64;
	// mr r4,r30
	ctx.r4.u64 = ctx.r30.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F5618;
	sub_8239CB70(ctx, base);
	// addi r29,r29,-1
	ctx.r29.s64 = ctx.r29.s64 + -1;
	// add r30,r27,r30
	ctx.r30.u64 = ctx.r27.u64 + ctx.r30.u64;
	// add r31,r23,r31
	ctx.r31.u64 = ctx.r23.u64 + ctx.r31.u64;
	// cmplwi cr6,r29,0
	ctx.cr6.compare<uint32_t>(ctx.r29.u32, 0, ctx.xer);
	// bne cr6,0x825f5608
	if (!ctx.cr6.eq) goto loc_825F5608;
loc_825F562C:
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// ble cr6,0x825f565c
	if (!ctx.cr6.gt) goto loc_825F565C;
	// mr r30,r22
	ctx.r30.u64 = ctx.r22.u64;
loc_825F5638:
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F5648;
	sub_8239CB70(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r28,r25,r28
	ctx.r28.u64 = ctx.r25.u64 + ctx.r28.u64;
	// add r31,r24,r31
	ctx.r31.u64 = ctx.r24.u64 + ctx.r31.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f5638
	if (!ctx.cr6.eq) goto loc_825F5638;
loc_825F565C:
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x825f5804
	if (!ctx.cr6.gt) goto loc_825F5804;
	// mr r30,r20
	ctx.r30.u64 = ctx.r20.u64;
loc_825F5668:
	// mr r5,r21
	ctx.r5.u64 = ctx.r21.u64;
	// mr r4,r26
	ctx.r4.u64 = ctx.r26.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F5678;
	sub_8239CB70(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r26,r25,r26
	ctx.r26.u64 = ctx.r25.u64 + ctx.r26.u64;
	// add r31,r21,r31
	ctx.r31.u64 = ctx.r21.u64 + ctx.r31.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f5668
	if (!ctx.cr6.eq) goto loc_825F5668;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825F5698:
	// cmpwi cr6,r3,2
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 2, ctx.xer);
	// bne cr6,0x825f5810
	if (!ctx.cr6.eq) goto loc_825F5810;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// mr r29,r31
	ctx.r29.u64 = ctx.r31.u64;
	// lwz r10,36(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 36);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r3,12(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 12);
	// lwz r8,16(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 16);
	// lhz r11,14(r7)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 14);
	// lwz r31,16(r7)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// cmplwi cr6,r31,3
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 3, ctx.xer);
	// addi r31,r10,31
	ctx.r31.s64 = ctx.r10.s64 + 31;
	// mullw r10,r3,r11
	ctx.r10.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r11.s32);
	// rlwinm r3,r31,0,0,26
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 0) & 0xFFFFFFE0;
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// srawi r3,r3,3
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 3;
	// rlwinm r31,r10,0,0,26
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFE0;
	// addze r10,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r3,r31,3
	ctx.xer.ca = (ctx.r31.s32 < 0) & ((ctx.r31.u32 & 0x7) != 0);
	ctx.r3.s64 = ctx.r31.s32 >> 3;
	// rlwinm r27,r10,1,0,30
	ctx.r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addze r28,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r28.s64 = temp.s64;
	// srawi r3,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 1;
	// addze r30,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r30.s64 = temp.s64;
	// bgt cr6,0x825f570c
	if (ctx.cr6.gt) goto loc_825F570C;
	// lwz r7,8(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// ble cr6,0x825f570c
	if (!ctx.cr6.gt) goto loc_825F570C;
	// li r6,1
	ctx.r6.s64 = 1;
loc_825F570C:
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bne cr6,0x825f5770
	if (!ctx.cr6.eq) goto loc_825F5770;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// bne cr6,0x825f5738
	if (!ctx.cr6.eq) goto loc_825F5738;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// mullw r11,r8,r11
	ctx.r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// b 0x825f57d0
	goto loc_825F57D0;
loc_825F5738:
	// lwz r7,40(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// lwz r6,8(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// srawi r5,r7,31
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// lwz r9,4(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// xor r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// mullw r11,r9,r11
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// subf r9,r5,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r5.s64;
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// subf r7,r6,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r6.s64;
	// addze r9,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r9.s64 = temp.s64;
	// subf r11,r8,r7
	ctx.r11.s64 = ctx.r7.s64 - ctx.r8.s64;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// b 0x825f57d4
	goto loc_825F57D4;
loc_825F5770:
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// bne cr6,0x825f579c
	if (!ctx.cr6.eq) goto loc_825F579C;
	// lwz r8,4(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// lwz r9,8(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// mullw r11,r8,r11
	ctx.r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// addze r9,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r9.s64 = temp.s64;
	// mullw r11,r8,r10
	ctx.r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// b 0x825f57d4
	goto loc_825F57D4;
loc_825F579C:
	// lwz r7,40(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 40);
	// lwz r6,8(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 8);
	// srawi r5,r7,31
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// lwz r3,4(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 4);
	// subfic r9,r6,1
	ctx.xer.ca = ctx.r6.u32 <= 1;
	ctx.r9.s64 = 1 - ctx.r6.s64;
	// xor r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r9,r5,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r5.s64;
	// mullw r11,r3,r11
	ctx.r11.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r11.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r8,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r11.s32 >> 3;
	// mullw r11,r9,r10
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// addze r10,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r10.s64 = temp.s64;
loc_825F57D0:
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
loc_825F57D4:
	// add r31,r11,r4
	ctx.r31.u64 = ctx.r11.u64 + ctx.r4.u64;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// ble cr6,0x825f5804
	if (!ctx.cr6.gt) goto loc_825F5804;
loc_825F57E0:
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F57F0;
	sub_8239CB70(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r31,r27,r31
	ctx.r31.u64 = ctx.r27.u64 + ctx.r31.u64;
	// add r29,r29,r28
	ctx.r29.u64 = ctx.r29.u64 + ctx.r28.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f57e0
	if (!ctx.cr6.eq) goto loc_825F57E0;
loc_825F5804:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825F5810:
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f5830
	if (!ctx.cr6.eq) goto loc_825F5830;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// addi r3,r11,-22756
	ctx.r3.s64 = ctx.r11.s64 + -22756;
	// bl 0x82691b58
	ctx.lr = 0x825F5824;
	sub_82691B58(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825F5830:
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// addi r3,r11,-22768
	ctx.r3.s64 = ctx.r11.s64 + -22768;
	// bl 0x82691b58
	ctx.lr = 0x825F583C;
	sub_82691B58(ctx, base);
loc_825F583C:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
}

__attribute__((alias("__imp__sub_825F5848"))) PPC_WEAK_FUNC(sub_825F5848);
PPC_FUNC_IMPL(__imp__sub_825F5848) {
	PPC_FUNC_PROLOGUE();
	PPCRegister temp{};
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239b9f8
	ctx.lr = 0x825F5850;
	sub_8239B9F8(ctx, base);
	// stwu r1,-192(r1)
	ea = -192 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r9,r3
	ctx.r9.u64 = ctx.r3.u64;
	// cmplwi cr6,r4,0
	ctx.cr6.compare<uint32_t>(ctx.r4.u32, 0, ctx.xer);
	// beq cr6,0x825f5d60
	if (ctx.cr6.eq) goto loc_825F5D60;
	// lwz r11,304(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 304);
	// cmplwi cr6,r11,0
	ctx.cr6.compare<uint32_t>(ctx.r11.u32, 0, ctx.xer);
	// beq cr6,0x825f5d60
	if (ctx.cr6.eq) goto loc_825F5D60;
	// bl 0x825f4af8
	ctx.lr = 0x825F5870;
	sub_825F4AF8(ctx, base);
	// cmpwi cr6,r3,1
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 1, ctx.xer);
	// bne cr6,0x825f5bbc
	if (!ctx.cr6.eq) goto loc_825F5BBC;
	// lwz r11,0(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// lis r10,22101
	ctx.r10.s64 = 1448411136;
	// li r30,0
	ctx.r30.s64 = 0;
	// ori r7,r10,22857
	ctx.r7.u64 = ctx.r10.u64 | 22857;
	// li r29,0
	ctx.r29.s64 = 0;
	// li r27,0
	ctx.r27.s64 = 0;
	// lwz r8,16(r11)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// li r24,0
	ctx.r24.s64 = 0;
	// li r23,0
	ctx.r23.s64 = 0;
	// li r21,0
	ctx.r21.s64 = 0;
	// li r28,0
	ctx.r28.s64 = 0;
	// li r22,0
	ctx.r22.s64 = 0;
	// li r20,0
	ctx.r20.s64 = 0;
	// li r11,0
	ctx.r11.s64 = 0;
	// li r10,0
	ctx.r10.s64 = 0;
	// cmpw cr6,r8,r7
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, ctx.xer);
	// beq cr6,0x825f599c
	if (ctx.cr6.eq) goto loc_825F599C;
	// lis r7,12338
	ctx.r7.s64 = 808583168;
	// ori r7,r7,13385
	ctx.r7.u64 = ctx.r7.u64 | 13385;
	// cmpw cr6,r8,r7
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, ctx.xer);
	// beq cr6,0x825f599c
	if (ctx.cr6.eq) goto loc_825F599C;
	// lis r7,12849
	ctx.r7.s64 = 842072064;
	// ori r7,r7,22105
	ctx.r7.u64 = ctx.r7.u64 | 22105;
	// cmpw cr6,r8,r7
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, ctx.xer);
	// beq cr6,0x825f599c
	if (ctx.cr6.eq) goto loc_825F599C;
	// lis r7,12593
	ctx.r7.s64 = 825294848;
	// ori r7,r7,13392
	ctx.r7.u64 = ctx.r7.u64 | 13392;
	// cmpw cr6,r8,r7
	ctx.cr6.compare<int32_t>(ctx.r8.s32, ctx.r7.s32, ctx.xer);
	// bne cr6,0x825f5b18
	if (!ctx.cr6.eq) goto loc_825F5B18;
	// lwz r24,28(r9)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// lwz r10,32(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// srawi r7,r24,2
	ctx.xer.ca = (ctx.r24.s32 < 0) & ((ctx.r24.u32 & 0x3) != 0);
	ctx.r7.s64 = ctx.r24.s32 >> 2;
	// lwz r11,44(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	// lwz r8,48(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	// addze r23,r7
	temp.s64 = ctx.r7.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r23.s64 = temp.s64;
	// lwz r6,24(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// lwz r7,20(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// mr r21,r23
	ctx.r21.u64 = ctx.r23.u64;
	// addze r28,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r28.s64 = temp.s64;
	// srawi r10,r11,2
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x3) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 2;
	// mr r22,r28
	ctx.r22.u64 = ctx.r28.u64;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// mr r20,r28
	ctx.r20.u64 = ctx.r28.u64;
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
	// bne cr6,0x825f596c
	if (!ctx.cr6.eq) goto loc_825F596C;
	// rlwinm r31,r8,2,0,29
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r5,r6,r10
	ctx.r5.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r10.s32);
	// mullw r6,r6,r11
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r11.s32);
	// srawi r3,r7,2
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r7.s32 >> 2;
	// add r31,r8,r31
	ctx.r31.u64 = ctx.r8.u64 + ctx.r31.u64;
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// addze r3,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r3.s64 = temp.s64;
	// srawi r6,r31,2
	ctx.xer.ca = (ctx.r31.s32 < 0) & ((ctx.r31.u32 & 0x3) != 0);
	ctx.r6.s64 = ctx.r31.s32 >> 2;
	// add r8,r3,r8
	ctx.r8.u64 = ctx.r3.u64 + ctx.r8.u64;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// add r8,r8,r5
	ctx.r8.u64 = ctx.r8.u64 + ctx.r5.u64;
	// add r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// b 0x825f5b0c
	goto loc_825F5B0C;
loc_825F596C:
	// rlwinm r3,r8,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// srawi r5,r7,2
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x3) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 2;
	// add r3,r8,r3
	ctx.r3.u64 = ctx.r8.u64 + ctx.r3.u64;
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r3,r3,2
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x3) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 2;
	// addi r6,r6,1
	ctx.r6.s64 = ctx.r6.s64 + 1;
	// addze r31,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r31.s64 = temp.s64;
	// mullw r3,r6,r10
	ctx.r3.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r10.s32);
	// mullw r6,r6,r11
	ctx.r6.s64 = int64_t(ctx.r6.s32) * int64_t(ctx.r11.s32);
	// add r7,r6,r7
	ctx.r7.u64 = ctx.r6.u64 + ctx.r7.u64;
	// add r6,r31,r5
	ctx.r6.u64 = ctx.r31.u64 + ctx.r5.u64;
	// b 0x825f5b00
	goto loc_825F5B00;
loc_825F599C:
	// lwz r7,124(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 124);
	// lwz r24,28(r9)
	ctx.r24.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// lwz r10,32(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// srawi r7,r24,1
	ctx.xer.ca = (ctx.r24.s32 < 0) & ((ctx.r24.u32 & 0x1) != 0);
	ctx.r7.s64 = ctx.r24.s32 >> 1;
	// lwz r11,44(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	// lwz r8,48(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	// addze r23,r7
	temp.s64 = ctx.r7.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r7.u32;
	ctx.r23.s64 = temp.s64;
	// srawi r10,r10,1
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 1;
	// mr r21,r23
	ctx.r21.u64 = ctx.r23.u64;
	// addze r28,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r28.s64 = temp.s64;
	// srawi r10,r28,1
	ctx.xer.ca = (ctx.r28.s32 < 0) & ((ctx.r28.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r28.s32 >> 1;
	// addze r22,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r22.s64 = temp.s64;
	// srawi r10,r11,1
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x1) != 0);
	ctx.r10.s64 = ctx.r11.s32 >> 1;
	// mr r20,r22
	ctx.r20.u64 = ctx.r22.u64;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// beq cr6,0x825f5a74
	if (ctx.cr6.eq) goto loc_825F5A74;
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// lwz r11,144(r9)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r9.u32 + 144);
	// lwz r10,148(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 148);
	// lwz r5,152(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 152);
	// lwz r7,20(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// lwz r8,24(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// bne cr6,0x825f5a28
	if (!ctx.cr6.eq) goto loc_825F5A28;
	// srawi r6,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 1;
	// mullw r4,r8,r10
	ctx.r4.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// srawi r4,r4,1
	ctx.xer.ca = (ctx.r4.s32 < 0) & ((ctx.r4.u32 & 0x1) != 0);
	ctx.r4.s64 = ctx.r4.s32 >> 1;
	// mullw r5,r8,r5
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
	// addze r4,r4
	temp.s64 = ctx.r4.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r4.u32;
	ctx.r4.s64 = temp.s64;
	// srawi r5,r5,1
	ctx.xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// add r7,r8,r7
	ctx.r7.u64 = ctx.r8.u64 + ctx.r7.u64;
	// b 0x825f5a50
	goto loc_825F5A50;
loc_825F5A28:
	// srawi r6,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r8.s32 >> 1;
	// addi r4,r8,1
	ctx.r4.s64 = ctx.r8.s64 + 1;
	// addze r8,r6
	temp.s64 = ctx.r6.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r8.s64 = temp.s64;
	// mullw r4,r4,r11
	ctx.r4.s64 = int64_t(ctx.r4.s32) * int64_t(ctx.r11.s32);
	// srawi r6,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r6.s64 = ctx.r7.s32 >> 1;
	// addi r8,r8,1
	ctx.r8.s64 = ctx.r8.s64 + 1;
	// add r7,r4,r7
	ctx.r7.u64 = ctx.r4.u64 + ctx.r7.u64;
	// addze r6,r6
	temp.s64 = ctx.r6.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r6.u32;
	ctx.r6.s64 = temp.s64;
	// mullw r4,r8,r10
	ctx.r4.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// mullw r5,r8,r5
	ctx.r5.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r5.s32);
loc_825F5A50:
	// add r8,r4,r6
	ctx.r8.u64 = ctx.r4.u64 + ctx.r6.u64;
	// lwz r3,132(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 132);
	// add r6,r5,r6
	ctx.r6.u64 = ctx.r5.u64 + ctx.r6.u64;
	// lwz r4,136(r9)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r9.u32 + 136);
	// lwz r5,140(r9)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r9.u32 + 140);
	// add r30,r3,r7
	ctx.r30.u64 = ctx.r3.u64 + ctx.r7.u64;
	// add r29,r4,r8
	ctx.r29.u64 = ctx.r4.u64 + ctx.r8.u64;
	// add r27,r5,r6
	ctx.r27.u64 = ctx.r5.u64 + ctx.r6.u64;
	// b 0x825f5b18
	goto loc_825F5B18;
loc_825F5A74:
	// lwz r7,24(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// lwz r6,20(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// mullw r8,r8,r11
	ctx.r8.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
	// bne cr6,0x825f5ac8
	if (!ctx.cr6.eq) goto loc_825F5AC8;
	// mullw r5,r7,r10
	ctx.r5.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r10.s32);
	// srawi r5,r5,1
	ctx.xer.ca = (ctx.r5.s32 < 0) & ((ctx.r5.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r5.s32 >> 1;
	// rlwinm r3,r8,2,0,29
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r31,r6,1
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r31.s64 = ctx.r6.s32 >> 1;
	// add r30,r8,r3
	ctx.r30.u64 = ctx.r8.u64 + ctx.r3.u64;
	// addze r3,r31
	temp.s64 = ctx.r31.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r31.u32;
	ctx.r3.s64 = temp.s64;
	// srawi r31,r30,2
	ctx.xer.ca = (ctx.r30.s32 < 0) & ((ctx.r30.u32 & 0x3) != 0);
	ctx.r31.s64 = ctx.r30.s32 >> 2;
	// mullw r7,r7,r11
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
	// addze r30,r31
	temp.s64 = ctx.r31.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r31.u32;
	ctx.r30.s64 = temp.s64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// add r31,r3,r5
	ctx.r31.u64 = ctx.r3.u64 + ctx.r5.u64;
	// add r6,r30,r3
	ctx.r6.u64 = ctx.r30.u64 + ctx.r3.u64;
	// add r8,r31,r8
	ctx.r8.u64 = ctx.r31.u64 + ctx.r8.u64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
	// b 0x825f5b0c
	goto loc_825F5B0C;
loc_825F5AC8:
	// srawi r5,r7,1
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x1) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 1;
	// addi r7,r7,1
	ctx.r7.s64 = ctx.r7.s64 + 1;
	// addze r5,r5
	temp.s64 = ctx.r5.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r5.u32;
	ctx.r5.s64 = temp.s64;
	// srawi r3,r6,1
	ctx.xer.ca = (ctx.r6.s32 < 0) & ((ctx.r6.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r6.s32 >> 1;
	// addi r31,r5,1
	ctx.r31.s64 = ctx.r5.s64 + 1;
	// addze r5,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r5.s64 = temp.s64;
	// mullw r3,r31,r10
	ctx.r3.s64 = int64_t(ctx.r31.s32) * int64_t(ctx.r10.s32);
	// rlwinm r31,r8,2,0,29
	ctx.r31.u64 = __builtin_rotateleft64(ctx.r8.u32 | (ctx.r8.u64 << 32), 2) & 0xFFFFFFFC;
	// mullw r7,r7,r11
	ctx.r7.s64 = int64_t(ctx.r7.s32) * int64_t(ctx.r11.s32);
	// add r31,r8,r31
	ctx.r31.u64 = ctx.r8.u64 + ctx.r31.u64;
	// add r7,r7,r6
	ctx.r7.u64 = ctx.r7.u64 + ctx.r6.u64;
	// srawi r31,r31,2
	ctx.xer.ca = (ctx.r31.s32 < 0) & ((ctx.r31.u32 & 0x3) != 0);
	ctx.r31.s64 = ctx.r31.s32 >> 2;
	// addze r6,r31
	temp.s64 = ctx.r31.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r31.u32;
	ctx.r6.s64 = temp.s64;
	// add r6,r6,r5
	ctx.r6.u64 = ctx.r6.u64 + ctx.r5.u64;
loc_825F5B00:
	// add r5,r5,r3
	ctx.r5.u64 = ctx.r5.u64 + ctx.r3.u64;
	// add r6,r6,r3
	ctx.r6.u64 = ctx.r6.u64 + ctx.r3.u64;
	// add r8,r5,r8
	ctx.r8.u64 = ctx.r5.u64 + ctx.r8.u64;
loc_825F5B0C:
	// add r27,r6,r4
	ctx.r27.u64 = ctx.r6.u64 + ctx.r4.u64;
	// add r29,r8,r4
	ctx.r29.u64 = ctx.r8.u64 + ctx.r4.u64;
	// add r30,r7,r4
	ctx.r30.u64 = ctx.r7.u64 + ctx.r4.u64;
loc_825F5B18:
	// lwz r31,308(r9)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r9.u32 + 308);
	// rlwinm r26,r11,1,0,30
	ctx.r26.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 1) & 0xFFFFFFFE;
	// rlwinm r25,r10,1,0,30
	ctx.r25.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// cmpwi cr6,r28,0
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 0, ctx.xer);
	// ble cr6,0x825f5b50
	if (!ctx.cr6.gt) goto loc_825F5B50;
loc_825F5B2C:
	// mr r5,r24
	ctx.r5.u64 = ctx.r24.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F5B3C;
	sub_8239CB70(ctx, base);
	// addi r28,r28,-1
	ctx.r28.s64 = ctx.r28.s64 + -1;
	// add r30,r26,r30
	ctx.r30.u64 = ctx.r26.u64 + ctx.r30.u64;
	// add r31,r24,r31
	ctx.r31.u64 = ctx.r24.u64 + ctx.r31.u64;
	// cmplwi cr6,r28,0
	ctx.cr6.compare<uint32_t>(ctx.r28.u32, 0, ctx.xer);
	// bne cr6,0x825f5b2c
	if (!ctx.cr6.eq) goto loc_825F5B2C;
loc_825F5B50:
	// cmpwi cr6,r22,0
	ctx.cr6.compare<int32_t>(ctx.r22.s32, 0, ctx.xer);
	// ble cr6,0x825f5b80
	if (!ctx.cr6.gt) goto loc_825F5B80;
	// mr r30,r22
	ctx.r30.u64 = ctx.r22.u64;
loc_825F5B5C:
	// mr r5,r23
	ctx.r5.u64 = ctx.r23.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r29
	ctx.r3.u64 = ctx.r29.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F5B6C;
	sub_8239CB70(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r29,r25,r29
	ctx.r29.u64 = ctx.r25.u64 + ctx.r29.u64;
	// add r31,r23,r31
	ctx.r31.u64 = ctx.r23.u64 + ctx.r31.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f5b5c
	if (!ctx.cr6.eq) goto loc_825F5B5C;
loc_825F5B80:
	// cmpwi cr6,r20,0
	ctx.cr6.compare<int32_t>(ctx.r20.s32, 0, ctx.xer);
	// ble cr6,0x825f5d28
	if (!ctx.cr6.gt) goto loc_825F5D28;
	// mr r30,r20
	ctx.r30.u64 = ctx.r20.u64;
loc_825F5B8C:
	// mr r5,r21
	ctx.r5.u64 = ctx.r21.u64;
	// mr r4,r31
	ctx.r4.u64 = ctx.r31.u64;
	// mr r3,r27
	ctx.r3.u64 = ctx.r27.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F5B9C;
	sub_8239CB70(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r27,r25,r27
	ctx.r27.u64 = ctx.r25.u64 + ctx.r27.u64;
	// add r31,r21,r31
	ctx.r31.u64 = ctx.r21.u64 + ctx.r31.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f5b8c
	if (!ctx.cr6.eq) goto loc_825F5B8C;
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825F5BBC:
	// cmpwi cr6,r3,2
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 2, ctx.xer);
	// bne cr6,0x825f5d34
	if (!ctx.cr6.eq) goto loc_825F5D34;
	// lwz r7,0(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 0);
	// li r6,0
	ctx.r6.s64 = 0;
	// lwz r10,28(r9)
	ctx.r10.u64 = PPC_LOAD_U32(ctx.r9.u32 + 28);
	// lwz r3,44(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 44);
	// lwz r8,32(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 32);
	// lwz r29,308(r9)
	ctx.r29.u64 = PPC_LOAD_U32(ctx.r9.u32 + 308);
	// lhz r11,14(r7)
	ctx.r11.u64 = PPC_LOAD_U16(ctx.r7.u32 + 14);
	// lwz r31,16(r7)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r7.u32 + 16);
	// mullw r10,r10,r11
	ctx.r10.s64 = int64_t(ctx.r10.s32) * int64_t(ctx.r11.s32);
	// cmplwi cr6,r31,3
	ctx.cr6.compare<uint32_t>(ctx.r31.u32, 3, ctx.xer);
	// addi r31,r10,31
	ctx.r31.s64 = ctx.r10.s64 + 31;
	// mullw r10,r3,r11
	ctx.r10.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r11.s32);
	// rlwinm r3,r31,0,0,26
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r31.u32 | (ctx.r31.u64 << 32), 0) & 0xFFFFFFE0;
	// addi r10,r10,31
	ctx.r10.s64 = ctx.r10.s64 + 31;
	// srawi r3,r3,3
	ctx.xer.ca = (ctx.r3.s32 < 0) & ((ctx.r3.u32 & 0x7) != 0);
	ctx.r3.s64 = ctx.r3.s32 >> 3;
	// rlwinm r10,r10,0,0,26
	ctx.r10.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 0) & 0xFFFFFFE0;
	// addze r28,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r28.s64 = temp.s64;
	// srawi r10,r10,3
	ctx.xer.ca = (ctx.r10.s32 < 0) & ((ctx.r10.u32 & 0x7) != 0);
	ctx.r10.s64 = ctx.r10.s32 >> 3;
	// addze r10,r10
	temp.s64 = ctx.r10.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r10.u32;
	ctx.r10.s64 = temp.s64;
	// srawi r3,r8,1
	ctx.xer.ca = (ctx.r8.s32 < 0) & ((ctx.r8.u32 & 0x1) != 0);
	ctx.r3.s64 = ctx.r8.s32 >> 1;
	// rlwinm r27,r10,1,0,30
	ctx.r27.u64 = __builtin_rotateleft64(ctx.r10.u32 | (ctx.r10.u64 << 32), 1) & 0xFFFFFFFE;
	// addze r30,r3
	temp.s64 = ctx.r3.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r3.u32;
	ctx.r30.s64 = temp.s64;
	// bgt cr6,0x825f5c30
	if (ctx.cr6.gt) goto loc_825F5C30;
	// lwz r7,8(r7)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r7.u32 + 8);
	// cmpwi cr6,r7,0
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 0, ctx.xer);
	// ble cr6,0x825f5c30
	if (!ctx.cr6.gt) goto loc_825F5C30;
	// li r6,1
	ctx.r6.s64 = 1;
loc_825F5C30:
	// cmpwi cr6,r5,0
	ctx.cr6.compare<int32_t>(ctx.r5.s32, 0, ctx.xer);
	// bne cr6,0x825f5c94
	if (!ctx.cr6.eq) goto loc_825F5C94;
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// bne cr6,0x825f5c5c
	if (!ctx.cr6.eq) goto loc_825F5C5C;
	// lwz r8,20(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// lwz r9,24(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// mullw r11,r8,r11
	ctx.r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// mullw r10,r9,r10
	ctx.r10.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// addze r11,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r11.s64 = temp.s64;
	// b 0x825f5cf4
	goto loc_825F5CF4;
loc_825F5C5C:
	// lwz r7,48(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	// lwz r6,24(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// srawi r5,r7,31
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// lwz r9,20(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// xor r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// mullw r11,r9,r11
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r11.s32);
	// subf r9,r5,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r5.s64;
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// subf r7,r6,r9
	ctx.r7.s64 = ctx.r9.s64 - ctx.r6.s64;
	// addze r9,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r9.s64 = temp.s64;
	// subf r11,r8,r7
	ctx.r11.s64 = ctx.r7.s64 - ctx.r8.s64;
	// mullw r11,r11,r10
	ctx.r11.s64 = int64_t(ctx.r11.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// b 0x825f5cf8
	goto loc_825F5CF8;
loc_825F5C94:
	// cmpwi cr6,r6,0
	ctx.cr6.compare<int32_t>(ctx.r6.s32, 0, ctx.xer);
	// bne cr6,0x825f5cc0
	if (!ctx.cr6.eq) goto loc_825F5CC0;
	// lwz r8,20(r9)
	ctx.r8.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// lwz r9,24(r9)
	ctx.r9.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// mullw r11,r8,r11
	ctx.r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r11.s32);
	// srawi r11,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r11.s64 = ctx.r11.s32 >> 3;
	// addi r8,r9,1
	ctx.r8.s64 = ctx.r9.s64 + 1;
	// addze r9,r11
	temp.s64 = ctx.r11.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r11.u32;
	ctx.r9.s64 = temp.s64;
	// mullw r11,r8,r10
	ctx.r11.s64 = int64_t(ctx.r8.s32) * int64_t(ctx.r10.s32);
	// add r11,r11,r9
	ctx.r11.u64 = ctx.r11.u64 + ctx.r9.u64;
	// b 0x825f5cf8
	goto loc_825F5CF8;
loc_825F5CC0:
	// lwz r7,48(r9)
	ctx.r7.u64 = PPC_LOAD_U32(ctx.r9.u32 + 48);
	// lwz r6,24(r9)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r9.u32 + 24);
	// srawi r5,r7,31
	ctx.xer.ca = (ctx.r7.s32 < 0) & ((ctx.r7.u32 & 0x7FFFFFFF) != 0);
	ctx.r5.s64 = ctx.r7.s32 >> 31;
	// lwz r3,20(r9)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r9.u32 + 20);
	// subfic r9,r6,1
	ctx.xer.ca = ctx.r6.u32 <= 1;
	ctx.r9.s64 = 1 - ctx.r6.s64;
	// xor r7,r7,r5
	ctx.r7.u64 = ctx.r7.u64 ^ ctx.r5.u64;
	// subf r8,r8,r9
	ctx.r8.s64 = ctx.r9.s64 - ctx.r8.s64;
	// subf r9,r5,r7
	ctx.r9.s64 = ctx.r7.s64 - ctx.r5.s64;
	// mullw r11,r3,r11
	ctx.r11.s64 = int64_t(ctx.r3.s32) * int64_t(ctx.r11.s32);
	// add r9,r9,r8
	ctx.r9.u64 = ctx.r9.u64 + ctx.r8.u64;
	// srawi r8,r11,3
	ctx.xer.ca = (ctx.r11.s32 < 0) & ((ctx.r11.u32 & 0x7) != 0);
	ctx.r8.s64 = ctx.r11.s32 >> 3;
	// mullw r11,r9,r10
	ctx.r11.s64 = int64_t(ctx.r9.s32) * int64_t(ctx.r10.s32);
	// addze r10,r8
	temp.s64 = ctx.r8.s64 + ctx.xer.ca;
	ctx.xer.ca = temp.u32 < ctx.r8.u32;
	ctx.r10.s64 = temp.s64;
loc_825F5CF4:
	// add r11,r11,r10
	ctx.r11.u64 = ctx.r11.u64 + ctx.r10.u64;
loc_825F5CF8:
	// add r31,r11,r4
	ctx.r31.u64 = ctx.r11.u64 + ctx.r4.u64;
	// cmpwi cr6,r30,0
	ctx.cr6.compare<int32_t>(ctx.r30.s32, 0, ctx.xer);
	// ble cr6,0x825f5d28
	if (!ctx.cr6.gt) goto loc_825F5D28;
loc_825F5D04:
	// mr r5,r28
	ctx.r5.u64 = ctx.r28.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// bl 0x8239cb70
	ctx.lr = 0x825F5D14;
	sub_8239CB70(ctx, base);
	// addi r30,r30,-1
	ctx.r30.s64 = ctx.r30.s64 + -1;
	// add r31,r27,r31
	ctx.r31.u64 = ctx.r27.u64 + ctx.r31.u64;
	// add r29,r29,r28
	ctx.r29.u64 = ctx.r29.u64 + ctx.r28.u64;
	// cmplwi cr6,r30,0
	ctx.cr6.compare<uint32_t>(ctx.r30.u32, 0, ctx.xer);
	// bne cr6,0x825f5d04
	if (!ctx.cr6.eq) goto loc_825F5D04;
loc_825F5D28:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825F5D34:
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f5d54
	if (!ctx.cr6.eq) goto loc_825F5D54;
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// addi r3,r11,-22756
	ctx.r3.s64 = ctx.r11.s64 + -22756;
	// bl 0x82691b58
	ctx.lr = 0x825F5D48;
	sub_82691B58(ctx, base);
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
loc_825F5D54:
	// lis r11,-32244
	ctx.r11.s64 = -2113142784;
	// addi r3,r11,-22768
	ctx.r3.s64 = ctx.r11.s64 + -22768;
	// bl 0x82691b58
	ctx.lr = 0x825F5D60;
	sub_82691B58(ctx, base);
loc_825F5D60:
	// li r3,0
	ctx.r3.s64 = 0;
	// addi r1,r1,192
	ctx.r1.s64 = ctx.r1.s64 + 192;
	// b 0x8239ba48
	// ERROR 8239BA48
	return;
}

__attribute__((alias("__imp__sub_825F5D6C"))) PPC_WEAK_FUNC(sub_825F5D6C);
PPC_FUNC_IMPL(__imp__sub_825F5D6C) {
	PPC_FUNC_PROLOGUE();
	// .long 0x0
}

__attribute__((alias("__imp__sub_825F5D70"))) PPC_WEAK_FUNC(sub_825F5D70);
PPC_FUNC_IMPL(__imp__sub_825F5D70) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// stw r12,-8(r1)
	PPC_STORE_U32(ctx.r1.u32 + -8, ctx.r12.u32);
	// std r30,-24(r1)
	PPC_STORE_U64(ctx.r1.u32 + -24, ctx.r30.u64);
	// std r31,-16(r1)
	PPC_STORE_U64(ctx.r1.u32 + -16, ctx.r31.u64);
	// stwu r1,-112(r1)
	ea = -112 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// mr r31,r3
	ctx.r31.u64 = ctx.r3.u64;
	// li r30,0
	ctx.r30.s64 = 0;
	// lwz r3,304(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 304);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825f5da0
	if (ctx.cr6.eq) goto loc_825F5DA0;
	// bl 0x825edb28
	ctx.lr = 0x825F5D9C;
	sub_825EDB28(ctx, base);
	// stw r30,304(r31)
	PPC_STORE_U32(ctx.r31.u32 + 304, ctx.r30.u32);
loc_825F5DA0:
	// lwz r3,308(r31)
	ctx.r3.u64 = PPC_LOAD_U32(ctx.r31.u32 + 308);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// beq cr6,0x825f5db4
	if (ctx.cr6.eq) goto loc_825F5DB4;
	// bl 0x825edb28
	ctx.lr = 0x825F5DB0;
	sub_825EDB28(ctx, base);
	// stw r30,308(r31)
	PPC_STORE_U32(ctx.r31.u32 + 308, ctx.r30.u32);
loc_825F5DB4:
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,40(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// lwz r4,36(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// bl 0x825f4b60
	ctx.lr = 0x825F5DC4;
	sub_825F4B60(ctx, base);
	// mr r8,r3
	ctx.r8.u64 = ctx.r3.u64;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// lwz r5,48(r31)
	ctx.r5.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// lwz r4,44(r31)
	ctx.r4.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// bl 0x825f4b60
	ctx.lr = 0x825F5DD8;
	sub_825F4B60(ctx, base);
	// mr r30,r3
	ctx.r30.u64 = ctx.r3.u64;
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r8
	ctx.r3.u64 = ctx.r8.u64;
	// bl 0x825edb18
	ctx.lr = 0x825F5DE8;
	sub_825EDB18(ctx, base);
	// cmplwi cr6,r3,0
	ctx.cr6.compare<uint32_t>(ctx.r3.u32, 0, ctx.xer);
	// stw r3,304(r31)
	PPC_STORE_U32(ctx.r31.u32 + 304, ctx.r3.u32);
	// bne cr6,0x825f5dfc
	if (!ctx.cr6.eq) goto loc_825F5DFC;
	// li r3,0
	ctx.r3.s64 = 0;
	// b 0x825f5e18
	goto loc_825F5E18;
loc_825F5DFC:
	// li r4,0
	ctx.r4.s64 = 0;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825edb18
	ctx.lr = 0x825F5E08;
	sub_825EDB18(ctx, base);
	// cntlzw r11,r3
	ctx.r11.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// stw r3,308(r31)
	PPC_STORE_U32(ctx.r31.u32 + 308, ctx.r3.u32);
	// rlwinm r11,r11,27,31,31
	ctx.r11.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// xori r3,r11,1
	ctx.r3.u64 = ctx.r11.u64 ^ 1;
loc_825F5E18:
	// addi r1,r1,112
	ctx.r1.s64 = ctx.r1.s64 + 112;
	// lwz r12,-8(r1)
	ctx.r12.u64 = PPC_LOAD_U32(ctx.r1.u32 + -8);
	// mtlr r12
	ctx.lr = ctx.r12.u64;
	// ld r30,-24(r1)
	ctx.r30.u64 = PPC_LOAD_U64(ctx.r1.u32 + -24);
	// ld r31,-16(r1)
	ctx.r31.u64 = PPC_LOAD_U64(ctx.r1.u32 + -16);
	// blr 
	return;
}

__attribute__((alias("__imp__sub_825F5E30"))) PPC_WEAK_FUNC(sub_825F5E30);
PPC_FUNC_IMPL(__imp__sub_825F5E30) {
	PPC_FUNC_PROLOGUE();
	uint32_t ea{};
	// mflr r12
	ctx.r12.u64 = ctx.lr;
	// bl 0x8239ba10
	ctx.lr = 0x825F5E38;
	sub_8239BA10(ctx, base);
	// stwu r1,-144(r1)
	ea = -144 + ctx.r1.u32;
	PPC_STORE_U32(ea, ctx.r1.u32);
	ctx.r1.u32 = ea;
	// lwz r31,0(r3)
	ctx.r31.u64 = PPC_LOAD_U32(ctx.r3.u32 + 0);
	// mr r29,r4
	ctx.r29.u64 = ctx.r4.u64;
	// mr r27,r5
	ctx.r27.u64 = ctx.r5.u64;
	// mr r28,r6
	ctx.r28.u64 = ctx.r6.u64;
	// cmpwi cr6,r29,0
	ctx.cr6.compare<int32_t>(ctx.r29.s32, 0, ctx.xer);
	// lwz r11,0(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 0);
	// lwz r30,16(r11)
	ctx.r30.u64 = PPC_LOAD_U32(ctx.r11.u32 + 16);
	// blt cr6,0x825f5fa4
	if (ctx.cr6.lt) goto loc_825F5FA4;
	// cmpwi cr6,r27,0
	ctx.cr6.compare<int32_t>(ctx.r27.s32, 0, ctx.xer);
	// blt cr6,0x825f5fa4
	if (ctx.cr6.lt) goto loc_825F5FA4;
	// cmpwi cr6,r28,1
	ctx.cr6.compare<int32_t>(ctx.r28.s32, 1, ctx.xer);
	// blt cr6,0x825f5fa4
	if (ctx.cr6.lt) goto loc_825F5FA4;
	// cmpwi cr6,r7,1
	ctx.cr6.compare<int32_t>(ctx.r7.s32, 1, ctx.xer);
	// blt cr6,0x825f5fa4
	if (ctx.cr6.lt) goto loc_825F5FA4;
	// cmpwi cr6,r8,0
	ctx.cr6.compare<int32_t>(ctx.r8.s32, 0, ctx.xer);
	// blt cr6,0x825f5fa4
	if (ctx.cr6.lt) goto loc_825F5FA4;
	// cmpwi cr6,r9,0
	ctx.cr6.compare<int32_t>(ctx.r9.s32, 0, ctx.xer);
	// blt cr6,0x825f5fa4
	if (ctx.cr6.lt) goto loc_825F5FA4;
	// cmpwi cr6,r10,1
	ctx.cr6.compare<int32_t>(ctx.r10.s32, 1, ctx.xer);
	// blt cr6,0x825f5fa4
	if (ctx.cr6.lt) goto loc_825F5FA4;
	// lwz r26,228(r1)
	ctx.r26.u64 = PPC_LOAD_U32(ctx.r1.u32 + 228);
	// cmpwi cr6,r26,1
	ctx.cr6.compare<int32_t>(ctx.r26.s32, 1, ctx.xer);
	// blt cr6,0x825f5fa4
	if (ctx.cr6.lt) goto loc_825F5FA4;
	// lwz r11,36(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 36);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x825f5ea8
	if (ctx.cr6.gt) goto loc_825F5EA8;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
loc_825F5EA8:
	// add r6,r29,r28
	ctx.r6.u64 = ctx.r29.u64 + ctx.r28.u64;
	// cmpw cr6,r6,r11
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r11.s32, ctx.xer);
	// bgt cr6,0x825f5fa4
	if (ctx.cr6.gt) goto loc_825F5FA4;
	// lwz r11,40(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 40);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x825f5ec4
	if (ctx.cr6.gt) goto loc_825F5EC4;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
loc_825F5EC4:
	// add r6,r27,r7
	ctx.r6.u64 = ctx.r27.u64 + ctx.r7.u64;
	// cmpw cr6,r6,r11
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r11.s32, ctx.xer);
	// bgt cr6,0x825f5fa4
	if (ctx.cr6.gt) goto loc_825F5FA4;
	// lwz r11,44(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 44);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x825f5ee0
	if (ctx.cr6.gt) goto loc_825F5EE0;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
loc_825F5EE0:
	// add r6,r8,r10
	ctx.r6.u64 = ctx.r8.u64 + ctx.r10.u64;
	// cmpw cr6,r6,r11
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r11.s32, ctx.xer);
	// bgt cr6,0x825f5fa4
	if (ctx.cr6.gt) goto loc_825F5FA4;
	// lwz r11,48(r31)
	ctx.r11.u64 = PPC_LOAD_U32(ctx.r31.u32 + 48);
	// cmpwi cr6,r11,0
	ctx.cr6.compare<int32_t>(ctx.r11.s32, 0, ctx.xer);
	// bgt cr6,0x825f5efc
	if (ctx.cr6.gt) goto loc_825F5EFC;
	// neg r11,r11
	ctx.r11.s64 = -ctx.r11.s64;
loc_825F5EFC:
	// add r6,r9,r26
	ctx.r6.u64 = ctx.r9.u64 + ctx.r26.u64;
	// cmpw cr6,r6,r11
	ctx.cr6.compare<int32_t>(ctx.r6.s32, ctx.r11.s32, ctx.xer);
	// bgt cr6,0x825f5fa4
	if (ctx.cr6.gt) goto loc_825F5FA4;
	// mr r5,r7
	ctx.r5.u64 = ctx.r7.u64;
	// lwz r6,296(r31)
	ctx.r6.u64 = PPC_LOAD_U32(ctx.r31.u32 + 296);
	// mr r4,r28
	ctx.r4.u64 = ctx.r28.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825f48c8
	ctx.lr = 0x825F5F1C;
	sub_825F48C8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f5fa4
	if (!ctx.cr6.eq) goto loc_825F5FA4;
	// mr r5,r26
	ctx.r5.u64 = ctx.r26.u64;
	// mr r4,r10
	ctx.r4.u64 = ctx.r10.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825f48c8
	ctx.lr = 0x825F5F34;
	sub_825F48C8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f5fa4
	if (!ctx.cr6.eq) goto loc_825F5FA4;
	// mr r5,r27
	ctx.r5.u64 = ctx.r27.u64;
	// mr r4,r29
	ctx.r4.u64 = ctx.r29.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825f48c8
	ctx.lr = 0x825F5F4C;
	sub_825F48C8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f5fa4
	if (!ctx.cr6.eq) goto loc_825F5FA4;
	// mr r5,r9
	ctx.r5.u64 = ctx.r9.u64;
	// mr r4,r8
	ctx.r4.u64 = ctx.r8.u64;
	// mr r3,r30
	ctx.r3.u64 = ctx.r30.u64;
	// bl 0x825f48c8
	ctx.lr = 0x825F5F64;
	sub_825F48C8(ctx, base);
	// cmpwi cr6,r3,0
	ctx.cr6.compare<int32_t>(ctx.r3.s32, 0, ctx.xer);
	// bne cr6,0x825f5fa4
	if (!ctx.cr6.eq) goto loc_825F5FA4;
	// mr r3,r31
	ctx.r3.u64 = ctx.r31.u64;
	// stw r29,4(r31)
	PPC_STORE_U32(ctx.r31.u32 + 4, ctx.r29.u32);
	// stw r27,8(r31)
	PPC_STORE_U32(ctx.r31.u32 + 8, ctx.r27.u32);
	// stw r8,20(r31)
	PPC_STORE_U32(ctx.r31.u32 + 20, ctx.r8.u32);
	// stw r9,24(r31)
	PPC_STORE_U32(ctx.r31.u32 + 24, ctx.r9.u32);
	// stw r28,12(r31)
	PPC_STORE_U32(ctx.r31.u32 + 12, ctx.r28.u32);
	// stw r7,16(r31)
	PPC_STORE_U32(ctx.r31.u32 + 16, ctx.r7.u32);
	// stw r10,28(r31)
	PPC_STORE_U32(ctx.r31.u32 + 28, ctx.r10.u32);
	// stw r26,32(r31)
	PPC_STORE_U32(ctx.r31.u32 + 32, ctx.r26.u32);
	// bl 0x825f4c00
	ctx.lr = 0x825F5F94;
	sub_825F4C00(ctx, base);
	// cntlzw r11,r3
	ctx.r11.u64 = ctx.r3.u32 == 0 ? 32 : __builtin_clz(ctx.r3.u32);
	// rlwinm r3,r11,27,31,31
	ctx.r3.u64 = __builtin_rotateleft64(ctx.r11.u32 | (ctx.r11.u64 << 32), 27) & 0x1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
loc_825F5FA4:
	// li r3,1
	ctx.r3.s64 = 1;
	// addi r1,r1,144
	ctx.r1.s64 = ctx.r1.s64 + 144;
	// b 0x8239ba60
	// ERROR 8239BA60
	return;
}

